(window.webpackJsonp=window.webpackJsonp||[]).push([[0],[]]);!function(n){function e(e){for(var r,s,o=e[0],l=e[1],c=e[2],p=0,u=[];p<o.length;p++)s=o[p],Object.prototype.hasOwnProperty.call(i,s)&&i[s]&&u.push(i[s][0]),i[s]=0;for(r in l)Object.prototype.hasOwnProperty.call(l,r)&&(n[r]=l[r]);for(d&&d(e);u.length;)u.shift()();return a.push.apply(a,c||[]),t()}function t(){for(var n,e=0;e<a.length;e++){for(var t=a[e],r=!0,o=1;o<t.length;o++){var l=t[o];0!==i[l]&&(r=!1)}r&&(a.splice(e--,1),n=s(s.s=t[0]))}return n}var r={},i={1:0},a=[];function s(e){if(r[e])return r[e].exports;var t=r[e]={i:e,l:!1,exports:{}};return n[e].call(t.exports,t,t.exports,s),t.l=!0,t.exports}s.e=function(n){var e=[],t=i[n];if(0!==t)if(t)e.push(t[2]);else{var r=new Promise((function(e,r){t=i[n]=[e,r]}));e.push(t[2]=r);var a,o=document.createElement("script");o.charset="utf-8",o.timeout=120,s.nc&&o.setAttribute("nonce",s.nc),o.src=function(n){return s.p+"assets/js/"+({}[n]||n)+"."+{2:"fc657b59",3:"e0ec9727",4:"7bd8ec5a",5:"bc317ee9",6:"da0024ba",7:"1d00dc1d",8:"b16448cb",9:"1b8d4154",10:"eb4f1714",11:"1f4ed2b0",12:"c55c2945",13:"b5382928",14:"9bf49d25",15:"47e2fe44",16:"709e62a0",17:"b149e3eb",18:"5b57a250",19:"d2aedd09",20:"13fd637f",21:"91dc71fa",22:"b44ebaaf",23:"85274bf7",24:"fdcf9dad",25:"d7834455",26:"149bb0bb",27:"fe47c303",28:"106a053b",29:"5cbd445a",30:"bc8be6db",31:"776bb22e",32:"38f1617c",33:"764d5e8e",34:"ee4b3664",35:"0c8a4552",36:"3114f265",37:"6623c809",38:"47b2ae3c",39:"66477360",40:"3c1369b3",41:"ba422769",42:"d8feaaee",43:"7a023784",44:"578065d4",45:"599d10a9",46:"68c58a57",47:"a0e48134",48:"a8edc4f3",49:"a98d9884",50:"7001c5b8",51:"43d17700",52:"9ad031a9",53:"8347b740",54:"73a1db12",55:"06ac4334",56:"caf42a7a",57:"c8af04ff",58:"259c0df6",59:"4a73da6e",60:"c88072d5",61:"a3a06ae4",62:"e5adc7b0",63:"48cc1305",64:"86f8647c",65:"005e380e",66:"67bff065",67:"e12b4c13",68:"0161e8c6",69:"5aaf7b82",70:"6a645155",71:"001a3225",72:"2850a293",73:"d250e0c6",74:"dafa5bc3",75:"22c6b92e",76:"24bc0a51",77:"e38213b8",78:"a22de10e",79:"b0e7fed2",80:"67472401",81:"1df69f3e",82:"022c9317",83:"8894e626",84:"e9f038d8",85:"ff3971f7",86:"24e53411",87:"95b56dc2",88:"4557cc7d",89:"1337890b",90:"29d24983",91:"84b2e3eb",92:"23e2abeb",93:"162e9b76",94:"9ada79ba",95:"5cc24e13",96:"0ee67f9e",97:"9f98bf38",98:"58600000",99:"f8a36889",100:"60572966",101:"d94775a6",102:"354c87f2",103:"0322d8d3",104:"330ce470",105:"f2c0cf7b",106:"9b15e481",107:"f83846c8",108:"fd868043",109:"144984d2",110:"393075aa",111:"8e2be411",112:"f5db1301",113:"c5a49956",114:"f7400c29",115:"0d4d9691",116:"5e2d0bba",117:"b9b6cd5c",118:"cfffe6ca",119:"faeb006b",120:"2c87041a",121:"6d6d525b",122:"a378a709",123:"8a44d7e0",124:"eef03f70",125:"20627b3b",126:"02bdd70b",127:"6aaf85c7",128:"2f2f6b78",129:"b1f7d667",130:"aadb5b72",131:"0a0ad3b4",132:"a05a0c00",133:"584b8785",134:"ab61764f",135:"ccae4853",136:"38fdaa61",137:"502d74c0",138:"12a12ee7",139:"5ed042c0",140:"32ffb0bc",141:"3b99bc3f",142:"afd3e1e7",143:"ad7b5934",144:"7bdbdbd9",145:"3f68756b",146:"a48f80c3",147:"eb3a30a3",148:"563f4279",149:"7627333e",150:"7cf4b6fd",151:"200e2d36",152:"68d5a6f8",153:"4019d1b1",154:"7c56b873",155:"8a895428",156:"9a106f9c",157:"b585e3f5",158:"a9ffbcc1",159:"9bd41101"}[n]+".js"}(n);var l=new Error;a=function(e){o.onerror=o.onload=null,clearTimeout(c);var t=i[n];if(0!==t){if(t){var r=e&&("load"===e.type?"missing":e.type),a=e&&e.target&&e.target.src;l.message="Loading chunk "+n+" failed.\n("+r+": "+a+")",l.name="ChunkLoadError",l.type=r,l.request=a,t[1](l)}i[n]=void 0}};var c=setTimeout((function(){a({type:"timeout",target:o})}),12e4);o.onerror=o.onload=a,document.head.appendChild(o)}return Promise.all(e)},s.m=n,s.c=r,s.d=function(n,e,t){s.o(n,e)||Object.defineProperty(n,e,{enumerable:!0,get:t})},s.r=function(n){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(n,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(n,"__esModule",{value:!0})},s.t=function(n,e){if(1&e&&(n=s(n)),8&e)return n;if(4&e&&"object"==typeof n&&n&&n.__esModule)return n;var t=Object.create(null);if(s.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:n}),2&e&&"string"!=typeof n)for(var r in n)s.d(t,r,function(e){return n[e]}.bind(null,r));return t},s.n=function(n){var e=n&&n.__esModule?function(){return n.default}:function(){return n};return s.d(e,"a",e),e},s.o=function(n,e){return Object.prototype.hasOwnProperty.call(n,e)},s.p="/",s.oe=function(n){throw console.error(n),n};var o=window.webpackJsonp=window.webpackJsonp||[],l=o.push.bind(o);o.push=e,o=o.slice();for(var c=0;c<o.length;c++)e(o[c]);var d=l;a.push([109,0]),t()}([function(n,e,t){"use strict";t.d(e,"d",(function(){return v})),t.d(e,"c",(function(){return y})),t.d(e,"b",(function(){return x})),t.d(e,"e",(function(){return k})),t.d(e,"a",(function(){return w})),t.d(e,"f",(function(){return S})),t.d(e,"g",(function(){return j})),t.d(e,"h",(function(){return E}));t(28),t(136);var r=t(1),i={NotFound:()=>Promise.all([t.e(0),t.e(5)]).then(t.bind(null,343)),Layout:()=>Promise.all([t.e(0),t.e(2)]).then(t.bind(null,341))},a={"v-74ef8d6a":()=>t.e(6).then(t.bind(null,344)),"v-0db05437":()=>t.e(10).then(t.bind(null,345)),"v-122e0ca4":()=>t.e(9).then(t.bind(null,346)),"v-cf325208":()=>t.e(8).then(t.bind(null,347)),"v-9db0ef34":()=>t.e(7).then(t.bind(null,348)),"v-166a22fe":()=>t.e(12).then(t.bind(null,349)),"v-a76b5b9a":()=>t.e(11).then(t.bind(null,350)),"v-2a8eeee9":()=>t.e(14).then(t.bind(null,351)),"v-1c9e69dc":()=>t.e(15).then(t.bind(null,352)),"v-539d06af":()=>t.e(13).then(t.bind(null,353)),"v-a41da114":()=>t.e(17).then(t.bind(null,354)),"v-75b1c1bd":()=>t.e(18).then(t.bind(null,355)),"v-6ab9acf7":()=>t.e(16).then(t.bind(null,356)),"v-b72fe238":()=>t.e(19).then(t.bind(null,357)),"v-4515ebe1":()=>t.e(20).then(t.bind(null,358)),"v-00877d3f":()=>t.e(22).then(t.bind(null,359)),"v-3ab071ee":()=>t.e(23).then(t.bind(null,360)),"v-751012fa":()=>t.e(24).then(t.bind(null,361)),"v-9882acb6":()=>t.e(21).then(t.bind(null,362)),"v-05b14bdd":()=>t.e(25).then(t.bind(null,363)),"v-5d250c21":()=>t.e(26).then(t.bind(null,364)),"v-d4cda7d8":()=>t.e(27).then(t.bind(null,365)),"v-0186e5b2":()=>t.e(29).then(t.bind(null,366)),"v-2e0b9ea9":()=>t.e(31).then(t.bind(null,367)),"v-552a3e17":()=>t.e(30).then(t.bind(null,368)),"v-66b680f6":()=>t.e(32).then(t.bind(null,369)),"v-ce7d277a":()=>t.e(33).then(t.bind(null,370)),"v-29dae873":()=>t.e(34).then(t.bind(null,371)),"v-f09d403a":()=>t.e(36).then(t.bind(null,372)),"v-17c18643":()=>t.e(35).then(t.bind(null,373)),"v-80ad8e7a":()=>t.e(37).then(t.bind(null,374)),"v-4c45192e":()=>t.e(39).then(t.bind(null,375)),"v-3d891436":()=>t.e(28).then(t.bind(null,376)),"v-8d62e9e4":()=>t.e(42).then(t.bind(null,377)),"v-0d97dac8":()=>t.e(40).then(t.bind(null,378)),"v-3f96471e":()=>t.e(38).then(t.bind(null,379)),"v-edd93478":()=>t.e(45).then(t.bind(null,380)),"v-3bed72fe":()=>t.e(48).then(t.bind(null,381)),"v-420ed683":()=>t.e(46).then(t.bind(null,382)),"v-4382d8a8":()=>t.e(41).then(t.bind(null,383)),"v-11f9a58f":()=>t.e(47).then(t.bind(null,384)),"v-35eddcf9":()=>t.e(43).then(t.bind(null,385)),"v-02f207c3":()=>t.e(49).then(t.bind(null,386)),"v-360acd55":()=>t.e(50).then(t.bind(null,387)),"v-3bfd7f0c":()=>t.e(54).then(t.bind(null,388)),"v-3bf8a54c":()=>t.e(51).then(t.bind(null,389)),"v-a0cd1b6e":()=>t.e(52).then(t.bind(null,390)),"v-6229bca2":()=>t.e(55).then(t.bind(null,391)),"v-6daf2e3a":()=>t.e(44).then(t.bind(null,392)),"v-1fa3135c":()=>t.e(53).then(t.bind(null,393)),"v-3632f31f":()=>t.e(58).then(t.bind(null,394)),"v-c740fd8a":()=>t.e(56).then(t.bind(null,395)),"v-b33a99d2":()=>t.e(57).then(t.bind(null,396)),"v-351e657c":()=>t.e(60).then(t.bind(null,397)),"v-c3bd5d9a":()=>t.e(63).then(t.bind(null,398)),"v-701ec247":()=>t.e(64).then(t.bind(null,399)),"v-4ae15768":()=>t.e(62).then(t.bind(null,400)),"v-39774543":()=>t.e(65).then(t.bind(null,401)),"v-13f7fe09":()=>t.e(59).then(t.bind(null,402)),"v-a024d1dc":()=>t.e(68).then(t.bind(null,403)),"v-3934d917":()=>t.e(61).then(t.bind(null,404)),"v-0b5ac23a":()=>t.e(67).then(t.bind(null,405)),"v-3a6aa74c":()=>t.e(66).then(t.bind(null,406)),"v-4b3039e8":()=>t.e(70).then(t.bind(null,407)),"v-726095f8":()=>t.e(69).then(t.bind(null,408)),"v-6a2f133e":()=>t.e(72).then(t.bind(null,409)),"v-3df805f8":()=>t.e(73).then(t.bind(null,410)),"v-057fb4a2":()=>t.e(71).then(t.bind(null,411)),"v-e327c356":()=>t.e(75).then(t.bind(null,412)),"v-5c23f220":()=>t.e(76).then(t.bind(null,413)),"v-6e2eff3a":()=>t.e(77).then(t.bind(null,414)),"v-626f8d1c":()=>t.e(74).then(t.bind(null,415)),"v-61a116d1":()=>t.e(82).then(t.bind(null,416)),"v-427aa158":()=>t.e(79).then(t.bind(null,417)),"v-56deed3f":()=>t.e(81).then(t.bind(null,418)),"v-2ba678ac":()=>t.e(78).then(t.bind(null,419)),"v-8f19b34e":()=>t.e(80).then(t.bind(null,420)),"v-17caaee6":()=>t.e(83).then(t.bind(null,421)),"v-243fb89a":()=>t.e(85).then(t.bind(null,422)),"v-d2252dc2":()=>t.e(88).then(t.bind(null,423)),"v-508e8e26":()=>t.e(87).then(t.bind(null,424)),"v-06f84544":()=>t.e(86).then(t.bind(null,425)),"v-29cee25f":()=>t.e(84).then(t.bind(null,426)),"v-413ff23e":()=>t.e(89).then(t.bind(null,427)),"v-41d84bb8":()=>t.e(91).then(t.bind(null,428)),"v-3d750c14":()=>t.e(92).then(t.bind(null,429)),"v-07eaf4f2":()=>t.e(94).then(t.bind(null,430)),"v-fb5f7d52":()=>t.e(93).then(t.bind(null,431)),"v-ba455d3c":()=>t.e(96).then(t.bind(null,432)),"v-31baad0c":()=>t.e(90).then(t.bind(null,433)),"v-4000ac32":()=>t.e(95).then(t.bind(null,434)),"v-06d0c40e":()=>t.e(97).then(t.bind(null,435)),"v-4ae8fe0c":()=>t.e(99).then(t.bind(null,436)),"v-3a15b156":()=>t.e(102).then(t.bind(null,437)),"v-2fa7be82":()=>t.e(100).then(t.bind(null,438)),"v-3ba4cd3c":()=>t.e(98).then(t.bind(null,439)),"v-af4ce9ea":()=>t.e(106).then(t.bind(null,440)),"v-7c74bfe6":()=>t.e(101).then(t.bind(null,441)),"v-b54bf03c":()=>t.e(105).then(t.bind(null,442)),"v-3f4db490":()=>t.e(104).then(t.bind(null,443)),"v-4fafeb1f":()=>t.e(103).then(t.bind(null,444)),"v-716c9de8":()=>t.e(107).then(t.bind(null,445)),"v-18ec41e8":()=>t.e(108).then(t.bind(null,446)),"v-02b6dc42":()=>t.e(110).then(t.bind(null,447)),"v-11e6d886":()=>t.e(109).then(t.bind(null,448)),"v-2acc9b91":()=>t.e(111).then(t.bind(null,449)),"v-26d66c54":()=>t.e(112).then(t.bind(null,450)),"v-03a351e0":()=>t.e(113).then(t.bind(null,451)),"v-2803fd6e":()=>t.e(114).then(t.bind(null,452)),"v-69c2fec1":()=>t.e(115).then(t.bind(null,453)),"v-45672518":()=>t.e(117).then(t.bind(null,454)),"v-87bd982c":()=>t.e(116).then(t.bind(null,455)),"v-2e0d1200":()=>t.e(118).then(t.bind(null,456)),"v-16ecaa80":()=>t.e(121).then(t.bind(null,457)),"v-6becdf80":()=>t.e(119).then(t.bind(null,458)),"v-baa1c044":()=>t.e(120).then(t.bind(null,459)),"v-6b5af5a0":()=>t.e(122).then(t.bind(null,460)),"v-1b852971":()=>t.e(123).then(t.bind(null,461)),"v-b34a9bcc":()=>t.e(124).then(t.bind(null,462)),"v-b94aee00":()=>t.e(125).then(t.bind(null,463)),"v-43f967eb":()=>t.e(126).then(t.bind(null,464)),"v-0774325c":()=>t.e(128).then(t.bind(null,465)),"v-383faa3a":()=>t.e(130).then(t.bind(null,466)),"v-611bc590":()=>t.e(129).then(t.bind(null,467)),"v-22994320":()=>t.e(127).then(t.bind(null,468)),"v-6b145c20":()=>t.e(131).then(t.bind(null,469)),"v-2275b9f7":()=>t.e(132).then(t.bind(null,470)),"v-5aae938c":()=>t.e(133).then(t.bind(null,471)),"v-c5de5724":()=>t.e(135).then(t.bind(null,472)),"v-bdb21a50":()=>t.e(134).then(t.bind(null,473)),"v-423b419f":()=>t.e(136).then(t.bind(null,474)),"v-3bea15f6":()=>t.e(138).then(t.bind(null,475)),"v-4946107a":()=>t.e(137).then(t.bind(null,476)),"v-4e60c2d6":()=>t.e(142).then(t.bind(null,477)),"v-3c2c77d5":()=>t.e(140).then(t.bind(null,478)),"v-51430cb6":()=>t.e(141).then(t.bind(null,479)),"v-2b2717ba":()=>t.e(144).then(t.bind(null,480)),"v-9044c656":()=>t.e(143).then(t.bind(null,481)),"v-1db4fa63":()=>t.e(139).then(t.bind(null,482)),"v-3a79afdc":()=>t.e(145).then(t.bind(null,483)),"v-142bc356":()=>t.e(146).then(t.bind(null,484)),"v-a797ba30":()=>t.e(147).then(t.bind(null,485)),"v-a382d2da":()=>t.e(148).then(t.bind(null,486)),"v-b795f742":()=>t.e(149).then(t.bind(null,487)),"v-567d2eac":()=>t.e(150).then(t.bind(null,488)),"v-6cf60d0d":()=>t.e(151).then(t.bind(null,489)),"v-0bfafa42":()=>t.e(155).then(t.bind(null,490)),"v-b1e04020":()=>t.e(153).then(t.bind(null,491)),"v-306f947f":()=>t.e(154).then(t.bind(null,492)),"v-56be318a":()=>t.e(152).then(t.bind(null,493)),"v-d87ad802":()=>t.e(156).then(t.bind(null,494)),"v-2fc6a049":()=>t.e(157).then(t.bind(null,495)),"v-a21049fc":()=>t.e(158).then(t.bind(null,496))};function s(n){const e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}const o=/-(\w)/g,l=s(n=>n.replace(o,(n,e)=>e?e.toUpperCase():"")),c=/\B([A-Z])/g,d=s(n=>n.replace(c,"-$1").toLowerCase()),p=s(n=>n.charAt(0).toUpperCase()+n.slice(1));function u(n,e){if(!e)return;if(n(e))return n(e);return e.includes("-")?n(p(l(e))):n(p(e))||n(d(e))}const m=Object.assign({},i,a),g=n=>m[n],h=n=>a[n],b=n=>i[n],f=n=>r.a.component(n);function v(n){return u(h,n)}function y(n){return u(b,n)}function x(n){return u(g,n)}function k(n){return u(f,n)}function w(...n){return Promise.all(n.filter(n=>n).map(async n=>{if(!k(n)&&x(n)){const e=await x(n)();r.a.component(n,e.default)}}))}function S(n,e,t){switch(e){case"components":n[e]||(n[e]={}),Object.assign(n[e],t);break;case"mixins":n[e]||(n[e]=[]),n[e].push(...t);break;default:throw new Error("Unknown option name.")}}function j(n,e){const{$localePath:t}=n;return"object"==typeof e&&e[t]?e[t]:e}function E(n,e){"undefined"!=typeof window&&window.__VUEPRESS__&&(window.__VUEPRESS__[n]=e)}},function(n,e,t){"use strict";t.d(e,"a",(function(){return Qt}));
/*!
 * Vue.js v2.7.14
 * (c) 2014-2022 Evan You
 * Released under the MIT License.
 */
var r=Object.freeze({}),i=Array.isArray;function a(n){return null==n}function s(n){return null!=n}function o(n){return!0===n}function l(n){return"string"==typeof n||"number"==typeof n||"symbol"==typeof n||"boolean"==typeof n}function c(n){return"function"==typeof n}function d(n){return null!==n&&"object"==typeof n}var p=Object.prototype.toString;function u(n){return"[object Object]"===p.call(n)}function m(n){return"[object RegExp]"===p.call(n)}function g(n){var e=parseFloat(String(n));return e>=0&&Math.floor(e)===e&&isFinite(n)}function h(n){return s(n)&&"function"==typeof n.then&&"function"==typeof n.catch}function b(n){return null==n?"":Array.isArray(n)||u(n)&&n.toString===p?JSON.stringify(n,null,2):String(n)}function f(n){var e=parseFloat(n);return isNaN(e)?n:e}function v(n,e){for(var t=Object.create(null),r=n.split(","),i=0;i<r.length;i++)t[r[i]]=!0;return e?function(n){return t[n.toLowerCase()]}:function(n){return t[n]}}v("slot,component",!0);var y=v("key,ref,slot,slot-scope,is");function x(n,e){var t=n.length;if(t){if(e===n[t-1])return void(n.length=t-1);var r=n.indexOf(e);if(r>-1)return n.splice(r,1)}}var k=Object.prototype.hasOwnProperty;function w(n,e){return k.call(n,e)}function S(n){var e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}var j=/-(\w)/g,E=S((function(n){return n.replace(j,(function(n,e){return e?e.toUpperCase():""}))})),T=S((function(n){return n.charAt(0).toUpperCase()+n.slice(1)})),q=/\B([A-Z])/g,_=S((function(n){return n.replace(q,"-$1").toLowerCase()}));var I=Function.prototype.bind?function(n,e){return n.bind(e)}:function(n,e){function t(t){var r=arguments.length;return r?r>1?n.apply(e,arguments):n.call(e,t):n.call(e)}return t._length=n.length,t};function C(n,e){e=e||0;for(var t=n.length-e,r=new Array(t);t--;)r[t]=n[t+e];return r}function A(n,e){for(var t in e)n[t]=e[t];return n}function R(n){for(var e={},t=0;t<n.length;t++)n[t]&&A(e,n[t]);return e}function z(n,e,t){}var O=function(n,e,t){return!1},M=function(n){return n};function B(n,e){if(n===e)return!0;var t=d(n),r=d(e);if(!t||!r)return!t&&!r&&String(n)===String(e);try{var i=Array.isArray(n),a=Array.isArray(e);if(i&&a)return n.length===e.length&&n.every((function(n,t){return B(n,e[t])}));if(n instanceof Date&&e instanceof Date)return n.getTime()===e.getTime();if(i||a)return!1;var s=Object.keys(n),o=Object.keys(e);return s.length===o.length&&s.every((function(t){return B(n[t],e[t])}))}catch(n){return!1}}function L(n,e){for(var t=0;t<n.length;t++)if(B(n[t],e))return t;return-1}function D(n){var e=!1;return function(){e||(e=!0,n.apply(this,arguments))}}function F(n,e){return n===e?0===n&&1/n!=1/e:n==n||e==e}var P=["component","directive","filter"],N=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch","renderTracked","renderTriggered"],U={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:O,isReservedAttr:O,isUnknownElement:O,getTagNamespace:z,parsePlatformTagName:M,mustUseProp:O,async:!0,_lifecycleHooks:N},H=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function J(n){var e=(n+"").charCodeAt(0);return 36===e||95===e}function $(n,e,t,r){Object.defineProperty(n,e,{value:t,enumerable:!!r,writable:!0,configurable:!0})}var Q=new RegExp("[^".concat(H.source,".$_\\d]"));var G="__proto__"in{},V="undefined"!=typeof window,W=V&&window.navigator.userAgent.toLowerCase(),K=W&&/msie|trident/.test(W),Z=W&&W.indexOf("msie 9.0")>0,X=W&&W.indexOf("edge/")>0;W&&W.indexOf("android");var Y=W&&/iphone|ipad|ipod|ios/.test(W);W&&/chrome\/\d+/.test(W),W&&/phantomjs/.test(W);var nn,en=W&&W.match(/firefox\/(\d+)/),tn={}.watch,rn=!1;if(V)try{var an={};Object.defineProperty(an,"passive",{get:function(){rn=!0}}),window.addEventListener("test-passive",null,an)}catch(n){}var sn=function(){return void 0===nn&&(nn=!V&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),nn},on=V&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function ln(n){return"function"==typeof n&&/native code/.test(n.toString())}var cn,dn="undefined"!=typeof Symbol&&ln(Symbol)&&"undefined"!=typeof Reflect&&ln(Reflect.ownKeys);cn="undefined"!=typeof Set&&ln(Set)?Set:function(){function n(){this.set=Object.create(null)}return n.prototype.has=function(n){return!0===this.set[n]},n.prototype.add=function(n){this.set[n]=!0},n.prototype.clear=function(){this.set=Object.create(null)},n}();var pn=null;function un(n){void 0===n&&(n=null),n||pn&&pn._scope.off(),pn=n,n&&n._scope.on()}var mn=function(){function n(n,e,t,r,i,a,s,o){this.tag=n,this.data=e,this.children=t,this.text=r,this.elm=i,this.ns=void 0,this.context=a,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=e&&e.key,this.componentOptions=s,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=o,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1}return Object.defineProperty(n.prototype,"child",{get:function(){return this.componentInstance},enumerable:!1,configurable:!0}),n}(),gn=function(n){void 0===n&&(n="");var e=new mn;return e.text=n,e.isComment=!0,e};function hn(n){return new mn(void 0,void 0,void 0,String(n))}function bn(n){var e=new mn(n.tag,n.data,n.children&&n.children.slice(),n.text,n.elm,n.context,n.componentOptions,n.asyncFactory);return e.ns=n.ns,e.isStatic=n.isStatic,e.key=n.key,e.isComment=n.isComment,e.fnContext=n.fnContext,e.fnOptions=n.fnOptions,e.fnScopeId=n.fnScopeId,e.asyncMeta=n.asyncMeta,e.isCloned=!0,e}var fn=0,vn=[],yn=function(){function n(){this._pending=!1,this.id=fn++,this.subs=[]}return n.prototype.addSub=function(n){this.subs.push(n)},n.prototype.removeSub=function(n){this.subs[this.subs.indexOf(n)]=null,this._pending||(this._pending=!0,vn.push(this))},n.prototype.depend=function(e){n.target&&n.target.addDep(this)},n.prototype.notify=function(n){var e=this.subs.filter((function(n){return n}));for(var t=0,r=e.length;t<r;t++){0,e[t].update()}},n}();yn.target=null;var xn=[];function kn(n){xn.push(n),yn.target=n}function wn(){xn.pop(),yn.target=xn[xn.length-1]}var Sn=Array.prototype,jn=Object.create(Sn);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(n){var e=Sn[n];$(jn,n,(function(){for(var t=[],r=0;r<arguments.length;r++)t[r]=arguments[r];var i,a=e.apply(this,t),s=this.__ob__;switch(n){case"push":case"unshift":i=t;break;case"splice":i=t.slice(2)}return i&&s.observeArray(i),s.dep.notify(),a}))}));var En=Object.getOwnPropertyNames(jn),Tn={},qn=!0;function _n(n){qn=n}var In={notify:z,depend:z,addSub:z,removeSub:z},Cn=function(){function n(n,e,t){if(void 0===e&&(e=!1),void 0===t&&(t=!1),this.value=n,this.shallow=e,this.mock=t,this.dep=t?In:new yn,this.vmCount=0,$(n,"__ob__",this),i(n)){if(!t)if(G)n.__proto__=jn;else for(var r=0,a=En.length;r<a;r++){$(n,o=En[r],jn[o])}e||this.observeArray(n)}else{var s=Object.keys(n);for(r=0;r<s.length;r++){var o;Rn(n,o=s[r],Tn,void 0,e,t)}}}return n.prototype.observeArray=function(n){for(var e=0,t=n.length;e<t;e++)An(n[e],!1,this.mock)},n}();function An(n,e,t){return n&&w(n,"__ob__")&&n.__ob__ instanceof Cn?n.__ob__:!qn||!t&&sn()||!i(n)&&!u(n)||!Object.isExtensible(n)||n.__v_skip||Fn(n)||n instanceof mn?void 0:new Cn(n,e,t)}function Rn(n,e,t,r,a,s){var o=new yn,l=Object.getOwnPropertyDescriptor(n,e);if(!l||!1!==l.configurable){var c=l&&l.get,d=l&&l.set;c&&!d||t!==Tn&&2!==arguments.length||(t=n[e]);var p=!a&&An(t,!1,s);return Object.defineProperty(n,e,{enumerable:!0,configurable:!0,get:function(){var e=c?c.call(n):t;return yn.target&&(o.depend(),p&&(p.dep.depend(),i(e)&&Mn(e))),Fn(e)&&!a?e.value:e},set:function(e){var r=c?c.call(n):t;if(F(r,e)){if(d)d.call(n,e);else{if(c)return;if(!a&&Fn(r)&&!Fn(e))return void(r.value=e);t=e}p=!a&&An(e,!1,s),o.notify()}}}),o}}function zn(n,e,t){if(!Dn(n)){var r=n.__ob__;return i(n)&&g(e)?(n.length=Math.max(n.length,e),n.splice(e,1,t),r&&!r.shallow&&r.mock&&An(t,!1,!0),t):e in n&&!(e in Object.prototype)?(n[e]=t,t):n._isVue||r&&r.vmCount?t:r?(Rn(r.value,e,t,void 0,r.shallow,r.mock),r.dep.notify(),t):(n[e]=t,t)}}function On(n,e){if(i(n)&&g(e))n.splice(e,1);else{var t=n.__ob__;n._isVue||t&&t.vmCount||Dn(n)||w(n,e)&&(delete n[e],t&&t.dep.notify())}}function Mn(n){for(var e=void 0,t=0,r=n.length;t<r;t++)(e=n[t])&&e.__ob__&&e.__ob__.dep.depend(),i(e)&&Mn(e)}function Bn(n){return Ln(n,!0),$(n,"__v_isShallow",!0),n}function Ln(n,e){if(!Dn(n)){An(n,e,sn());0}}function Dn(n){return!(!n||!n.__v_isReadonly)}function Fn(n){return!(!n||!0!==n.__v_isRef)}function Pn(n,e,t){Object.defineProperty(n,t,{enumerable:!0,configurable:!0,get:function(){var n=e[t];if(Fn(n))return n.value;var r=n&&n.__ob__;return r&&r.dep.depend(),n},set:function(n){var r=e[t];Fn(r)&&!Fn(n)?r.value=n:e[t]=n}})}"".concat("watcher"," callback"),"".concat("watcher"," getter"),"".concat("watcher"," cleanup");var Nn;var Un=function(){function n(n){void 0===n&&(n=!1),this.detached=n,this.active=!0,this.effects=[],this.cleanups=[],this.parent=Nn,!n&&Nn&&(this.index=(Nn.scopes||(Nn.scopes=[])).push(this)-1)}return n.prototype.run=function(n){if(this.active){var e=Nn;try{return Nn=this,n()}finally{Nn=e}}else 0},n.prototype.on=function(){Nn=this},n.prototype.off=function(){Nn=this.parent},n.prototype.stop=function(n){if(this.active){var e=void 0,t=void 0;for(e=0,t=this.effects.length;e<t;e++)this.effects[e].teardown();for(e=0,t=this.cleanups.length;e<t;e++)this.cleanups[e]();if(this.scopes)for(e=0,t=this.scopes.length;e<t;e++)this.scopes[e].stop(!0);if(!this.detached&&this.parent&&!n){var r=this.parent.scopes.pop();r&&r!==this&&(this.parent.scopes[this.index]=r,r.index=this.index)}this.parent=void 0,this.active=!1}},n}();function Hn(n){var e=n._provided,t=n.$parent&&n.$parent._provided;return t===e?n._provided=Object.create(t):e}var Jn=S((function(n){var e="&"===n.charAt(0),t="~"===(n=e?n.slice(1):n).charAt(0),r="!"===(n=t?n.slice(1):n).charAt(0);return{name:n=r?n.slice(1):n,once:t,capture:r,passive:e}}));function $n(n,e){function t(){var n=t.fns;if(!i(n))return qe(n,null,arguments,e,"v-on handler");for(var r=n.slice(),a=0;a<r.length;a++)qe(r[a],null,arguments,e,"v-on handler")}return t.fns=n,t}function Qn(n,e,t,r,i,s){var l,c,d,p;for(l in n)c=n[l],d=e[l],p=Jn(l),a(c)||(a(d)?(a(c.fns)&&(c=n[l]=$n(c,s)),o(p.once)&&(c=n[l]=i(p.name,c,p.capture)),t(p.name,c,p.capture,p.passive,p.params)):c!==d&&(d.fns=c,n[l]=d));for(l in e)a(n[l])&&r((p=Jn(l)).name,e[l],p.capture)}function Gn(n,e,t){var r;n instanceof mn&&(n=n.data.hook||(n.data.hook={}));var i=n[e];function l(){t.apply(this,arguments),x(r.fns,l)}a(i)?r=$n([l]):s(i.fns)&&o(i.merged)?(r=i).fns.push(l):r=$n([i,l]),r.merged=!0,n[e]=r}function Vn(n,e,t,r,i){if(s(e)){if(w(e,t))return n[t]=e[t],i||delete e[t],!0;if(w(e,r))return n[t]=e[r],i||delete e[r],!0}return!1}function Wn(n){return l(n)?[hn(n)]:i(n)?function n(e,t){var r,c,d,p,u=[];for(r=0;r<e.length;r++)a(c=e[r])||"boolean"==typeof c||(d=u.length-1,p=u[d],i(c)?c.length>0&&(Kn((c=n(c,"".concat(t||"","_").concat(r)))[0])&&Kn(p)&&(u[d]=hn(p.text+c[0].text),c.shift()),u.push.apply(u,c)):l(c)?Kn(p)?u[d]=hn(p.text+c):""!==c&&u.push(hn(c)):Kn(c)&&Kn(p)?u[d]=hn(p.text+c.text):(o(e._isVList)&&s(c.tag)&&a(c.key)&&s(t)&&(c.key="__vlist".concat(t,"_").concat(r,"__")),u.push(c)));return u}(n):void 0}function Kn(n){return s(n)&&s(n.text)&&!1===n.isComment}function Zn(n,e){var t,r,a,o,l=null;if(i(n)||"string"==typeof n)for(l=new Array(n.length),t=0,r=n.length;t<r;t++)l[t]=e(n[t],t);else if("number"==typeof n)for(l=new Array(n),t=0;t<n;t++)l[t]=e(t+1,t);else if(d(n))if(dn&&n[Symbol.iterator]){l=[];for(var c=n[Symbol.iterator](),p=c.next();!p.done;)l.push(e(p.value,l.length)),p=c.next()}else for(a=Object.keys(n),l=new Array(a.length),t=0,r=a.length;t<r;t++)o=a[t],l[t]=e(n[o],o,t);return s(l)||(l=[]),l._isVList=!0,l}function Xn(n,e,t,r){var i,a=this.$scopedSlots[n];a?(t=t||{},r&&(t=A(A({},r),t)),i=a(t)||(c(e)?e():e)):i=this.$slots[n]||(c(e)?e():e);var s=t&&t.slot;return s?this.$createElement("template",{slot:s},i):i}function Yn(n){return Ct(this.$options,"filters",n,!0)||M}function ne(n,e){return i(n)?-1===n.indexOf(e):n!==e}function ee(n,e,t,r,i){var a=U.keyCodes[e]||t;return i&&r&&!U.keyCodes[e]?ne(i,r):a?ne(a,n):r?_(r)!==e:void 0===n}function te(n,e,t,r,a){if(t)if(d(t)){i(t)&&(t=R(t));var s=void 0,o=function(i){if("class"===i||"style"===i||y(i))s=n;else{var o=n.attrs&&n.attrs.type;s=r||U.mustUseProp(e,o,i)?n.domProps||(n.domProps={}):n.attrs||(n.attrs={})}var l=E(i),c=_(i);l in s||c in s||(s[i]=t[i],a&&((n.on||(n.on={}))["update:".concat(i)]=function(n){t[i]=n}))};for(var l in t)o(l)}else;return n}function re(n,e){var t=this._staticTrees||(this._staticTrees=[]),r=t[n];return r&&!e||ae(r=t[n]=this.$options.staticRenderFns[n].call(this._renderProxy,this._c,this),"__static__".concat(n),!1),r}function ie(n,e,t){return ae(n,"__once__".concat(e).concat(t?"_".concat(t):""),!0),n}function ae(n,e,t){if(i(n))for(var r=0;r<n.length;r++)n[r]&&"string"!=typeof n[r]&&se(n[r],"".concat(e,"_").concat(r),t);else se(n,e,t)}function se(n,e,t){n.isStatic=!0,n.key=e,n.isOnce=t}function oe(n,e){if(e)if(u(e)){var t=n.on=n.on?A({},n.on):{};for(var r in e){var i=t[r],a=e[r];t[r]=i?[].concat(i,a):a}}else;return n}function le(n,e,t,r){e=e||{$stable:!t};for(var a=0;a<n.length;a++){var s=n[a];i(s)?le(s,e,t):s&&(s.proxy&&(s.fn.proxy=!0),e[s.key]=s.fn)}return r&&(e.$key=r),e}function ce(n,e){for(var t=0;t<e.length;t+=2){var r=e[t];"string"==typeof r&&r&&(n[e[t]]=e[t+1])}return n}function de(n,e){return"string"==typeof n?e+n:n}function pe(n){n._o=ie,n._n=f,n._s=b,n._l=Zn,n._t=Xn,n._q=B,n._i=L,n._m=re,n._f=Yn,n._k=ee,n._b=te,n._v=hn,n._e=gn,n._u=le,n._g=oe,n._d=ce,n._p=de}function ue(n,e){if(!n||!n.length)return{};for(var t={},r=0,i=n.length;r<i;r++){var a=n[r],s=a.data;if(s&&s.attrs&&s.attrs.slot&&delete s.attrs.slot,a.context!==e&&a.fnContext!==e||!s||null==s.slot)(t.default||(t.default=[])).push(a);else{var o=s.slot,l=t[o]||(t[o]=[]);"template"===a.tag?l.push.apply(l,a.children||[]):l.push(a)}}for(var c in t)t[c].every(me)&&delete t[c];return t}function me(n){return n.isComment&&!n.asyncFactory||" "===n.text}function ge(n){return n.isComment&&n.asyncFactory}function he(n,e,t,i){var a,s=Object.keys(t).length>0,o=e?!!e.$stable:!s,l=e&&e.$key;if(e){if(e._normalized)return e._normalized;if(o&&i&&i!==r&&l===i.$key&&!s&&!i.$hasNormal)return i;for(var c in a={},e)e[c]&&"$"!==c[0]&&(a[c]=be(n,t,c,e[c]))}else a={};for(var d in t)d in a||(a[d]=fe(t,d));return e&&Object.isExtensible(e)&&(e._normalized=a),$(a,"$stable",o),$(a,"$key",l),$(a,"$hasNormal",s),a}function be(n,e,t,r){var a=function(){var e=pn;un(n);var t=arguments.length?r.apply(null,arguments):r({}),a=(t=t&&"object"==typeof t&&!i(t)?[t]:Wn(t))&&t[0];return un(e),t&&(!a||1===t.length&&a.isComment&&!ge(a))?void 0:t};return r.proxy&&Object.defineProperty(e,t,{get:a,enumerable:!0,configurable:!0}),a}function fe(n,e){return function(){return n[e]}}function ve(n){return{get attrs(){if(!n._attrsProxy){var e=n._attrsProxy={};$(e,"_v_attr_proxy",!0),ye(e,n.$attrs,r,n,"$attrs")}return n._attrsProxy},get listeners(){n._listenersProxy||ye(n._listenersProxy={},n.$listeners,r,n,"$listeners");return n._listenersProxy},get slots(){return function(n){n._slotsProxy||ke(n._slotsProxy={},n.$scopedSlots);return n._slotsProxy}(n)},emit:I(n.$emit,n),expose:function(e){e&&Object.keys(e).forEach((function(t){return Pn(n,e,t)}))}}}function ye(n,e,t,r,i){var a=!1;for(var s in e)s in n?e[s]!==t[s]&&(a=!0):(a=!0,xe(n,s,r,i));for(var s in n)s in e||(a=!0,delete n[s]);return a}function xe(n,e,t,r){Object.defineProperty(n,e,{enumerable:!0,configurable:!0,get:function(){return t[r][e]}})}function ke(n,e){for(var t in e)n[t]=e[t];for(var t in n)t in e||delete n[t]}var we=null;function Se(n,e){return(n.__esModule||dn&&"Module"===n[Symbol.toStringTag])&&(n=n.default),d(n)?e.extend(n):n}function je(n){if(i(n))for(var e=0;e<n.length;e++){var t=n[e];if(s(t)&&(s(t.componentOptions)||ge(t)))return t}}function Ee(n,e,t,r,p,u){return(i(t)||l(t))&&(p=r,r=t,t=void 0),o(u)&&(p=2),function(n,e,t,r,l){if(s(t)&&s(t.__ob__))return gn();s(t)&&s(t.is)&&(e=t.is);if(!e)return gn();0;i(r)&&c(r[0])&&((t=t||{}).scopedSlots={default:r[0]},r.length=0);2===l?r=Wn(r):1===l&&(r=function(n){for(var e=0;e<n.length;e++)if(i(n[e]))return Array.prototype.concat.apply([],n);return n}(r));var p,u;if("string"==typeof e){var m=void 0;u=n.$vnode&&n.$vnode.ns||U.getTagNamespace(e),p=U.isReservedTag(e)?new mn(U.parsePlatformTagName(e),t,r,void 0,void 0,n):t&&t.pre||!s(m=Ct(n.$options,"components",e))?new mn(e,t,r,void 0,void 0,n):xt(m,t,n,r,e)}else p=xt(e,t,n,r);return i(p)?p:s(p)?(s(u)&&function n(e,t,r){e.ns=t,"foreignObject"===e.tag&&(t=void 0,r=!0);if(s(e.children))for(var i=0,l=e.children.length;i<l;i++){var c=e.children[i];s(c.tag)&&(a(c.ns)||o(r)&&"svg"!==c.tag)&&n(c,t,r)}}(p,u),s(t)&&function(n){d(n.style)&&Ue(n.style);d(n.class)&&Ue(n.class)}(t),p):gn()}(n,e,t,r,p)}function Te(n,e,t){kn();try{if(e)for(var r=e;r=r.$parent;){var i=r.$options.errorCaptured;if(i)for(var a=0;a<i.length;a++)try{if(!1===i[a].call(r,n,e,t))return}catch(n){_e(n,r,"errorCaptured hook")}}_e(n,e,t)}finally{wn()}}function qe(n,e,t,r,i){var a;try{(a=t?n.apply(e,t):n.call(e))&&!a._isVue&&h(a)&&!a._handled&&(a.catch((function(n){return Te(n,r,i+" (Promise/async)")})),a._handled=!0)}catch(n){Te(n,r,i)}return a}function _e(n,e,t){if(U.errorHandler)try{return U.errorHandler.call(null,n,e,t)}catch(e){e!==n&&Ie(e,null,"config.errorHandler")}Ie(n,e,t)}function Ie(n,e,t){if(!V||"undefined"==typeof console)throw n;console.error(n)}var Ce,Ae=!1,Re=[],ze=!1;function Oe(){ze=!1;var n=Re.slice(0);Re.length=0;for(var e=0;e<n.length;e++)n[e]()}if("undefined"!=typeof Promise&&ln(Promise)){var Me=Promise.resolve();Ce=function(){Me.then(Oe),Y&&setTimeout(z)},Ae=!0}else if(K||"undefined"==typeof MutationObserver||!ln(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())Ce="undefined"!=typeof setImmediate&&ln(setImmediate)?function(){setImmediate(Oe)}:function(){setTimeout(Oe,0)};else{var Be=1,Le=new MutationObserver(Oe),De=document.createTextNode(String(Be));Le.observe(De,{characterData:!0}),Ce=function(){Be=(Be+1)%2,De.data=String(Be)},Ae=!0}function Fe(n,e){var t;if(Re.push((function(){if(n)try{n.call(e)}catch(n){Te(n,e,"nextTick")}else t&&t(e)})),ze||(ze=!0,Ce()),!n&&"undefined"!=typeof Promise)return new Promise((function(n){t=n}))}function Pe(n){return function(e,t){if(void 0===t&&(t=pn),t)return function(n,e,t){var r=n.$options;r[e]=Tt(r[e],t)}(t,n,e)}}Pe("beforeMount"),Pe("mounted"),Pe("beforeUpdate"),Pe("updated"),Pe("beforeDestroy"),Pe("destroyed"),Pe("activated"),Pe("deactivated"),Pe("serverPrefetch"),Pe("renderTracked"),Pe("renderTriggered"),Pe("errorCaptured");var Ne=new cn;function Ue(n){return function n(e,t){var r,a,s=i(e);if(!s&&!d(e)||e.__v_skip||Object.isFrozen(e)||e instanceof mn)return;if(e.__ob__){var o=e.__ob__.dep.id;if(t.has(o))return;t.add(o)}if(s)for(r=e.length;r--;)n(e[r],t);else if(Fn(e))n(e.value,t);else for(a=Object.keys(e),r=a.length;r--;)n(e[a[r]],t)}(n,Ne),Ne.clear(),n}var He,Je=0,$e=function(){function n(n,e,t,r,i){var a,s;a=this,void 0===(s=Nn&&!Nn._vm?Nn:n?n._scope:void 0)&&(s=Nn),s&&s.active&&s.effects.push(a),(this.vm=n)&&i&&(n._watcher=this),r?(this.deep=!!r.deep,this.user=!!r.user,this.lazy=!!r.lazy,this.sync=!!r.sync,this.before=r.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=t,this.id=++Je,this.active=!0,this.post=!1,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new cn,this.newDepIds=new cn,this.expression="",c(e)?this.getter=e:(this.getter=function(n){if(!Q.test(n)){var e=n.split(".");return function(n){for(var t=0;t<e.length;t++){if(!n)return;n=n[e[t]]}return n}}}(e),this.getter||(this.getter=z)),this.value=this.lazy?void 0:this.get()}return n.prototype.get=function(){var n;kn(this);var e=this.vm;try{n=this.getter.call(e,e)}catch(n){if(!this.user)throw n;Te(n,e,'getter for watcher "'.concat(this.expression,'"'))}finally{this.deep&&Ue(n),wn(),this.cleanupDeps()}return n},n.prototype.addDep=function(n){var e=n.id;this.newDepIds.has(e)||(this.newDepIds.add(e),this.newDeps.push(n),this.depIds.has(e)||n.addSub(this))},n.prototype.cleanupDeps=function(){for(var n=this.deps.length;n--;){var e=this.deps[n];this.newDepIds.has(e.id)||e.removeSub(this)}var t=this.depIds;this.depIds=this.newDepIds,this.newDepIds=t,this.newDepIds.clear(),t=this.deps,this.deps=this.newDeps,this.newDeps=t,this.newDeps.length=0},n.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():ut(this)},n.prototype.run=function(){if(this.active){var n=this.get();if(n!==this.value||d(n)||this.deep){var e=this.value;if(this.value=n,this.user){var t='callback for watcher "'.concat(this.expression,'"');qe(this.cb,this.vm,[n,e],this.vm,t)}else this.cb.call(this.vm,n,e)}}},n.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},n.prototype.depend=function(){for(var n=this.deps.length;n--;)this.deps[n].depend()},n.prototype.teardown=function(){if(this.vm&&!this.vm._isBeingDestroyed&&x(this.vm._scope.effects,this),this.active){for(var n=this.deps.length;n--;)this.deps[n].removeSub(this);this.active=!1,this.onStop&&this.onStop()}},n}();function Qe(n,e){He.$on(n,e)}function Ge(n,e){He.$off(n,e)}function Ve(n,e){var t=He;return function r(){var i=e.apply(null,arguments);null!==i&&t.$off(n,r)}}function We(n,e,t){He=n,Qn(e,t||{},Qe,Ge,Ve,n),He=void 0}var Ke=null;function Ze(n){var e=Ke;return Ke=n,function(){Ke=e}}function Xe(n){for(;n&&(n=n.$parent);)if(n._inactive)return!0;return!1}function Ye(n,e){if(e){if(n._directInactive=!1,Xe(n))return}else if(n._directInactive)return;if(n._inactive||null===n._inactive){n._inactive=!1;for(var t=0;t<n.$children.length;t++)Ye(n.$children[t]);nt(n,"activated")}}function nt(n,e,t,r){void 0===r&&(r=!0),kn();var i=pn;r&&un(n);var a=n.$options[e],s="".concat(e," hook");if(a)for(var o=0,l=a.length;o<l;o++)qe(a[o],n,t||null,n,s);n._hasHookEvent&&n.$emit("hook:"+e),r&&un(i),wn()}var et=[],tt=[],rt={},it=!1,at=!1,st=0;var ot=0,lt=Date.now;if(V&&!K){var ct=window.performance;ct&&"function"==typeof ct.now&&lt()>document.createEvent("Event").timeStamp&&(lt=function(){return ct.now()})}var dt=function(n,e){if(n.post){if(!e.post)return 1}else if(e.post)return-1;return n.id-e.id};function pt(){var n,e;for(ot=lt(),at=!0,et.sort(dt),st=0;st<et.length;st++)(n=et[st]).before&&n.before(),e=n.id,rt[e]=null,n.run();var t=tt.slice(),r=et.slice();st=et.length=tt.length=0,rt={},it=at=!1,function(n){for(var e=0;e<n.length;e++)n[e]._inactive=!0,Ye(n[e],!0)}(t),function(n){var e=n.length;for(;e--;){var t=n[e],r=t.vm;r&&r._watcher===t&&r._isMounted&&!r._isDestroyed&&nt(r,"updated")}}(r),function(){for(var n=0;n<vn.length;n++){var e=vn[n];e.subs=e.subs.filter((function(n){return n})),e._pending=!1}vn.length=0}(),on&&U.devtools&&on.emit("flush")}function ut(n){var e=n.id;if(null==rt[e]&&(n!==yn.target||!n.noRecurse)){if(rt[e]=!0,at){for(var t=et.length-1;t>st&&et[t].id>n.id;)t--;et.splice(t+1,0,n)}else et.push(n);it||(it=!0,Fe(pt))}}function mt(n,e){if(n){for(var t=Object.create(null),r=dn?Reflect.ownKeys(n):Object.keys(n),i=0;i<r.length;i++){var a=r[i];if("__ob__"!==a){var s=n[a].from;if(s in e._provided)t[a]=e._provided[s];else if("default"in n[a]){var o=n[a].default;t[a]=c(o)?o.call(e):o}else 0}}return t}}function gt(n,e,t,a,s){var l,c=this,d=s.options;w(a,"_uid")?(l=Object.create(a))._original=a:(l=a,a=a._original);var p=o(d._compiled),u=!p;this.data=n,this.props=e,this.children=t,this.parent=a,this.listeners=n.on||r,this.injections=mt(d.inject,a),this.slots=function(){return c.$slots||he(a,n.scopedSlots,c.$slots=ue(t,a)),c.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return he(a,n.scopedSlots,this.slots())}}),p&&(this.$options=d,this.$slots=this.slots(),this.$scopedSlots=he(a,n.scopedSlots,this.$slots)),d._scopeId?this._c=function(n,e,t,r){var s=Ee(l,n,e,t,r,u);return s&&!i(s)&&(s.fnScopeId=d._scopeId,s.fnContext=a),s}:this._c=function(n,e,t,r){return Ee(l,n,e,t,r,u)}}function ht(n,e,t,r,i){var a=bn(n);return a.fnContext=t,a.fnOptions=r,e.slot&&((a.data||(a.data={})).slot=e.slot),a}function bt(n,e){for(var t in e)n[E(t)]=e[t]}function ft(n){return n.name||n.__name||n._componentTag}pe(gt.prototype);var vt={init:function(n,e){if(n.componentInstance&&!n.componentInstance._isDestroyed&&n.data.keepAlive){var t=n;vt.prepatch(t,t)}else{(n.componentInstance=function(n,e){var t={_isComponent:!0,_parentVnode:n,parent:e},r=n.data.inlineTemplate;s(r)&&(t.render=r.render,t.staticRenderFns=r.staticRenderFns);return new n.componentOptions.Ctor(t)}(n,Ke)).$mount(e?n.elm:void 0,e)}},prepatch:function(n,e){var t=e.componentOptions;!function(n,e,t,i,a){var s=i.data.scopedSlots,o=n.$scopedSlots,l=!!(s&&!s.$stable||o!==r&&!o.$stable||s&&n.$scopedSlots.$key!==s.$key||!s&&n.$scopedSlots.$key),c=!!(a||n.$options._renderChildren||l),d=n.$vnode;n.$options._parentVnode=i,n.$vnode=i,n._vnode&&(n._vnode.parent=i),n.$options._renderChildren=a;var p=i.data.attrs||r;n._attrsProxy&&ye(n._attrsProxy,p,d.data&&d.data.attrs||r,n,"$attrs")&&(c=!0),n.$attrs=p,t=t||r;var u=n.$options._parentListeners;if(n._listenersProxy&&ye(n._listenersProxy,t,u||r,n,"$listeners"),n.$listeners=n.$options._parentListeners=t,We(n,t,u),e&&n.$options.props){_n(!1);for(var m=n._props,g=n.$options._propKeys||[],h=0;h<g.length;h++){var b=g[h],f=n.$options.props;m[b]=At(b,f,e,n)}_n(!0),n.$options.propsData=e}c&&(n.$slots=ue(a,i.context),n.$forceUpdate())}(e.componentInstance=n.componentInstance,t.propsData,t.listeners,e,t.children)},insert:function(n){var e,t=n.context,r=n.componentInstance;r._isMounted||(r._isMounted=!0,nt(r,"mounted")),n.data.keepAlive&&(t._isMounted?((e=r)._inactive=!1,tt.push(e)):Ye(r,!0))},destroy:function(n){var e=n.componentInstance;e._isDestroyed||(n.data.keepAlive?function n(e,t){if(!(t&&(e._directInactive=!0,Xe(e))||e._inactive)){e._inactive=!0;for(var r=0;r<e.$children.length;r++)n(e.$children[r]);nt(e,"deactivated")}}(e,!0):e.$destroy())}},yt=Object.keys(vt);function xt(n,e,t,l,c){if(!a(n)){var p=t.$options._base;if(d(n)&&(n=p.extend(n)),"function"==typeof n){var u;if(a(n.cid)&&void 0===(n=function(n,e){if(o(n.error)&&s(n.errorComp))return n.errorComp;if(s(n.resolved))return n.resolved;var t=we;if(t&&s(n.owners)&&-1===n.owners.indexOf(t)&&n.owners.push(t),o(n.loading)&&s(n.loadingComp))return n.loadingComp;if(t&&!s(n.owners)){var r=n.owners=[t],i=!0,l=null,c=null;t.$on("hook:destroyed",(function(){return x(r,t)}));var p=function(n){for(var e=0,t=r.length;e<t;e++)r[e].$forceUpdate();n&&(r.length=0,null!==l&&(clearTimeout(l),l=null),null!==c&&(clearTimeout(c),c=null))},u=D((function(t){n.resolved=Se(t,e),i?r.length=0:p(!0)})),m=D((function(e){s(n.errorComp)&&(n.error=!0,p(!0))})),g=n(u,m);return d(g)&&(h(g)?a(n.resolved)&&g.then(u,m):h(g.component)&&(g.component.then(u,m),s(g.error)&&(n.errorComp=Se(g.error,e)),s(g.loading)&&(n.loadingComp=Se(g.loading,e),0===g.delay?n.loading=!0:l=setTimeout((function(){l=null,a(n.resolved)&&a(n.error)&&(n.loading=!0,p(!1))}),g.delay||200)),s(g.timeout)&&(c=setTimeout((function(){c=null,a(n.resolved)&&m(null)}),g.timeout)))),i=!1,n.loading?n.loadingComp:n.resolved}}(u=n,p)))return function(n,e,t,r,i){var a=gn();return a.asyncFactory=n,a.asyncMeta={data:e,context:t,children:r,tag:i},a}(u,e,t,l,c);e=e||{},$t(n),s(e.model)&&function(n,e){var t=n.model&&n.model.prop||"value",r=n.model&&n.model.event||"input";(e.attrs||(e.attrs={}))[t]=e.model.value;var a=e.on||(e.on={}),o=a[r],l=e.model.callback;s(o)?(i(o)?-1===o.indexOf(l):o!==l)&&(a[r]=[l].concat(o)):a[r]=l}(n.options,e);var m=function(n,e,t){var r=e.options.props;if(!a(r)){var i={},o=n.attrs,l=n.props;if(s(o)||s(l))for(var c in r){var d=_(c);Vn(i,l,c,d,!0)||Vn(i,o,c,d,!1)}return i}}(e,n);if(o(n.options.functional))return function(n,e,t,a,o){var l=n.options,c={},d=l.props;if(s(d))for(var p in d)c[p]=At(p,d,e||r);else s(t.attrs)&&bt(c,t.attrs),s(t.props)&&bt(c,t.props);var u=new gt(t,c,o,a,n),m=l.render.call(null,u._c,u);if(m instanceof mn)return ht(m,t,u.parent,l,u);if(i(m)){for(var g=Wn(m)||[],h=new Array(g.length),b=0;b<g.length;b++)h[b]=ht(g[b],t,u.parent,l,u);return h}}(n,m,e,t,l);var g=e.on;if(e.on=e.nativeOn,o(n.options.abstract)){var b=e.slot;e={},b&&(e.slot=b)}!function(n){for(var e=n.hook||(n.hook={}),t=0;t<yt.length;t++){var r=yt[t],i=e[r],a=vt[r];i===a||i&&i._merged||(e[r]=i?kt(a,i):a)}}(e);var f=ft(n.options)||c;return new mn("vue-component-".concat(n.cid).concat(f?"-".concat(f):""),e,void 0,void 0,void 0,t,{Ctor:n,propsData:m,listeners:g,tag:c,children:l},u)}}}function kt(n,e){var t=function(t,r){n(t,r),e(t,r)};return t._merged=!0,t}var wt=z,St=U.optionMergeStrategies;function jt(n,e,t){if(void 0===t&&(t=!0),!e)return n;for(var r,i,a,s=dn?Reflect.ownKeys(e):Object.keys(e),o=0;o<s.length;o++)"__ob__"!==(r=s[o])&&(i=n[r],a=e[r],t&&w(n,r)?i!==a&&u(i)&&u(a)&&jt(i,a):zn(n,r,a));return n}function Et(n,e,t){return t?function(){var r=c(e)?e.call(t,t):e,i=c(n)?n.call(t,t):n;return r?jt(r,i):i}:e?n?function(){return jt(c(e)?e.call(this,this):e,c(n)?n.call(this,this):n)}:e:n}function Tt(n,e){var t=e?n?n.concat(e):i(e)?e:[e]:n;return t?function(n){for(var e=[],t=0;t<n.length;t++)-1===e.indexOf(n[t])&&e.push(n[t]);return e}(t):t}function qt(n,e,t,r){var i=Object.create(n||null);return e?A(i,e):i}St.data=function(n,e,t){return t?Et(n,e,t):e&&"function"!=typeof e?n:Et(n,e)},N.forEach((function(n){St[n]=Tt})),P.forEach((function(n){St[n+"s"]=qt})),St.watch=function(n,e,t,r){if(n===tn&&(n=void 0),e===tn&&(e=void 0),!e)return Object.create(n||null);if(!n)return e;var a={};for(var s in A(a,n),e){var o=a[s],l=e[s];o&&!i(o)&&(o=[o]),a[s]=o?o.concat(l):i(l)?l:[l]}return a},St.props=St.methods=St.inject=St.computed=function(n,e,t,r){if(!n)return e;var i=Object.create(null);return A(i,n),e&&A(i,e),i},St.provide=function(n,e){return n?function(){var t=Object.create(null);return jt(t,c(n)?n.call(this):n),e&&jt(t,c(e)?e.call(this):e,!1),t}:e};var _t=function(n,e){return void 0===e?n:e};function It(n,e,t){if(c(e)&&(e=e.options),function(n,e){var t=n.props;if(t){var r,a,s={};if(i(t))for(r=t.length;r--;)"string"==typeof(a=t[r])&&(s[E(a)]={type:null});else if(u(t))for(var o in t)a=t[o],s[E(o)]=u(a)?a:{type:a};else 0;n.props=s}}(e),function(n,e){var t=n.inject;if(t){var r=n.inject={};if(i(t))for(var a=0;a<t.length;a++)r[t[a]]={from:t[a]};else if(u(t))for(var s in t){var o=t[s];r[s]=u(o)?A({from:s},o):{from:o}}else 0}}(e),function(n){var e=n.directives;if(e)for(var t in e){var r=e[t];c(r)&&(e[t]={bind:r,update:r})}}(e),!e._base&&(e.extends&&(n=It(n,e.extends,t)),e.mixins))for(var r=0,a=e.mixins.length;r<a;r++)n=It(n,e.mixins[r],t);var s,o={};for(s in n)l(s);for(s in e)w(n,s)||l(s);function l(r){var i=St[r]||_t;o[r]=i(n[r],e[r],t,r)}return o}function Ct(n,e,t,r){if("string"==typeof t){var i=n[e];if(w(i,t))return i[t];var a=E(t);if(w(i,a))return i[a];var s=T(a);return w(i,s)?i[s]:i[t]||i[a]||i[s]}}function At(n,e,t,r){var i=e[n],a=!w(t,n),s=t[n],o=Mt(Boolean,i.type);if(o>-1)if(a&&!w(i,"default"))s=!1;else if(""===s||s===_(n)){var l=Mt(String,i.type);(l<0||o<l)&&(s=!0)}if(void 0===s){s=function(n,e,t){if(!w(e,"default"))return;var r=e.default;0;if(n&&n.$options.propsData&&void 0===n.$options.propsData[t]&&void 0!==n._props[t])return n._props[t];return c(r)&&"Function"!==zt(e.type)?r.call(n):r}(r,i,n);var d=qn;_n(!0),An(s),_n(d)}return s}var Rt=/^\s*function (\w+)/;function zt(n){var e=n&&n.toString().match(Rt);return e?e[1]:""}function Ot(n,e){return zt(n)===zt(e)}function Mt(n,e){if(!i(e))return Ot(e,n)?0:-1;for(var t=0,r=e.length;t<r;t++)if(Ot(e[t],n))return t;return-1}var Bt={enumerable:!0,configurable:!0,get:z,set:z};function Lt(n,e,t){Bt.get=function(){return this[e][t]},Bt.set=function(n){this[e][t]=n},Object.defineProperty(n,t,Bt)}function Dt(n){var e=n.$options;if(e.props&&function(n,e){var t=n.$options.propsData||{},r=n._props=Bn({}),i=n.$options._propKeys=[];n.$parent&&_n(!1);var a=function(a){i.push(a);var s=At(a,e,t,n);Rn(r,a,s),a in n||Lt(n,"_props",a)};for(var s in e)a(s);_n(!0)}(n,e.props),function(n){var e=n.$options,t=e.setup;if(t){var r=n._setupContext=ve(n);un(n),kn();var i=qe(t,null,[n._props||Bn({}),r],n,"setup");if(wn(),un(),c(i))e.render=i;else if(d(i))if(n._setupState=i,i.__sfc){var a=n._setupProxy={};for(var s in i)"__sfc"!==s&&Pn(a,i,s)}else for(var s in i)J(s)||Pn(n,i,s);else 0}}(n),e.methods&&function(n,e){n.$options.props;for(var t in e)n[t]="function"!=typeof e[t]?z:I(e[t],n)}(n,e.methods),e.data)!function(n){var e=n.$options.data;u(e=n._data=c(e)?function(n,e){kn();try{return n.call(e,e)}catch(n){return Te(n,e,"data()"),{}}finally{wn()}}(e,n):e||{})||(e={});var t=Object.keys(e),r=n.$options.props,i=(n.$options.methods,t.length);for(;i--;){var a=t[i];0,r&&w(r,a)||J(a)||Lt(n,"_data",a)}var s=An(e);s&&s.vmCount++}(n);else{var t=An(n._data={});t&&t.vmCount++}e.computed&&function(n,e){var t=n._computedWatchers=Object.create(null),r=sn();for(var i in e){var a=e[i],s=c(a)?a:a.get;0,r||(t[i]=new $e(n,s||z,z,Ft)),i in n||Pt(n,i,a)}}(n,e.computed),e.watch&&e.watch!==tn&&function(n,e){for(var t in e){var r=e[t];if(i(r))for(var a=0;a<r.length;a++)Ht(n,t,r[a]);else Ht(n,t,r)}}(n,e.watch)}var Ft={lazy:!0};function Pt(n,e,t){var r=!sn();c(t)?(Bt.get=r?Nt(e):Ut(t),Bt.set=z):(Bt.get=t.get?r&&!1!==t.cache?Nt(e):Ut(t.get):z,Bt.set=t.set||z),Object.defineProperty(n,e,Bt)}function Nt(n){return function(){var e=this._computedWatchers&&this._computedWatchers[n];if(e)return e.dirty&&e.evaluate(),yn.target&&e.depend(),e.value}}function Ut(n){return function(){return n.call(this,this)}}function Ht(n,e,t,r){return u(t)&&(r=t,t=t.handler),"string"==typeof t&&(t=n[t]),n.$watch(e,t,r)}var Jt=0;function $t(n){var e=n.options;if(n.super){var t=$t(n.super);if(t!==n.superOptions){n.superOptions=t;var r=function(n){var e,t=n.options,r=n.sealedOptions;for(var i in t)t[i]!==r[i]&&(e||(e={}),e[i]=t[i]);return e}(n);r&&A(n.extendOptions,r),(e=n.options=It(t,n.extendOptions)).name&&(e.components[e.name]=n)}}return e}function Qt(n){this._init(n)}function Gt(n){n.cid=0;var e=1;n.extend=function(n){n=n||{};var t=this,r=t.cid,i=n._Ctor||(n._Ctor={});if(i[r])return i[r];var a=ft(n)||ft(t.options);var s=function(n){this._init(n)};return(s.prototype=Object.create(t.prototype)).constructor=s,s.cid=e++,s.options=It(t.options,n),s.super=t,s.options.props&&function(n){var e=n.options.props;for(var t in e)Lt(n.prototype,"_props",t)}(s),s.options.computed&&function(n){var e=n.options.computed;for(var t in e)Pt(n.prototype,t,e[t])}(s),s.extend=t.extend,s.mixin=t.mixin,s.use=t.use,P.forEach((function(n){s[n]=t[n]})),a&&(s.options.components[a]=s),s.superOptions=t.options,s.extendOptions=n,s.sealedOptions=A({},s.options),i[r]=s,s}}function Vt(n){return n&&(ft(n.Ctor.options)||n.tag)}function Wt(n,e){return i(n)?n.indexOf(e)>-1:"string"==typeof n?n.split(",").indexOf(e)>-1:!!m(n)&&n.test(e)}function Kt(n,e){var t=n.cache,r=n.keys,i=n._vnode;for(var a in t){var s=t[a];if(s){var o=s.name;o&&!e(o)&&Zt(t,a,r,i)}}}function Zt(n,e,t,r){var i=n[e];!i||r&&i.tag===r.tag||i.componentInstance.$destroy(),n[e]=null,x(t,e)}!function(n){n.prototype._init=function(n){var e=this;e._uid=Jt++,e._isVue=!0,e.__v_skip=!0,e._scope=new Un(!0),e._scope._vm=!0,n&&n._isComponent?function(n,e){var t=n.$options=Object.create(n.constructor.options),r=e._parentVnode;t.parent=e.parent,t._parentVnode=r;var i=r.componentOptions;t.propsData=i.propsData,t._parentListeners=i.listeners,t._renderChildren=i.children,t._componentTag=i.tag,e.render&&(t.render=e.render,t.staticRenderFns=e.staticRenderFns)}(e,n):e.$options=It($t(e.constructor),n||{},e),e._renderProxy=e,e._self=e,function(n){var e=n.$options,t=e.parent;if(t&&!e.abstract){for(;t.$options.abstract&&t.$parent;)t=t.$parent;t.$children.push(n)}n.$parent=t,n.$root=t?t.$root:n,n.$children=[],n.$refs={},n._provided=t?t._provided:Object.create(null),n._watcher=null,n._inactive=null,n._directInactive=!1,n._isMounted=!1,n._isDestroyed=!1,n._isBeingDestroyed=!1}(e),function(n){n._events=Object.create(null),n._hasHookEvent=!1;var e=n.$options._parentListeners;e&&We(n,e)}(e),function(n){n._vnode=null,n._staticTrees=null;var e=n.$options,t=n.$vnode=e._parentVnode,i=t&&t.context;n.$slots=ue(e._renderChildren,i),n.$scopedSlots=t?he(n.$parent,t.data.scopedSlots,n.$slots):r,n._c=function(e,t,r,i){return Ee(n,e,t,r,i,!1)},n.$createElement=function(e,t,r,i){return Ee(n,e,t,r,i,!0)};var a=t&&t.data;Rn(n,"$attrs",a&&a.attrs||r,null,!0),Rn(n,"$listeners",e._parentListeners||r,null,!0)}(e),nt(e,"beforeCreate",void 0,!1),function(n){var e=mt(n.$options.inject,n);e&&(_n(!1),Object.keys(e).forEach((function(t){Rn(n,t,e[t])})),_n(!0))}(e),Dt(e),function(n){var e=n.$options.provide;if(e){var t=c(e)?e.call(n):e;if(!d(t))return;for(var r=Hn(n),i=dn?Reflect.ownKeys(t):Object.keys(t),a=0;a<i.length;a++){var s=i[a];Object.defineProperty(r,s,Object.getOwnPropertyDescriptor(t,s))}}}(e),nt(e,"created"),e.$options.el&&e.$mount(e.$options.el)}}(Qt),function(n){var e={get:function(){return this._data}},t={get:function(){return this._props}};Object.defineProperty(n.prototype,"$data",e),Object.defineProperty(n.prototype,"$props",t),n.prototype.$set=zn,n.prototype.$delete=On,n.prototype.$watch=function(n,e,t){if(u(e))return Ht(this,n,e,t);(t=t||{}).user=!0;var r=new $e(this,n,e,t);if(t.immediate){var i='callback for immediate watcher "'.concat(r.expression,'"');kn(),qe(e,this,[r.value],this,i),wn()}return function(){r.teardown()}}}(Qt),function(n){var e=/^hook:/;n.prototype.$on=function(n,t){var r=this;if(i(n))for(var a=0,s=n.length;a<s;a++)r.$on(n[a],t);else(r._events[n]||(r._events[n]=[])).push(t),e.test(n)&&(r._hasHookEvent=!0);return r},n.prototype.$once=function(n,e){var t=this;function r(){t.$off(n,r),e.apply(t,arguments)}return r.fn=e,t.$on(n,r),t},n.prototype.$off=function(n,e){var t=this;if(!arguments.length)return t._events=Object.create(null),t;if(i(n)){for(var r=0,a=n.length;r<a;r++)t.$off(n[r],e);return t}var s,o=t._events[n];if(!o)return t;if(!e)return t._events[n]=null,t;for(var l=o.length;l--;)if((s=o[l])===e||s.fn===e){o.splice(l,1);break}return t},n.prototype.$emit=function(n){var e=this,t=e._events[n];if(t){t=t.length>1?C(t):t;for(var r=C(arguments,1),i='event handler for "'.concat(n,'"'),a=0,s=t.length;a<s;a++)qe(t[a],e,r,e,i)}return e}}(Qt),function(n){n.prototype._update=function(n,e){var t=this,r=t.$el,i=t._vnode,a=Ze(t);t._vnode=n,t.$el=i?t.__patch__(i,n):t.__patch__(t.$el,n,e,!1),a(),r&&(r.__vue__=null),t.$el&&(t.$el.__vue__=t);for(var s=t;s&&s.$vnode&&s.$parent&&s.$vnode===s.$parent._vnode;)s.$parent.$el=s.$el,s=s.$parent},n.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},n.prototype.$destroy=function(){var n=this;if(!n._isBeingDestroyed){nt(n,"beforeDestroy"),n._isBeingDestroyed=!0;var e=n.$parent;!e||e._isBeingDestroyed||n.$options.abstract||x(e.$children,n),n._scope.stop(),n._data.__ob__&&n._data.__ob__.vmCount--,n._isDestroyed=!0,n.__patch__(n._vnode,null),nt(n,"destroyed"),n.$off(),n.$el&&(n.$el.__vue__=null),n.$vnode&&(n.$vnode.parent=null)}}}(Qt),function(n){pe(n.prototype),n.prototype.$nextTick=function(n){return Fe(n,this)},n.prototype._render=function(){var n,e=this,t=e.$options,r=t.render,a=t._parentVnode;a&&e._isMounted&&(e.$scopedSlots=he(e.$parent,a.data.scopedSlots,e.$slots,e.$scopedSlots),e._slotsProxy&&ke(e._slotsProxy,e.$scopedSlots)),e.$vnode=a;try{un(e),we=e,n=r.call(e._renderProxy,e.$createElement)}catch(t){Te(t,e,"render"),n=e._vnode}finally{we=null,un()}return i(n)&&1===n.length&&(n=n[0]),n instanceof mn||(n=gn()),n.parent=a,n}}(Qt);var Xt=[String,RegExp,Array],Yt={KeepAlive:{name:"keep-alive",abstract:!0,props:{include:Xt,exclude:Xt,max:[String,Number]},methods:{cacheVNode:function(){var n=this.cache,e=this.keys,t=this.vnodeToCache,r=this.keyToCache;if(t){var i=t.tag,a=t.componentInstance,s=t.componentOptions;n[r]={name:Vt(s),tag:i,componentInstance:a},e.push(r),this.max&&e.length>parseInt(this.max)&&Zt(n,e[0],e,this._vnode),this.vnodeToCache=null}}},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var n in this.cache)Zt(this.cache,n,this.keys)},mounted:function(){var n=this;this.cacheVNode(),this.$watch("include",(function(e){Kt(n,(function(n){return Wt(e,n)}))})),this.$watch("exclude",(function(e){Kt(n,(function(n){return!Wt(e,n)}))}))},updated:function(){this.cacheVNode()},render:function(){var n=this.$slots.default,e=je(n),t=e&&e.componentOptions;if(t){var r=Vt(t),i=this.include,a=this.exclude;if(i&&(!r||!Wt(i,r))||a&&r&&Wt(a,r))return e;var s=this.cache,o=this.keys,l=null==e.key?t.Ctor.cid+(t.tag?"::".concat(t.tag):""):e.key;s[l]?(e.componentInstance=s[l].componentInstance,x(o,l),o.push(l)):(this.vnodeToCache=e,this.keyToCache=l),e.data.keepAlive=!0}return e||n&&n[0]}}};!function(n){var e={get:function(){return U}};Object.defineProperty(n,"config",e),n.util={warn:wt,extend:A,mergeOptions:It,defineReactive:Rn},n.set=zn,n.delete=On,n.nextTick=Fe,n.observable=function(n){return An(n),n},n.options=Object.create(null),P.forEach((function(e){n.options[e+"s"]=Object.create(null)})),n.options._base=n,A(n.options.components,Yt),function(n){n.use=function(n){var e=this._installedPlugins||(this._installedPlugins=[]);if(e.indexOf(n)>-1)return this;var t=C(arguments,1);return t.unshift(this),c(n.install)?n.install.apply(n,t):c(n)&&n.apply(null,t),e.push(n),this}}(n),function(n){n.mixin=function(n){return this.options=It(this.options,n),this}}(n),Gt(n),function(n){P.forEach((function(e){n[e]=function(n,t){return t?("component"===e&&u(t)&&(t.name=t.name||n,t=this.options._base.extend(t)),"directive"===e&&c(t)&&(t={bind:t,update:t}),this.options[e+"s"][n]=t,t):this.options[e+"s"][n]}}))}(n)}(Qt),Object.defineProperty(Qt.prototype,"$isServer",{get:sn}),Object.defineProperty(Qt.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(Qt,"FunctionalRenderContext",{value:gt}),Qt.version="2.7.14";var nr=v("style,class"),er=v("input,textarea,option,select,progress"),tr=v("contenteditable,draggable,spellcheck"),rr=v("events,caret,typing,plaintext-only"),ir=v("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,truespeed,typemustmatch,visible"),ar="http://www.w3.org/1999/xlink",sr=function(n){return":"===n.charAt(5)&&"xlink"===n.slice(0,5)},or=function(n){return sr(n)?n.slice(6,n.length):""},lr=function(n){return null==n||!1===n};function cr(n){for(var e=n.data,t=n,r=n;s(r.componentInstance);)(r=r.componentInstance._vnode)&&r.data&&(e=dr(r.data,e));for(;s(t=t.parent);)t&&t.data&&(e=dr(e,t.data));return function(n,e){if(s(n)||s(e))return pr(n,ur(e));return""}(e.staticClass,e.class)}function dr(n,e){return{staticClass:pr(n.staticClass,e.staticClass),class:s(n.class)?[n.class,e.class]:e.class}}function pr(n,e){return n?e?n+" "+e:n:e||""}function ur(n){return Array.isArray(n)?function(n){for(var e,t="",r=0,i=n.length;r<i;r++)s(e=ur(n[r]))&&""!==e&&(t&&(t+=" "),t+=e);return t}(n):d(n)?function(n){var e="";for(var t in n)n[t]&&(e&&(e+=" "),e+=t);return e}(n):"string"==typeof n?n:""}var mr={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},gr=v("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),hr=v("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignobject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),br=function(n){return gr(n)||hr(n)};var fr=Object.create(null);var vr=v("text,number,password,search,email,tel,url");var yr=Object.freeze({__proto__:null,createElement:function(n,e){var t=document.createElement(n);return"select"!==n||e.data&&e.data.attrs&&void 0!==e.data.attrs.multiple&&t.setAttribute("multiple","multiple"),t},createElementNS:function(n,e){return document.createElementNS(mr[n],e)},createTextNode:function(n){return document.createTextNode(n)},createComment:function(n){return document.createComment(n)},insertBefore:function(n,e,t){n.insertBefore(e,t)},removeChild:function(n,e){n.removeChild(e)},appendChild:function(n,e){n.appendChild(e)},parentNode:function(n){return n.parentNode},nextSibling:function(n){return n.nextSibling},tagName:function(n){return n.tagName},setTextContent:function(n,e){n.textContent=e},setStyleScope:function(n,e){n.setAttribute(e,"")}}),xr={create:function(n,e){kr(e)},update:function(n,e){n.data.ref!==e.data.ref&&(kr(n,!0),kr(e))},destroy:function(n){kr(n,!0)}};function kr(n,e){var t=n.data.ref;if(s(t)){var r=n.context,a=n.componentInstance||n.elm,o=e?null:a,l=e?void 0:a;if(c(t))qe(t,r,[o],r,"template ref function");else{var d=n.data.refInFor,p="string"==typeof t||"number"==typeof t,u=Fn(t),m=r.$refs;if(p||u)if(d){var g=p?m[t]:t.value;e?i(g)&&x(g,a):i(g)?g.includes(a)||g.push(a):p?(m[t]=[a],wr(r,t,m[t])):t.value=[a]}else if(p){if(e&&m[t]!==a)return;m[t]=l,wr(r,t,o)}else if(u){if(e&&t.value!==a)return;t.value=o}else 0}}}function wr(n,e,t){var r=n._setupState;r&&w(r,e)&&(Fn(r[e])?r[e].value=t:r[e]=t)}var Sr=new mn("",{},[]),jr=["create","activate","update","remove","destroy"];function Er(n,e){return n.key===e.key&&n.asyncFactory===e.asyncFactory&&(n.tag===e.tag&&n.isComment===e.isComment&&s(n.data)===s(e.data)&&function(n,e){if("input"!==n.tag)return!0;var t,r=s(t=n.data)&&s(t=t.attrs)&&t.type,i=s(t=e.data)&&s(t=t.attrs)&&t.type;return r===i||vr(r)&&vr(i)}(n,e)||o(n.isAsyncPlaceholder)&&a(e.asyncFactory.error))}function Tr(n,e,t){var r,i,a={};for(r=e;r<=t;++r)s(i=n[r].key)&&(a[i]=r);return a}var qr={create:_r,update:_r,destroy:function(n){_r(n,Sr)}};function _r(n,e){(n.data.directives||e.data.directives)&&function(n,e){var t,r,i,a=n===Sr,s=e===Sr,o=Cr(n.data.directives,n.context),l=Cr(e.data.directives,e.context),c=[],d=[];for(t in l)r=o[t],i=l[t],r?(i.oldValue=r.value,i.oldArg=r.arg,Rr(i,"update",e,n),i.def&&i.def.componentUpdated&&d.push(i)):(Rr(i,"bind",e,n),i.def&&i.def.inserted&&c.push(i));if(c.length){var p=function(){for(var t=0;t<c.length;t++)Rr(c[t],"inserted",e,n)};a?Gn(e,"insert",p):p()}d.length&&Gn(e,"postpatch",(function(){for(var t=0;t<d.length;t++)Rr(d[t],"componentUpdated",e,n)}));if(!a)for(t in o)l[t]||Rr(o[t],"unbind",n,n,s)}(n,e)}var Ir=Object.create(null);function Cr(n,e){var t,r,i=Object.create(null);if(!n)return i;for(t=0;t<n.length;t++){if((r=n[t]).modifiers||(r.modifiers=Ir),i[Ar(r)]=r,e._setupState&&e._setupState.__sfc){var a=r.def||Ct(e,"_setupState","v-"+r.name);r.def="function"==typeof a?{bind:a,update:a}:a}r.def=r.def||Ct(e.$options,"directives",r.name)}return i}function Ar(n){return n.rawName||"".concat(n.name,".").concat(Object.keys(n.modifiers||{}).join("."))}function Rr(n,e,t,r,i){var a=n.def&&n.def[e];if(a)try{a(t.elm,n,t,r,i)}catch(r){Te(r,t.context,"directive ".concat(n.name," ").concat(e," hook"))}}var zr=[xr,qr];function Or(n,e){var t=e.componentOptions;if(!(s(t)&&!1===t.Ctor.options.inheritAttrs||a(n.data.attrs)&&a(e.data.attrs))){var r,i,l=e.elm,c=n.data.attrs||{},d=e.data.attrs||{};for(r in(s(d.__ob__)||o(d._v_attr_proxy))&&(d=e.data.attrs=A({},d)),d)i=d[r],c[r]!==i&&Mr(l,r,i,e.data.pre);for(r in(K||X)&&d.value!==c.value&&Mr(l,"value",d.value),c)a(d[r])&&(sr(r)?l.removeAttributeNS(ar,or(r)):tr(r)||l.removeAttribute(r))}}function Mr(n,e,t,r){r||n.tagName.indexOf("-")>-1?Br(n,e,t):ir(e)?lr(t)?n.removeAttribute(e):(t="allowfullscreen"===e&&"EMBED"===n.tagName?"true":e,n.setAttribute(e,t)):tr(e)?n.setAttribute(e,function(n,e){return lr(e)||"false"===e?"false":"contenteditable"===n&&rr(e)?e:"true"}(e,t)):sr(e)?lr(t)?n.removeAttributeNS(ar,or(e)):n.setAttributeNS(ar,e,t):Br(n,e,t)}function Br(n,e,t){if(lr(t))n.removeAttribute(e);else{if(K&&!Z&&"TEXTAREA"===n.tagName&&"placeholder"===e&&""!==t&&!n.__ieph){var r=function(e){e.stopImmediatePropagation(),n.removeEventListener("input",r)};n.addEventListener("input",r),n.__ieph=!0}n.setAttribute(e,t)}}var Lr={create:Or,update:Or};function Dr(n,e){var t=e.elm,r=e.data,i=n.data;if(!(a(r.staticClass)&&a(r.class)&&(a(i)||a(i.staticClass)&&a(i.class)))){var o=cr(e),l=t._transitionClasses;s(l)&&(o=pr(o,ur(l))),o!==t._prevClass&&(t.setAttribute("class",o),t._prevClass=o)}}var Fr,Pr={create:Dr,update:Dr};function Nr(n,e,t){var r=Fr;return function i(){var a=e.apply(null,arguments);null!==a&&Jr(n,i,t,r)}}var Ur=Ae&&!(en&&Number(en[1])<=53);function Hr(n,e,t,r){if(Ur){var i=ot,a=e;e=a._wrapper=function(n){if(n.target===n.currentTarget||n.timeStamp>=i||n.timeStamp<=0||n.target.ownerDocument!==document)return a.apply(this,arguments)}}Fr.addEventListener(n,e,rn?{capture:t,passive:r}:t)}function Jr(n,e,t,r){(r||Fr).removeEventListener(n,e._wrapper||e,t)}function $r(n,e){if(!a(n.data.on)||!a(e.data.on)){var t=e.data.on||{},r=n.data.on||{};Fr=e.elm||n.elm,function(n){if(s(n.__r)){var e=K?"change":"input";n[e]=[].concat(n.__r,n[e]||[]),delete n.__r}s(n.__c)&&(n.change=[].concat(n.__c,n.change||[]),delete n.__c)}(t),Qn(t,r,Hr,Jr,Nr,e.context),Fr=void 0}}var Qr,Gr={create:$r,update:$r,destroy:function(n){return $r(n,Sr)}};function Vr(n,e){if(!a(n.data.domProps)||!a(e.data.domProps)){var t,r,i=e.elm,l=n.data.domProps||{},c=e.data.domProps||{};for(t in(s(c.__ob__)||o(c._v_attr_proxy))&&(c=e.data.domProps=A({},c)),l)t in c||(i[t]="");for(t in c){if(r=c[t],"textContent"===t||"innerHTML"===t){if(e.children&&(e.children.length=0),r===l[t])continue;1===i.childNodes.length&&i.removeChild(i.childNodes[0])}if("value"===t&&"PROGRESS"!==i.tagName){i._value=r;var d=a(r)?"":String(r);Wr(i,d)&&(i.value=d)}else if("innerHTML"===t&&hr(i.tagName)&&a(i.innerHTML)){(Qr=Qr||document.createElement("div")).innerHTML="<svg>".concat(r,"</svg>");for(var p=Qr.firstChild;i.firstChild;)i.removeChild(i.firstChild);for(;p.firstChild;)i.appendChild(p.firstChild)}else if(r!==l[t])try{i[t]=r}catch(n){}}}}function Wr(n,e){return!n.composing&&("OPTION"===n.tagName||function(n,e){var t=!0;try{t=document.activeElement!==n}catch(n){}return t&&n.value!==e}(n,e)||function(n,e){var t=n.value,r=n._vModifiers;if(s(r)){if(r.number)return f(t)!==f(e);if(r.trim)return t.trim()!==e.trim()}return t!==e}(n,e))}var Kr={create:Vr,update:Vr},Zr=S((function(n){var e={},t=/:(.+)/;return n.split(/;(?![^(]*\))/g).forEach((function(n){if(n){var r=n.split(t);r.length>1&&(e[r[0].trim()]=r[1].trim())}})),e}));function Xr(n){var e=Yr(n.style);return n.staticStyle?A(n.staticStyle,e):e}function Yr(n){return Array.isArray(n)?R(n):"string"==typeof n?Zr(n):n}var ni,ei=/^--/,ti=/\s*!important$/,ri=function(n,e,t){if(ei.test(e))n.style.setProperty(e,t);else if(ti.test(t))n.style.setProperty(_(e),t.replace(ti,""),"important");else{var r=ai(e);if(Array.isArray(t))for(var i=0,a=t.length;i<a;i++)n.style[r]=t[i];else n.style[r]=t}},ii=["Webkit","Moz","ms"],ai=S((function(n){if(ni=ni||document.createElement("div").style,"filter"!==(n=E(n))&&n in ni)return n;for(var e=n.charAt(0).toUpperCase()+n.slice(1),t=0;t<ii.length;t++){var r=ii[t]+e;if(r in ni)return r}}));function si(n,e){var t=e.data,r=n.data;if(!(a(t.staticStyle)&&a(t.style)&&a(r.staticStyle)&&a(r.style))){var i,o,l=e.elm,c=r.staticStyle,d=r.normalizedStyle||r.style||{},p=c||d,u=Yr(e.data.style)||{};e.data.normalizedStyle=s(u.__ob__)?A({},u):u;var m=function(n,e){var t,r={};if(e)for(var i=n;i.componentInstance;)(i=i.componentInstance._vnode)&&i.data&&(t=Xr(i.data))&&A(r,t);(t=Xr(n.data))&&A(r,t);for(var a=n;a=a.parent;)a.data&&(t=Xr(a.data))&&A(r,t);return r}(e,!0);for(o in p)a(m[o])&&ri(l,o,"");for(o in m)(i=m[o])!==p[o]&&ri(l,o,null==i?"":i)}}var oi={create:si,update:si},li=/\s+/;function ci(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(li).forEach((function(e){return n.classList.add(e)})):n.classList.add(e);else{var t=" ".concat(n.getAttribute("class")||""," ");t.indexOf(" "+e+" ")<0&&n.setAttribute("class",(t+e).trim())}}function di(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(li).forEach((function(e){return n.classList.remove(e)})):n.classList.remove(e),n.classList.length||n.removeAttribute("class");else{for(var t=" ".concat(n.getAttribute("class")||""," "),r=" "+e+" ";t.indexOf(r)>=0;)t=t.replace(r," ");(t=t.trim())?n.setAttribute("class",t):n.removeAttribute("class")}}function pi(n){if(n){if("object"==typeof n){var e={};return!1!==n.css&&A(e,ui(n.name||"v")),A(e,n),e}return"string"==typeof n?ui(n):void 0}}var ui=S((function(n){return{enterClass:"".concat(n,"-enter"),enterToClass:"".concat(n,"-enter-to"),enterActiveClass:"".concat(n,"-enter-active"),leaveClass:"".concat(n,"-leave"),leaveToClass:"".concat(n,"-leave-to"),leaveActiveClass:"".concat(n,"-leave-active")}})),mi=V&&!Z,gi="transition",hi="transitionend",bi="animation",fi="animationend";mi&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(gi="WebkitTransition",hi="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&(bi="WebkitAnimation",fi="webkitAnimationEnd"));var vi=V?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(n){return n()};function yi(n){vi((function(){vi(n)}))}function xi(n,e){var t=n._transitionClasses||(n._transitionClasses=[]);t.indexOf(e)<0&&(t.push(e),ci(n,e))}function ki(n,e){n._transitionClasses&&x(n._transitionClasses,e),di(n,e)}function wi(n,e,t){var r=ji(n,e),i=r.type,a=r.timeout,s=r.propCount;if(!i)return t();var o="transition"===i?hi:fi,l=0,c=function(){n.removeEventListener(o,d),t()},d=function(e){e.target===n&&++l>=s&&c()};setTimeout((function(){l<s&&c()}),a+1),n.addEventListener(o,d)}var Si=/\b(transform|all)(,|$)/;function ji(n,e){var t,r=window.getComputedStyle(n),i=(r[gi+"Delay"]||"").split(", "),a=(r[gi+"Duration"]||"").split(", "),s=Ei(i,a),o=(r[bi+"Delay"]||"").split(", "),l=(r[bi+"Duration"]||"").split(", "),c=Ei(o,l),d=0,p=0;return"transition"===e?s>0&&(t="transition",d=s,p=a.length):"animation"===e?c>0&&(t="animation",d=c,p=l.length):p=(t=(d=Math.max(s,c))>0?s>c?"transition":"animation":null)?"transition"===t?a.length:l.length:0,{type:t,timeout:d,propCount:p,hasTransform:"transition"===t&&Si.test(r[gi+"Property"])}}function Ei(n,e){for(;n.length<e.length;)n=n.concat(n);return Math.max.apply(null,e.map((function(e,t){return Ti(e)+Ti(n[t])})))}function Ti(n){return 1e3*Number(n.slice(0,-1).replace(",","."))}function qi(n,e){var t=n.elm;s(t._leaveCb)&&(t._leaveCb.cancelled=!0,t._leaveCb());var r=pi(n.data.transition);if(!a(r)&&!s(t._enterCb)&&1===t.nodeType){for(var i=r.css,o=r.type,l=r.enterClass,p=r.enterToClass,u=r.enterActiveClass,m=r.appearClass,g=r.appearToClass,h=r.appearActiveClass,b=r.beforeEnter,v=r.enter,y=r.afterEnter,x=r.enterCancelled,k=r.beforeAppear,w=r.appear,S=r.afterAppear,j=r.appearCancelled,E=r.duration,T=Ke,q=Ke.$vnode;q&&q.parent;)T=q.context,q=q.parent;var _=!T._isMounted||!n.isRootInsert;if(!_||w||""===w){var I=_&&m?m:l,C=_&&h?h:u,A=_&&g?g:p,R=_&&k||b,z=_&&c(w)?w:v,O=_&&S||y,M=_&&j||x,B=f(d(E)?E.enter:E);0;var L=!1!==i&&!Z,F=Ci(z),P=t._enterCb=D((function(){L&&(ki(t,A),ki(t,C)),P.cancelled?(L&&ki(t,I),M&&M(t)):O&&O(t),t._enterCb=null}));n.data.show||Gn(n,"insert",(function(){var e=t.parentNode,r=e&&e._pending&&e._pending[n.key];r&&r.tag===n.tag&&r.elm._leaveCb&&r.elm._leaveCb(),z&&z(t,P)})),R&&R(t),L&&(xi(t,I),xi(t,C),yi((function(){ki(t,I),P.cancelled||(xi(t,A),F||(Ii(B)?setTimeout(P,B):wi(t,o,P)))}))),n.data.show&&(e&&e(),z&&z(t,P)),L||F||P()}}}function _i(n,e){var t=n.elm;s(t._enterCb)&&(t._enterCb.cancelled=!0,t._enterCb());var r=pi(n.data.transition);if(a(r)||1!==t.nodeType)return e();if(!s(t._leaveCb)){var i=r.css,o=r.type,l=r.leaveClass,c=r.leaveToClass,p=r.leaveActiveClass,u=r.beforeLeave,m=r.leave,g=r.afterLeave,h=r.leaveCancelled,b=r.delayLeave,v=r.duration,y=!1!==i&&!Z,x=Ci(m),k=f(d(v)?v.leave:v);0;var w=t._leaveCb=D((function(){t.parentNode&&t.parentNode._pending&&(t.parentNode._pending[n.key]=null),y&&(ki(t,c),ki(t,p)),w.cancelled?(y&&ki(t,l),h&&h(t)):(e(),g&&g(t)),t._leaveCb=null}));b?b(S):S()}function S(){w.cancelled||(!n.data.show&&t.parentNode&&((t.parentNode._pending||(t.parentNode._pending={}))[n.key]=n),u&&u(t),y&&(xi(t,l),xi(t,p),yi((function(){ki(t,l),w.cancelled||(xi(t,c),x||(Ii(k)?setTimeout(w,k):wi(t,o,w)))}))),m&&m(t,w),y||x||w())}}function Ii(n){return"number"==typeof n&&!isNaN(n)}function Ci(n){if(a(n))return!1;var e=n.fns;return s(e)?Ci(Array.isArray(e)?e[0]:e):(n._length||n.length)>1}function Ai(n,e){!0!==e.data.show&&qi(e)}var Ri=function(n){var e,t,r={},c=n.modules,d=n.nodeOps;for(e=0;e<jr.length;++e)for(r[jr[e]]=[],t=0;t<c.length;++t)s(c[t][jr[e]])&&r[jr[e]].push(c[t][jr[e]]);function p(n){var e=d.parentNode(n);s(e)&&d.removeChild(e,n)}function u(n,e,t,i,a,l,c){if(s(n.elm)&&s(l)&&(n=l[c]=bn(n)),n.isRootInsert=!a,!function(n,e,t,i){var a=n.data;if(s(a)){var l=s(n.componentInstance)&&a.keepAlive;if(s(a=a.hook)&&s(a=a.init)&&a(n,!1),s(n.componentInstance))return m(n,e),g(t,n.elm,i),o(l)&&function(n,e,t,i){var a,o=n;for(;o.componentInstance;)if(o=o.componentInstance._vnode,s(a=o.data)&&s(a=a.transition)){for(a=0;a<r.activate.length;++a)r.activate[a](Sr,o);e.push(o);break}g(t,n.elm,i)}(n,e,t,i),!0}}(n,e,t,i)){var p=n.data,u=n.children,b=n.tag;s(b)?(n.elm=n.ns?d.createElementNS(n.ns,b):d.createElement(b,n),y(n),h(n,u,e),s(p)&&f(n,e),g(t,n.elm,i)):o(n.isComment)?(n.elm=d.createComment(n.text),g(t,n.elm,i)):(n.elm=d.createTextNode(n.text),g(t,n.elm,i))}}function m(n,e){s(n.data.pendingInsert)&&(e.push.apply(e,n.data.pendingInsert),n.data.pendingInsert=null),n.elm=n.componentInstance.$el,b(n)?(f(n,e),y(n)):(kr(n),e.push(n))}function g(n,e,t){s(n)&&(s(t)?d.parentNode(t)===n&&d.insertBefore(n,e,t):d.appendChild(n,e))}function h(n,e,t){if(i(e)){0;for(var r=0;r<e.length;++r)u(e[r],t,n.elm,null,!0,e,r)}else l(n.text)&&d.appendChild(n.elm,d.createTextNode(String(n.text)))}function b(n){for(;n.componentInstance;)n=n.componentInstance._vnode;return s(n.tag)}function f(n,t){for(var i=0;i<r.create.length;++i)r.create[i](Sr,n);s(e=n.data.hook)&&(s(e.create)&&e.create(Sr,n),s(e.insert)&&t.push(n))}function y(n){var e;if(s(e=n.fnScopeId))d.setStyleScope(n.elm,e);else for(var t=n;t;)s(e=t.context)&&s(e=e.$options._scopeId)&&d.setStyleScope(n.elm,e),t=t.parent;s(e=Ke)&&e!==n.context&&e!==n.fnContext&&s(e=e.$options._scopeId)&&d.setStyleScope(n.elm,e)}function x(n,e,t,r,i,a){for(;r<=i;++r)u(t[r],a,n,e,!1,t,r)}function k(n){var e,t,i=n.data;if(s(i))for(s(e=i.hook)&&s(e=e.destroy)&&e(n),e=0;e<r.destroy.length;++e)r.destroy[e](n);if(s(e=n.children))for(t=0;t<n.children.length;++t)k(n.children[t])}function w(n,e,t){for(;e<=t;++e){var r=n[e];s(r)&&(s(r.tag)?(S(r),k(r)):p(r.elm))}}function S(n,e){if(s(e)||s(n.data)){var t,i=r.remove.length+1;for(s(e)?e.listeners+=i:e=function(n,e){function t(){0==--t.listeners&&p(n)}return t.listeners=e,t}(n.elm,i),s(t=n.componentInstance)&&s(t=t._vnode)&&s(t.data)&&S(t,e),t=0;t<r.remove.length;++t)r.remove[t](n,e);s(t=n.data.hook)&&s(t=t.remove)?t(n,e):e()}else p(n.elm)}function j(n,e,t,r){for(var i=t;i<r;i++){var a=e[i];if(s(a)&&Er(n,a))return i}}function E(n,e,t,i,l,c){if(n!==e){s(e.elm)&&s(i)&&(e=i[l]=bn(e));var p=e.elm=n.elm;if(o(n.isAsyncPlaceholder))s(e.asyncFactory.resolved)?_(n.elm,e,t):e.isAsyncPlaceholder=!0;else if(o(e.isStatic)&&o(n.isStatic)&&e.key===n.key&&(o(e.isCloned)||o(e.isOnce)))e.componentInstance=n.componentInstance;else{var m,g=e.data;s(g)&&s(m=g.hook)&&s(m=m.prepatch)&&m(n,e);var h=n.children,f=e.children;if(s(g)&&b(e)){for(m=0;m<r.update.length;++m)r.update[m](n,e);s(m=g.hook)&&s(m=m.update)&&m(n,e)}a(e.text)?s(h)&&s(f)?h!==f&&function(n,e,t,r,i){var o,l,c,p=0,m=0,g=e.length-1,h=e[0],b=e[g],f=t.length-1,v=t[0],y=t[f],k=!i;for(0;p<=g&&m<=f;)a(h)?h=e[++p]:a(b)?b=e[--g]:Er(h,v)?(E(h,v,r,t,m),h=e[++p],v=t[++m]):Er(b,y)?(E(b,y,r,t,f),b=e[--g],y=t[--f]):Er(h,y)?(E(h,y,r,t,f),k&&d.insertBefore(n,h.elm,d.nextSibling(b.elm)),h=e[++p],y=t[--f]):Er(b,v)?(E(b,v,r,t,m),k&&d.insertBefore(n,b.elm,h.elm),b=e[--g],v=t[++m]):(a(o)&&(o=Tr(e,p,g)),a(l=s(v.key)?o[v.key]:j(v,e,p,g))?u(v,r,n,h.elm,!1,t,m):Er(c=e[l],v)?(E(c,v,r,t,m),e[l]=void 0,k&&d.insertBefore(n,c.elm,h.elm)):u(v,r,n,h.elm,!1,t,m),v=t[++m]);p>g?x(n,a(t[f+1])?null:t[f+1].elm,t,m,f,r):m>f&&w(e,p,g)}(p,h,f,t,c):s(f)?(s(n.text)&&d.setTextContent(p,""),x(p,null,f,0,f.length-1,t)):s(h)?w(h,0,h.length-1):s(n.text)&&d.setTextContent(p,""):n.text!==e.text&&d.setTextContent(p,e.text),s(g)&&s(m=g.hook)&&s(m=m.postpatch)&&m(n,e)}}}function T(n,e,t){if(o(t)&&s(n.parent))n.parent.data.pendingInsert=e;else for(var r=0;r<e.length;++r)e[r].data.hook.insert(e[r])}var q=v("attrs,class,staticClass,staticStyle,key");function _(n,e,t,r){var i,a=e.tag,l=e.data,c=e.children;if(r=r||l&&l.pre,e.elm=n,o(e.isComment)&&s(e.asyncFactory))return e.isAsyncPlaceholder=!0,!0;if(s(l)&&(s(i=l.hook)&&s(i=i.init)&&i(e,!0),s(i=e.componentInstance)))return m(e,t),!0;if(s(a)){if(s(c))if(n.hasChildNodes())if(s(i=l)&&s(i=i.domProps)&&s(i=i.innerHTML)){if(i!==n.innerHTML)return!1}else{for(var d=!0,p=n.firstChild,u=0;u<c.length;u++){if(!p||!_(p,c[u],t,r)){d=!1;break}p=p.nextSibling}if(!d||p)return!1}else h(e,c,t);if(s(l)){var g=!1;for(var b in l)if(!q(b)){g=!0,f(e,t);break}!g&&l.class&&Ue(l.class)}}else n.data!==e.text&&(n.data=e.text);return!0}return function(n,e,t,i){if(!a(e)){var l,c=!1,p=[];if(a(n))c=!0,u(e,p);else{var m=s(n.nodeType);if(!m&&Er(n,e))E(n,e,p,null,null,i);else{if(m){if(1===n.nodeType&&n.hasAttribute("data-server-rendered")&&(n.removeAttribute("data-server-rendered"),t=!0),o(t)&&_(n,e,p))return T(e,p,!0),n;l=n,n=new mn(d.tagName(l).toLowerCase(),{},[],void 0,l)}var g=n.elm,h=d.parentNode(g);if(u(e,p,g._leaveCb?null:h,d.nextSibling(g)),s(e.parent))for(var f=e.parent,v=b(e);f;){for(var y=0;y<r.destroy.length;++y)r.destroy[y](f);if(f.elm=e.elm,v){for(var x=0;x<r.create.length;++x)r.create[x](Sr,f);var S=f.data.hook.insert;if(S.merged)for(var j=1;j<S.fns.length;j++)S.fns[j]()}else kr(f);f=f.parent}s(h)?w([n],0,0):s(n.tag)&&k(n)}}return T(e,p,c),e.elm}s(n)&&k(n)}}({nodeOps:yr,modules:[Lr,Pr,Gr,Kr,oi,V?{create:Ai,activate:Ai,remove:function(n,e){!0!==n.data.show?_i(n,e):e()}}:{}].concat(zr)});Z&&document.addEventListener("selectionchange",(function(){var n=document.activeElement;n&&n.vmodel&&Pi(n,"input")}));var zi={inserted:function(n,e,t,r){"select"===t.tag?(r.elm&&!r.elm._vOptions?Gn(t,"postpatch",(function(){zi.componentUpdated(n,e,t)})):Oi(n,e,t.context),n._vOptions=[].map.call(n.options,Li)):("textarea"===t.tag||vr(n.type))&&(n._vModifiers=e.modifiers,e.modifiers.lazy||(n.addEventListener("compositionstart",Di),n.addEventListener("compositionend",Fi),n.addEventListener("change",Fi),Z&&(n.vmodel=!0)))},componentUpdated:function(n,e,t){if("select"===t.tag){Oi(n,e,t.context);var r=n._vOptions,i=n._vOptions=[].map.call(n.options,Li);if(i.some((function(n,e){return!B(n,r[e])})))(n.multiple?e.value.some((function(n){return Bi(n,i)})):e.value!==e.oldValue&&Bi(e.value,i))&&Pi(n,"change")}}};function Oi(n,e,t){Mi(n,e,t),(K||X)&&setTimeout((function(){Mi(n,e,t)}),0)}function Mi(n,e,t){var r=e.value,i=n.multiple;if(!i||Array.isArray(r)){for(var a,s,o=0,l=n.options.length;o<l;o++)if(s=n.options[o],i)a=L(r,Li(s))>-1,s.selected!==a&&(s.selected=a);else if(B(Li(s),r))return void(n.selectedIndex!==o&&(n.selectedIndex=o));i||(n.selectedIndex=-1)}}function Bi(n,e){return e.every((function(e){return!B(e,n)}))}function Li(n){return"_value"in n?n._value:n.value}function Di(n){n.target.composing=!0}function Fi(n){n.target.composing&&(n.target.composing=!1,Pi(n.target,"input"))}function Pi(n,e){var t=document.createEvent("HTMLEvents");t.initEvent(e,!0,!0),n.dispatchEvent(t)}function Ni(n){return!n.componentInstance||n.data&&n.data.transition?n:Ni(n.componentInstance._vnode)}var Ui={model:zi,show:{bind:function(n,e,t){var r=e.value,i=(t=Ni(t)).data&&t.data.transition,a=n.__vOriginalDisplay="none"===n.style.display?"":n.style.display;r&&i?(t.data.show=!0,qi(t,(function(){n.style.display=a}))):n.style.display=r?a:"none"},update:function(n,e,t){var r=e.value;!r!=!e.oldValue&&((t=Ni(t)).data&&t.data.transition?(t.data.show=!0,r?qi(t,(function(){n.style.display=n.__vOriginalDisplay})):_i(t,(function(){n.style.display="none"}))):n.style.display=r?n.__vOriginalDisplay:"none")},unbind:function(n,e,t,r,i){i||(n.style.display=n.__vOriginalDisplay)}}},Hi={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function Ji(n){var e=n&&n.componentOptions;return e&&e.Ctor.options.abstract?Ji(je(e.children)):n}function $i(n){var e={},t=n.$options;for(var r in t.propsData)e[r]=n[r];var i=t._parentListeners;for(var r in i)e[E(r)]=i[r];return e}function Qi(n,e){if(/\d-keep-alive$/.test(e.tag))return n("keep-alive",{props:e.componentOptions.propsData})}var Gi=function(n){return n.tag||ge(n)},Vi=function(n){return"show"===n.name},Wi={name:"transition",props:Hi,abstract:!0,render:function(n){var e=this,t=this.$slots.default;if(t&&(t=t.filter(Gi)).length){0;var r=this.mode;0;var i=t[0];if(function(n){for(;n=n.parent;)if(n.data.transition)return!0}(this.$vnode))return i;var a=Ji(i);if(!a)return i;if(this._leaving)return Qi(n,i);var s="__transition-".concat(this._uid,"-");a.key=null==a.key?a.isComment?s+"comment":s+a.tag:l(a.key)?0===String(a.key).indexOf(s)?a.key:s+a.key:a.key;var o=(a.data||(a.data={})).transition=$i(this),c=this._vnode,d=Ji(c);if(a.data.directives&&a.data.directives.some(Vi)&&(a.data.show=!0),d&&d.data&&!function(n,e){return e.key===n.key&&e.tag===n.tag}(a,d)&&!ge(d)&&(!d.componentInstance||!d.componentInstance._vnode.isComment)){var p=d.data.transition=A({},o);if("out-in"===r)return this._leaving=!0,Gn(p,"afterLeave",(function(){e._leaving=!1,e.$forceUpdate()})),Qi(n,i);if("in-out"===r){if(ge(a))return c;var u,m=function(){u()};Gn(o,"afterEnter",m),Gn(o,"enterCancelled",m),Gn(p,"delayLeave",(function(n){u=n}))}}return i}}},Ki=A({tag:String,moveClass:String},Hi);function Zi(n){n.elm._moveCb&&n.elm._moveCb(),n.elm._enterCb&&n.elm._enterCb()}function Xi(n){n.data.newPos=n.elm.getBoundingClientRect()}function Yi(n){var e=n.data.pos,t=n.data.newPos,r=e.left-t.left,i=e.top-t.top;if(r||i){n.data.moved=!0;var a=n.elm.style;a.transform=a.WebkitTransform="translate(".concat(r,"px,").concat(i,"px)"),a.transitionDuration="0s"}}delete Ki.mode;var na={Transition:Wi,TransitionGroup:{props:Ki,beforeMount:function(){var n=this,e=this._update;this._update=function(t,r){var i=Ze(n);n.__patch__(n._vnode,n.kept,!1,!0),n._vnode=n.kept,i(),e.call(n,t,r)}},render:function(n){for(var e=this.tag||this.$vnode.data.tag||"span",t=Object.create(null),r=this.prevChildren=this.children,i=this.$slots.default||[],a=this.children=[],s=$i(this),o=0;o<i.length;o++){if((d=i[o]).tag)if(null!=d.key&&0!==String(d.key).indexOf("__vlist"))a.push(d),t[d.key]=d,(d.data||(d.data={})).transition=s;else;}if(r){var l=[],c=[];for(o=0;o<r.length;o++){var d;(d=r[o]).data.transition=s,d.data.pos=d.elm.getBoundingClientRect(),t[d.key]?l.push(d):c.push(d)}this.kept=n(e,null,l),this.removed=c}return n(e,null,a)},updated:function(){var n=this.prevChildren,e=this.moveClass||(this.name||"v")+"-move";n.length&&this.hasMove(n[0].elm,e)&&(n.forEach(Zi),n.forEach(Xi),n.forEach(Yi),this._reflow=document.body.offsetHeight,n.forEach((function(n){if(n.data.moved){var t=n.elm,r=t.style;xi(t,e),r.transform=r.WebkitTransform=r.transitionDuration="",t.addEventListener(hi,t._moveCb=function n(r){r&&r.target!==t||r&&!/transform$/.test(r.propertyName)||(t.removeEventListener(hi,n),t._moveCb=null,ki(t,e))})}})))},methods:{hasMove:function(n,e){if(!mi)return!1;if(this._hasMove)return this._hasMove;var t=n.cloneNode();n._transitionClasses&&n._transitionClasses.forEach((function(n){di(t,n)})),ci(t,e),t.style.display="none",this.$el.appendChild(t);var r=ji(t);return this.$el.removeChild(t),this._hasMove=r.hasTransform}}}};Qt.config.mustUseProp=function(n,e,t){return"value"===t&&er(n)&&"button"!==e||"selected"===t&&"option"===n||"checked"===t&&"input"===n||"muted"===t&&"video"===n},Qt.config.isReservedTag=br,Qt.config.isReservedAttr=nr,Qt.config.getTagNamespace=function(n){return hr(n)?"svg":"math"===n?"math":void 0},Qt.config.isUnknownElement=function(n){if(!V)return!0;if(br(n))return!1;if(n=n.toLowerCase(),null!=fr[n])return fr[n];var e=document.createElement(n);return n.indexOf("-")>-1?fr[n]=e.constructor===window.HTMLUnknownElement||e.constructor===window.HTMLElement:fr[n]=/HTMLUnknownElement/.test(e.toString())},A(Qt.options.directives,Ui),A(Qt.options.components,na),Qt.prototype.__patch__=V?Ri:z,Qt.prototype.$mount=function(n,e){return function(n,e,t){var r;n.$el=e,n.$options.render||(n.$options.render=gn),nt(n,"beforeMount"),r=function(){n._update(n._render(),t)},new $e(n,r,z,{before:function(){n._isMounted&&!n._isDestroyed&&nt(n,"beforeUpdate")}},!0),t=!1;var i=n._preWatchers;if(i)for(var a=0;a<i.length;a++)i[a].run();return null==n.$vnode&&(n._isMounted=!0,nt(n,"mounted")),n}(this,n=n&&V?function(n){if("string"==typeof n){var e=document.querySelector(n);return e||document.createElement("div")}return n}(n):void 0,e)},V&&setTimeout((function(){U.devtools&&on&&on.emit("init",Qt)}),0)},function(n,e,t){"use strict";function r(n,e,t,r,i,a,s,o){var l,c="function"==typeof n?n.options:n;if(e&&(c.render=e,c.staticRenderFns=t,c._compiled=!0),r&&(c.functional=!0),a&&(c._scopeId="data-v-"+a),s?(l=function(n){(n=n||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(n=__VUE_SSR_CONTEXT__),i&&i.call(this,n),n&&n._registeredComponents&&n._registeredComponents.add(s)},c._ssrRegister=l):i&&(l=o?function(){i.call(this,(c.functional?this.parent:this).$root.$options.shadowRoot)}:i),l)if(c.functional){c._injectStyles=l;var d=c.render;c.render=function(n,e){return l.call(e),d(n,e)}}else{var p=c.beforeCreate;c.beforeCreate=p?[].concat(p,l):[l]}return{exports:n,options:c}}t.d(e,"a",(function(){return r}))},function(n,e,t){var r=t(58),i=r.all;n.exports=r.IS_HTMLDDA?function(n){return"function"==typeof n||n===i}:function(n){return"function"==typeof n}},function(n,e,t){var r=t(31),i=Function.prototype,a=i.call,s=r&&i.bind.bind(a,a);n.exports=r?s:function(n){return function(){return a.apply(n,arguments)}}},function(n,e){var t=function(n){return n&&n.Math==Math&&n};n.exports=t("object"==typeof globalThis&&globalThis)||t("object"==typeof window&&window)||t("object"==typeof self&&self)||t("object"==typeof global&&global)||function(){return this}()||this||Function("return this")()},function(n,e){n.exports=function(n){try{return!!n()}catch(n){return!0}}},function(n,e,t){var r=t(6);n.exports=!r((function(){return 7!=Object.defineProperty({},1,{get:function(){return 7}})[1]}))},function(n,e){var t=Array.isArray;n.exports=t},function(n,e,t){var r=t(72),i="object"==typeof self&&self&&self.Object===Object&&self,a=r||i||Function("return this")();n.exports=a},function(n,e,t){var r=t(4),i=t(36),a=r({}.hasOwnProperty);n.exports=Object.hasOwn||function(n,e){return a(i(n),e)}},function(n,e,t){var r=t(3),i=t(58),a=i.all;n.exports=i.IS_HTMLDDA?function(n){return"object"==typeof n?null!==n:r(n)||n===a}:function(n){return"object"==typeof n?null!==n:r(n)}},function(n,e,t){var r=t(169),i=t(172);n.exports=function(n,e){var t=i(n,e);return r(t)?t:void 0}},function(n,e,t){"use strict";t.d(e,"e",(function(){return r})),t.d(e,"b",(function(){return a})),t.d(e,"j",(function(){return s})),t.d(e,"g",(function(){return l})),t.d(e,"h",(function(){return c})),t.d(e,"i",(function(){return d})),t.d(e,"c",(function(){return p})),t.d(e,"f",(function(){return u})),t.d(e,"l",(function(){return m})),t.d(e,"m",(function(){return g})),t.d(e,"d",(function(){return b})),t.d(e,"k",(function(){return f})),t.d(e,"n",(function(){return v})),t.d(e,"a",(function(){return x}));t(28);const r=/#.*$/,i=/\.(md|html)$/,a=/\/$/,s=/^[a-z]+:/i;function o(n){return decodeURI(n).replace(r,"").replace(i,"")}function l(n){return s.test(n)}function c(n){return/^mailto:/.test(n)}function d(n){return/^tel:/.test(n)}function p(n){if(l(n))return n;if(!n)return"404";const e=n.match(r),t=e?e[0]:"",i=o(n);return a.test(i)?n:i+".html"+t}function u(n,e){const t=n.hash,i=function(n){const e=n&&n.match(r);if(e)return e[0]}(e);if(i&&t!==i)return!1;return o(n.path)===o(e)}function m(n,e,t){if(l(e))return{type:"external",path:e};t&&(e=function(n,e,t){const r=n.charAt(0);if("/"===r)return n;if("?"===r||"#"===r)return e+n;const i=e.split("/");t&&i[i.length-1]||i.pop();const a=n.replace(/^\//,"").split("/");for(let n=0;n<a.length;n++){const e=a[n];".."===e?i.pop():"."!==e&&i.push(e)}""!==i[0]&&i.unshift("");return i.join("/")}(e,t));const r=o(e);for(let e=0;e<n.length;e++)if(o(n[e].regularPath)===r)return Object.assign({},n[e],{type:"page",path:p(n[e].path)});return console.error(`[vuepress] No matching page found for sidebar item "${e}"`),{}}function g(n,e,t,r){const{pages:i,themeConfig:a}=t,s=r&&a.locales&&a.locales[r]||a;if("auto"===(n.frontmatter.sidebar||s.sidebar||a.sidebar))return h(n);const o=s.sidebar||a.sidebar;if(o){const{base:t,config:r}=function(n,e){if(Array.isArray(e))return{base:"/",config:e};for(const r in e)if(0===(t=n,/(\.html|\/)$/.test(t)?t:t+"/").indexOf(encodeURI(r)))return{base:r,config:e[r]};var t;return{}}(e,o);return"auto"===r?h(n):r?r.map(n=>function n(e,t,r,i=1){if("string"==typeof e)return m(t,e,r);if(Array.isArray(e))return Object.assign(m(t,e[0],r),{title:e[1]});{i>3&&console.error("[vuepress] detected a too deep nested sidebar group.");const a=e.children||[];return 0===a.length&&e.path?Object.assign(m(t,e.path,r),{title:e.title}):{type:"group",path:e.path,title:e.title,sidebarDepth:e.sidebarDepth,initialOpenGroupIndex:e.initialOpenGroupIndex,children:a.map(e=>n(e,t,r,i+1)),collapsable:!1!==e.collapsable}}}(n,i,t)):[]}return[]}function h(n){const e=b(n.headers||[]);return[{type:"group",collapsable:!1,title:n.title,path:null,children:e.map(e=>({type:"auto",title:e.title,basePath:n.path,path:n.path+"#"+e.slug,children:e.children||[]}))}]}function b(n){let e;return(n=n.map(n=>Object.assign({},n))).forEach(n=>{2===n.level?e=n:e&&(e.children||(e.children=[])).push(n)}),n.filter(n=>2===n.level)}function f(n){return Object.assign(n,{type:n.items&&n.items.length?"links":"link"})}function v(n){return Object.prototype.toString.call(n).match(/\[object (.*?)\]/)[1].toLowerCase()}function y(n){let e=n.frontmatter.date||n.lastUpdated||new Date,t=new Date(e);return"Invalid Date"==t&&e&&(t=new Date(e.replace(/-/g,"/"))),t.getTime()}function x(n,e){return y(e)-y(n)}},function(n,e){n.exports=function(n){return null!=n&&"object"==typeof n}},function(n,e,t){"use strict";var r=t(1);e.a=new r.a},function(n,e,t){var r=t(18),i=t(154),a=t(155),s=r?r.toStringTag:void 0;n.exports=function(n){return null==n?void 0===n?"[object Undefined]":"[object Null]":s&&s in Object(n)?i(n):a(n)}},function(n,e,t){var r=t(7),i=t(19),a=t(39);n.exports=r?function(n,e,t){return i.f(n,e,a(1,t))}:function(n,e,t){return n[e]=t,n}},function(n,e,t){var r=t(9).Symbol;n.exports=r},function(n,e,t){var r=t(7),i=t(67),a=t(104),s=t(29),o=t(57),l=TypeError,c=Object.defineProperty,d=Object.getOwnPropertyDescriptor;e.f=r?a?function(n,e,t){if(s(n),e=o(e),s(t),"function"==typeof n&&"prototype"===e&&"value"in t&&"writable"in t&&!t.writable){var r=d(n,e);r&&r.writable&&(n[e]=t.value,t={configurable:"configurable"in t?t.configurable:r.configurable,enumerable:"enumerable"in t?t.enumerable:r.enumerable,writable:!1})}return c(n,e,t)}:c:function(n,e,t){if(s(n),e=o(e),s(t),i)try{return c(n,e,t)}catch(n){}if("get"in t||"set"in t)throw l("Accessors not supported");return"value"in t&&(n[e]=t.value),n}},function(n,e,t){var r=t(4),i=r({}.toString),a=r("".slice);n.exports=function(n){return a(i(n),8,-1)}},function(n,e,t){var r=t(159),i=t(160),a=t(161),s=t(162),o=t(163);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var r=n[e];this.set(r[0],r[1])}}l.prototype.clear=r,l.prototype.delete=i,l.prototype.get=a,l.prototype.has=s,l.prototype.set=o,n.exports=l},function(n,e,t){var r=t(74);n.exports=function(n,e){for(var t=n.length;t--;)if(r(n[t][0],e))return t;return-1}},function(n,e,t){var r=t(12)(Object,"create");n.exports=r},function(n,e,t){var r=t(181);n.exports=function(n,e){var t=n.__data__;return r(e)?t["string"==typeof e?"string":"hash"]:t.map}},function(n,e,t){var r=t(49);n.exports=function(n){if("string"==typeof n||r(n))return n;var e=n+"";return"0"==e&&1/n==-1/0?"-0":e}},function(n,e){var t=/^\s+|\s+$/g,r=/^[-+]0x[0-9a-f]+$/i,i=/^0b[01]+$/i,a=/^0o[0-7]+$/i,s=parseInt,o="object"==typeof global&&global&&global.Object===Object&&global,l="object"==typeof self&&self&&self.Object===Object&&self,c=o||l||Function("return this")(),d=Object.prototype.toString,p=Math.max,u=Math.min,m=function(){return c.Date.now()};function g(n){var e=typeof n;return!!n&&("object"==e||"function"==e)}function h(n){if("number"==typeof n)return n;if(function(n){return"symbol"==typeof n||function(n){return!!n&&"object"==typeof n}(n)&&"[object Symbol]"==d.call(n)}(n))return NaN;if(g(n)){var e="function"==typeof n.valueOf?n.valueOf():n;n=g(e)?e+"":e}if("string"!=typeof n)return 0===n?n:+n;n=n.replace(t,"");var o=i.test(n);return o||a.test(n)?s(n.slice(2),o?2:8):r.test(n)?NaN:+n}n.exports=function(n,e,t){var r,i,a,s,o,l,c=0,d=!1,b=!1,f=!0;if("function"!=typeof n)throw new TypeError("Expected a function");function v(e){var t=r,a=i;return r=i=void 0,c=e,s=n.apply(a,t)}function y(n){return c=n,o=setTimeout(k,e),d?v(n):s}function x(n){var t=n-l;return void 0===l||t>=e||t<0||b&&n-c>=a}function k(){var n=m();if(x(n))return w(n);o=setTimeout(k,function(n){var t=e-(n-l);return b?u(t,a-(n-c)):t}(n))}function w(n){return o=void 0,f&&r?v(n):(r=i=void 0,s)}function S(){var n=m(),t=x(n);if(r=arguments,i=this,l=n,t){if(void 0===o)return y(l);if(b)return o=setTimeout(k,e),v(l)}return void 0===o&&(o=setTimeout(k,e)),s}return e=h(e)||0,g(t)&&(d=!!t.leading,a=(b="maxWait"in t)?p(h(t.maxWait)||0,e):a,f="trailing"in t?!!t.trailing:f),S.cancel=function(){void 0!==o&&clearTimeout(o),c=0,r=l=i=o=void 0},S.flush=function(){return void 0===o?s:w(m())},S}},function(n,e,t){var r,i;
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */void 0===(i="function"==typeof(r=function(){var n,e,t={version:"0.2.0"},r=t.settings={minimum:.08,easing:"ease",positionUsing:"",speed:200,trickle:!0,trickleRate:.02,trickleSpeed:800,showSpinner:!0,barSelector:'[role="bar"]',spinnerSelector:'[role="spinner"]',parent:"body",template:'<div class="bar" role="bar"><div class="peg"></div></div><div class="spinner" role="spinner"><div class="spinner-icon"></div></div>'};function i(n,e,t){return n<e?e:n>t?t:n}function a(n){return 100*(-1+n)}t.configure=function(n){var e,t;for(e in n)void 0!==(t=n[e])&&n.hasOwnProperty(e)&&(r[e]=t);return this},t.status=null,t.set=function(n){var e=t.isStarted();n=i(n,r.minimum,1),t.status=1===n?null:n;var l=t.render(!e),c=l.querySelector(r.barSelector),d=r.speed,p=r.easing;return l.offsetWidth,s((function(e){""===r.positionUsing&&(r.positionUsing=t.getPositioningCSS()),o(c,function(n,e,t){var i;return(i="translate3d"===r.positionUsing?{transform:"translate3d("+a(n)+"%,0,0)"}:"translate"===r.positionUsing?{transform:"translate("+a(n)+"%,0)"}:{"margin-left":a(n)+"%"}).transition="all "+e+"ms "+t,i}(n,d,p)),1===n?(o(l,{transition:"none",opacity:1}),l.offsetWidth,setTimeout((function(){o(l,{transition:"all "+d+"ms linear",opacity:0}),setTimeout((function(){t.remove(),e()}),d)}),d)):setTimeout(e,d)})),this},t.isStarted=function(){return"number"==typeof t.status},t.start=function(){t.status||t.set(0);var n=function(){setTimeout((function(){t.status&&(t.trickle(),n())}),r.trickleSpeed)};return r.trickle&&n(),this},t.done=function(n){return n||t.status?t.inc(.3+.5*Math.random()).set(1):this},t.inc=function(n){var e=t.status;return e?("number"!=typeof n&&(n=(1-e)*i(Math.random()*e,.1,.95)),e=i(e+n,0,.994),t.set(e)):t.start()},t.trickle=function(){return t.inc(Math.random()*r.trickleRate)},n=0,e=0,t.promise=function(r){return r&&"resolved"!==r.state()?(0===e&&t.start(),n++,e++,r.always((function(){0==--e?(n=0,t.done()):t.set((n-e)/n)})),this):this},t.render=function(n){if(t.isRendered())return document.getElementById("nprogress");c(document.documentElement,"nprogress-busy");var e=document.createElement("div");e.id="nprogress",e.innerHTML=r.template;var i,s=e.querySelector(r.barSelector),l=n?"-100":a(t.status||0),d=document.querySelector(r.parent);return o(s,{transition:"all 0 linear",transform:"translate3d("+l+"%,0,0)"}),r.showSpinner||(i=e.querySelector(r.spinnerSelector))&&u(i),d!=document.body&&c(d,"nprogress-custom-parent"),d.appendChild(e),e},t.remove=function(){d(document.documentElement,"nprogress-busy"),d(document.querySelector(r.parent),"nprogress-custom-parent");var n=document.getElementById("nprogress");n&&u(n)},t.isRendered=function(){return!!document.getElementById("nprogress")},t.getPositioningCSS=function(){var n=document.body.style,e="WebkitTransform"in n?"Webkit":"MozTransform"in n?"Moz":"msTransform"in n?"ms":"OTransform"in n?"O":"";return e+"Perspective"in n?"translate3d":e+"Transform"in n?"translate":"margin"};var s=function(){var n=[];function e(){var t=n.shift();t&&t(e)}return function(t){n.push(t),1==n.length&&e()}}(),o=function(){var n=["Webkit","O","Moz","ms"],e={};function t(t){return t=t.replace(/^-ms-/,"ms-").replace(/-([\da-z])/gi,(function(n,e){return e.toUpperCase()})),e[t]||(e[t]=function(e){var t=document.body.style;if(e in t)return e;for(var r,i=n.length,a=e.charAt(0).toUpperCase()+e.slice(1);i--;)if((r=n[i]+a)in t)return r;return e}(t))}function r(n,e,r){e=t(e),n.style[e]=r}return function(n,e){var t,i,a=arguments;if(2==a.length)for(t in e)void 0!==(i=e[t])&&e.hasOwnProperty(t)&&r(n,t,i);else r(n,a[1],a[2])}}();function l(n,e){return("string"==typeof n?n:p(n)).indexOf(" "+e+" ")>=0}function c(n,e){var t=p(n),r=t+e;l(t,e)||(n.className=r.substring(1))}function d(n,e){var t,r=p(n);l(n,e)&&(t=r.replace(" "+e+" "," "),n.className=t.substring(1,t.length-1))}function p(n){return(" "+(n.className||"")+" ").replace(/\s+/gi," ")}function u(n){n&&n.parentNode&&n.parentNode.removeChild(n)}return t})?r.call(e,t,e,n):r)||(n.exports=i)},function(n,e,t){"use strict";var r=t(30),i=t(36),a=t(37),s=t(133),o=t(135);r({target:"Array",proto:!0,arity:1,forced:t(6)((function(){return 4294967297!==[].push.call({length:4294967296},1)}))||!function(){try{Object.defineProperty([],"length",{writable:!1}).push()}catch(n){return n instanceof TypeError}}()},{push:function(n){var e=i(this),t=a(e),r=arguments.length;o(t+r);for(var l=0;l<r;l++)e[t]=arguments[l],t++;return s(e,t),t}})},function(n,e,t){var r=t(11),i=String,a=TypeError;n.exports=function(n){if(r(n))return n;throw a(i(n)+" is not an object")}},function(n,e,t){var r=t(5),i=t(54).f,a=t(17),s=t(117),o=t(41),l=t(68),c=t(129);n.exports=function(n,e){var t,d,p,u,m,g=n.target,h=n.global,b=n.stat;if(t=h?r:b?r[g]||o(g,{}):(r[g]||{}).prototype)for(d in e){if(u=e[d],p=n.dontCallGetSet?(m=i(t,d))&&m.value:t[d],!c(h?d:g+(b?".":"#")+d,n.forced)&&void 0!==p){if(typeof u==typeof p)continue;l(u,p)}(n.sham||p&&p.sham)&&a(u,"sham",!0),s(t,d,u,n)}}},function(n,e,t){var r=t(6);n.exports=!r((function(){var n=function(){}.bind();return"function"!=typeof n||n.hasOwnProperty("prototype")}))},function(n,e,t){var r=t(51),i=t(55);n.exports=function(n){return r(i(n))}},function(n,e,t){var r=t(5),i=t(3),a=function(n){return i(n)?n:void 0};n.exports=function(n,e){return arguments.length<2?a(r[n]):r[n]&&r[n][e]}},function(n,e,t){var r=t(3),i=t(115),a=TypeError;n.exports=function(n){if(r(n))return n;throw a(i(n)+" is not a function")}},function(n,e,t){var r=t(5),i=t(64),a=t(10),s=t(66),o=t(62),l=t(61),c=r.Symbol,d=i("wks"),p=l?c.for||c:c&&c.withoutSetter||s;n.exports=function(n){return a(d,n)||(d[n]=o&&a(c,n)?c[n]:p("Symbol."+n)),d[n]}},function(n,e,t){var r=t(55),i=Object;n.exports=function(n){return i(r(n))}},function(n,e,t){var r=t(127);n.exports=function(n){return r(n.length)}},function(n,e,t){var r=t(31),i=Function.prototype.call;n.exports=r?i.bind(i):function(){return i.apply(i,arguments)}},function(n,e){n.exports=function(n,e){return{enumerable:!(1&n),configurable:!(2&n),writable:!(4&n),value:e}}},function(n,e,t){var r=t(5),i=t(41),a=r["__core-js_shared__"]||i("__core-js_shared__",{});n.exports=a},function(n,e,t){var r=t(5),i=Object.defineProperty;n.exports=function(n,e){try{i(r,n,{value:e,configurable:!0,writable:!0})}catch(t){r[n]=e}return e}},function(n,e,t){var r=t(153),i=t(14),a=Object.prototype,s=a.hasOwnProperty,o=a.propertyIsEnumerable,l=r(function(){return arguments}())?r:function(n){return i(n)&&s.call(n,"callee")&&!o.call(n,"callee")};n.exports=l},function(n,e,t){var r=t(12)(t(9),"Map");n.exports=r},function(n,e){n.exports=function(n){var e=typeof n;return null!=n&&("object"==e||"function"==e)}},function(n,e,t){var r=t(173),i=t(180),a=t(182),s=t(183),o=t(184);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var r=n[e];this.set(r[0],r[1])}}l.prototype.clear=r,l.prototype.delete=i,l.prototype.get=a,l.prototype.has=s,l.prototype.set=o,n.exports=l},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n){t[++e]=n})),t}},function(n,e){n.exports=function(n){return"number"==typeof n&&n>-1&&n%1==0&&n<=9007199254740991}},function(n,e,t){var r=t(8),i=t(49),a=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,s=/^\w*$/;n.exports=function(n,e){if(r(n))return!1;var t=typeof n;return!("number"!=t&&"symbol"!=t&&"boolean"!=t&&null!=n&&!i(n))||(s.test(n)||!a.test(n)||null!=e&&n in Object(e))}},function(n,e,t){var r=t(16),i=t(14);n.exports=function(n){return"symbol"==typeof n||i(n)&&"[object Symbol]"==r(n)}},function(n,e){n.exports=function(n){return n}},function(n,e,t){var r=t(4),i=t(6),a=t(20),s=Object,o=r("".split);n.exports=i((function(){return!s("z").propertyIsEnumerable(0)}))?function(n){return"String"==a(n)?o(n,""):s(n)}:s},function(n,e){n.exports={}},function(n,e){n.exports=function(n){return n.webpackPolyfill||(n.deprecate=function(){},n.paths=[],n.children||(n.children=[]),Object.defineProperty(n,"loaded",{enumerable:!0,get:function(){return n.l}}),Object.defineProperty(n,"id",{enumerable:!0,get:function(){return n.i}}),n.webpackPolyfill=1),n}},function(n,e,t){var r=t(7),i=t(38),a=t(111),s=t(39),o=t(32),l=t(57),c=t(10),d=t(67),p=Object.getOwnPropertyDescriptor;e.f=r?p:function(n,e){if(n=o(n),e=l(e),d)try{return p(n,e)}catch(n){}if(c(n,e))return s(!i(a.f,n,e),n[e])}},function(n,e,t){var r=t(56),i=TypeError;n.exports=function(n){if(r(n))throw i("Can't call method on "+n);return n}},function(n,e){n.exports=function(n){return null==n}},function(n,e,t){var r=t(112),i=t(59);n.exports=function(n){var e=r(n,"string");return i(e)?e:e+""}},function(n,e){var t="object"==typeof document&&document.all,r=void 0===t&&void 0!==t;n.exports={all:t,IS_HTMLDDA:r}},function(n,e,t){var r=t(33),i=t(3),a=t(60),s=t(61),o=Object;n.exports=s?function(n){return"symbol"==typeof n}:function(n){var e=r("Symbol");return i(e)&&a(e.prototype,o(n))}},function(n,e,t){var r=t(4);n.exports=r({}.isPrototypeOf)},function(n,e,t){var r=t(62);n.exports=r&&!Symbol.sham&&"symbol"==typeof Symbol.iterator},function(n,e,t){var r=t(63),i=t(6),a=t(5).String;n.exports=!!Object.getOwnPropertySymbols&&!i((function(){var n=Symbol();return!a(n)||!(Object(n)instanceof Symbol)||!Symbol.sham&&r&&r<41}))},function(n,e,t){var r,i,a=t(5),s=t(113),o=a.process,l=a.Deno,c=o&&o.versions||l&&l.version,d=c&&c.v8;d&&(i=(r=d.split("."))[0]>0&&r[0]<4?1:+(r[0]+r[1])),!i&&s&&(!(r=s.match(/Edge\/(\d+)/))||r[1]>=74)&&(r=s.match(/Chrome\/(\d+)/))&&(i=+r[1]),n.exports=i},function(n,e,t){var r=t(65),i=t(40);(n.exports=function(n,e){return i[n]||(i[n]=void 0!==e?e:{})})("versions",[]).push({version:"3.31.0",mode:r?"pure":"global",copyright:"© 2014-2023 Denis Pushkarev (zloirock.ru)",license:"https://github.com/zloirock/core-js/blob/v3.31.0/LICENSE",source:"https://github.com/zloirock/core-js"})},function(n,e){n.exports=!1},function(n,e,t){var r=t(4),i=0,a=Math.random(),s=r(1..toString);n.exports=function(n){return"Symbol("+(void 0===n?"":n)+")_"+s(++i+a,36)}},function(n,e,t){var r=t(7),i=t(6),a=t(103);n.exports=!r&&!i((function(){return 7!=Object.defineProperty(a("div"),"a",{get:function(){return 7}}).a}))},function(n,e,t){var r=t(10),i=t(122),a=t(54),s=t(19);n.exports=function(n,e,t){for(var o=i(e),l=s.f,c=a.f,d=0;d<o.length;d++){var p=o[d];r(n,p)||t&&r(t,p)||l(n,p,c(e,p))}}},function(n,e,t){var r=t(126);n.exports=function(n){var e=+n;return e!=e||0===e?0:r(e)}},function(n,e,t){var r=t(139),i=t(29),a=t(140);n.exports=Object.setPrototypeOf||("__proto__"in{}?function(){var n,e=!1,t={};try{(n=r(Object.prototype,"__proto__","set"))(t,[]),e=t instanceof Array}catch(n){}return function(t,r){return i(t),a(r),e?n(t,r):t.__proto__=r,t}}():void 0)},function(n,e){n.exports=function(n,e){for(var t=-1,r=e.length,i=n.length;++t<r;)n[i+t]=e[t];return n}},function(n,e){var t="object"==typeof global&&global&&global.Object===Object&&global;n.exports=t},function(n,e,t){var r=t(21),i=t(164),a=t(165),s=t(166),o=t(167),l=t(168);function c(n){var e=this.__data__=new r(n);this.size=e.size}c.prototype.clear=i,c.prototype.delete=a,c.prototype.get=s,c.prototype.has=o,c.prototype.set=l,n.exports=c},function(n,e){n.exports=function(n,e){return n===e||n!=n&&e!=e}},function(n,e,t){var r=t(16),i=t(44);n.exports=function(n){if(!i(n))return!1;var e=r(n);return"[object Function]"==e||"[object GeneratorFunction]"==e||"[object AsyncFunction]"==e||"[object Proxy]"==e}},function(n,e){var t=Function.prototype.toString;n.exports=function(n){if(null!=n){try{return t.call(n)}catch(n){}try{return n+""}catch(n){}}return""}},function(n,e,t){var r=t(185),i=t(14);n.exports=function n(e,t,a,s,o){return e===t||(null==e||null==t||!i(e)&&!i(t)?e!=e&&t!=t:r(e,t,a,s,n,o))}},function(n,e,t){var r=t(79),i=t(188),a=t(80);n.exports=function(n,e,t,s,o,l){var c=1&t,d=n.length,p=e.length;if(d!=p&&!(c&&p>d))return!1;var u=l.get(n),m=l.get(e);if(u&&m)return u==e&&m==n;var g=-1,h=!0,b=2&t?new r:void 0;for(l.set(n,e),l.set(e,n);++g<d;){var f=n[g],v=e[g];if(s)var y=c?s(v,f,g,e,n,l):s(f,v,g,n,e,l);if(void 0!==y){if(y)continue;h=!1;break}if(b){if(!i(e,(function(n,e){if(!a(b,e)&&(f===n||o(f,n,t,s,l)))return b.push(e)}))){h=!1;break}}else if(f!==v&&!o(f,v,t,s,l)){h=!1;break}}return l.delete(n),l.delete(e),h}},function(n,e,t){var r=t(45),i=t(186),a=t(187);function s(n){var e=-1,t=null==n?0:n.length;for(this.__data__=new r;++e<t;)this.add(n[e])}s.prototype.add=s.prototype.push=i,s.prototype.has=a,n.exports=s},function(n,e){n.exports=function(n,e){return n.has(e)}},function(n,e,t){var r=t(198),i=t(204),a=t(85);n.exports=function(n){return a(n)?r(n):i(n)}},function(n,e,t){(function(n){var r=t(9),i=t(200),a=e&&!e.nodeType&&e,s=a&&"object"==typeof n&&n&&!n.nodeType&&n,o=s&&s.exports===a?r.Buffer:void 0,l=(o?o.isBuffer:void 0)||i;n.exports=l}).call(this,t(53)(n))},function(n,e){var t=/^(?:0|[1-9]\d*)$/;n.exports=function(n,e){var r=typeof n;return!!(e=null==e?9007199254740991:e)&&("number"==r||"symbol"!=r&&t.test(n))&&n>-1&&n%1==0&&n<e}},function(n,e,t){var r=t(201),i=t(202),a=t(203),s=a&&a.isTypedArray,o=s?i(s):r;n.exports=o},function(n,e,t){var r=t(75),i=t(47);n.exports=function(n){return null!=n&&i(n.length)&&!r(n)}},function(n,e,t){var r=t(12)(t(9),"Set");n.exports=r},function(n,e,t){var r=t(44);n.exports=function(n){return n==n&&!r(n)}},function(n,e){n.exports=function(n,e){return function(t){return null!=t&&(t[n]===e&&(void 0!==e||n in Object(t)))}}},function(n,e,t){var r=t(90),i=t(25);n.exports=function(n,e){for(var t=0,a=(e=r(e,n)).length;null!=n&&t<a;)n=n[i(e[t++])];return t&&t==a?n:void 0}},function(n,e,t){var r=t(8),i=t(48),a=t(215),s=t(218);n.exports=function(n,e){return r(n)?n:i(n,e)?[n]:a(s(n))}},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){var r=t(151),i=t(156),a=t(227),s=t(235),o=t(244),l=t(108),c=a((function(n){var e=l(n);return o(e)&&(e=void 0),s(r(n,1,o,!0),i(e,2))}));n.exports=c},function(n,e,t){"use strict";
/*!
 * escape-html
 * Copyright(c) 2012-2013 TJ Holowaychuk
 * Copyright(c) 2015 Andreas Lubbe
 * Copyright(c) 2015 Tiancheng "Timothy" Gu
 * MIT Licensed
 */var r=/["'&<>]/;n.exports=function(n){var e,t=""+n,i=r.exec(t);if(!i)return t;var a="",s=0,o=0;for(s=i.index;s<t.length;s++){switch(t.charCodeAt(s)){case 34:e="&quot;";break;case 38:e="&amp;";break;case 39:e="&#39;";break;case 60:e="&lt;";break;case 62:e="&gt;";break;default:continue}o!==s&&(a+=t.substring(o,s)),o=s+1,a+=e}return o!==s?a+t.substring(o,s):a}},function(n,e,t){"use strict";t.r(e);var r={name:"CodeBlock",props:{title:{type:String,required:!0},active:{type:Boolean,default:!1}}},i=(t(247),t(2)),a=Object(i.a)(r,(function(){return(0,this._self._c)("div",{staticClass:"theme-code-block",class:{"theme-code-block__active":this.active}},[this._t("default")],2)}),[],!1,null,"4f1e9d0c",null);e.default=a.exports},function(n,e,t){"use strict";t.r(e);var r={name:"CodeGroup",data:()=>({codeTabs:[],activeCodeTabIndex:-1}),watch:{activeCodeTabIndex(n){this.codeTabs.forEach(n=>{n.elm.classList.remove("theme-code-block__active")}),this.codeTabs[n].elm.classList.add("theme-code-block__active")}},mounted(){this.codeTabs=(this.$slots.default||[]).filter(n=>Boolean(n.componentOptions)).map((n,e)=>(""===n.componentOptions.propsData.active&&(this.activeCodeTabIndex=e),{title:n.componentOptions.propsData.title,elm:n.elm})),-1===this.activeCodeTabIndex&&this.codeTabs.length>0&&(this.activeCodeTabIndex=0)},methods:{changeCodeTab(n){this.activeCodeTabIndex=n}}},i=(t(248),t(2)),a=Object(i.a)(r,(function(){var n=this,e=n._self._c;return e("div",{staticClass:"theme-code-group"},[e("div",{staticClass:"theme-code-group__nav"},[e("ul",{staticClass:"theme-code-group__ul"},n._l(n.codeTabs,(function(t,r){return e("li",{key:t.title,staticClass:"theme-code-group__li"},[e("button",{staticClass:"theme-code-group__nav-tab",class:{"theme-code-group__nav-tab-active":r===n.activeCodeTabIndex},on:{click:function(e){return n.changeCodeTab(r)}}},[n._v("\n            "+n._s(t.title)+"\n          ")])])})),0)]),n._v(" "),n._t("default"),n._v(" "),n.codeTabs.length<1?e("pre",{staticClass:"pre-blank"},[n._v("// Make sure to add code blocks to your code group")]):n._e()],2)}),[],!1,null,"2f5f1757",null);e.default=a.exports},function(n,e){n.exports=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"]},function(n,e,t){var r=t(5),i=t(11),a=r.document,s=i(a)&&i(a.createElement);n.exports=function(n){return s?a.createElement(n):{}}},function(n,e,t){var r=t(7),i=t(6);n.exports=r&&i((function(){return 42!=Object.defineProperty((function(){}),"prototype",{value:42,writable:!1}).prototype}))},function(n,e,t){var r=t(4),i=t(6),a=t(3),s=t(10),o=t(7),l=t(118).CONFIGURABLE,c=t(119),d=t(120),p=d.enforce,u=d.get,m=String,g=Object.defineProperty,h=r("".slice),b=r("".replace),f=r([].join),v=o&&!i((function(){return 8!==g((function(){}),"length",{value:8}).length})),y=String(String).split("String"),x=n.exports=function(n,e,t){"Symbol("===h(m(e),0,7)&&(e="["+b(m(e),/^Symbol\(([^)]*)\)/,"$1")+"]"),t&&t.getter&&(e="get "+e),t&&t.setter&&(e="set "+e),(!s(n,"name")||l&&n.name!==e)&&(o?g(n,"name",{value:e,configurable:!0}):n.name=e),v&&t&&s(t,"arity")&&n.length!==t.arity&&g(n,"length",{value:t.arity});try{t&&s(t,"constructor")&&t.constructor?o&&g(n,"prototype",{writable:!1}):n.prototype&&(n.prototype=void 0)}catch(n){}var r=p(n);return s(r,"source")||(r.source=f(y,"string"==typeof e?e:"")),n};Function.prototype.toString=x((function(){return a(this)&&u(this).source||c(this)}),"toString")},function(n,e,t){var r=t(64),i=t(66),a=r("keys");n.exports=function(n){return a[n]||(a[n]=i(n))}},function(n,e,t){var r=t(4),i=t(10),a=t(32),s=t(124).indexOf,o=t(52),l=r([].push);n.exports=function(n,e){var t,r=a(n),c=0,d=[];for(t in r)!i(o,t)&&i(r,t)&&l(d,t);for(;e.length>c;)i(r,t=e[c++])&&(~s(d,t)||l(d,t));return d}},function(n,e){n.exports=function(n){var e=null==n?0:n.length;return e?n[e-1]:void 0}},function(n,e,t){n.exports=t(256)},function(n,e,t){"use strict";var r=t(30),i=t(130).left,a=t(131),s=t(63);r({target:"Array",proto:!0,forced:!t(132)&&s>79&&s<83||!a("reduce")},{reduce:function(n){var e=arguments.length;return i(this,n,e,e>1?arguments[1]:void 0)}})},function(n,e,t){"use strict";var r={}.propertyIsEnumerable,i=Object.getOwnPropertyDescriptor,a=i&&!r.call({1:2},1);e.f=a?function(n){var e=i(this,n);return!!e&&e.enumerable}:r},function(n,e,t){var r=t(38),i=t(11),a=t(59),s=t(114),o=t(116),l=t(35),c=TypeError,d=l("toPrimitive");n.exports=function(n,e){if(!i(n)||a(n))return n;var t,l=s(n,d);if(l){if(void 0===e&&(e="default"),t=r(l,n,e),!i(t)||a(t))return t;throw c("Can't convert object to primitive value")}return void 0===e&&(e="number"),o(n,e)}},function(n,e){n.exports="undefined"!=typeof navigator&&String(navigator.userAgent)||""},function(n,e,t){var r=t(34),i=t(56);n.exports=function(n,e){var t=n[e];return i(t)?void 0:r(t)}},function(n,e){var t=String;n.exports=function(n){try{return t(n)}catch(n){return"Object"}}},function(n,e,t){var r=t(38),i=t(3),a=t(11),s=TypeError;n.exports=function(n,e){var t,o;if("string"===e&&i(t=n.toString)&&!a(o=r(t,n)))return o;if(i(t=n.valueOf)&&!a(o=r(t,n)))return o;if("string"!==e&&i(t=n.toString)&&!a(o=r(t,n)))return o;throw s("Can't convert object to primitive value")}},function(n,e,t){var r=t(3),i=t(19),a=t(105),s=t(41);n.exports=function(n,e,t,o){o||(o={});var l=o.enumerable,c=void 0!==o.name?o.name:e;if(r(t)&&a(t,c,o),o.global)l?n[e]=t:s(e,t);else{try{o.unsafe?n[e]&&(l=!0):delete n[e]}catch(n){}l?n[e]=t:i.f(n,e,{value:t,enumerable:!1,configurable:!o.nonConfigurable,writable:!o.nonWritable})}return n}},function(n,e,t){var r=t(7),i=t(10),a=Function.prototype,s=r&&Object.getOwnPropertyDescriptor,o=i(a,"name"),l=o&&"something"===function(){}.name,c=o&&(!r||r&&s(a,"name").configurable);n.exports={EXISTS:o,PROPER:l,CONFIGURABLE:c}},function(n,e,t){var r=t(4),i=t(3),a=t(40),s=r(Function.toString);i(a.inspectSource)||(a.inspectSource=function(n){return s(n)}),n.exports=a.inspectSource},function(n,e,t){var r,i,a,s=t(121),o=t(5),l=t(11),c=t(17),d=t(10),p=t(40),u=t(106),m=t(52),g=o.TypeError,h=o.WeakMap;if(s||p.state){var b=p.state||(p.state=new h);b.get=b.get,b.has=b.has,b.set=b.set,r=function(n,e){if(b.has(n))throw g("Object already initialized");return e.facade=n,b.set(n,e),e},i=function(n){return b.get(n)||{}},a=function(n){return b.has(n)}}else{var f=u("state");m[f]=!0,r=function(n,e){if(d(n,f))throw g("Object already initialized");return e.facade=n,c(n,f,e),e},i=function(n){return d(n,f)?n[f]:{}},a=function(n){return d(n,f)}}n.exports={set:r,get:i,has:a,enforce:function(n){return a(n)?i(n):r(n,{})},getterFor:function(n){return function(e){var t;if(!l(e)||(t=i(e)).type!==n)throw g("Incompatible receiver, "+n+" required");return t}}}},function(n,e,t){var r=t(5),i=t(3),a=r.WeakMap;n.exports=i(a)&&/native code/.test(String(a))},function(n,e,t){var r=t(33),i=t(4),a=t(123),s=t(128),o=t(29),l=i([].concat);n.exports=r("Reflect","ownKeys")||function(n){var e=a.f(o(n)),t=s.f;return t?l(e,t(n)):e}},function(n,e,t){var r=t(107),i=t(102).concat("length","prototype");e.f=Object.getOwnPropertyNames||function(n){return r(n,i)}},function(n,e,t){var r=t(32),i=t(125),a=t(37),s=function(n){return function(e,t,s){var o,l=r(e),c=a(l),d=i(s,c);if(n&&t!=t){for(;c>d;)if((o=l[d++])!=o)return!0}else for(;c>d;d++)if((n||d in l)&&l[d]===t)return n||d||0;return!n&&-1}};n.exports={includes:s(!0),indexOf:s(!1)}},function(n,e,t){var r=t(69),i=Math.max,a=Math.min;n.exports=function(n,e){var t=r(n);return t<0?i(t+e,0):a(t,e)}},function(n,e){var t=Math.ceil,r=Math.floor;n.exports=Math.trunc||function(n){var e=+n;return(e>0?r:t)(e)}},function(n,e,t){var r=t(69),i=Math.min;n.exports=function(n){return n>0?i(r(n),9007199254740991):0}},function(n,e){e.f=Object.getOwnPropertySymbols},function(n,e,t){var r=t(6),i=t(3),a=/#|\.prototype\./,s=function(n,e){var t=l[o(n)];return t==d||t!=c&&(i(e)?r(e):!!e)},o=s.normalize=function(n){return String(n).replace(a,".").toLowerCase()},l=s.data={},c=s.NATIVE="N",d=s.POLYFILL="P";n.exports=s},function(n,e,t){var r=t(34),i=t(36),a=t(51),s=t(37),o=TypeError,l=function(n){return function(e,t,l,c){r(t);var d=i(e),p=a(d),u=s(d),m=n?u-1:0,g=n?-1:1;if(l<2)for(;;){if(m in p){c=p[m],m+=g;break}if(m+=g,n?m<0:u<=m)throw o("Reduce of empty array with no initial value")}for(;n?m>=0:u>m;m+=g)m in p&&(c=t(c,p[m],m,d));return c}};n.exports={left:l(!1),right:l(!0)}},function(n,e,t){"use strict";var r=t(6);n.exports=function(n,e){var t=[][n];return!!t&&r((function(){t.call(null,e||function(){return 1},1)}))}},function(n,e,t){var r=t(20);n.exports="undefined"!=typeof process&&"process"==r(process)},function(n,e,t){"use strict";var r=t(7),i=t(134),a=TypeError,s=Object.getOwnPropertyDescriptor,o=r&&!function(){if(void 0!==this)return!0;try{Object.defineProperty([],"length",{writable:!1}).length=1}catch(n){return n instanceof TypeError}}();n.exports=o?function(n,e){if(i(n)&&!s(n,"length").writable)throw a("Cannot set read only .length");return n.length=e}:function(n,e){return n.length=e}},function(n,e,t){var r=t(20);n.exports=Array.isArray||function(n){return"Array"==r(n)}},function(n,e){var t=TypeError;n.exports=function(n){if(n>9007199254740991)throw t("Maximum allowed index exceeded");return n}},function(n,e,t){var r=t(30),i=t(5),a=t(137),s=t(138),o=i.WebAssembly,l=7!==Error("e",{cause:7}).cause,c=function(n,e){var t={};t[n]=s(n,e,l),r({global:!0,constructor:!0,arity:1,forced:l},t)},d=function(n,e){if(o&&o[n]){var t={};t[n]=s("WebAssembly."+n,e,l),r({target:"WebAssembly",stat:!0,constructor:!0,arity:1,forced:l},t)}};c("Error",(function(n){return function(e){return a(n,this,arguments)}})),c("EvalError",(function(n){return function(e){return a(n,this,arguments)}})),c("RangeError",(function(n){return function(e){return a(n,this,arguments)}})),c("ReferenceError",(function(n){return function(e){return a(n,this,arguments)}})),c("SyntaxError",(function(n){return function(e){return a(n,this,arguments)}})),c("TypeError",(function(n){return function(e){return a(n,this,arguments)}})),c("URIError",(function(n){return function(e){return a(n,this,arguments)}})),d("CompileError",(function(n){return function(e){return a(n,this,arguments)}})),d("LinkError",(function(n){return function(e){return a(n,this,arguments)}})),d("RuntimeError",(function(n){return function(e){return a(n,this,arguments)}}))},function(n,e,t){var r=t(31),i=Function.prototype,a=i.apply,s=i.call;n.exports="object"==typeof Reflect&&Reflect.apply||(r?s.bind(a):function(){return s.apply(a,arguments)})},function(n,e,t){"use strict";var r=t(33),i=t(10),a=t(17),s=t(60),o=t(70),l=t(68),c=t(141),d=t(142),p=t(143),u=t(147),m=t(148),g=t(7),h=t(65);n.exports=function(n,e,t,b){var f=b?2:1,v=n.split("."),y=v[v.length-1],x=r.apply(null,v);if(x){var k=x.prototype;if(!h&&i(k,"cause")&&delete k.cause,!t)return x;var w=r("Error"),S=e((function(n,e){var t=p(b?e:n,void 0),r=b?new x(n):new x;return void 0!==t&&a(r,"message",t),m(r,S,r.stack,2),this&&s(k,this)&&d(r,this,S),arguments.length>f&&u(r,arguments[f]),r}));if(S.prototype=k,"Error"!==y?o?o(S,w):l(S,w,{name:!0}):g&&"stackTraceLimit"in x&&(c(S,x,"stackTraceLimit"),c(S,x,"prepareStackTrace")),l(S,x),!h)try{k.name!==y&&a(k,"name",y),k.constructor=S}catch(n){}return S}}},function(n,e,t){var r=t(4),i=t(34);n.exports=function(n,e,t){try{return r(i(Object.getOwnPropertyDescriptor(n,e)[t]))}catch(n){}}},function(n,e,t){var r=t(3),i=String,a=TypeError;n.exports=function(n){if("object"==typeof n||r(n))return n;throw a("Can't set "+i(n)+" as a prototype")}},function(n,e,t){var r=t(19).f;n.exports=function(n,e,t){t in n||r(n,t,{configurable:!0,get:function(){return e[t]},set:function(n){e[t]=n}})}},function(n,e,t){var r=t(3),i=t(11),a=t(70);n.exports=function(n,e,t){var s,o;return a&&r(s=e.constructor)&&s!==t&&i(o=s.prototype)&&o!==t.prototype&&a(n,o),n}},function(n,e,t){var r=t(144);n.exports=function(n,e){return void 0===n?arguments.length<2?"":e:r(n)}},function(n,e,t){var r=t(145),i=String;n.exports=function(n){if("Symbol"===r(n))throw TypeError("Cannot convert a Symbol value to a string");return i(n)}},function(n,e,t){var r=t(146),i=t(3),a=t(20),s=t(35)("toStringTag"),o=Object,l="Arguments"==a(function(){return arguments}());n.exports=r?a:function(n){var e,t,r;return void 0===n?"Undefined":null===n?"Null":"string"==typeof(t=function(n,e){try{return n[e]}catch(n){}}(e=o(n),s))?t:l?a(e):"Object"==(r=a(e))&&i(e.callee)?"Arguments":r}},function(n,e,t){var r={};r[t(35)("toStringTag")]="z",n.exports="[object z]"===String(r)},function(n,e,t){var r=t(11),i=t(17);n.exports=function(n,e){r(e)&&"cause"in e&&i(n,"cause",e.cause)}},function(n,e,t){var r=t(17),i=t(149),a=t(150),s=Error.captureStackTrace;n.exports=function(n,e,t,o){a&&(s?s(n,e):r(n,"stack",i(t,o)))}},function(n,e,t){var r=t(4),i=Error,a=r("".replace),s=String(i("zxcasd").stack),o=/\n\s*at [^:]*:[^\n]*/,l=o.test(s);n.exports=function(n,e){if(l&&"string"==typeof n&&!i.prepareStackTrace)for(;e--;)n=a(n,o,"");return n}},function(n,e,t){var r=t(6),i=t(39);n.exports=!r((function(){var n=Error("a");return!("stack"in n)||(Object.defineProperty(n,"stack",i(1,7)),7!==n.stack)}))},function(n,e,t){var r=t(71),i=t(152);n.exports=function n(e,t,a,s,o){var l=-1,c=e.length;for(a||(a=i),o||(o=[]);++l<c;){var d=e[l];t>0&&a(d)?t>1?n(d,t-1,a,s,o):r(o,d):s||(o[o.length]=d)}return o}},function(n,e,t){var r=t(18),i=t(42),a=t(8),s=r?r.isConcatSpreadable:void 0;n.exports=function(n){return a(n)||i(n)||!!(s&&n&&n[s])}},function(n,e,t){var r=t(16),i=t(14);n.exports=function(n){return i(n)&&"[object Arguments]"==r(n)}},function(n,e,t){var r=t(18),i=Object.prototype,a=i.hasOwnProperty,s=i.toString,o=r?r.toStringTag:void 0;n.exports=function(n){var e=a.call(n,o),t=n[o];try{n[o]=void 0;var r=!0}catch(n){}var i=s.call(n);return r&&(e?n[o]=t:delete n[o]),i}},function(n,e){var t=Object.prototype.toString;n.exports=function(n){return t.call(n)}},function(n,e,t){var r=t(157),i=t(213),a=t(50),s=t(8),o=t(224);n.exports=function(n){return"function"==typeof n?n:null==n?a:"object"==typeof n?s(n)?i(n[0],n[1]):r(n):o(n)}},function(n,e,t){var r=t(158),i=t(212),a=t(88);n.exports=function(n){var e=i(n);return 1==e.length&&e[0][2]?a(e[0][0],e[0][1]):function(t){return t===n||r(t,n,e)}}},function(n,e,t){var r=t(73),i=t(77);n.exports=function(n,e,t,a){var s=t.length,o=s,l=!a;if(null==n)return!o;for(n=Object(n);s--;){var c=t[s];if(l&&c[2]?c[1]!==n[c[0]]:!(c[0]in n))return!1}for(;++s<o;){var d=(c=t[s])[0],p=n[d],u=c[1];if(l&&c[2]){if(void 0===p&&!(d in n))return!1}else{var m=new r;if(a)var g=a(p,u,d,n,e,m);if(!(void 0===g?i(u,p,3,a,m):g))return!1}}return!0}},function(n,e){n.exports=function(){this.__data__=[],this.size=0}},function(n,e,t){var r=t(22),i=Array.prototype.splice;n.exports=function(n){var e=this.__data__,t=r(e,n);return!(t<0)&&(t==e.length-1?e.pop():i.call(e,t,1),--this.size,!0)}},function(n,e,t){var r=t(22);n.exports=function(n){var e=this.__data__,t=r(e,n);return t<0?void 0:e[t][1]}},function(n,e,t){var r=t(22);n.exports=function(n){return r(this.__data__,n)>-1}},function(n,e,t){var r=t(22);n.exports=function(n,e){var t=this.__data__,i=r(t,n);return i<0?(++this.size,t.push([n,e])):t[i][1]=e,this}},function(n,e,t){var r=t(21);n.exports=function(){this.__data__=new r,this.size=0}},function(n,e){n.exports=function(n){var e=this.__data__,t=e.delete(n);return this.size=e.size,t}},function(n,e){n.exports=function(n){return this.__data__.get(n)}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e,t){var r=t(21),i=t(43),a=t(45);n.exports=function(n,e){var t=this.__data__;if(t instanceof r){var s=t.__data__;if(!i||s.length<199)return s.push([n,e]),this.size=++t.size,this;t=this.__data__=new a(s)}return t.set(n,e),this.size=t.size,this}},function(n,e,t){var r=t(75),i=t(170),a=t(44),s=t(76),o=/^\[object .+?Constructor\]$/,l=Function.prototype,c=Object.prototype,d=l.toString,p=c.hasOwnProperty,u=RegExp("^"+d.call(p).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$");n.exports=function(n){return!(!a(n)||i(n))&&(r(n)?u:o).test(s(n))}},function(n,e,t){var r,i=t(171),a=(r=/[^.]+$/.exec(i&&i.keys&&i.keys.IE_PROTO||""))?"Symbol(src)_1."+r:"";n.exports=function(n){return!!a&&a in n}},function(n,e,t){var r=t(9)["__core-js_shared__"];n.exports=r},function(n,e){n.exports=function(n,e){return null==n?void 0:n[e]}},function(n,e,t){var r=t(174),i=t(21),a=t(43);n.exports=function(){this.size=0,this.__data__={hash:new r,map:new(a||i),string:new r}}},function(n,e,t){var r=t(175),i=t(176),a=t(177),s=t(178),o=t(179);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var r=n[e];this.set(r[0],r[1])}}l.prototype.clear=r,l.prototype.delete=i,l.prototype.get=a,l.prototype.has=s,l.prototype.set=o,n.exports=l},function(n,e,t){var r=t(23);n.exports=function(){this.__data__=r?r(null):{},this.size=0}},function(n,e){n.exports=function(n){var e=this.has(n)&&delete this.__data__[n];return this.size-=e?1:0,e}},function(n,e,t){var r=t(23),i=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;if(r){var t=e[n];return"__lodash_hash_undefined__"===t?void 0:t}return i.call(e,n)?e[n]:void 0}},function(n,e,t){var r=t(23),i=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;return r?void 0!==e[n]:i.call(e,n)}},function(n,e,t){var r=t(23);n.exports=function(n,e){var t=this.__data__;return this.size+=this.has(n)?0:1,t[n]=r&&void 0===e?"__lodash_hash_undefined__":e,this}},function(n,e,t){var r=t(24);n.exports=function(n){var e=r(this,n).delete(n);return this.size-=e?1:0,e}},function(n,e){n.exports=function(n){var e=typeof n;return"string"==e||"number"==e||"symbol"==e||"boolean"==e?"__proto__"!==n:null===n}},function(n,e,t){var r=t(24);n.exports=function(n){return r(this,n).get(n)}},function(n,e,t){var r=t(24);n.exports=function(n){return r(this,n).has(n)}},function(n,e,t){var r=t(24);n.exports=function(n,e){var t=r(this,n),i=t.size;return t.set(n,e),this.size+=t.size==i?0:1,this}},function(n,e,t){var r=t(73),i=t(78),a=t(189),s=t(192),o=t(208),l=t(8),c=t(82),d=t(84),p="[object Object]",u=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,m,g,h){var b=l(n),f=l(e),v=b?"[object Array]":o(n),y=f?"[object Array]":o(e),x=(v="[object Arguments]"==v?p:v)==p,k=(y="[object Arguments]"==y?p:y)==p,w=v==y;if(w&&c(n)){if(!c(e))return!1;b=!0,x=!1}if(w&&!x)return h||(h=new r),b||d(n)?i(n,e,t,m,g,h):a(n,e,v,t,m,g,h);if(!(1&t)){var S=x&&u.call(n,"__wrapped__"),j=k&&u.call(e,"__wrapped__");if(S||j){var E=S?n.value():n,T=j?e.value():e;return h||(h=new r),g(E,T,t,m,h)}}return!!w&&(h||(h=new r),s(n,e,t,m,g,h))}},function(n,e){n.exports=function(n){return this.__data__.set(n,"__lodash_hash_undefined__"),this}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e){n.exports=function(n,e){for(var t=-1,r=null==n?0:n.length;++t<r;)if(e(n[t],t,n))return!0;return!1}},function(n,e,t){var r=t(18),i=t(190),a=t(74),s=t(78),o=t(191),l=t(46),c=r?r.prototype:void 0,d=c?c.valueOf:void 0;n.exports=function(n,e,t,r,c,p,u){switch(t){case"[object DataView]":if(n.byteLength!=e.byteLength||n.byteOffset!=e.byteOffset)return!1;n=n.buffer,e=e.buffer;case"[object ArrayBuffer]":return!(n.byteLength!=e.byteLength||!p(new i(n),new i(e)));case"[object Boolean]":case"[object Date]":case"[object Number]":return a(+n,+e);case"[object Error]":return n.name==e.name&&n.message==e.message;case"[object RegExp]":case"[object String]":return n==e+"";case"[object Map]":var m=o;case"[object Set]":var g=1&r;if(m||(m=l),n.size!=e.size&&!g)return!1;var h=u.get(n);if(h)return h==e;r|=2,u.set(n,e);var b=s(m(n),m(e),r,c,p,u);return u.delete(n),b;case"[object Symbol]":if(d)return d.call(n)==d.call(e)}return!1}},function(n,e,t){var r=t(9).Uint8Array;n.exports=r},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n,r){t[++e]=[r,n]})),t}},function(n,e,t){var r=t(193),i=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,a,s,o){var l=1&t,c=r(n),d=c.length;if(d!=r(e).length&&!l)return!1;for(var p=d;p--;){var u=c[p];if(!(l?u in e:i.call(e,u)))return!1}var m=o.get(n),g=o.get(e);if(m&&g)return m==e&&g==n;var h=!0;o.set(n,e),o.set(e,n);for(var b=l;++p<d;){var f=n[u=c[p]],v=e[u];if(a)var y=l?a(v,f,u,e,n,o):a(f,v,u,n,e,o);if(!(void 0===y?f===v||s(f,v,t,a,o):y)){h=!1;break}b||(b="constructor"==u)}if(h&&!b){var x=n.constructor,k=e.constructor;x==k||!("constructor"in n)||!("constructor"in e)||"function"==typeof x&&x instanceof x&&"function"==typeof k&&k instanceof k||(h=!1)}return o.delete(n),o.delete(e),h}},function(n,e,t){var r=t(194),i=t(195),a=t(81);n.exports=function(n){return r(n,a,i)}},function(n,e,t){var r=t(71),i=t(8);n.exports=function(n,e,t){var a=e(n);return i(n)?a:r(a,t(n))}},function(n,e,t){var r=t(196),i=t(197),a=Object.prototype.propertyIsEnumerable,s=Object.getOwnPropertySymbols,o=s?function(n){return null==n?[]:(n=Object(n),r(s(n),(function(e){return a.call(n,e)})))}:i;n.exports=o},function(n,e){n.exports=function(n,e){for(var t=-1,r=null==n?0:n.length,i=0,a=[];++t<r;){var s=n[t];e(s,t,n)&&(a[i++]=s)}return a}},function(n,e){n.exports=function(){return[]}},function(n,e,t){var r=t(199),i=t(42),a=t(8),s=t(82),o=t(83),l=t(84),c=Object.prototype.hasOwnProperty;n.exports=function(n,e){var t=a(n),d=!t&&i(n),p=!t&&!d&&s(n),u=!t&&!d&&!p&&l(n),m=t||d||p||u,g=m?r(n.length,String):[],h=g.length;for(var b in n)!e&&!c.call(n,b)||m&&("length"==b||p&&("offset"==b||"parent"==b)||u&&("buffer"==b||"byteLength"==b||"byteOffset"==b)||o(b,h))||g.push(b);return g}},function(n,e){n.exports=function(n,e){for(var t=-1,r=Array(n);++t<n;)r[t]=e(t);return r}},function(n,e){n.exports=function(){return!1}},function(n,e,t){var r=t(16),i=t(47),a=t(14),s={};s["[object Float32Array]"]=s["[object Float64Array]"]=s["[object Int8Array]"]=s["[object Int16Array]"]=s["[object Int32Array]"]=s["[object Uint8Array]"]=s["[object Uint8ClampedArray]"]=s["[object Uint16Array]"]=s["[object Uint32Array]"]=!0,s["[object Arguments]"]=s["[object Array]"]=s["[object ArrayBuffer]"]=s["[object Boolean]"]=s["[object DataView]"]=s["[object Date]"]=s["[object Error]"]=s["[object Function]"]=s["[object Map]"]=s["[object Number]"]=s["[object Object]"]=s["[object RegExp]"]=s["[object Set]"]=s["[object String]"]=s["[object WeakMap]"]=!1,n.exports=function(n){return a(n)&&i(n.length)&&!!s[r(n)]}},function(n,e){n.exports=function(n){return function(e){return n(e)}}},function(n,e,t){(function(n){var r=t(72),i=e&&!e.nodeType&&e,a=i&&"object"==typeof n&&n&&!n.nodeType&&n,s=a&&a.exports===i&&r.process,o=function(){try{var n=a&&a.require&&a.require("util").types;return n||s&&s.binding&&s.binding("util")}catch(n){}}();n.exports=o}).call(this,t(53)(n))},function(n,e,t){var r=t(205),i=t(206),a=Object.prototype.hasOwnProperty;n.exports=function(n){if(!r(n))return i(n);var e=[];for(var t in Object(n))a.call(n,t)&&"constructor"!=t&&e.push(t);return e}},function(n,e){var t=Object.prototype;n.exports=function(n){var e=n&&n.constructor;return n===("function"==typeof e&&e.prototype||t)}},function(n,e,t){var r=t(207)(Object.keys,Object);n.exports=r},function(n,e){n.exports=function(n,e){return function(t){return n(e(t))}}},function(n,e,t){var r=t(209),i=t(43),a=t(210),s=t(86),o=t(211),l=t(16),c=t(76),d=c(r),p=c(i),u=c(a),m=c(s),g=c(o),h=l;(r&&"[object DataView]"!=h(new r(new ArrayBuffer(1)))||i&&"[object Map]"!=h(new i)||a&&"[object Promise]"!=h(a.resolve())||s&&"[object Set]"!=h(new s)||o&&"[object WeakMap]"!=h(new o))&&(h=function(n){var e=l(n),t="[object Object]"==e?n.constructor:void 0,r=t?c(t):"";if(r)switch(r){case d:return"[object DataView]";case p:return"[object Map]";case u:return"[object Promise]";case m:return"[object Set]";case g:return"[object WeakMap]"}return e}),n.exports=h},function(n,e,t){var r=t(12)(t(9),"DataView");n.exports=r},function(n,e,t){var r=t(12)(t(9),"Promise");n.exports=r},function(n,e,t){var r=t(12)(t(9),"WeakMap");n.exports=r},function(n,e,t){var r=t(87),i=t(81);n.exports=function(n){for(var e=i(n),t=e.length;t--;){var a=e[t],s=n[a];e[t]=[a,s,r(s)]}return e}},function(n,e,t){var r=t(77),i=t(214),a=t(221),s=t(48),o=t(87),l=t(88),c=t(25);n.exports=function(n,e){return s(n)&&o(e)?l(c(n),e):function(t){var s=i(t,n);return void 0===s&&s===e?a(t,n):r(e,s,3)}}},function(n,e,t){var r=t(89);n.exports=function(n,e,t){var i=null==n?void 0:r(n,e);return void 0===i?t:i}},function(n,e,t){var r=t(216),i=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,a=/\\(\\)?/g,s=r((function(n){var e=[];return 46===n.charCodeAt(0)&&e.push(""),n.replace(i,(function(n,t,r,i){e.push(r?i.replace(a,"$1"):t||n)})),e}));n.exports=s},function(n,e,t){var r=t(217);n.exports=function(n){var e=r(n,(function(n){return 500===t.size&&t.clear(),n})),t=e.cache;return e}},function(n,e,t){var r=t(45);function i(n,e){if("function"!=typeof n||null!=e&&"function"!=typeof e)throw new TypeError("Expected a function");var t=function(){var r=arguments,i=e?e.apply(this,r):r[0],a=t.cache;if(a.has(i))return a.get(i);var s=n.apply(this,r);return t.cache=a.set(i,s)||a,s};return t.cache=new(i.Cache||r),t}i.Cache=r,n.exports=i},function(n,e,t){var r=t(219);n.exports=function(n){return null==n?"":r(n)}},function(n,e,t){var r=t(18),i=t(220),a=t(8),s=t(49),o=r?r.prototype:void 0,l=o?o.toString:void 0;n.exports=function n(e){if("string"==typeof e)return e;if(a(e))return i(e,n)+"";if(s(e))return l?l.call(e):"";var t=e+"";return"0"==t&&1/e==-1/0?"-0":t}},function(n,e){n.exports=function(n,e){for(var t=-1,r=null==n?0:n.length,i=Array(r);++t<r;)i[t]=e(n[t],t,n);return i}},function(n,e,t){var r=t(222),i=t(223);n.exports=function(n,e){return null!=n&&i(n,e,r)}},function(n,e){n.exports=function(n,e){return null!=n&&e in Object(n)}},function(n,e,t){var r=t(90),i=t(42),a=t(8),s=t(83),o=t(47),l=t(25);n.exports=function(n,e,t){for(var c=-1,d=(e=r(e,n)).length,p=!1;++c<d;){var u=l(e[c]);if(!(p=null!=n&&t(n,u)))break;n=n[u]}return p||++c!=d?p:!!(d=null==n?0:n.length)&&o(d)&&s(u,d)&&(a(n)||i(n))}},function(n,e,t){var r=t(225),i=t(226),a=t(48),s=t(25);n.exports=function(n){return a(n)?r(s(n)):i(n)}},function(n,e){n.exports=function(n){return function(e){return null==e?void 0:e[n]}}},function(n,e,t){var r=t(89);n.exports=function(n){return function(e){return r(e,n)}}},function(n,e,t){var r=t(50),i=t(228),a=t(230);n.exports=function(n,e){return a(i(n,e,r),n+"")}},function(n,e,t){var r=t(229),i=Math.max;n.exports=function(n,e,t){return e=i(void 0===e?n.length-1:e,0),function(){for(var a=arguments,s=-1,o=i(a.length-e,0),l=Array(o);++s<o;)l[s]=a[e+s];s=-1;for(var c=Array(e+1);++s<e;)c[s]=a[s];return c[e]=t(l),r(n,this,c)}}},function(n,e){n.exports=function(n,e,t){switch(t.length){case 0:return n.call(e);case 1:return n.call(e,t[0]);case 2:return n.call(e,t[0],t[1]);case 3:return n.call(e,t[0],t[1],t[2])}return n.apply(e,t)}},function(n,e,t){var r=t(231),i=t(234)(r);n.exports=i},function(n,e,t){var r=t(232),i=t(233),a=t(50),s=i?function(n,e){return i(n,"toString",{configurable:!0,enumerable:!1,value:r(e),writable:!0})}:a;n.exports=s},function(n,e){n.exports=function(n){return function(){return n}}},function(n,e,t){var r=t(12),i=function(){try{var n=r(Object,"defineProperty");return n({},"",{}),n}catch(n){}}();n.exports=i},function(n,e){var t=Date.now;n.exports=function(n){var e=0,r=0;return function(){var i=t(),a=16-(i-r);if(r=i,a>0){if(++e>=800)return arguments[0]}else e=0;return n.apply(void 0,arguments)}}},function(n,e,t){var r=t(79),i=t(236),a=t(241),s=t(80),o=t(242),l=t(46);n.exports=function(n,e,t){var c=-1,d=i,p=n.length,u=!0,m=[],g=m;if(t)u=!1,d=a;else if(p>=200){var h=e?null:o(n);if(h)return l(h);u=!1,d=s,g=new r}else g=e?[]:m;n:for(;++c<p;){var b=n[c],f=e?e(b):b;if(b=t||0!==b?b:0,u&&f==f){for(var v=g.length;v--;)if(g[v]===f)continue n;e&&g.push(f),m.push(b)}else d(g,f,t)||(g!==m&&g.push(f),m.push(b))}return m}},function(n,e,t){var r=t(237);n.exports=function(n,e){return!!(null==n?0:n.length)&&r(n,e,0)>-1}},function(n,e,t){var r=t(238),i=t(239),a=t(240);n.exports=function(n,e,t){return e==e?a(n,e,t):r(n,i,t)}},function(n,e){n.exports=function(n,e,t,r){for(var i=n.length,a=t+(r?1:-1);r?a--:++a<i;)if(e(n[a],a,n))return a;return-1}},function(n,e){n.exports=function(n){return n!=n}},function(n,e){n.exports=function(n,e,t){for(var r=t-1,i=n.length;++r<i;)if(n[r]===e)return r;return-1}},function(n,e){n.exports=function(n,e,t){for(var r=-1,i=null==n?0:n.length;++r<i;)if(t(e,n[r]))return!0;return!1}},function(n,e,t){var r=t(86),i=t(243),a=t(46),s=r&&1/a(new r([,-0]))[1]==1/0?function(n){return new r(n)}:i;n.exports=s},function(n,e){n.exports=function(){}},function(n,e,t){var r=t(85),i=t(14);n.exports=function(n){return i(n)&&r(n)}},function(n,e,t){},function(n,e,t){},function(n,e,t){"use strict";t(91)},function(n,e,t){"use strict";t(92)},function(n,e,t){},function(n,e,t){},function(n,e,t){"use strict";t(93)},function(n,e,t){"use strict";t(94)},function(n,e,t){"use strict";t(95)},function(n,e,t){"use strict";t(96)},function(n,e,t){"use strict";t(97)},function(n,e,t){"use strict";t.r(e);var r=t(1);
/*!
  * vue-router v3.6.5
  * (c) 2022 Evan You
  * @license MIT
  */function i(n,e){for(var t in e)n[t]=e[t];return n}var a=/[!'()*]/g,s=function(n){return"%"+n.charCodeAt(0).toString(16)},o=/%2C/g,l=function(n){return encodeURIComponent(n).replace(a,s).replace(o,",")};function c(n){try{return decodeURIComponent(n)}catch(n){0}return n}var d=function(n){return null==n||"object"==typeof n?n:String(n)};function p(n){var e={};return(n=n.trim().replace(/^(\?|#|&)/,""))?(n.split("&").forEach((function(n){var t=n.replace(/\+/g," ").split("="),r=c(t.shift()),i=t.length>0?c(t.join("=")):null;void 0===e[r]?e[r]=i:Array.isArray(e[r])?e[r].push(i):e[r]=[e[r],i]})),e):e}function u(n){var e=n?Object.keys(n).map((function(e){var t=n[e];if(void 0===t)return"";if(null===t)return l(e);if(Array.isArray(t)){var r=[];return t.forEach((function(n){void 0!==n&&(null===n?r.push(l(e)):r.push(l(e)+"="+l(n)))})),r.join("&")}return l(e)+"="+l(t)})).filter((function(n){return n.length>0})).join("&"):null;return e?"?"+e:""}var m=/\/?$/;function g(n,e,t,r){var i=r&&r.options.stringifyQuery,a=e.query||{};try{a=h(a)}catch(n){}var s={name:e.name||n&&n.name,meta:n&&n.meta||{},path:e.path||"/",hash:e.hash||"",query:a,params:e.params||{},fullPath:v(e,i),matched:n?f(n):[]};return t&&(s.redirectedFrom=v(t,i)),Object.freeze(s)}function h(n){if(Array.isArray(n))return n.map(h);if(n&&"object"==typeof n){var e={};for(var t in n)e[t]=h(n[t]);return e}return n}var b=g(null,{path:"/"});function f(n){for(var e=[];n;)e.unshift(n),n=n.parent;return e}function v(n,e){var t=n.path,r=n.query;void 0===r&&(r={});var i=n.hash;return void 0===i&&(i=""),(t||"/")+(e||u)(r)+i}function y(n,e,t){return e===b?n===e:!!e&&(n.path&&e.path?n.path.replace(m,"")===e.path.replace(m,"")&&(t||n.hash===e.hash&&x(n.query,e.query)):!(!n.name||!e.name)&&(n.name===e.name&&(t||n.hash===e.hash&&x(n.query,e.query)&&x(n.params,e.params))))}function x(n,e){if(void 0===n&&(n={}),void 0===e&&(e={}),!n||!e)return n===e;var t=Object.keys(n).sort(),r=Object.keys(e).sort();return t.length===r.length&&t.every((function(t,i){var a=n[t];if(r[i]!==t)return!1;var s=e[t];return null==a||null==s?a===s:"object"==typeof a&&"object"==typeof s?x(a,s):String(a)===String(s)}))}function k(n){for(var e=0;e<n.matched.length;e++){var t=n.matched[e];for(var r in t.instances){var i=t.instances[r],a=t.enteredCbs[r];if(i&&a){delete t.enteredCbs[r];for(var s=0;s<a.length;s++)i._isBeingDestroyed||a[s](i)}}}}var w={name:"RouterView",functional:!0,props:{name:{type:String,default:"default"}},render:function(n,e){var t=e.props,r=e.children,a=e.parent,s=e.data;s.routerView=!0;for(var o=a.$createElement,l=t.name,c=a.$route,d=a._routerViewCache||(a._routerViewCache={}),p=0,u=!1;a&&a._routerRoot!==a;){var m=a.$vnode?a.$vnode.data:{};m.routerView&&p++,m.keepAlive&&a._directInactive&&a._inactive&&(u=!0),a=a.$parent}if(s.routerViewDepth=p,u){var g=d[l],h=g&&g.component;return h?(g.configProps&&S(h,s,g.route,g.configProps),o(h,s,r)):o()}var b=c.matched[p],f=b&&b.components[l];if(!b||!f)return d[l]=null,o();d[l]={component:f},s.registerRouteInstance=function(n,e){var t=b.instances[l];(e&&t!==n||!e&&t===n)&&(b.instances[l]=e)},(s.hook||(s.hook={})).prepatch=function(n,e){b.instances[l]=e.componentInstance},s.hook.init=function(n){n.data.keepAlive&&n.componentInstance&&n.componentInstance!==b.instances[l]&&(b.instances[l]=n.componentInstance),k(c)};var v=b.props&&b.props[l];return v&&(i(d[l],{route:c,configProps:v}),S(f,s,c,v)),o(f,s,r)}};function S(n,e,t,r){var a=e.props=function(n,e){switch(typeof e){case"undefined":return;case"object":return e;case"function":return e(n);case"boolean":return e?n.params:void 0;default:0}}(t,r);if(a){a=e.props=i({},a);var s=e.attrs=e.attrs||{};for(var o in a)n.props&&o in n.props||(s[o]=a[o],delete a[o])}}function j(n,e,t){var r=n.charAt(0);if("/"===r)return n;if("?"===r||"#"===r)return e+n;var i=e.split("/");t&&i[i.length-1]||i.pop();for(var a=n.replace(/^\//,"").split("/"),s=0;s<a.length;s++){var o=a[s];".."===o?i.pop():"."!==o&&i.push(o)}return""!==i[0]&&i.unshift(""),i.join("/")}function E(n){return n.replace(/\/(?:\s*\/)+/g,"/")}var T=Array.isArray||function(n){return"[object Array]"==Object.prototype.toString.call(n)},q=N,_=z,I=function(n,e){return M(z(n,e),e)},C=M,A=P,R=new RegExp(["(\\\\.)","([\\/.])?(?:(?:\\:(\\w+)(?:\\(((?:\\\\.|[^\\\\()])+)\\))?|\\(((?:\\\\.|[^\\\\()])+)\\))([+*?])?|(\\*))"].join("|"),"g");function z(n,e){for(var t,r=[],i=0,a=0,s="",o=e&&e.delimiter||"/";null!=(t=R.exec(n));){var l=t[0],c=t[1],d=t.index;if(s+=n.slice(a,d),a=d+l.length,c)s+=c[1];else{var p=n[a],u=t[2],m=t[3],g=t[4],h=t[5],b=t[6],f=t[7];s&&(r.push(s),s="");var v=null!=u&&null!=p&&p!==u,y="+"===b||"*"===b,x="?"===b||"*"===b,k=t[2]||o,w=g||h;r.push({name:m||i++,prefix:u||"",delimiter:k,optional:x,repeat:y,partial:v,asterisk:!!f,pattern:w?L(w):f?".*":"[^"+B(k)+"]+?"})}}return a<n.length&&(s+=n.substr(a)),s&&r.push(s),r}function O(n){return encodeURI(n).replace(/[\/?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()}))}function M(n,e){for(var t=new Array(n.length),r=0;r<n.length;r++)"object"==typeof n[r]&&(t[r]=new RegExp("^(?:"+n[r].pattern+")$",F(e)));return function(e,r){for(var i="",a=e||{},s=(r||{}).pretty?O:encodeURIComponent,o=0;o<n.length;o++){var l=n[o];if("string"!=typeof l){var c,d=a[l.name];if(null==d){if(l.optional){l.partial&&(i+=l.prefix);continue}throw new TypeError('Expected "'+l.name+'" to be defined')}if(T(d)){if(!l.repeat)throw new TypeError('Expected "'+l.name+'" to not repeat, but received `'+JSON.stringify(d)+"`");if(0===d.length){if(l.optional)continue;throw new TypeError('Expected "'+l.name+'" to not be empty')}for(var p=0;p<d.length;p++){if(c=s(d[p]),!t[o].test(c))throw new TypeError('Expected all "'+l.name+'" to match "'+l.pattern+'", but received `'+JSON.stringify(c)+"`");i+=(0===p?l.prefix:l.delimiter)+c}}else{if(c=l.asterisk?encodeURI(d).replace(/[?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()})):s(d),!t[o].test(c))throw new TypeError('Expected "'+l.name+'" to match "'+l.pattern+'", but received "'+c+'"');i+=l.prefix+c}}else i+=l}return i}}function B(n){return n.replace(/([.+*?=^!:${}()[\]|\/\\])/g,"\\$1")}function L(n){return n.replace(/([=!:$\/()])/g,"\\$1")}function D(n,e){return n.keys=e,n}function F(n){return n&&n.sensitive?"":"i"}function P(n,e,t){T(e)||(t=e||t,e=[]);for(var r=(t=t||{}).strict,i=!1!==t.end,a="",s=0;s<n.length;s++){var o=n[s];if("string"==typeof o)a+=B(o);else{var l=B(o.prefix),c="(?:"+o.pattern+")";e.push(o),o.repeat&&(c+="(?:"+l+c+")*"),a+=c=o.optional?o.partial?l+"("+c+")?":"(?:"+l+"("+c+"))?":l+"("+c+")"}}var d=B(t.delimiter||"/"),p=a.slice(-d.length)===d;return r||(a=(p?a.slice(0,-d.length):a)+"(?:"+d+"(?=$))?"),a+=i?"$":r&&p?"":"(?="+d+"|$)",D(new RegExp("^"+a,F(t)),e)}function N(n,e,t){return T(e)||(t=e||t,e=[]),t=t||{},n instanceof RegExp?function(n,e){var t=n.source.match(/\((?!\?)/g);if(t)for(var r=0;r<t.length;r++)e.push({name:r,prefix:null,delimiter:null,optional:!1,repeat:!1,partial:!1,asterisk:!1,pattern:null});return D(n,e)}(n,e):T(n)?function(n,e,t){for(var r=[],i=0;i<n.length;i++)r.push(N(n[i],e,t).source);return D(new RegExp("(?:"+r.join("|")+")",F(t)),e)}(n,e,t):function(n,e,t){return P(z(n,t),e,t)}(n,e,t)}q.parse=_,q.compile=I,q.tokensToFunction=C,q.tokensToRegExp=A;var U=Object.create(null);function H(n,e,t){e=e||{};try{var r=U[n]||(U[n]=q.compile(n));return"string"==typeof e.pathMatch&&(e[0]=e.pathMatch),r(e,{pretty:!0})}catch(n){return""}finally{delete e[0]}}function J(n,e,t,r){var a="string"==typeof n?{path:n}:n;if(a._normalized)return a;if(a.name){var s=(a=i({},n)).params;return s&&"object"==typeof s&&(a.params=i({},s)),a}if(!a.path&&a.params&&e){(a=i({},a))._normalized=!0;var o=i(i({},e.params),a.params);if(e.name)a.name=e.name,a.params=o;else if(e.matched.length){var l=e.matched[e.matched.length-1].path;a.path=H(l,o,e.path)}else 0;return a}var c=function(n){var e="",t="",r=n.indexOf("#");r>=0&&(e=n.slice(r),n=n.slice(0,r));var i=n.indexOf("?");return i>=0&&(t=n.slice(i+1),n=n.slice(0,i)),{path:n,query:t,hash:e}}(a.path||""),u=e&&e.path||"/",m=c.path?j(c.path,u,t||a.append):u,g=function(n,e,t){void 0===e&&(e={});var r,i=t||p;try{r=i(n||"")}catch(n){r={}}for(var a in e){var s=e[a];r[a]=Array.isArray(s)?s.map(d):d(s)}return r}(c.query,a.query,r&&r.options.parseQuery),h=a.hash||c.hash;return h&&"#"!==h.charAt(0)&&(h="#"+h),{_normalized:!0,path:m,query:g,hash:h}}var $,Q=function(){},G={name:"RouterLink",props:{to:{type:[String,Object],required:!0},tag:{type:String,default:"a"},custom:Boolean,exact:Boolean,exactPath:Boolean,append:Boolean,replace:Boolean,activeClass:String,exactActiveClass:String,ariaCurrentValue:{type:String,default:"page"},event:{type:[String,Array],default:"click"}},render:function(n){var e=this,t=this.$router,r=this.$route,a=t.resolve(this.to,r,this.append),s=a.location,o=a.route,l=a.href,c={},d=t.options.linkActiveClass,p=t.options.linkExactActiveClass,u=null==d?"router-link-active":d,h=null==p?"router-link-exact-active":p,b=null==this.activeClass?u:this.activeClass,f=null==this.exactActiveClass?h:this.exactActiveClass,v=o.redirectedFrom?g(null,J(o.redirectedFrom),null,t):o;c[f]=y(r,v,this.exactPath),c[b]=this.exact||this.exactPath?c[f]:function(n,e){return 0===n.path.replace(m,"/").indexOf(e.path.replace(m,"/"))&&(!e.hash||n.hash===e.hash)&&function(n,e){for(var t in e)if(!(t in n))return!1;return!0}(n.query,e.query)}(r,v);var x=c[f]?this.ariaCurrentValue:null,k=function(n){V(n)&&(e.replace?t.replace(s,Q):t.push(s,Q))},w={click:V};Array.isArray(this.event)?this.event.forEach((function(n){w[n]=k})):w[this.event]=k;var S={class:c},j=!this.$scopedSlots.$hasNormal&&this.$scopedSlots.default&&this.$scopedSlots.default({href:l,route:o,navigate:k,isActive:c[b],isExactActive:c[f]});if(j){if(1===j.length)return j[0];if(j.length>1||!j.length)return 0===j.length?n():n("span",{},j)}if("a"===this.tag)S.on=w,S.attrs={href:l,"aria-current":x};else{var E=function n(e){var t;if(e)for(var r=0;r<e.length;r++){if("a"===(t=e[r]).tag)return t;if(t.children&&(t=n(t.children)))return t}}(this.$slots.default);if(E){E.isStatic=!1;var T=E.data=i({},E.data);for(var q in T.on=T.on||{},T.on){var _=T.on[q];q in w&&(T.on[q]=Array.isArray(_)?_:[_])}for(var I in w)I in T.on?T.on[I].push(w[I]):T.on[I]=k;var C=E.data.attrs=i({},E.data.attrs);C.href=l,C["aria-current"]=x}else S.on=w}return n(this.tag,S,this.$slots.default)}};function V(n){if(!(n.metaKey||n.altKey||n.ctrlKey||n.shiftKey||n.defaultPrevented||void 0!==n.button&&0!==n.button)){if(n.currentTarget&&n.currentTarget.getAttribute){var e=n.currentTarget.getAttribute("target");if(/\b_blank\b/i.test(e))return}return n.preventDefault&&n.preventDefault(),!0}}var W="undefined"!=typeof window;function K(n,e,t,r,i){var a=e||[],s=t||Object.create(null),o=r||Object.create(null);n.forEach((function(n){!function n(e,t,r,i,a,s){var o=i.path,l=i.name;0;var c=i.pathToRegexpOptions||{},d=function(n,e,t){t||(n=n.replace(/\/$/,""));if("/"===n[0])return n;if(null==e)return n;return E(e.path+"/"+n)}(o,a,c.strict);"boolean"==typeof i.caseSensitive&&(c.sensitive=i.caseSensitive);var p={path:d,regex:Z(d,c),components:i.components||{default:i.component},alias:i.alias?"string"==typeof i.alias?[i.alias]:i.alias:[],instances:{},enteredCbs:{},name:l,parent:a,matchAs:s,redirect:i.redirect,beforeEnter:i.beforeEnter,meta:i.meta||{},props:null==i.props?{}:i.components?i.props:{default:i.props}};i.children&&i.children.forEach((function(i){var a=s?E(s+"/"+i.path):void 0;n(e,t,r,i,p,a)}));t[p.path]||(e.push(p.path),t[p.path]=p);if(void 0!==i.alias)for(var u=Array.isArray(i.alias)?i.alias:[i.alias],m=0;m<u.length;++m){0;var g={path:u[m],children:i.children};n(e,t,r,g,a,p.path||"/")}l&&(r[l]||(r[l]=p))}(a,s,o,n,i)}));for(var l=0,c=a.length;l<c;l++)"*"===a[l]&&(a.push(a.splice(l,1)[0]),c--,l--);return{pathList:a,pathMap:s,nameMap:o}}function Z(n,e){return q(n,[],e)}function X(n,e){var t=K(n),r=t.pathList,i=t.pathMap,a=t.nameMap;function s(n,t,s){var o=J(n,t,!1,e),c=o.name;if(c){var d=a[c];if(!d)return l(null,o);var p=d.regex.keys.filter((function(n){return!n.optional})).map((function(n){return n.name}));if("object"!=typeof o.params&&(o.params={}),t&&"object"==typeof t.params)for(var u in t.params)!(u in o.params)&&p.indexOf(u)>-1&&(o.params[u]=t.params[u]);return o.path=H(d.path,o.params),l(d,o,s)}if(o.path){o.params={};for(var m=0;m<r.length;m++){var g=r[m],h=i[g];if(Y(h.regex,o.path,o.params))return l(h,o,s)}}return l(null,o)}function o(n,t){var r=n.redirect,i="function"==typeof r?r(g(n,t,null,e)):r;if("string"==typeof i&&(i={path:i}),!i||"object"!=typeof i)return l(null,t);var o=i,c=o.name,d=o.path,p=t.query,u=t.hash,m=t.params;if(p=o.hasOwnProperty("query")?o.query:p,u=o.hasOwnProperty("hash")?o.hash:u,m=o.hasOwnProperty("params")?o.params:m,c){a[c];return s({_normalized:!0,name:c,query:p,hash:u,params:m},void 0,t)}if(d){var h=function(n,e){return j(n,e.parent?e.parent.path:"/",!0)}(d,n);return s({_normalized:!0,path:H(h,m),query:p,hash:u},void 0,t)}return l(null,t)}function l(n,t,r){return n&&n.redirect?o(n,r||t):n&&n.matchAs?function(n,e,t){var r=s({_normalized:!0,path:H(t,e.params)});if(r){var i=r.matched,a=i[i.length-1];return e.params=r.params,l(a,e)}return l(null,e)}(0,t,n.matchAs):g(n,t,r,e)}return{match:s,addRoute:function(n,e){var t="object"!=typeof n?a[n]:void 0;K([e||n],r,i,a,t),t&&t.alias.length&&K(t.alias.map((function(n){return{path:n,children:[e]}})),r,i,a,t)},getRoutes:function(){return r.map((function(n){return i[n]}))},addRoutes:function(n){K(n,r,i,a)}}}function Y(n,e,t){var r=e.match(n);if(!r)return!1;if(!t)return!0;for(var i=1,a=r.length;i<a;++i){var s=n.keys[i-1];s&&(t[s.name||"pathMatch"]="string"==typeof r[i]?c(r[i]):r[i])}return!0}var nn=W&&window.performance&&window.performance.now?window.performance:Date;function en(){return nn.now().toFixed(3)}var tn=en();function rn(){return tn}function an(n){return tn=n}var sn=Object.create(null);function on(){"scrollRestoration"in window.history&&(window.history.scrollRestoration="manual");var n=window.location.protocol+"//"+window.location.host,e=window.location.href.replace(n,""),t=i({},window.history.state);return t.key=rn(),window.history.replaceState(t,"",e),window.addEventListener("popstate",dn),function(){window.removeEventListener("popstate",dn)}}function ln(n,e,t,r){if(n.app){var i=n.options.scrollBehavior;i&&n.app.$nextTick((function(){var a=function(){var n=rn();if(n)return sn[n]}(),s=i.call(n,e,t,r?a:null);s&&("function"==typeof s.then?s.then((function(n){hn(n,a)})).catch((function(n){0})):hn(s,a))}))}}function cn(){var n=rn();n&&(sn[n]={x:window.pageXOffset,y:window.pageYOffset})}function dn(n){cn(),n.state&&n.state.key&&an(n.state.key)}function pn(n){return mn(n.x)||mn(n.y)}function un(n){return{x:mn(n.x)?n.x:window.pageXOffset,y:mn(n.y)?n.y:window.pageYOffset}}function mn(n){return"number"==typeof n}var gn=/^#\d/;function hn(n,e){var t,r="object"==typeof n;if(r&&"string"==typeof n.selector){var i=gn.test(n.selector)?document.getElementById(n.selector.slice(1)):document.querySelector(n.selector);if(i){var a=n.offset&&"object"==typeof n.offset?n.offset:{};e=function(n,e){var t=document.documentElement.getBoundingClientRect(),r=n.getBoundingClientRect();return{x:r.left-t.left-e.x,y:r.top-t.top-e.y}}(i,a={x:mn((t=a).x)?t.x:0,y:mn(t.y)?t.y:0})}else pn(n)&&(e=un(n))}else r&&pn(n)&&(e=un(n));e&&("scrollBehavior"in document.documentElement.style?window.scrollTo({left:e.x,top:e.y,behavior:n.behavior}):window.scrollTo(e.x,e.y))}var bn,fn=W&&((-1===(bn=window.navigator.userAgent).indexOf("Android 2.")&&-1===bn.indexOf("Android 4.0")||-1===bn.indexOf("Mobile Safari")||-1!==bn.indexOf("Chrome")||-1!==bn.indexOf("Windows Phone"))&&window.history&&"function"==typeof window.history.pushState);function vn(n,e){cn();var t=window.history;try{if(e){var r=i({},t.state);r.key=rn(),t.replaceState(r,"",n)}else t.pushState({key:an(en())},"",n)}catch(t){window.location[e?"replace":"assign"](n)}}function yn(n){vn(n,!0)}var xn={redirected:2,aborted:4,cancelled:8,duplicated:16};function kn(n,e){return Sn(n,e,xn.redirected,'Redirected when going from "'+n.fullPath+'" to "'+function(n){if("string"==typeof n)return n;if("path"in n)return n.path;var e={};return jn.forEach((function(t){t in n&&(e[t]=n[t])})),JSON.stringify(e,null,2)}(e)+'" via a navigation guard.')}function wn(n,e){return Sn(n,e,xn.cancelled,'Navigation cancelled from "'+n.fullPath+'" to "'+e.fullPath+'" with a new navigation.')}function Sn(n,e,t,r){var i=new Error(r);return i._isRouter=!0,i.from=n,i.to=e,i.type=t,i}var jn=["params","query","hash"];function En(n){return Object.prototype.toString.call(n).indexOf("Error")>-1}function Tn(n,e){return En(n)&&n._isRouter&&(null==e||n.type===e)}function qn(n,e,t){var r=function(i){i>=n.length?t():n[i]?e(n[i],(function(){r(i+1)})):r(i+1)};r(0)}function _n(n){return function(e,t,r){var i=!1,a=0,s=null;In(n,(function(n,e,t,o){if("function"==typeof n&&void 0===n.cid){i=!0,a++;var l,c=Rn((function(e){var i;((i=e).__esModule||An&&"Module"===i[Symbol.toStringTag])&&(e=e.default),n.resolved="function"==typeof e?e:$.extend(e),t.components[o]=e,--a<=0&&r()})),d=Rn((function(n){var e="Failed to resolve async component "+o+": "+n;s||(s=En(n)?n:new Error(e),r(s))}));try{l=n(c,d)}catch(n){d(n)}if(l)if("function"==typeof l.then)l.then(c,d);else{var p=l.component;p&&"function"==typeof p.then&&p.then(c,d)}}})),i||r()}}function In(n,e){return Cn(n.map((function(n){return Object.keys(n.components).map((function(t){return e(n.components[t],n.instances[t],n,t)}))})))}function Cn(n){return Array.prototype.concat.apply([],n)}var An="function"==typeof Symbol&&"symbol"==typeof Symbol.toStringTag;function Rn(n){var e=!1;return function(){for(var t=[],r=arguments.length;r--;)t[r]=arguments[r];if(!e)return e=!0,n.apply(this,t)}}var zn=function(n,e){this.router=n,this.base=function(n){if(!n)if(W){var e=document.querySelector("base");n=(n=e&&e.getAttribute("href")||"/").replace(/^https?:\/\/[^\/]+/,"")}else n="/";"/"!==n.charAt(0)&&(n="/"+n);return n.replace(/\/$/,"")}(e),this.current=b,this.pending=null,this.ready=!1,this.readyCbs=[],this.readyErrorCbs=[],this.errorCbs=[],this.listeners=[]};function On(n,e,t,r){var i=In(n,(function(n,r,i,a){var s=function(n,e){"function"!=typeof n&&(n=$.extend(n));return n.options[e]}(n,e);if(s)return Array.isArray(s)?s.map((function(n){return t(n,r,i,a)})):t(s,r,i,a)}));return Cn(r?i.reverse():i)}function Mn(n,e){if(e)return function(){return n.apply(e,arguments)}}zn.prototype.listen=function(n){this.cb=n},zn.prototype.onReady=function(n,e){this.ready?n():(this.readyCbs.push(n),e&&this.readyErrorCbs.push(e))},zn.prototype.onError=function(n){this.errorCbs.push(n)},zn.prototype.transitionTo=function(n,e,t){var r,i=this;try{r=this.router.match(n,this.current)}catch(n){throw this.errorCbs.forEach((function(e){e(n)})),n}var a=this.current;this.confirmTransition(r,(function(){i.updateRoute(r),e&&e(r),i.ensureURL(),i.router.afterHooks.forEach((function(n){n&&n(r,a)})),i.ready||(i.ready=!0,i.readyCbs.forEach((function(n){n(r)})))}),(function(n){t&&t(n),n&&!i.ready&&(Tn(n,xn.redirected)&&a===b||(i.ready=!0,i.readyErrorCbs.forEach((function(e){e(n)}))))}))},zn.prototype.confirmTransition=function(n,e,t){var r=this,i=this.current;this.pending=n;var a,s,o=function(n){!Tn(n)&&En(n)&&(r.errorCbs.length?r.errorCbs.forEach((function(e){e(n)})):console.error(n)),t&&t(n)},l=n.matched.length-1,c=i.matched.length-1;if(y(n,i)&&l===c&&n.matched[l]===i.matched[c])return this.ensureURL(),n.hash&&ln(this.router,i,n,!1),o(((s=Sn(a=i,n,xn.duplicated,'Avoided redundant navigation to current location: "'+a.fullPath+'".')).name="NavigationDuplicated",s));var d=function(n,e){var t,r=Math.max(n.length,e.length);for(t=0;t<r&&n[t]===e[t];t++);return{updated:e.slice(0,t),activated:e.slice(t),deactivated:n.slice(t)}}(this.current.matched,n.matched),p=d.updated,u=d.deactivated,m=d.activated,g=[].concat(function(n){return On(n,"beforeRouteLeave",Mn,!0)}(u),this.router.beforeHooks,function(n){return On(n,"beforeRouteUpdate",Mn)}(p),m.map((function(n){return n.beforeEnter})),_n(m)),h=function(e,t){if(r.pending!==n)return o(wn(i,n));try{e(n,i,(function(e){!1===e?(r.ensureURL(!0),o(function(n,e){return Sn(n,e,xn.aborted,'Navigation aborted from "'+n.fullPath+'" to "'+e.fullPath+'" via a navigation guard.')}(i,n))):En(e)?(r.ensureURL(!0),o(e)):"string"==typeof e||"object"==typeof e&&("string"==typeof e.path||"string"==typeof e.name)?(o(kn(i,n)),"object"==typeof e&&e.replace?r.replace(e):r.push(e)):t(e)}))}catch(n){o(n)}};qn(g,h,(function(){qn(function(n){return On(n,"beforeRouteEnter",(function(n,e,t,r){return function(n,e,t){return function(r,i,a){return n(r,i,(function(n){"function"==typeof n&&(e.enteredCbs[t]||(e.enteredCbs[t]=[]),e.enteredCbs[t].push(n)),a(n)}))}}(n,t,r)}))}(m).concat(r.router.resolveHooks),h,(function(){if(r.pending!==n)return o(wn(i,n));r.pending=null,e(n),r.router.app&&r.router.app.$nextTick((function(){k(n)}))}))}))},zn.prototype.updateRoute=function(n){this.current=n,this.cb&&this.cb(n)},zn.prototype.setupListeners=function(){},zn.prototype.teardown=function(){this.listeners.forEach((function(n){n()})),this.listeners=[],this.current=b,this.pending=null};var Bn=function(n){function e(e,t){n.call(this,e,t),this._startLocation=Ln(this.base)}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router,t=e.options.scrollBehavior,r=fn&&t;r&&this.listeners.push(on());var i=function(){var t=n.current,i=Ln(n.base);n.current===b&&i===n._startLocation||n.transitionTo(i,(function(n){r&&ln(e,n,t,!0)}))};window.addEventListener("popstate",i),this.listeners.push((function(){window.removeEventListener("popstate",i)}))}},e.prototype.go=function(n){window.history.go(n)},e.prototype.push=function(n,e,t){var r=this,i=this.current;this.transitionTo(n,(function(n){vn(E(r.base+n.fullPath)),ln(r.router,n,i,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var r=this,i=this.current;this.transitionTo(n,(function(n){yn(E(r.base+n.fullPath)),ln(r.router,n,i,!1),e&&e(n)}),t)},e.prototype.ensureURL=function(n){if(Ln(this.base)!==this.current.fullPath){var e=E(this.base+this.current.fullPath);n?vn(e):yn(e)}},e.prototype.getCurrentLocation=function(){return Ln(this.base)},e}(zn);function Ln(n){var e=window.location.pathname,t=e.toLowerCase(),r=n.toLowerCase();return!n||t!==r&&0!==t.indexOf(E(r+"/"))||(e=e.slice(n.length)),(e||"/")+window.location.search+window.location.hash}var Dn=function(n){function e(e,t,r){n.call(this,e,t),r&&function(n){var e=Ln(n);if(!/^\/#/.test(e))return window.location.replace(E(n+"/#"+e)),!0}(this.base)||Fn()}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router.options.scrollBehavior,t=fn&&e;t&&this.listeners.push(on());var r=function(){var e=n.current;Fn()&&n.transitionTo(Pn(),(function(r){t&&ln(n.router,r,e,!0),fn||Hn(r.fullPath)}))},i=fn?"popstate":"hashchange";window.addEventListener(i,r),this.listeners.push((function(){window.removeEventListener(i,r)}))}},e.prototype.push=function(n,e,t){var r=this,i=this.current;this.transitionTo(n,(function(n){Un(n.fullPath),ln(r.router,n,i,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var r=this,i=this.current;this.transitionTo(n,(function(n){Hn(n.fullPath),ln(r.router,n,i,!1),e&&e(n)}),t)},e.prototype.go=function(n){window.history.go(n)},e.prototype.ensureURL=function(n){var e=this.current.fullPath;Pn()!==e&&(n?Un(e):Hn(e))},e.prototype.getCurrentLocation=function(){return Pn()},e}(zn);function Fn(){var n=Pn();return"/"===n.charAt(0)||(Hn("/"+n),!1)}function Pn(){var n=window.location.href,e=n.indexOf("#");return e<0?"":n=n.slice(e+1)}function Nn(n){var e=window.location.href,t=e.indexOf("#");return(t>=0?e.slice(0,t):e)+"#"+n}function Un(n){fn?vn(Nn(n)):window.location.hash=n}function Hn(n){fn?yn(Nn(n)):window.location.replace(Nn(n))}var Jn=function(n){function e(e,t){n.call(this,e,t),this.stack=[],this.index=-1}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.push=function(n,e,t){var r=this;this.transitionTo(n,(function(n){r.stack=r.stack.slice(0,r.index+1).concat(n),r.index++,e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var r=this;this.transitionTo(n,(function(n){r.stack=r.stack.slice(0,r.index).concat(n),e&&e(n)}),t)},e.prototype.go=function(n){var e=this,t=this.index+n;if(!(t<0||t>=this.stack.length)){var r=this.stack[t];this.confirmTransition(r,(function(){var n=e.current;e.index=t,e.updateRoute(r),e.router.afterHooks.forEach((function(e){e&&e(r,n)}))}),(function(n){Tn(n,xn.duplicated)&&(e.index=t)}))}},e.prototype.getCurrentLocation=function(){var n=this.stack[this.stack.length-1];return n?n.fullPath:"/"},e.prototype.ensureURL=function(){},e}(zn),$n=function(n){void 0===n&&(n={}),this.app=null,this.apps=[],this.options=n,this.beforeHooks=[],this.resolveHooks=[],this.afterHooks=[],this.matcher=X(n.routes||[],this);var e=n.mode||"hash";switch(this.fallback="history"===e&&!fn&&!1!==n.fallback,this.fallback&&(e="hash"),W||(e="abstract"),this.mode=e,e){case"history":this.history=new Bn(this,n.base);break;case"hash":this.history=new Dn(this,n.base,this.fallback);break;case"abstract":this.history=new Jn(this,n.base);break;default:0}},Qn={currentRoute:{configurable:!0}};$n.prototype.match=function(n,e,t){return this.matcher.match(n,e,t)},Qn.currentRoute.get=function(){return this.history&&this.history.current},$n.prototype.init=function(n){var e=this;if(this.apps.push(n),n.$once("hook:destroyed",(function(){var t=e.apps.indexOf(n);t>-1&&e.apps.splice(t,1),e.app===n&&(e.app=e.apps[0]||null),e.app||e.history.teardown()})),!this.app){this.app=n;var t=this.history;if(t instanceof Bn||t instanceof Dn){var r=function(n){t.setupListeners(),function(n){var r=t.current,i=e.options.scrollBehavior;fn&&i&&"fullPath"in n&&ln(e,n,r,!1)}(n)};t.transitionTo(t.getCurrentLocation(),r,r)}t.listen((function(n){e.apps.forEach((function(e){e._route=n}))}))}},$n.prototype.beforeEach=function(n){return Vn(this.beforeHooks,n)},$n.prototype.beforeResolve=function(n){return Vn(this.resolveHooks,n)},$n.prototype.afterEach=function(n){return Vn(this.afterHooks,n)},$n.prototype.onReady=function(n,e){this.history.onReady(n,e)},$n.prototype.onError=function(n){this.history.onError(n)},$n.prototype.push=function(n,e,t){var r=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){r.history.push(n,e,t)}));this.history.push(n,e,t)},$n.prototype.replace=function(n,e,t){var r=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){r.history.replace(n,e,t)}));this.history.replace(n,e,t)},$n.prototype.go=function(n){this.history.go(n)},$n.prototype.back=function(){this.go(-1)},$n.prototype.forward=function(){this.go(1)},$n.prototype.getMatchedComponents=function(n){var e=n?n.matched?n:this.resolve(n).route:this.currentRoute;return e?[].concat.apply([],e.matched.map((function(n){return Object.keys(n.components).map((function(e){return n.components[e]}))}))):[]},$n.prototype.resolve=function(n,e,t){var r=J(n,e=e||this.history.current,t,this),i=this.match(r,e),a=i.redirectedFrom||i.fullPath;return{location:r,route:i,href:function(n,e,t){var r="hash"===t?"#"+e:e;return n?E(n+"/"+r):r}(this.history.base,a,this.mode),normalizedTo:r,resolved:i}},$n.prototype.getRoutes=function(){return this.matcher.getRoutes()},$n.prototype.addRoute=function(n,e){this.matcher.addRoute(n,e),this.history.current!==b&&this.history.transitionTo(this.history.getCurrentLocation())},$n.prototype.addRoutes=function(n){this.matcher.addRoutes(n),this.history.current!==b&&this.history.transitionTo(this.history.getCurrentLocation())},Object.defineProperties($n.prototype,Qn);var Gn=$n;function Vn(n,e){return n.push(e),function(){var t=n.indexOf(e);t>-1&&n.splice(t,1)}}$n.install=function n(e){if(!n.installed||$!==e){n.installed=!0,$=e;var t=function(n){return void 0!==n},r=function(n,e){var r=n.$options._parentVnode;t(r)&&t(r=r.data)&&t(r=r.registerRouteInstance)&&r(n,e)};e.mixin({beforeCreate:function(){t(this.$options.router)?(this._routerRoot=this,this._router=this.$options.router,this._router.init(this),e.util.defineReactive(this,"_route",this._router.history.current)):this._routerRoot=this.$parent&&this.$parent._routerRoot||this,r(this,this)},destroyed:function(){r(this)}}),Object.defineProperty(e.prototype,"$router",{get:function(){return this._routerRoot._router}}),Object.defineProperty(e.prototype,"$route",{get:function(){return this._routerRoot._route}}),e.component("RouterView",w),e.component("RouterLink",G);var i=e.config.optionMergeStrategies;i.beforeRouteEnter=i.beforeRouteLeave=i.beforeRouteUpdate=i.created}},$n.version="3.6.5",$n.isNavigationFailure=Tn,$n.NavigationFailureType=xn,$n.START_LOCATION=b,W&&window.Vue&&window.Vue.use($n);t(110);var Wn=t(0),Kn=t(98),Zn=t.n(Kn),Xn=t(99),Yn=t.n(Xn),ne={created(){if(this.siteMeta=this.$site.headTags.filter(([n])=>"meta"===n).map(([n,e])=>e),this.$ssrContext){const e=this.getMergedMetaTags();this.$ssrContext.title=this.$title,this.$ssrContext.lang=this.$lang,this.$ssrContext.pageMeta=(n=e)?n.map(n=>{let e="<meta";return Object.keys(n).forEach(t=>{e+=` ${t}="${Yn()(n[t])}"`}),e+">"}).join("\n    "):"",this.$ssrContext.canonicalLink=te(this.$canonicalUrl)}var n},mounted(){this.currentMetaTags=[...document.querySelectorAll("meta")],this.updateMeta(),this.updateCanonicalLink()},methods:{updateMeta(){document.title=this.$title,document.documentElement.lang=this.$lang;const n=this.getMergedMetaTags();this.currentMetaTags=re(n,this.currentMetaTags)},getMergedMetaTags(){const n=this.$page.frontmatter.meta||[];return Zn()([{name:"description",content:this.$description}],n,this.siteMeta,ie)},updateCanonicalLink(){ee(),this.$canonicalUrl&&document.head.insertAdjacentHTML("beforeend",te(this.$canonicalUrl))}},watch:{$page(){this.updateMeta(),this.updateCanonicalLink()}},beforeDestroy(){re(null,this.currentMetaTags),ee()}};function ee(){const n=document.querySelector("link[rel='canonical']");n&&n.remove()}function te(n=""){return n?`<link href="${n}" rel="canonical" />`:""}function re(n,e){if(e&&[...e].filter(n=>n.parentNode===document.head).forEach(n=>document.head.removeChild(n)),n)return n.map(n=>{const e=document.createElement("meta");return Object.keys(n).forEach(t=>{e.setAttribute(t,n[t])}),document.head.appendChild(e),e})}function ie(n){for(const e of["name","property","itemprop"])if(n.hasOwnProperty(e))return n[e]+e;return JSON.stringify(n)}var ae=t(26),se=t.n(ae),oe={mounted(){window.addEventListener("scroll",this.onScroll)},methods:{onScroll:se()((function(){this.setActiveHash()}),300),setActiveHash(){const n=[].slice.call(document.querySelectorAll(".sidebar-link")),e=[].slice.call(document.querySelectorAll(".header-anchor")).filter(e=>n.some(n=>n.hash===e.hash)),t=Math.max(window.pageYOffset,document.documentElement.scrollTop,document.body.scrollTop),r=Math.max(document.documentElement.scrollHeight,document.body.scrollHeight),i=window.innerHeight+t;for(let n=0;n<e.length;n++){const a=e[n],s=e[n+1],o=0===n&&0===t||t>=a.parentElement.offsetTop+10&&(!s||t<s.parentElement.offsetTop-10),l=decodeURIComponent(this.$route.hash);if(o&&l!==decodeURIComponent(a.hash)){const t=a;if(i===r)for(let t=n+1;t<e.length;t++)if(l===decodeURIComponent(e[t].hash))return;return this.$vuepress.$set("disableScrollBehavior",!0),void this.$router.replace(decodeURIComponent(t.hash),()=>{this.$nextTick(()=>{this.$vuepress.$set("disableScrollBehavior",!1)})})}}}},beforeDestroy(){window.removeEventListener("scroll",this.onScroll)}},le=t(27),ce=t.n(le),de={mounted(){ce.a.configure({showSpinner:!1}),this.$router.beforeEach((n,e,t)=>{n.path===e.path||r.a.component(n.name)||ce.a.start(),t()}),this.$router.afterEach(()=>{ce.a.done(),this.isSidebarOpen=!1})}};t(245),t(246);class pe{constructor(){this.containerEl=document.getElementById("message-container"),this.containerEl||(this.containerEl=document.createElement("div"),this.containerEl.id="message-container",document.body.appendChild(this.containerEl))}show({text:n="",duration:e=3e3}){let t=document.createElement("div");t.className="message move-in",t.innerHTML=`\n      <i style="fill: #06a35a;font-size: 14px;display:inline-flex;align-items: center;">\n        <svg style="fill: #06a35a;font-size: 14px;" t="1572421810237" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2323" width="16" height="16"><path d="M822.811993 824.617989c-83.075838 81.99224-188.546032 124.613757-316.049383 127.86455-122.085362-3.250794-223.943563-45.87231-305.935802-127.86455s-124.613757-184.21164-127.86455-305.935802c3.250794-127.503351 45.87231-232.973545 127.86455-316.049383 81.99224-83.075838 184.21164-126.058554 305.935802-129.309347 127.503351 3.250794 232.973545 46.23351 316.049383 129.309347 83.075838 83.075838 126.058554 188.546032 129.309347 316.049383C949.231746 640.406349 905.887831 742.62575 822.811993 824.617989zM432.716755 684.111464c3.973192 3.973192 8.307584 5.779189 13.364374 6.140388 5.05679 0.361199 9.752381-1.444797 13.364374-5.417989l292.571429-287.514638c3.973192-3.973192 5.779189-8.307584 5.779189-13.364374 0-5.05679-1.805996-9.752381-5.779189-13.364374l1.805996 1.805996c-3.973192-3.973192-8.668783-5.779189-14.086772-6.140388-5.417989-0.361199-10.47478 1.444797-14.809171 5.417989l-264.397884 220.33157c-3.973192 3.250794-8.668783 4.695591-14.447972 4.695591-5.779189 0-10.835979-1.444797-15.53157-3.973192l-94.273016-72.962257c-4.334392-3.250794-9.391182-4.334392-14.447972-3.973192s-9.391182 3.250794-12.641975 7.585185l-2.889594 3.973192c-3.250794 4.334392-4.334392 9.391182-3.973192 14.809171 0.722399 5.417989 2.528395 10.11358 5.779189 14.086772L432.716755 684.111464z" p-id="2324"></path></svg>\n      </i>\n      <div class="text">${n}</div>\n    `,this.containerEl.appendChild(t),e>0&&setTimeout(()=>{this.close(t)},e)}close(n){n.className=n.className.replace("move-in",""),n.className+="move-out",n.addEventListener("animationend",()=>{n.remove()})}}var ue={mounted(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},updated(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},methods:{updateCopy(){setTimeout(()=>{(['div[class*="language-"] pre','div[class*="aside-code"] aside']instanceof Array||Array.isArray(['div[class*="language-"] pre','div[class*="aside-code"] aside']))&&['div[class*="language-"] pre','div[class*="aside-code"] aside'].forEach(n=>{document.querySelectorAll(n).forEach(this.generateCopyButton)})},1e3)},generateCopyButton(n){if(n.classList.contains("codecopy-enabled"))return;const e=document.createElement("i");e.className="code-copy",e.innerHTML='<svg  style="color:#aaa;font-size:14px" t="1572422231464" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3201" width="14" height="14"><path d="M866.461538 39.384615H354.461538c-43.323077 0-78.769231 35.446154-78.76923 78.769231v39.384616h472.615384c43.323077 0 78.769231 35.446154 78.769231 78.76923v551.384616h39.384615c43.323077 0 78.769231-35.446154 78.769231-78.769231V118.153846c0-43.323077-35.446154-78.769231-78.769231-78.769231z m-118.153846 275.692308c0-43.323077-35.446154-78.769231-78.76923-78.769231H157.538462c-43.323077 0-78.769231 35.446154-78.769231 78.769231v590.769231c0 43.323077 35.446154 78.769231 78.769231 78.769231h512c43.323077 0 78.769231-35.446154 78.76923-78.769231V315.076923z m-354.461538 137.846154c0 11.815385-7.876923 19.692308-19.692308 19.692308h-157.538461c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h157.538461c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z m157.538461 315.076923c0 11.815385-7.876923 19.692308-19.692307 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h315.076923c11.815385 0 19.692308 7.876923 19.692307 19.692308v39.384615z m78.769231-157.538462c0 11.815385-7.876923 19.692308-19.692308 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h393.846153c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z" p-id="3202"></path></svg>',e.title="Copy to clipboard",e.addEventListener("click",()=>{this.copyToClipboard(n.innerText)}),n.appendChild(e),n.classList.add("codecopy-enabled")},copyToClipboard(n){const e=document.createElement("textarea");e.value=n,e.setAttribute("readonly",""),e.style.position="absolute",e.style.left="-9999px",document.body.appendChild(e);const t=document.getSelection().rangeCount>0&&document.getSelection().getRangeAt(0);e.select(),document.execCommand("copy");(new pe).show({text:"复制成功",duration:1e3}),document.body.removeChild(e),t&&(document.getSelection().removeAllRanges(),document.getSelection().addRange(t))}}},me="auto",ge="zoom-in",he="zoom-out",be="grab",fe="move";function ve(n,e,t){var r=!(arguments.length>3&&void 0!==arguments[3])||arguments[3],i={passive:!1};r?n.addEventListener(e,t,i):n.removeEventListener(e,t,i)}function ye(n,e){if(n){var t=new Image;t.onload=function(){e&&e(t)},t.src=n}}function xe(n){return n.dataset.original?n.dataset.original:"A"===n.parentNode.tagName?n.parentNode.getAttribute("href"):null}function ke(n,e,t){!function(n){var e=we,t=Se;if(n.transition){var r=n.transition;delete n.transition,n[e]=r}if(n.transform){var i=n.transform;delete n.transform,n[t]=i}}(e);var r=n.style,i={};for(var a in e)t&&(i[a]=r[a]||""),r[a]=e[a];return i}var we="transition",Se="transform",je="transform",Ee="transitionend";var Te=function(){},qe={enableGrab:!0,preloadImage:!1,closeOnWindowResize:!0,transitionDuration:.4,transitionTimingFunction:"cubic-bezier(0.4, 0, 0, 1)",bgColor:"rgb(255, 255, 255)",bgOpacity:1,scaleBase:1,scaleExtra:.5,scrollThreshold:40,zIndex:998,customSize:null,onOpen:Te,onClose:Te,onGrab:Te,onMove:Te,onRelease:Te,onBeforeOpen:Te,onBeforeClose:Te,onBeforeGrab:Te,onBeforeRelease:Te,onImageLoading:Te,onImageLoaded:Te},_e={init:function(n){var e,t;e=this,t=n,Object.getOwnPropertyNames(Object.getPrototypeOf(e)).forEach((function(n){e[n]=e[n].bind(t)}))},click:function(n){if(n.preventDefault(),Ce(n))return window.open(this.target.srcOriginal||n.currentTarget.src,"_blank");this.shown?this.released?this.close():this.release():this.open(n.currentTarget)},scroll:function(){var n=document.documentElement||document.body.parentNode||document.body,e=window.pageXOffset||n.scrollLeft,t=window.pageYOffset||n.scrollTop;null===this.lastScrollPosition&&(this.lastScrollPosition={x:e,y:t});var r=this.lastScrollPosition.x-e,i=this.lastScrollPosition.y-t,a=this.options.scrollThreshold;(Math.abs(i)>=a||Math.abs(r)>=a)&&(this.lastScrollPosition=null,this.close())},keydown:function(n){(function(n){return"Escape"===(n.key||n.code)||27===n.keyCode})(n)&&(this.released?this.close():this.release(this.close))},mousedown:function(n){if(Ie(n)&&!Ce(n)){n.preventDefault();var e=n.clientX,t=n.clientY;this.pressTimer=setTimeout(function(){this.grab(e,t)}.bind(this),200)}},mousemove:function(n){this.released||this.move(n.clientX,n.clientY)},mouseup:function(n){Ie(n)&&!Ce(n)&&(clearTimeout(this.pressTimer),this.released?this.close():this.release())},touchstart:function(n){n.preventDefault();var e=n.touches[0],t=e.clientX,r=e.clientY;this.pressTimer=setTimeout(function(){this.grab(t,r)}.bind(this),200)},touchmove:function(n){if(!this.released){var e=n.touches[0],t=e.clientX,r=e.clientY;this.move(t,r)}},touchend:function(n){(function(n){n.targetTouches.length})(n)||(clearTimeout(this.pressTimer),this.released?this.close():this.release())},clickOverlay:function(){this.close()},resizeWindow:function(){this.close()}};function Ie(n){return 0===n.button}function Ce(n){return n.metaKey||n.ctrlKey}var Ae={init:function(n){this.el=document.createElement("div"),this.instance=n,this.parent=document.body,ke(this.el,{position:"fixed",top:0,left:0,right:0,bottom:0,opacity:0}),this.updateStyle(n.options),ve(this.el,"click",n.handler.clickOverlay.bind(n))},updateStyle:function(n){ke(this.el,{zIndex:n.zIndex,backgroundColor:n.bgColor,transition:"opacity\n        "+n.transitionDuration+"s\n        "+n.transitionTimingFunction})},insert:function(){this.parent.appendChild(this.el)},remove:function(){this.parent.removeChild(this.el)},fadeIn:function(){this.el.offsetWidth,this.el.style.opacity=this.instance.options.bgOpacity},fadeOut:function(){this.el.style.opacity=0}},Re="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(n){return typeof n}:function(n){return n&&"function"==typeof Symbol&&n.constructor===Symbol&&n!==Symbol.prototype?"symbol":typeof n},ze=function(){function n(n,e){for(var t=0;t<e.length;t++){var r=e[t];r.enumerable=r.enumerable||!1,r.configurable=!0,"value"in r&&(r.writable=!0),Object.defineProperty(n,r.key,r)}}return function(e,t,r){return t&&n(e.prototype,t),r&&n(e,r),e}}(),Oe=Object.assign||function(n){for(var e=1;e<arguments.length;e++){var t=arguments[e];for(var r in t)Object.prototype.hasOwnProperty.call(t,r)&&(n[r]=t[r])}return n},Me={init:function(n,e){this.el=n,this.instance=e,this.srcThumbnail=this.el.getAttribute("src"),this.srcset=this.el.getAttribute("srcset"),this.srcOriginal=xe(this.el),this.rect=this.el.getBoundingClientRect(),this.translate=null,this.scale=null,this.styleOpen=null,this.styleClose=null},zoomIn:function(){var n=this.instance.options,e=n.zIndex,t=n.enableGrab,r=n.transitionDuration,i=n.transitionTimingFunction;this.translate=this.calculateTranslate(),this.scale=this.calculateScale(),this.styleOpen={position:"relative",zIndex:e+1,cursor:t?be:he,transition:je+"\n        "+r+"s\n        "+i,transform:"translate3d("+this.translate.x+"px, "+this.translate.y+"px, 0px)\n        scale("+this.scale.x+","+this.scale.y+")",height:this.rect.height+"px",width:this.rect.width+"px"},this.el.offsetWidth,this.styleClose=ke(this.el,this.styleOpen,!0)},zoomOut:function(){this.el.offsetWidth,ke(this.el,{transform:"none"})},grab:function(n,e,t){var r=Be(),i=r.x-n,a=r.y-e;ke(this.el,{cursor:fe,transform:"translate3d(\n        "+(this.translate.x+i)+"px, "+(this.translate.y+a)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},move:function(n,e,t){var r=Be(),i=r.x-n,a=r.y-e;ke(this.el,{transition:je,transform:"translate3d(\n        "+(this.translate.x+i)+"px, "+(this.translate.y+a)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},restoreCloseStyle:function(){ke(this.el,this.styleClose)},restoreOpenStyle:function(){ke(this.el,this.styleOpen)},upgradeSource:function(){if(this.srcOriginal){var n=this.el.parentNode;this.srcset&&this.el.removeAttribute("srcset");var e=this.el.cloneNode(!1);e.setAttribute("src",this.srcOriginal),e.style.position="fixed",e.style.visibility="hidden",n.appendChild(e),setTimeout(function(){this.el.setAttribute("src",this.srcOriginal),n.removeChild(e)}.bind(this),50)}},downgradeSource:function(){this.srcOriginal&&(this.srcset&&this.el.setAttribute("srcset",this.srcset),this.el.setAttribute("src",this.srcThumbnail))},calculateTranslate:function(){var n=Be(),e=this.rect.left+this.rect.width/2,t=this.rect.top+this.rect.height/2;return{x:n.x-e,y:n.y-t}},calculateScale:function(){var n=this.el.dataset,e=n.zoomingHeight,t=n.zoomingWidth,r=this.instance.options,i=r.customSize,a=r.scaleBase;if(!i&&e&&t)return{x:t/this.rect.width,y:e/this.rect.height};if(i&&"object"===(void 0===i?"undefined":Re(i)))return{x:i.width/this.rect.width,y:i.height/this.rect.height};var s=this.rect.width/2,o=this.rect.height/2,l=Be(),c={x:l.x-s,y:l.y-o},d=c.x/s,p=c.y/o,u=a+Math.min(d,p);if(i&&"string"==typeof i){var m=t||this.el.naturalWidth,g=e||this.el.naturalHeight,h=parseFloat(i)*m/(100*this.rect.width),b=parseFloat(i)*g/(100*this.rect.height);if(u>h||u>b)return{x:h,y:b}}return{x:u,y:u}}};function Be(){var n=document.documentElement;return{x:Math.min(n.clientWidth,window.innerWidth)/2,y:Math.min(n.clientHeight,window.innerHeight)/2}}function Le(n,e,t){["mousedown","mousemove","mouseup","touchstart","touchmove","touchend"].forEach((function(r){ve(n,r,e[r],t)}))}var De=function(){function n(e){!function(n,e){if(!(n instanceof e))throw new TypeError("Cannot call a class as a function")}(this,n),this.target=Object.create(Me),this.overlay=Object.create(Ae),this.handler=Object.create(_e),this.body=document.body,this.shown=!1,this.lock=!1,this.released=!0,this.lastScrollPosition=null,this.pressTimer=null,this.options=Oe({},qe,e),this.overlay.init(this),this.handler.init(this)}return ze(n,[{key:"listen",value:function(n){if("string"==typeof n)for(var e=document.querySelectorAll(n),t=e.length;t--;)this.listen(e[t]);else"IMG"===n.tagName&&(n.style.cursor=ge,ve(n,"click",this.handler.click),this.options.preloadImage&&ye(xe(n)));return this}},{key:"config",value:function(n){return n?(Oe(this.options,n),this.overlay.updateStyle(this.options),this):this.options}},{key:"open",value:function(n){var e=this,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.options.onOpen;if(!this.shown&&!this.lock){var r="string"==typeof n?document.querySelector(n):n;if("IMG"===r.tagName){if(this.options.onBeforeOpen(r),this.target.init(r,this),!this.options.preloadImage){var i=this.target.srcOriginal;null!=i&&(this.options.onImageLoading(r),ye(i,this.options.onImageLoaded))}this.shown=!0,this.lock=!0,this.target.zoomIn(),this.overlay.insert(),this.overlay.fadeIn(),ve(document,"scroll",this.handler.scroll),ve(document,"keydown",this.handler.keydown),this.options.closeOnWindowResize&&ve(window,"resize",this.handler.resizeWindow);var a=function n(){ve(r,Ee,n,!1),e.lock=!1,e.target.upgradeSource(),e.options.enableGrab&&Le(document,e.handler,!0),t(r)};return ve(r,Ee,a),this}}}},{key:"close",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onClose;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeClose(t),this.lock=!0,this.body.style.cursor=me,this.overlay.fadeOut(),this.target.zoomOut(),ve(document,"scroll",this.handler.scroll,!1),ve(document,"keydown",this.handler.keydown,!1),this.options.closeOnWindowResize&&ve(window,"resize",this.handler.resizeWindow,!1);var r=function r(){ve(t,Ee,r,!1),n.shown=!1,n.lock=!1,n.target.downgradeSource(),n.options.enableGrab&&Le(document,n.handler,!1),n.target.restoreCloseStyle(),n.overlay.remove(),e(t)};return ve(t,Ee,r),this}}},{key:"grab",value:function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,r=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onGrab;if(this.shown&&!this.lock){var i=this.target.el;this.options.onBeforeGrab(i),this.released=!1,this.target.grab(n,e,t);var a=function n(){ve(i,Ee,n,!1),r(i)};return ve(i,Ee,a),this}}},{key:"move",value:function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,r=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onMove;if(this.shown&&!this.lock){this.released=!1,this.body.style.cursor=fe,this.target.move(n,e,t);var i=this.target.el,a=function n(){ve(i,Ee,n,!1),r(i)};return ve(i,Ee,a),this}}},{key:"release",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onRelease;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeRelease(t),this.lock=!0,this.body.style.cursor=me,this.target.restoreOpenStyle();var r=function r(){ve(t,Ee,r,!1),n.lock=!1,n.released=!0,e(t)};return ve(t,Ee,r),this}}}]),n}();const Fe=JSON.parse('{"bgColor":"rgba(0,0,0,0.6)"}'),Pe=Number("500");class Ne{constructor(){this.instance=new De(Fe)}update(n=".theme-vdoing-content img:not(.no-zoom)"){"undefined"!=typeof window&&this.instance.listen(n)}updateDelay(n=".theme-vdoing-content img:not(.no-zoom)",e=Pe){setTimeout(()=>this.update(n),e)}}var Ue=[ne,oe,de,ue,{watch:{"$page.path"(){void 0!==this.$vuepress.zooming&&this.$vuepress.zooming.updateDelay()}},mounted(){this.$vuepress.zooming=new Ne,this.$vuepress.zooming.updateDelay()}}],He={name:"GlobalLayout",computed:{layout(){const n=this.getLayout();return Object(Wn.h)("layout",n),r.a.component(n)}},methods:{getLayout(){if(this.$page.path){const n=this.$page.frontmatter.layout;return n&&(this.$vuepress.getLayoutAsyncComponent(n)||this.$vuepress.getVueComponent(n))?n:"Layout"}return"NotFound"}}},Je=t(2),$e=Object(Je.a)(He,(function(){return(0,this._self._c)(this.layout,{tag:"component"})}),[],!1,null,null,null).exports;Object(Wn.f)($e,"mixins",Ue);const Qe=[{name:"v-74ef8d6a",path:"/pages/6862b9/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-74ef8d6a").then(t)}},{path:"/pages/6862b9/index.html",redirect:"/pages/6862b9/"},{path:"/01.后端/10.Java/50.集合/10.Map基础.html",redirect:"/pages/6862b9/"},{name:"v-0db05437",path:"/pages/8b78d9/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-0db05437").then(t)}},{path:"/pages/8b78d9/index.html",redirect:"/pages/8b78d9/"},{path:"/01.后端/10.Java/50.集合/40.综合练习.html",redirect:"/pages/8b78d9/"},{name:"v-122e0ca4",path:"/pages/9f2f57/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-122e0ca4").then(t)}},{path:"/pages/9f2f57/index.html",redirect:"/pages/9f2f57/"},{path:"/01.后端/10.Java/50.集合/30.可变参数.html",redirect:"/pages/9f2f57/"},{name:"v-cf325208",path:"/pages/bb8a3e/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-cf325208").then(t)}},{path:"/pages/bb8a3e/index.html",redirect:"/pages/bb8a3e/"},{path:"/01.后端/10.Java/50.集合/25.TreeMap.html",redirect:"/pages/bb8a3e/"},{name:"v-9db0ef34",path:"/pages/cfbd66/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-9db0ef34").then(t)}},{path:"/pages/cfbd66/index.html",redirect:"/pages/cfbd66/"},{path:"/01.后端/10.Java/50.集合/20.HashMap.html",redirect:"/pages/cfbd66/"},{name:"v-166a22fe",path:"/pages/d8ef9d/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-166a22fe").then(t)}},{path:"/pages/d8ef9d/index.html",redirect:"/pages/d8ef9d/"},{path:"/01.后端/10.Java/50.集合/60.不可变集合.html",redirect:"/pages/d8ef9d/"},{name:"v-a76b5b9a",path:"/pages/5208ee/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-a76b5b9a").then(t)}},{path:"/pages/5208ee/index.html",redirect:"/pages/5208ee/"},{path:"/01.后端/10.Java/50.集合/50.斗地主练习.html",redirect:"/pages/5208ee/"},{name:"v-2a8eeee9",path:"/pages/eeb1b8/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-2a8eeee9").then(t)}},{path:"/pages/eeb1b8/index.html",redirect:"/pages/eeb1b8/"},{path:"/01.后端/10.Java/50.集合/80.方法引用.html",redirect:"/pages/eeb1b8/"},{name:"v-1c9e69dc",path:"/pages/2ff7b9/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-1c9e69dc").then(t)}},{path:"/pages/2ff7b9/index.html",redirect:"/pages/2ff7b9/"},{path:"/01.后端/10.Java/60.流/05.异常.html",redirect:"/pages/2ff7b9/"},{name:"v-539d06af",path:"/pages/bc84f5/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-539d06af").then(t)}},{path:"/pages/bc84f5/index.html",redirect:"/pages/bc84f5/"},{path:"/01.后端/10.Java/50.集合/70.stream流.html",redirect:"/pages/bc84f5/"},{name:"v-a41da114",path:"/pages/cb6708/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-a41da114").then(t)}},{path:"/pages/cb6708/index.html",redirect:"/pages/cb6708/"},{path:"/01.后端/10.Java/60.流/15.File综合练习.html",redirect:"/pages/cb6708/"},{name:"v-75b1c1bd",path:"/pages/4fe65a/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-75b1c1bd").then(t)}},{path:"/pages/4fe65a/index.html",redirect:"/pages/4fe65a/"},{path:"/01.后端/10.Java/60.流/20.IO流概述.html",redirect:"/pages/4fe65a/"},{name:"v-6ab9acf7",path:"/pages/217f27/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-6ab9acf7").then(t)}},{path:"/pages/217f27/index.html",redirect:"/pages/217f27/"},{path:"/01.后端/10.Java/60.流/10.File基础.html",redirect:"/pages/217f27/"},{name:"v-b72fe238",path:"/pages/674de2/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-b72fe238").then(t)}},{path:"/pages/674de2/index.html",redirect:"/pages/674de2/"},{path:"/01.后端/10.Java/60.流/25.字节流.html",redirect:"/pages/674de2/"},{name:"v-4515ebe1",path:"/pages/f0c1fd/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-4515ebe1").then(t)}},{path:"/pages/f0c1fd/index.html",redirect:"/pages/f0c1fd/"},{path:"/01.后端/10.Java/60.流/30.字符流.html",redirect:"/pages/f0c1fd/"},{name:"v-00877d3f",path:"/pages/6e842a/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-00877d3f").then(t)}},{path:"/pages/6e842a/index.html",redirect:"/pages/6e842a/"},{path:"/01.后端/10.Java/60.流/40.IO流综合练习.html",redirect:"/pages/6e842a/"},{name:"v-3ab071ee",path:"/pages/bce7f1/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-3ab071ee").then(t)}},{path:"/pages/bce7f1/index.html",redirect:"/pages/bce7f1/"},{path:"/01.后端/10.Java/60.流/45.缓冲流.html",redirect:"/pages/bce7f1/"},{name:"v-751012fa",path:"/pages/d2a882/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-751012fa").then(t)}},{path:"/pages/d2a882/index.html",redirect:"/pages/d2a882/"},{path:"/01.后端/10.Java/60.流/47.转换流.html",redirect:"/pages/d2a882/"},{name:"v-9882acb6",path:"/pages/2ca28f/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-9882acb6").then(t)}},{path:"/pages/2ca28f/index.html",redirect:"/pages/2ca28f/"},{path:"/01.后端/10.Java/60.流/35.IO流异常.html",redirect:"/pages/2ca28f/"},{name:"v-05b14bdd",path:"/pages/57821f/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-05b14bdd").then(t)}},{path:"/pages/57821f/index.html",redirect:"/pages/57821f/"},{path:"/01.后端/10.Java/60.流/49.序列流.html",redirect:"/pages/57821f/"},{name:"v-5d250c21",path:"/pages/a9cd19/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-5d250c21").then(t)}},{path:"/pages/a9cd19/index.html",redirect:"/pages/a9cd19/"},{path:"/01.后端/10.Java/60.流/51.打印流.html",redirect:"/pages/a9cd19/"},{name:"v-d4cda7d8",path:"/pages/6201c3/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-d4cda7d8").then(t)}},{path:"/pages/6201c3/index.html",redirect:"/pages/6201c3/"},{path:"/01.后端/10.Java/60.流/53.压缩流.html",redirect:"/pages/6201c3/"},{name:"v-0186e5b2",path:"/back/java/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-0186e5b2").then(t)}},{path:"/back/java/index.html",redirect:"/back/java/"},{path:"/01.后端/10.Java/",redirect:"/back/java/"},{name:"v-2e0b9ea9",path:"/pages/3b2939/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-2e0b9ea9").then(t)}},{path:"/pages/3b2939/index.html",redirect:"/pages/3b2939/"},{path:"/01.后端/20.JavaWeb/15.Listener.html",redirect:"/pages/3b2939/"},{name:"v-552a3e17",path:"/pages/7a0933/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-552a3e17").then(t)}},{path:"/pages/7a0933/index.html",redirect:"/pages/7a0933/"},{path:"/01.后端/20.JavaWeb/10.Filter.html",redirect:"/pages/7a0933/"},{name:"v-66b680f6",path:"/pages/36c3d1/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-66b680f6").then(t)}},{path:"/pages/36c3d1/index.html",redirect:"/pages/36c3d1/"},{path:"/01.后端/20.JavaWeb/20.Ajax.html",redirect:"/pages/36c3d1/"},{name:"v-ce7d277a",path:"/pages/140b55/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-ce7d277a").then(t)}},{path:"/pages/140b55/index.html",redirect:"/pages/140b55/"},{path:"/01.后端/20.JavaWeb/25.Axios.html",redirect:"/pages/140b55/"},{name:"v-29dae873",path:"/pages/63836e/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-29dae873").then(t)}},{path:"/pages/63836e/index.html",redirect:"/pages/63836e/"},{path:"/01.后端/20.JavaWeb/30.JSON.html",redirect:"/pages/63836e/"},{name:"v-f09d403a",path:"/pages/7250e2/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-f09d403a").then(t)}},{path:"/pages/7250e2/index.html",redirect:"/pages/7250e2/"},{path:"/01.后端/20.JavaWeb/40.VUE.html",redirect:"/pages/7250e2/"},{name:"v-17c18643",path:"/pages/4f9642/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-17c18643").then(t)}},{path:"/pages/4f9642/index.html",redirect:"/pages/4f9642/"},{path:"/01.后端/20.JavaWeb/35.JSP.html",redirect:"/pages/4f9642/"},{name:"v-80ad8e7a",path:"/pages/f1ca40/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-80ad8e7a").then(t)}},{path:"/pages/f1ca40/index.html",redirect:"/pages/f1ca40/"},{path:"/01.后端/20.JavaWeb/45.Element.html",redirect:"/pages/f1ca40/"},{name:"v-4c45192e",path:"/pages/4f0756/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-4c45192e").then(t)}},{path:"/pages/4f0756/index.html",redirect:"/pages/4f0756/"},{path:"/01.后端/20.JavaWeb/55.用户登录注册案例.html",redirect:"/pages/4f0756/"},{name:"v-3d891436",path:"/pages/732670/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-3d891436").then(t)}},{path:"/pages/732670/index.html",redirect:"/pages/732670/"},{path:"/01.后端/10.Java/60.流/70.工具包.html",redirect:"/pages/732670/"},{name:"v-8d62e9e4",path:"/pages/e00b6b/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-8d62e9e4").then(t)}},{path:"/pages/e00b6b/index.html",redirect:"/pages/e00b6b/"},{path:"/01.后端/30.数据库/01.MySQL/05.函数.html",redirect:"/pages/e00b6b/"},{name:"v-0d97dac8",path:"/back/javaWeb/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-0d97dac8").then(t)}},{path:"/back/javaWeb/index.html",redirect:"/back/javaWeb/"},{path:"/01.后端/20.JavaWeb/",redirect:"/back/javaWeb/"},{name:"v-3f96471e",path:"/pages/a450d7/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-3f96471e").then(t)}},{path:"/pages/a450d7/index.html",redirect:"/pages/a450d7/"},{path:"/01.后端/20.JavaWeb/50.会话技术.html",redirect:"/pages/a450d7/"},{name:"v-edd93478",path:"/pages/05f9b5/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-edd93478").then(t)}},{path:"/pages/05f9b5/index.html",redirect:"/pages/05f9b5/"},{path:"/01.后端/30.数据库/01.MySQL/20.事务.html",redirect:"/pages/05f9b5/"},{name:"v-3bed72fe",path:"/pages/2d9387/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-3bed72fe").then(t)}},{path:"/pages/2d9387/index.html",redirect:"/pages/2d9387/"},{path:"/01.后端/30.数据库/05.Redis/101.单机安装Redis.html",redirect:"/pages/2d9387/"},{name:"v-420ed683",path:"/pages/31e22b/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-420ed683").then(t)}},{path:"/pages/31e22b/index.html",redirect:"/pages/31e22b/"},{path:"/01.后端/30.数据库/05.Redis/05.Redis基础.html",redirect:"/pages/31e22b/"},{name:"v-4382d8a8",path:"/pages/e5c957/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-4382d8a8").then(t)}},{path:"/pages/e5c957/index.html",redirect:"/pages/e5c957/"},{path:"/01.后端/30.数据库/01.MySQL/01.SQL语句.html",redirect:"/pages/e5c957/"},{name:"v-11f9a58f",path:"/pages/948263/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-11f9a58f").then(t)}},{path:"/pages/948263/index.html",redirect:"/pages/948263/"},{path:"/01.后端/30.数据库/05.Redis/10.Redis常见命令.html",redirect:"/pages/948263/"},{name:"v-35eddcf9",path:"/pages/ab672a/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-35eddcf9").then(t)}},{path:"/pages/ab672a/index.html",redirect:"/pages/ab672a/"},{path:"/01.后端/30.数据库/01.MySQL/10.约束.html",redirect:"/pages/ab672a/"},{name:"v-02f207c3",path:"/pages/564491/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-02f207c3").then(t)}},{path:"/pages/564491/index.html",redirect:"/pages/564491/"},{path:"/01.后端/30.数据库/05.Redis/102.主从集群搭建.html",redirect:"/pages/564491/"},{name:"v-360acd55",path:"/pages/ca02f2/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-360acd55").then(t)}},{path:"/pages/ca02f2/index.html",redirect:"/pages/ca02f2/"},{path:"/01.后端/30.数据库/05.Redis/103.哨兵集群搭建.html",redirect:"/pages/ca02f2/"},{name:"v-3bfd7f0c",path:"/pages/7c7a2d/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-3bfd7f0c").then(t)}},{path:"/pages/7c7a2d/index.html",redirect:"/pages/7c7a2d/"},{path:"/01.后端/30.数据库/05.Redis/23.Redis持久化.html",redirect:"/pages/7c7a2d/"},{name:"v-3bf8a54c",path:"/pages/fdb88d/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-3bf8a54c").then(t)}},{path:"/pages/fdb88d/index.html",redirect:"/pages/fdb88d/"},{path:"/01.后端/30.数据库/05.Redis/104.分片集群搭建.html",redirect:"/pages/fdb88d/"},{name:"v-a0cd1b6e",path:"/pages/94a219/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-a0cd1b6e").then(t)}},{path:"/pages/94a219/index.html",redirect:"/pages/94a219/"},{path:"/01.后端/30.数据库/05.Redis/15.Redis-API.html",redirect:"/pages/94a219/"},{name:"v-6229bca2",path:"/pages/ef6419/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-6229bca2").then(t)}},{path:"/pages/ef6419/index.html",redirect:"/pages/ef6419/"},{path:"/01.后端/30.数据库/05.Redis/25.Redis主从.html",redirect:"/pages/ef6419/"},{name:"v-6daf2e3a",path:"/pages/a367f0/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-6daf2e3a").then(t)}},{path:"/pages/a367f0/index.html",redirect:"/pages/a367f0/"},{path:"/01.后端/30.数据库/01.MySQL/15.多表查询.html",redirect:"/pages/a367f0/"},{name:"v-1fa3135c",path:"/pages/444956/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-1fa3135c").then(t)}},{path:"/pages/444956/index.html",redirect:"/pages/444956/"},{path:"/01.后端/30.数据库/05.Redis/20.Redis优化.html",redirect:"/pages/444956/"},{name:"v-3632f31f",path:"/pages/981287/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-3632f31f").then(t)}},{path:"/pages/981287/index.html",redirect:"/pages/981287/"},{path:"/01.后端/30.数据库/05.Redis/35.分布式缓存总结.html",redirect:"/pages/981287/"},{name:"v-c740fd8a",path:"/pages/74c463/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-c740fd8a").then(t)}},{path:"/pages/74c463/index.html",redirect:"/pages/74c463/"},{path:"/01.后端/30.数据库/05.Redis/27.Redis哨兵.html",redirect:"/pages/74c463/"},{name:"v-b33a99d2",path:"/pages/f86562/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-b33a99d2").then(t)}},{path:"/pages/f86562/index.html",redirect:"/pages/f86562/"},{path:"/01.后端/30.数据库/05.Redis/30.Redis分片集群.html",redirect:"/pages/f86562/"},{name:"v-351e657c",path:"/pages/11137c/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-351e657c").then(t)}},{path:"/pages/11137c/index.html",redirect:"/pages/11137c/"},{path:"/01.后端/30.数据库/05.Redis/45.Redis网络模型.html",redirect:"/pages/11137c/"},{name:"v-c3bd5d9a",path:"/back/sql/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-c3bd5d9a").then(t)}},{path:"/back/sql/index.html",redirect:"/back/sql/"},{path:"/01.后端/30.数据库/",redirect:"/back/sql/"},{name:"v-701ec247",path:"/pages/b2f859/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-701ec247").then(t)}},{path:"/pages/b2f859/index.html",redirect:"/pages/b2f859/"},{path:"/01.后端/40.SSM/05.Spring.html",redirect:"/pages/b2f859/"},{name:"v-4ae15768",path:"/pages/11b084/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-4ae15768").then(t)}},{path:"/pages/11b084/index.html",redirect:"/pages/11b084/"},{path:"/01.后端/30.数据库/05.Redis/55.Redis内存回收.html",redirect:"/pages/11b084/"},{name:"v-39774543",path:"/pages/1d17ee/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-39774543").then(t)}},{path:"/pages/1d17ee/index.html",redirect:"/pages/1d17ee/"},{path:"/01.后端/40.SSM/10.SpringMVC-XML.html",redirect:"/pages/1d17ee/"},{name:"v-13f7fe09",path:"/pages/a0085d/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-13f7fe09").then(t)}},{path:"/pages/a0085d/index.html",redirect:"/pages/a0085d/"},{path:"/01.后端/30.数据库/05.Redis/40.Redis数据结构.html",redirect:"/pages/a0085d/"},{name:"v-a024d1dc",path:"/pages/f036b6/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-a024d1dc").then(t)}},{path:"/pages/f036b6/index.html",redirect:"/pages/f036b6/"},{path:"/01.后端/40.SSM/25.SSM 整合-XML方式.html",redirect:"/pages/f036b6/"},{name:"v-3934d917",path:"/pages/638026/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-3934d917").then(t)}},{path:"/pages/638026/index.html",redirect:"/pages/638026/"},{path:"/01.后端/30.数据库/05.Redis/50.Redis通信协议.html",redirect:"/pages/638026/"},{name:"v-0b5ac23a",path:"/pages/3c9c2a/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-0b5ac23a").then(t)}},{path:"/pages/3c9c2a/index.html",redirect:"/pages/3c9c2a/"},{path:"/01.后端/40.SSM/20.MyBatisPlus.html",redirect:"/pages/3c9c2a/"},{name:"v-3a6aa74c",path:"/pages/ee4b7e/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-3a6aa74c").then(t)}},{path:"/pages/ee4b7e/index.html",redirect:"/pages/ee4b7e/"},{path:"/01.后端/40.SSM/15.SpringMVC-注解.html",redirect:"/pages/ee4b7e/"},{name:"v-4b3039e8",path:"/back/SSM/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-4b3039e8").then(t)}},{path:"/back/SSM/index.html",redirect:"/back/SSM/"},{path:"/01.后端/40.SSM/",redirect:"/back/SSM/"},{name:"v-726095f8",path:"/pages/e67134/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-726095f8").then(t)}},{path:"/pages/e67134/index.html",redirect:"/pages/e67134/"},{path:"/01.后端/40.SSM/30.SSM整合-注解.html",redirect:"/pages/e67134/"},{name:"v-6a2f133e",path:"/pages/164be2/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-6a2f133e").then(t)}},{path:"/pages/164be2/index.html",redirect:"/pages/164be2/"},{path:"/01.后端/50.SpringBoot/10.开发实用篇.html",redirect:"/pages/164be2/"},{name:"v-3df805f8",path:"/pages/0446de/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-3df805f8").then(t)}},{path:"/pages/0446de/index.html",redirect:"/pages/0446de/"},{path:"/01.后端/50.SpringBoot/15.数据层解决方案.html",redirect:"/pages/0446de/"},{name:"v-057fb4a2",path:"/pages/d74c9d/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-057fb4a2").then(t)}},{path:"/pages/d74c9d/index.html",redirect:"/pages/d74c9d/"},{path:"/01.后端/50.SpringBoot/05.基础篇.html",redirect:"/pages/d74c9d/"},{name:"v-e327c356",path:"/pages/cd183e/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-e327c356").then(t)}},{path:"/pages/cd183e/index.html",redirect:"/pages/cd183e/"},{path:"/01.后端/50.SpringBoot/50.原理篇.html",redirect:"/pages/cd183e/"},{name:"v-5c23f220",path:"/back/SpringBoot/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-5c23f220").then(t)}},{path:"/back/SpringBoot/index.html",redirect:"/back/SpringBoot/"},{path:"/01.后端/50.SpringBoot/",redirect:"/back/SpringBoot/"},{name:"v-6e2eff3a",path:"/pages/04da56/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-6e2eff3a").then(t)}},{path:"/pages/04da56/index.html",redirect:"/pages/04da56/"},{path:"/01.后端/60.微服务/10.Docker/05.安装Docker.html",redirect:"/pages/04da56/"},{name:"v-626f8d1c",path:"/pages/e76fff/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-626f8d1c").then(t)}},{path:"/pages/e76fff/index.html",redirect:"/pages/e76fff/"},{path:"/01.后端/50.SpringBoot/20.第三方技术整合.html",redirect:"/pages/e76fff/"},{name:"v-61a116d1",path:"/pages/7624a9/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-61a116d1").then(t)}},{path:"/pages/7624a9/index.html",redirect:"/pages/7624a9/"},{path:"/01.后端/60.微服务/20.ES/15.文档操作.html",redirect:"/pages/7624a9/"},{name:"v-427aa158",path:"/pages/e9e22c/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-427aa158").then(t)}},{path:"/pages/e9e22c/index.html",redirect:"/pages/e9e22c/"},{path:"/01.后端/60.微服务/20.ES/05.ES基础.html",redirect:"/pages/e9e22c/"},{name:"v-56deed3f",path:"/pages/06e2e5/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-56deed3f").then(t)}},{path:"/pages/06e2e5/index.html",redirect:"/pages/06e2e5/"},{path:"/01.后端/60.微服务/20.ES/100.es操作文档.html",redirect:"/pages/06e2e5/"},{name:"v-2ba678ac",path:"/pages/247ef5/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-2ba678ac").then(t)}},{path:"/pages/247ef5/index.html",redirect:"/pages/247ef5/"},{path:"/01.后端/60.微服务/10.Docker/10.Docker实用篇.html",redirect:"/pages/247ef5/"},{name:"v-8f19b34e",path:"/pages/7bb700/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-8f19b34e").then(t)}},{path:"/pages/7bb700/index.html",redirect:"/pages/7bb700/"},{path:"/01.后端/60.微服务/20.ES/10.索引.html",redirect:"/pages/7bb700/"},{name:"v-17caaee6",path:"/pages/dac0d9/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-17caaee6").then(t)}},{path:"/pages/dac0d9/index.html",redirect:"/pages/dac0d9/"},{path:"/01.后端/60.微服务/20.ES/20.操作API.html",redirect:"/pages/dac0d9/"},{name:"v-243fb89a",path:"/pages/5b74f8/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-243fb89a").then(t)}},{path:"/pages/5b74f8/index.html",redirect:"/pages/5b74f8/"},{path:"/01.后端/60.微服务/20.ES/30.案例练习.html",redirect:"/pages/5b74f8/"},{name:"v-d2252dc2",path:"/pages/bdbadd/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-d2252dc2").then(t)}},{path:"/pages/bdbadd/index.html",redirect:"/pages/bdbadd/"},{path:"/01.后端/60.微服务/30.RabbitMQ/10.RabbitMQ入门.html",redirect:"/pages/bdbadd/"},{name:"v-508e8e26",path:"/pages/0af5eb/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-508e8e26").then(t)}},{path:"/pages/0af5eb/index.html",redirect:"/pages/0af5eb/"},{path:"/01.后端/60.微服务/30.RabbitMQ/05.RabbitMQ介绍.html",redirect:"/pages/0af5eb/"},{name:"v-06f84544",path:"/pages/18dc29/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-06f84544").then(t)}},{path:"/pages/18dc29/index.html",redirect:"/pages/18dc29/"},{path:"/01.后端/60.微服务/20.ES/35.ES进阶.html",redirect:"/pages/18dc29/"},{name:"v-29cee25f",path:"/pages/a93137/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-29cee25f").then(t)}},{path:"/pages/a93137/index.html",redirect:"/pages/a93137/"},{path:"/01.后端/60.微服务/20.ES/25.文档搜索.html",redirect:"/pages/a93137/"},{name:"v-413ff23e",path:"/pages/4b3e75/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-413ff23e").then(t)}},{path:"/pages/4b3e75/index.html",redirect:"/pages/4b3e75/"},{path:"/01.后端/60.微服务/30.RabbitMQ/15.SpringAMQP.html",redirect:"/pages/4b3e75/"},{name:"v-41d84bb8",path:"/pages/49c1ac/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-41d84bb8").then(t)}},{path:"/pages/49c1ac/index.html",redirect:"/pages/49c1ac/"},{path:"/01.后端/60.微服务/40.SpringCloud/05.认识微服务.html",redirect:"/pages/49c1ac/"},{name:"v-3d750c14",path:"/pages/1b3d4d/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-3d750c14").then(t)}},{path:"/pages/1b3d4d/index.html",redirect:"/pages/1b3d4d/"},{path:"/01.后端/60.微服务/40.SpringCloud/10.服务拆分和远程调用.html",redirect:"/pages/1b3d4d/"},{name:"v-07eaf4f2",path:"/pages/5546ef/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-07eaf4f2").then(t)}},{path:"/pages/5546ef/index.html",redirect:"/pages/5546ef/"},{path:"/01.后端/60.微服务/40.SpringCloud/30.Ribbon负载均衡.html",redirect:"/pages/5546ef/"},{name:"v-fb5f7d52",path:"/pages/938522/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-fb5f7d52").then(t)}},{path:"/pages/938522/index.html",redirect:"/pages/938522/"},{path:"/01.后端/60.微服务/40.SpringCloud/20.Eureka注册中心.html",redirect:"/pages/938522/"},{name:"v-ba455d3c",path:"/pages/ffc82f/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-ba455d3c").then(t)}},{path:"/pages/ffc82f/index.html",redirect:"/pages/ffc82f/"},{path:"/01.后端/60.微服务/40.SpringCloud/50.Nacos配置管理.html",redirect:"/pages/ffc82f/"},{name:"v-31baad0c",path:"/pages/bac98f/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-31baad0c").then(t)}},{path:"/pages/bac98f/index.html",redirect:"/pages/bac98f/"},{path:"/01.后端/60.微服务/30.RabbitMQ/20.RabbitMQ进阶.html",redirect:"/pages/bac98f/"},{name:"v-4000ac32",path:"/pages/9b099d/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-4000ac32").then(t)}},{path:"/pages/9b099d/index.html",redirect:"/pages/9b099d/"},{path:"/01.后端/60.微服务/40.SpringCloud/40.Nacos注册中心.html",redirect:"/pages/9b099d/"},{name:"v-06d0c40e",path:"/pages/314ef0/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-06d0c40e").then(t)}},{path:"/pages/314ef0/index.html",redirect:"/pages/314ef0/"},{path:"/01.后端/60.微服务/40.SpringCloud/60.Feign远程调用.html",redirect:"/pages/314ef0/"},{name:"v-4ae8fe0c",path:"/pages/267113/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-4ae8fe0c").then(t)}},{path:"/pages/267113/index.html",redirect:"/pages/267113/"},{path:"/01.后端/60.微服务/50.多级缓存/05.多级缓存.html",redirect:"/pages/267113/"},{name:"v-3a15b156",path:"/pages/1725f1/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-3a15b156").then(t)}},{path:"/pages/1725f1/index.html",redirect:"/pages/1725f1/"},{path:"/01.后端/60.微服务/50.多级缓存/15.Lua语法入门.html",redirect:"/pages/1725f1/"},{name:"v-2fa7be82",path:"/pages/dc166e/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-2fa7be82").then(t)}},{path:"/pages/dc166e/index.html",redirect:"/pages/dc166e/"},{path:"/01.后端/60.微服务/50.多级缓存/10.JVM进程缓存.html",redirect:"/pages/dc166e/"},{name:"v-3ba4cd3c",path:"/pages/89fc28/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-3ba4cd3c").then(t)}},{path:"/pages/89fc28/index.html",redirect:"/pages/89fc28/"},{path:"/01.后端/60.微服务/40.SpringCloud/70.Gateway服务网关.html",redirect:"/pages/89fc28/"},{name:"v-af4ce9ea",path:"/pages/8ffa15/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-af4ce9ea").then(t)}},{path:"/pages/8ffa15/index.html",redirect:"/pages/8ffa15/"},{path:"/01.后端/60.微服务/60.分布式事务/10.理论基础.html",redirect:"/pages/8ffa15/"},{name:"v-7c74bfe6",path:"/pages/52d2bc/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-7c74bfe6").then(t)}},{path:"/pages/52d2bc/index.html",redirect:"/pages/52d2bc/"},{path:"/01.后端/60.微服务/50.多级缓存/100.多级缓存总结.html",redirect:"/pages/52d2bc/"},{name:"v-b54bf03c",path:"/pages/d48085/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-b54bf03c").then(t)}},{path:"/pages/d48085/index.html",redirect:"/pages/d48085/"},{path:"/01.后端/60.微服务/60.分布式事务/05.分布式事务问题.html",redirect:"/pages/d48085/"},{name:"v-3f4db490",path:"/pages/92ac01/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-3f4db490").then(t)}},{path:"/pages/92ac01/index.html",redirect:"/pages/92ac01/"},{path:"/01.后端/60.微服务/50.多级缓存/25.缓存同步.html",redirect:"/pages/92ac01/"},{name:"v-4fafeb1f",path:"/pages/387b25/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-4fafeb1f").then(t)}},{path:"/pages/387b25/index.html",redirect:"/pages/387b25/"},{path:"/01.后端/60.微服务/50.多级缓存/20.实现多级缓存.html",redirect:"/pages/387b25/"},{name:"v-716c9de8",path:"/pages/f26c95/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-716c9de8").then(t)}},{path:"/pages/f26c95/index.html",redirect:"/pages/f26c95/"},{path:"/01.后端/60.微服务/60.分布式事务/20.Seata基础.html",redirect:"/pages/f26c95/"},{name:"v-18ec41e8",path:"/pages/2bcba9/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-18ec41e8").then(t)}},{path:"/pages/2bcba9/index.html",redirect:"/pages/2bcba9/"},{path:"/01.后端/60.微服务/60.分布式事务/25.Seata事务模式.html",redirect:"/pages/2bcba9/"},{name:"v-02b6dc42",path:"/pages/0f114c/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-02b6dc42").then(t)}},{path:"/pages/0f114c/index.html",redirect:"/pages/0f114c/"},{path:"/01.后端/60.微服务/70.微服务保护/20.流量控制.html",redirect:"/pages/0f114c/"},{name:"v-11e6d886",path:"/pages/6e3b45/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-11e6d886").then(t)}},{path:"/pages/6e3b45/index.html",redirect:"/pages/6e3b45/"},{path:"/01.后端/60.微服务/70.微服务保护/05.Sentinel基础.html",redirect:"/pages/6e3b45/"},{name:"v-2acc9b91",path:"/pages/b25c7c/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-2acc9b91").then(t)}},{path:"/pages/b25c7c/index.html",redirect:"/pages/b25c7c/"},{path:"/01.后端/60.微服务/70.微服务保护/30.隔离和降级.html",redirect:"/pages/b25c7c/"},{name:"v-26d66c54",path:"/pages/d7aaea/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-26d66c54").then(t)}},{path:"/pages/d7aaea/index.html",redirect:"/pages/d7aaea/"},{path:"/01.后端/60.微服务/70.微服务保护/40.授权规则.html",redirect:"/pages/d7aaea/"},{name:"v-03a351e0",path:"/back/Microservice/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-03a351e0").then(t)}},{path:"/back/Microservice/index.html",redirect:"/back/Microservice/"},{path:"/01.后端/60.微服务/",redirect:"/back/Microservice/"},{name:"v-2803fd6e",path:"/Java-C/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-2803fd6e").then(t)}},{path:"/Java-C/index.html",redirect:"/Java-C/"},{path:"/01.后端/",redirect:"/Java-C/"},{name:"v-69c2fec1",path:"/pages/17f632/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-69c2fec1").then(t)}},{path:"/pages/17f632/index.html",redirect:"/pages/17f632/"},{path:"/02.算法/05.算法基础/05.数据结构与算法.html",redirect:"/pages/17f632/"},{name:"v-45672518",path:"/pages/552b44/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-45672518").then(t)}},{path:"/pages/552b44/index.html",redirect:"/pages/552b44/"},{path:"/02.算法/10.数组/05.数组.html",redirect:"/pages/552b44/"},{name:"v-87bd982c",path:"/pages/f85993/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-87bd982c").then(t)}},{path:"/pages/f85993/index.html",redirect:"/pages/f85993/"},{path:"/02.算法/05.算法基础/07.时空间复杂度.html",redirect:"/pages/f85993/"},{name:"v-2e0d1200",path:"/pages/1d3d1a/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-2e0d1200").then(t)}},{path:"/pages/1d3d1a/index.html",redirect:"/pages/1d3d1a/"},{path:"/02.算法/15.链表/05.链表.html",redirect:"/pages/1d3d1a/"},{name:"v-16ecaa80",path:"/pages/abbe04/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-16ecaa80").then(t)}},{path:"/pages/abbe04/index.html",redirect:"/pages/abbe04/"},{path:"/02.算法/30.堆/05.堆.html",redirect:"/pages/abbe04/"},{name:"v-6becdf80",path:"/pages/bfea35/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-6becdf80").then(t)}},{path:"/pages/bfea35/index.html",redirect:"/pages/bfea35/"},{path:"/02.算法/20.栈/05.栈.html",redirect:"/pages/bfea35/"},{name:"v-baa1c044",path:"/pages/7b2a41/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-baa1c044").then(t)}},{path:"/pages/7b2a41/index.html",redirect:"/pages/7b2a41/"},{path:"/02.算法/25.队列/05.队列.html",redirect:"/pages/7b2a41/"},{name:"v-6b5af5a0",path:"/pages/6215d9/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-6b5af5a0").then(t)}},{path:"/pages/6215d9/index.html",redirect:"/pages/6215d9/"},{path:"/02.算法/35.散列表/05.散列表.html",redirect:"/pages/6215d9/"},{name:"v-1b852971",path:"/pages/9227a5/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-1b852971").then(t)}},{path:"/pages/9227a5/index.html",redirect:"/pages/9227a5/"},{path:"/02.算法/35.散列表/10.哈希算法.html",redirect:"/pages/9227a5/"},{name:"v-b34a9bcc",path:"/pages/e28ee4/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-b34a9bcc").then(t)}},{path:"/pages/e28ee4/index.html",redirect:"/pages/e28ee4/"},{path:"/02.算法/38.跳表/05.跳表.html",redirect:"/pages/e28ee4/"},{name:"v-b94aee00",path:"/pages/658fbb/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-b94aee00").then(t)}},{path:"/pages/658fbb/index.html",redirect:"/pages/658fbb/"},{path:"/02.算法/40.字符串/05.字符串.html",redirect:"/pages/658fbb/"},{name:"v-43f967eb",path:"/pages/4d7338/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-43f967eb").then(t)}},{path:"/pages/4d7338/index.html",redirect:"/pages/4d7338/"},{path:"/02.算法/45.树/05.二叉树.html",redirect:"/pages/4d7338/"},{name:"v-0774325c",path:"/pages/60a4a5/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-0774325c").then(t)}},{path:"/pages/60a4a5/index.html",redirect:"/pages/60a4a5/"},{path:"/02.算法/60.排序/05.排序.html",redirect:"/pages/60a4a5/"},{name:"v-383faa3a",path:"/pages/1b322d/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-383faa3a").then(t)}},{path:"/pages/1b322d/index.html",redirect:"/pages/1b322d/"},{path:"/02.算法/75.贪心/01.贪心算法.html",redirect:"/pages/1b322d/"},{name:"v-611bc590",path:"/pages/4fe23f/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-611bc590").then(t)}},{path:"/pages/4fe23f/index.html",redirect:"/pages/4fe23f/"},{path:"/02.算法/70.二分查找/05.二分查找.html",redirect:"/pages/4fe23f/"},{name:"v-22994320",path:"/pages/dacecb/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-22994320").then(t)}},{path:"/pages/dacecb/index.html",redirect:"/pages/dacecb/"},{path:"/02.算法/50.图/05.图.html",redirect:"/pages/dacecb/"},{name:"v-6b145c20",path:"/pages/ec0ce8/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-6b145c20").then(t)}},{path:"/pages/ec0ce8/index.html",redirect:"/pages/ec0ce8/"},{path:"/02.算法/80.回溯法/01.回溯.html",redirect:"/pages/ec0ce8/"},{name:"v-2275b9f7",path:"/pages/1ee87b/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-2275b9f7").then(t)}},{path:"/pages/1ee87b/index.html",redirect:"/pages/1ee87b/"},{path:"/02.算法/80.回溯法/50.递归.html",redirect:"/pages/1ee87b/"},{name:"v-5aae938c",path:"/pages/f7cec8/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-5aae938c").then(t)}},{path:"/pages/f7cec8/index.html",redirect:"/pages/f7cec8/"},{path:"/02.算法/90.动态规划/01.动态规划.html",redirect:"/pages/f7cec8/"},{name:"v-c5de5724",path:"/Algorithm/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-c5de5724").then(t)}},{path:"/Algorithm/index.html",redirect:"/Algorithm/"},{path:"/02.算法/",redirect:"/Algorithm/"},{name:"v-bdb21a50",path:"/pages/5b1763/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-bdb21a50").then(t)}},{path:"/pages/5b1763/index.html",redirect:"/pages/5b1763/"},{path:"/02.算法/95.分治/01.分治算法.html",redirect:"/pages/5b1763/"},{name:"v-423b419f",path:"/pages/b02b7f/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-423b419f").then(t)}},{path:"/pages/b02b7f/index.html",redirect:"/pages/b02b7f/"},{path:"/03.技术/01.Git/05.Git 概述.html",redirect:"/pages/b02b7f/"},{name:"v-3bea15f6",path:"/technology/Git/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-3bea15f6").then(t)}},{path:"/technology/Git/index.html",redirect:"/technology/Git/"},{path:"/03.技术/01.Git/",redirect:"/technology/Git/"},{name:"v-4946107a",path:"/pages/91f9c0/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-4946107a").then(t)}},{path:"/pages/91f9c0/index.html",redirect:"/pages/91f9c0/"},{path:"/03.技术/01.Git/10.Git基础.html",redirect:"/pages/91f9c0/"},{name:"v-4e60c2d6",path:"/technology/Linux/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-4e60c2d6").then(t)}},{path:"/technology/Linux/index.html",redirect:"/technology/Linux/"},{path:"/03.技术/02.Linux/",redirect:"/technology/Linux/"},{name:"v-3c2c77d5",path:"/pages/549714/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-3c2c77d5").then(t)}},{path:"/pages/549714/index.html",redirect:"/pages/549714/"},{path:"/03.技术/02.Linux/20.vim.html",redirect:"/pages/549714/"},{name:"v-51430cb6",path:"/pages/343c9a/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-51430cb6").then(t)}},{path:"/pages/343c9a/index.html",redirect:"/pages/343c9a/"},{path:"/03.技术/02.Linux/50.Linux面试题.html",redirect:"/pages/343c9a/"},{name:"v-2b2717ba",path:"/technology/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-2b2717ba").then(t)}},{path:"/technology/index.html",redirect:"/technology/"},{path:"/03.技术/",redirect:"/technology/"},{name:"v-9044c656",path:"/pages/b5d182/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-9044c656").then(t)}},{path:"/pages/b5d182/index.html",redirect:"/pages/b5d182/"},{path:"/03.技术/10.正则表达式/05.正则表达式.html",redirect:"/pages/b5d182/"},{name:"v-1db4fa63",path:"/pages/373d5a/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-1db4fa63").then(t)}},{path:"/pages/373d5a/index.html",redirect:"/pages/373d5a/"},{path:"/03.技术/02.Linux/05.Linux基础.html",redirect:"/pages/373d5a/"},{name:"v-3a79afdc",path:"/pages/edb686/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-3a79afdc").then(t)}},{path:"/pages/edb686/index.html",redirect:"/pages/edb686/"},{path:"/04.面试/10.Redis.html",redirect:"/pages/edb686/"},{name:"v-142bc356",path:"/pages/fa00f8/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-142bc356").then(t)}},{path:"/pages/fa00f8/index.html",redirect:"/pages/fa00f8/"},{path:"/04.面试/20.微服务篇.html",redirect:"/pages/fa00f8/"},{name:"v-a797ba30",path:"/pages/8b5a99/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-a797ba30").then(t)}},{path:"/pages/8b5a99/index.html",redirect:"/pages/8b5a99/"},{path:"/04.面试/30.MQ.html",redirect:"/pages/8b5a99/"},{name:"v-a382d2da",path:"/pages/2aa63b/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-a382d2da").then(t)}},{path:"/pages/2aa63b/index.html",redirect:"/pages/2aa63b/"},{path:"/05.项目记录/05.blog/05.blog.html",redirect:"/pages/2aa63b/"},{name:"v-b795f742",path:"/DOA/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-b795f742").then(t)}},{path:"/DOA/index.html",redirect:"/DOA/"},{path:"/06.DOA/01.DOA.html",redirect:"/DOA/"},{name:"v-567d2eac",path:"/pages/1b70fb/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-567d2eac").then(t)}},{path:"/pages/1b70fb/index.html",redirect:"/pages/1b70fb/"},{path:"/06.DOA/",redirect:"/pages/1b70fb/"},{name:"v-6cf60d0d",path:"/pages/d591da/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-6cf60d0d").then(t)}},{path:"/pages/d591da/index.html",redirect:"/pages/d591da/"},{path:"/07.life/01.海浪.html",redirect:"/pages/d591da/"},{name:"v-0bfafa42",path:"/categories/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-0bfafa42").then(t)}},{path:"/categories/index.html",redirect:"/categories/"},{path:"/@pages/categoriesPage.html",redirect:"/categories/"},{name:"v-b1e04020",path:"/pages/961a83/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-b1e04020").then(t)}},{path:"/pages/961a83/index.html",redirect:"/pages/961a83/"},{path:"/08.任意门/02.站外导航.html",redirect:"/pages/961a83/"},{name:"v-306f947f",path:"/archives/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-306f947f").then(t)}},{path:"/archives/index.html",redirect:"/archives/"},{path:"/@pages/archivesPage.html",redirect:"/archives/"},{name:"v-56be318a",path:"/pages/4607dc/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-56be318a").then(t)}},{path:"/pages/4607dc/index.html",redirect:"/pages/4607dc/"},{path:"/07.life/05.edg.html",redirect:"/pages/4607dc/"},{name:"v-d87ad802",path:"/tags/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-d87ad802").then(t)}},{path:"/tags/index.html",redirect:"/tags/"},{path:"/@pages/tagsPage.html",redirect:"/tags/"},{name:"v-2fc6a049",path:"/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-2fc6a049").then(t)}},{path:"/index.html",redirect:"/"},{name:"v-a21049fc",path:"/pages/f2e63f/",component:$e,beforeEnter:(n,e,t)=>{Object(Wn.a)("Layout","v-a21049fc").then(t)}},{path:"/pages/f2e63f/index.html",redirect:"/pages/f2e63f/"},{path:"/_posts/随笔/鸡汤.html",redirect:"/pages/f2e63f/"},{path:"*",component:$e}],Ge={title:"凉冰的手记",description:"记录生活的精彩瞬间，记录一个程序猿的成长！",base:"/",headTags:[["link",{rel:"icon",href:"/favicon.ico"}],["meta",{name:"keywords",content:"琐事，小记"}],["meta",{name:"author",href:"凉冰"}],["link",{rel:"manifest",href:"/manifest.json"}],["meta",{name:"theme-color",content:"#2196f3"}],["meta",{name:"apple-mobile-web-app-capable",content:"yes"}],["meta",{name:"apple-mobile-web-app-status-bar-style",content:"black"}],["link",{rel:"apple-touch-icon",href:"/icons/192.png"}],["link",{rel:"mask-icon",href:"/icons/safari-pinned-tab.svg",color:"#2196f3"}],["meta",{name:"msapplication-TileImage",content:"/icons/144.png"}],["meta",{name:"msapplication-TileColor",content:"#000000"}]],pages:[{title:"Map基础",frontmatter:{autoSort:99,title:"Map基础",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/6862b9/",categories:["后端","Java","集合"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/50.%E9%9B%86%E5%90%88/10.Map%E5%9F%BA%E7%A1%80.html",relativePath:"01.后端/10.Java/50.集合/10.Map基础.md",key:"v-74ef8d6a",path:"/pages/6862b9/",headers:[{level:2,title:"Map集合概述和特点",slug:"map集合概述和特点",normalizedTitle:"map集合概述和特点",charIndex:12},{level:2,title:"Map集合的基本功能",slug:"map集合的基本功能",normalizedTitle:"map集合的基本功能",charIndex:617},{level:2,title:"Map集合的获取功能",slug:"map集合的获取功能",normalizedTitle:"map集合的获取功能",charIndex:2022},{level:2,title:"Map集合的遍历(方式1)",slug:"map集合的遍历-方式1",normalizedTitle:"map集合的遍历(方式1)",charIndex:3110},{level:2,title:"Map集合的遍历(方式2)",slug:"map集合的遍历-方式2",normalizedTitle:"map集合的遍历(方式2)",charIndex:4007}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"Map集合概述和特点 Map集合的基本功能 Map集合的获取功能 Map集合的遍历(方式1) Map集合的遍历(方式2)",content:'# Map基础\n\n\n# Map集合概述和特点\n\n * Map集合概述\n   \n   interface Map<K,V>  K：键的类型；V：值的类型\n   \n\n * Map集合的特点\n   \n   * 双列集合,一个键对应一个值\n   \n   * 键不可以重复,值可以重复\n\n * Map集合的基本使用\n   \n   public class MapDemo01 {\n       public static void main(String[] args) {\n           //创建集合对象\n           Map<String,String> map = new HashMap<String,String>();\n   \n           //V put(K key, V value) 将指定的值与该映射中的指定键相关联\n           map.put("itheima001","林青霞");\n           map.put("itheima002","张曼玉");\n           map.put("itheima003","王祖贤");\n           map.put("itheima003","柳岩");\n   \n           //输出集合对象\n           System.out.println(map);\n       }\n   }\n   \n\n\n# Map集合的基本功能\n\n * 方法介绍\n   \n   方法名                                   说明\n   V put(K key,V value)                  添加元素\n   V remove(Object key)                  根据键删除键值对元素\n   void clear()                          移除所有的键值对元素\n   boolean containsKey(Object key)       判断集合是否包含指定的键\n   boolean containsValue(Object value)   判断集合是否包含指定的值\n   boolean isEmpty()                     判断集合是否为空\n   int size()                            集合的长度，也就是集合中键值对的个数\n\n * 示例代码\n   \n   public class MapDemo02 {\n       public static void main(String[] args) {\n           //创建集合对象\n           Map<String,String> map = new HashMap<String,String>();\n   \n           //V put(K key,V value)：添加元素\n           map.put("张无忌","赵敏");\n           map.put("郭靖","黄蓉");\n           map.put("杨过","小龙女");\n   \n           //V remove(Object key)：根据键删除键值对元素\n   //        System.out.println(map.remove("郭靖"));\n   //        System.out.println(map.remove("郭襄"));\n   \n           //void clear()：移除所有的键值对元素\n   //        map.clear();\n   \n           //boolean containsKey(Object key)：判断集合是否包含指定的键\n   //        System.out.println(map.containsKey("郭靖"));\n   //        System.out.println(map.containsKey("郭襄"));\n   \n           //boolean isEmpty()：判断集合是否为空\n   //        System.out.println(map.isEmpty());\n   \n           //int size()：集合的长度，也就是集合中键值对的个数\n           System.out.println(map.size());\n   \n           //输出集合对象\n           System.out.println(map);\n       }\n   }\n   \n\n\n# Map集合的获取功能\n\n * 方法介绍\n   \n   方法名                              说明\n   V get(Object key)                根据键获取值\n   Set<K> keySet()                  获取所有键的集合\n   Collection<V> values()           获取所有值的集合\n   Set<Map.Entry<K,V>> entrySet()   获取所有键值对对象的集合\n\n * 示例代码\n   \n   public class MapDemo03 {\n       public static void main(String[] args) {\n           //创建集合对象\n           Map<String, String> map = new HashMap<String, String>();\n   \n           //添加元素\n           map.put("张无忌", "赵敏");\n           map.put("郭靖", "黄蓉");\n           map.put("杨过", "小龙女");\n   \n           //V get(Object key):根据键获取值\n   //        System.out.println(map.get("张无忌"));\n   //        System.out.println(map.get("张三丰"));\n   \n           //Set<K> keySet():获取所有键的集合\n   //        Set<String> keySet = map.keySet();\n   //        for(String key : keySet) {\n   //            System.out.println(key);\n   //        }\n   \n           //Collection<V> values():获取所有值的集合\n           Collection<String> values = map.values();\n           for(String value : values) {\n               System.out.println(value);\n           }\n       }\n   }\n   \n\n\n# Map集合的遍历(方式1)\n\n * 遍历思路\n   \n   * 我们刚才存储的元素都是成对出现的，所以我们把Map看成是一个夫妻对的集合\n     \n     * 把所有的丈夫给集中起来\n     \n     * 遍历丈夫的集合，获取到每一个丈夫\n     \n     * 根据丈夫去找对应的妻子\n\n * 步骤分析\n   \n   * 获取所有键的集合。用keySet()方法实现\n   \n   * 遍历键的集合，获取到每一个键。用增强for实现\n   \n   * 根据键去找值。用get(Object key)方法实现\n\n * 代码实现\n   \n   public class MapDemo01 {\n       public static void main(String[] args) {\n           //创建集合对象\n           Map<String, String> map = new HashMap<String, String>();\n   \n           //添加元素\n           map.put("张无忌", "赵敏");\n           map.put("郭靖", "黄蓉");\n           map.put("杨过", "小龙女");\n   \n           //获取所有键的集合。用keySet()方法实现\n           Set<String> keySet = map.keySet();\n           //遍历键的集合，获取到每一个键。用增强for实现\n           for (String key : keySet) {\n               //根据键去找值。用get(Object key)方法实现\n               String value = map.get(key);\n               System.out.println(key + "," + value);\n           }\n       }\n   }\n   \n\n\n# Map集合的遍历(方式2)\n\n * 遍历思路\n   \n   * 我们刚才存储的元素都是成对出现的，所以我们把Map看成是一个夫妻对的集合\n     \n     * 获取所有结婚证的集合\n     \n     * 遍历结婚证的集合，得到每一个结婚证\n     \n     * 根据结婚证获取丈夫和妻子\n\n * 步骤分析\n   \n   * 获取所有键值对对象的集合\n     \n     * Set<Map.Entry<K,V>> entrySet()：获取所有键值对对象的集合\n   \n   * 遍历键值对对象的集合，得到每一个键值对对象\n     \n     * 用增强for实现，得到每一个Map.Entry\n   \n   * 根据键值对对象获取键和值\n     \n     * 用getKey()得到键\n     \n     * 用getValue()得到值\n\n * 代码实现\n   \n   public class MapDemo02 {\n       public static void main(String[] args) {\n           //创建集合对象\n           Map<String, String> map = new HashMap<String, String>();\n   \n           //添加元素\n           map.put("张无忌", "赵敏");\n           map.put("郭靖", "黄蓉");\n           map.put("杨过", "小龙女");\n   \n           //获取所有键值对对象的集合\n           Set<Map.Entry<String, String>> entrySet = map.entrySet();\n           //遍历键值对对象的集合，得到每一个键值对对象\n           for (Map.Entry<String, String> me : entrySet) {\n               //根据键值对对象获取键和值\n               String key = me.getKey();\n               String value = me.getValue();\n               System.out.println(key + "," + value);\n           }\n       }\n   }\n   ',normalizedContent:'# map基础\n\n\n# map集合概述和特点\n\n * map集合概述\n   \n   interface map<k,v>  k：键的类型；v：值的类型\n   \n\n * map集合的特点\n   \n   * 双列集合,一个键对应一个值\n   \n   * 键不可以重复,值可以重复\n\n * map集合的基本使用\n   \n   public class mapdemo01 {\n       public static void main(string[] args) {\n           //创建集合对象\n           map<string,string> map = new hashmap<string,string>();\n   \n           //v put(k key, v value) 将指定的值与该映射中的指定键相关联\n           map.put("itheima001","林青霞");\n           map.put("itheima002","张曼玉");\n           map.put("itheima003","王祖贤");\n           map.put("itheima003","柳岩");\n   \n           //输出集合对象\n           system.out.println(map);\n       }\n   }\n   \n\n\n# map集合的基本功能\n\n * 方法介绍\n   \n   方法名                                   说明\n   v put(k key,v value)                  添加元素\n   v remove(object key)                  根据键删除键值对元素\n   void clear()                          移除所有的键值对元素\n   boolean containskey(object key)       判断集合是否包含指定的键\n   boolean containsvalue(object value)   判断集合是否包含指定的值\n   boolean isempty()                     判断集合是否为空\n   int size()                            集合的长度，也就是集合中键值对的个数\n\n * 示例代码\n   \n   public class mapdemo02 {\n       public static void main(string[] args) {\n           //创建集合对象\n           map<string,string> map = new hashmap<string,string>();\n   \n           //v put(k key,v value)：添加元素\n           map.put("张无忌","赵敏");\n           map.put("郭靖","黄蓉");\n           map.put("杨过","小龙女");\n   \n           //v remove(object key)：根据键删除键值对元素\n   //        system.out.println(map.remove("郭靖"));\n   //        system.out.println(map.remove("郭襄"));\n   \n           //void clear()：移除所有的键值对元素\n   //        map.clear();\n   \n           //boolean containskey(object key)：判断集合是否包含指定的键\n   //        system.out.println(map.containskey("郭靖"));\n   //        system.out.println(map.containskey("郭襄"));\n   \n           //boolean isempty()：判断集合是否为空\n   //        system.out.println(map.isempty());\n   \n           //int size()：集合的长度，也就是集合中键值对的个数\n           system.out.println(map.size());\n   \n           //输出集合对象\n           system.out.println(map);\n       }\n   }\n   \n\n\n# map集合的获取功能\n\n * 方法介绍\n   \n   方法名                              说明\n   v get(object key)                根据键获取值\n   set<k> keyset()                  获取所有键的集合\n   collection<v> values()           获取所有值的集合\n   set<map.entry<k,v>> entryset()   获取所有键值对对象的集合\n\n * 示例代码\n   \n   public class mapdemo03 {\n       public static void main(string[] args) {\n           //创建集合对象\n           map<string, string> map = new hashmap<string, string>();\n   \n           //添加元素\n           map.put("张无忌", "赵敏");\n           map.put("郭靖", "黄蓉");\n           map.put("杨过", "小龙女");\n   \n           //v get(object key):根据键获取值\n   //        system.out.println(map.get("张无忌"));\n   //        system.out.println(map.get("张三丰"));\n   \n           //set<k> keyset():获取所有键的集合\n   //        set<string> keyset = map.keyset();\n   //        for(string key : keyset) {\n   //            system.out.println(key);\n   //        }\n   \n           //collection<v> values():获取所有值的集合\n           collection<string> values = map.values();\n           for(string value : values) {\n               system.out.println(value);\n           }\n       }\n   }\n   \n\n\n# map集合的遍历(方式1)\n\n * 遍历思路\n   \n   * 我们刚才存储的元素都是成对出现的，所以我们把map看成是一个夫妻对的集合\n     \n     * 把所有的丈夫给集中起来\n     \n     * 遍历丈夫的集合，获取到每一个丈夫\n     \n     * 根据丈夫去找对应的妻子\n\n * 步骤分析\n   \n   * 获取所有键的集合。用keyset()方法实现\n   \n   * 遍历键的集合，获取到每一个键。用增强for实现\n   \n   * 根据键去找值。用get(object key)方法实现\n\n * 代码实现\n   \n   public class mapdemo01 {\n       public static void main(string[] args) {\n           //创建集合对象\n           map<string, string> map = new hashmap<string, string>();\n   \n           //添加元素\n           map.put("张无忌", "赵敏");\n           map.put("郭靖", "黄蓉");\n           map.put("杨过", "小龙女");\n   \n           //获取所有键的集合。用keyset()方法实现\n           set<string> keyset = map.keyset();\n           //遍历键的集合，获取到每一个键。用增强for实现\n           for (string key : keyset) {\n               //根据键去找值。用get(object key)方法实现\n               string value = map.get(key);\n               system.out.println(key + "," + value);\n           }\n       }\n   }\n   \n\n\n# map集合的遍历(方式2)\n\n * 遍历思路\n   \n   * 我们刚才存储的元素都是成对出现的，所以我们把map看成是一个夫妻对的集合\n     \n     * 获取所有结婚证的集合\n     \n     * 遍历结婚证的集合，得到每一个结婚证\n     \n     * 根据结婚证获取丈夫和妻子\n\n * 步骤分析\n   \n   * 获取所有键值对对象的集合\n     \n     * set<map.entry<k,v>> entryset()：获取所有键值对对象的集合\n   \n   * 遍历键值对对象的集合，得到每一个键值对对象\n     \n     * 用增强for实现，得到每一个map.entry\n   \n   * 根据键值对对象获取键和值\n     \n     * 用getkey()得到键\n     \n     * 用getvalue()得到值\n\n * 代码实现\n   \n   public class mapdemo02 {\n       public static void main(string[] args) {\n           //创建集合对象\n           map<string, string> map = new hashmap<string, string>();\n   \n           //添加元素\n           map.put("张无忌", "赵敏");\n           map.put("郭靖", "黄蓉");\n           map.put("杨过", "小龙女");\n   \n           //获取所有键值对对象的集合\n           set<map.entry<string, string>> entryset = map.entryset();\n           //遍历键值对对象的集合，得到每一个键值对对象\n           for (map.entry<string, string> me : entryset) {\n               //根据键值对对象获取键和值\n               string key = me.getkey();\n               string value = me.getvalue();\n               system.out.println(key + "," + value);\n           }\n       }\n   }\n   ',charsets:{cjk:!0}},{title:"综合练习",frontmatter:{autoSort:95,title:"综合练习",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/8b78d9/",categories:["后端","Java","集合"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/50.%E9%9B%86%E5%90%88/40.%E7%BB%BC%E5%90%88%E7%BB%83%E4%B9%A0.html",relativePath:"01.后端/10.Java/50.集合/40.综合练习.md",key:"v-0db05437",path:"/pages/8b78d9/",headers:[{level:2,title:"练习1：随机点名器",slug:"练习1-随机点名器",normalizedTitle:"练习1：随机点名器",charIndex:11},{level:2,title:"练习2：带概率的随机",slug:"练习2-带概率的随机",normalizedTitle:"练习2：带概率的随机",charIndex:657},{level:2,title:"练习3：随机不重复",slug:"练习3-随机不重复",normalizedTitle:"练习3：随机不重复",charIndex:2104},{level:2,title:"练习4：集合的嵌套",slug:"练习4-集合的嵌套",normalizedTitle:"练习4：集合的嵌套",charIndex:3300}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"练习1：随机点名器 练习2：带概率的随机 练习3：随机不重复 练习4：集合的嵌套",content:'# 综合练习\n\n\n# 练习1：随机点名器\n\n需求：班级里有N个学生，实现随机点名器\n\n代码实现：\n\npublic class Test1 {\n    public static void main(String[] args) {\n        /* 班级里有N个学生，学生属性:姓名，年龄，性别。\n        实现随机点名器。*/\n\n\n        //1.定义集合\n        ArrayList<String> list = new ArrayList<>();\n        //2.添加数据\n        Collections.addAll(list,"范闲","范建","范统","杜子腾","杜琦燕","宋合泛","侯笼藤","朱益群","朱穆朗玛峰","袁明媛");\n        //3.随机点名\n        /* Random r = new Random();\n        int index = r.nextInt(list.size());\n        String name = list.get(index);\n        System.out.println(name);*/\n\n        //打乱\n        Collections.shuffle(list);\n\n        String name = list.get(0);\n        System.out.println(name);\n\n\n    }\n}\n\n\n\n# 练习2：带概率的随机\n\n需求：\n\n班级里有N个学生\n\n要求在随机的时候，70%的概率随机到男生，30%的概率随机到女生\n\n代码实现：\n\npublic class Test2 {\n    public static void main(String[] args) {\n        /* 班级里有N个学生\n        要求：\n        70%的概率随机到男生\n        30%的概率随机到女生\n\n        "范闲","范建","范统","杜子腾","宋合泛","侯笼藤","朱益群","朱穆朗玛峰",\n        "杜琦燕","袁明媛","李猜","田蜜蜜",\n        */\n        //1.创建集合\n        ArrayList<Integer> list = new ArrayList<>();\n        //2.添加数据\n        Collections.addAll(list,1,1,1,1,1,1,1);\n        Collections.addAll(list,0,0,0);\n        //3.打乱集合中的数据\n        Collections.shuffle(list);\n        //4.从list集合中随机抽取0或者1\n        Random r = new Random();\n        int index = r.nextInt(list.size());\n        int number = list.get(index);\n        System.out.println(number);\n        //5.创建两个集合分别存储男生和女生的名字\n        ArrayList<String> boyList = new ArrayList<>();\n        ArrayList<String> girlList = new ArrayList<>();\n\n        Collections.addAll(boyList,"范闲","范建","范统","杜子腾","宋合泛","侯笼藤","朱益群","朱穆朗玛峰");\n        Collections.addAll(girlList,"杜琦燕","袁明媛","李猜","田蜜蜜");\n\n        //6.判断此时是从boyList里面抽取还是从girlList里面抽取\n        if(number == 1){\n            //boyList\n            int boyIndex = r.nextInt(boyList.size());\n            String name = boyList.get(boyIndex);\n            System.out.println(name);\n        }else{\n            //girlList\n            int girlIndex = r.nextInt(girlList.size());\n            String name = girlList.get(girlIndex);\n            System.out.println(name);\n        }\n\n\n    }\n}\n\n\n\n# 练习3：随机不重复\n\n需求：\n\n班级里有N个学生，被点到的学生不会再被点到。但是如果班级中所有的学生都点完了， 需要重新开启第二轮点名。\n\n代码实现：\n\npublic class Test3 {\n    public static void main(String[] args) {\n       /* 班级里有5个学生\n        要求：\n        被点到的学生不会再被点到。\n        但是如果班级中所有的学生都点完了，需要重新开启第二轮点名。*/\n\n\n        //1.定义集合\n        ArrayList<String> list1 = new ArrayList<>();\n        //2.添加数据\n        Collections.addAll(list1, "范闲", "范建", "范统", "杜子腾", "杜琦燕", "宋合泛", "侯笼藤", "朱益群", "朱穆朗玛峰", "袁明媛");\n        //创建一个临时的集合，用来存已经被点到学生的名字\n        ArrayList<String> list2 = new ArrayList<>();\n        //外循环：表示轮数\n        for (int i = 1; i <= 10; i++) {\n            System.out.println("=========第" + i + "轮点名开始了======================");\n            //3.获取集合的长度\n            int count = list1.size();\n            //4.随机点名\n            Random r = new Random();\n            //内循环：每一轮中随机循环抽取的过程\n            for (int j = 0; j < count; j++) {\n                int index = r.nextInt(list1.size());\n                String name = list1.remove(index);\n                list2.add(name);\n                System.out.println(name);\n            }\n            //此时表示一轮点名结束\n            //list1 空了 list2 10个学生的名字\n            list1.addAll(list2);\n            list2.clear();\n\n        }\n    }\n}\n\n\n\n# 练习4：集合的嵌套\n\n需求：\n\n定义一个Map集合，键用表示省份名称province，值表示市city，但是市会有多个。\n\n添加完毕后，遍历结果格式如下：\n\n江苏省 = 南京市，扬州市，苏州市，无锡市，常州市\n\n湖北省 = 武汉市，孝感市，十堰市，宜昌市，鄂州市\n\n河北省 = 石家庄市，唐山市，邢台市，保定市，张家口市\n\n\n代码实现：\n\npublic class Test4 {\n    public static void main(String[] args) {\n        /* 需求\n        定义一个Map集合，键用表示省份名称province，值表示市city，但是市会有多个。\n        添加完毕后，遍历结果格式如下：\n                江苏省 = 南京市，扬州市，苏州市，无锡市，常州市\n                湖北省 = 武汉市，孝感市，十堰市，宜昌市，鄂州市\n                河北省 = 石家庄市，唐山市，邢台市，保定市，张家口市*/\n\n\n        //1.创建Map集合\n        HashMap<String, ArrayList<String>> hm = new HashMap<>();\n\n        //2.创建单列集合存储市\n        ArrayList<String> city1 = new ArrayList<>();\n        city1.add("南京市");\n        city1.add("扬州市");\n        city1.add("苏州市");\n        city1.add("无锡市");\n        city1.add("常州市");\n\n        ArrayList<String> city2 = new ArrayList<>();\n        city2.add("武汉市");\n        city2.add("孝感市");\n        city2.add("十堰市");\n        city2.add("宜昌市");\n        city2.add("鄂州市");\n\n        ArrayList<String> city3 = new ArrayList<>();\n        city3.add("石家庄市");\n        city3.add("唐山市");\n        city3.add("邢台市");\n        city3.add("保定市");\n        city3.add("张家口市");\n\n        //3.把省份和多个市添加到map集合\n        hm.put("江苏省",city1);\n        hm.put("湖北省",city2);\n        hm.put("河北省",city3);\n\n        Set<Map.Entry<String, ArrayList<String>>> entries = hm.entrySet();\n        for (Map.Entry<String, ArrayList<String>> entry : entries) {\n            //entry依次表示每一个键值对对象\n            String key = entry.getKey();\n            ArrayList<String> value = entry.getValue();\n            StringJoiner sj = new StringJoiner(", ","","");\n            for (String city : value) {\n                sj.add(city);\n            }\n            System.out.println(key + " = " + sj);\n\n        }\n    }\n}\n',normalizedContent:'# 综合练习\n\n\n# 练习1：随机点名器\n\n需求：班级里有n个学生，实现随机点名器\n\n代码实现：\n\npublic class test1 {\n    public static void main(string[] args) {\n        /* 班级里有n个学生，学生属性:姓名，年龄，性别。\n        实现随机点名器。*/\n\n\n        //1.定义集合\n        arraylist<string> list = new arraylist<>();\n        //2.添加数据\n        collections.addall(list,"范闲","范建","范统","杜子腾","杜琦燕","宋合泛","侯笼藤","朱益群","朱穆朗玛峰","袁明媛");\n        //3.随机点名\n        /* random r = new random();\n        int index = r.nextint(list.size());\n        string name = list.get(index);\n        system.out.println(name);*/\n\n        //打乱\n        collections.shuffle(list);\n\n        string name = list.get(0);\n        system.out.println(name);\n\n\n    }\n}\n\n\n\n# 练习2：带概率的随机\n\n需求：\n\n班级里有n个学生\n\n要求在随机的时候，70%的概率随机到男生，30%的概率随机到女生\n\n代码实现：\n\npublic class test2 {\n    public static void main(string[] args) {\n        /* 班级里有n个学生\n        要求：\n        70%的概率随机到男生\n        30%的概率随机到女生\n\n        "范闲","范建","范统","杜子腾","宋合泛","侯笼藤","朱益群","朱穆朗玛峰",\n        "杜琦燕","袁明媛","李猜","田蜜蜜",\n        */\n        //1.创建集合\n        arraylist<integer> list = new arraylist<>();\n        //2.添加数据\n        collections.addall(list,1,1,1,1,1,1,1);\n        collections.addall(list,0,0,0);\n        //3.打乱集合中的数据\n        collections.shuffle(list);\n        //4.从list集合中随机抽取0或者1\n        random r = new random();\n        int index = r.nextint(list.size());\n        int number = list.get(index);\n        system.out.println(number);\n        //5.创建两个集合分别存储男生和女生的名字\n        arraylist<string> boylist = new arraylist<>();\n        arraylist<string> girllist = new arraylist<>();\n\n        collections.addall(boylist,"范闲","范建","范统","杜子腾","宋合泛","侯笼藤","朱益群","朱穆朗玛峰");\n        collections.addall(girllist,"杜琦燕","袁明媛","李猜","田蜜蜜");\n\n        //6.判断此时是从boylist里面抽取还是从girllist里面抽取\n        if(number == 1){\n            //boylist\n            int boyindex = r.nextint(boylist.size());\n            string name = boylist.get(boyindex);\n            system.out.println(name);\n        }else{\n            //girllist\n            int girlindex = r.nextint(girllist.size());\n            string name = girllist.get(girlindex);\n            system.out.println(name);\n        }\n\n\n    }\n}\n\n\n\n# 练习3：随机不重复\n\n需求：\n\n班级里有n个学生，被点到的学生不会再被点到。但是如果班级中所有的学生都点完了， 需要重新开启第二轮点名。\n\n代码实现：\n\npublic class test3 {\n    public static void main(string[] args) {\n       /* 班级里有5个学生\n        要求：\n        被点到的学生不会再被点到。\n        但是如果班级中所有的学生都点完了，需要重新开启第二轮点名。*/\n\n\n        //1.定义集合\n        arraylist<string> list1 = new arraylist<>();\n        //2.添加数据\n        collections.addall(list1, "范闲", "范建", "范统", "杜子腾", "杜琦燕", "宋合泛", "侯笼藤", "朱益群", "朱穆朗玛峰", "袁明媛");\n        //创建一个临时的集合，用来存已经被点到学生的名字\n        arraylist<string> list2 = new arraylist<>();\n        //外循环：表示轮数\n        for (int i = 1; i <= 10; i++) {\n            system.out.println("=========第" + i + "轮点名开始了======================");\n            //3.获取集合的长度\n            int count = list1.size();\n            //4.随机点名\n            random r = new random();\n            //内循环：每一轮中随机循环抽取的过程\n            for (int j = 0; j < count; j++) {\n                int index = r.nextint(list1.size());\n                string name = list1.remove(index);\n                list2.add(name);\n                system.out.println(name);\n            }\n            //此时表示一轮点名结束\n            //list1 空了 list2 10个学生的名字\n            list1.addall(list2);\n            list2.clear();\n\n        }\n    }\n}\n\n\n\n# 练习4：集合的嵌套\n\n需求：\n\n定义一个map集合，键用表示省份名称province，值表示市city，但是市会有多个。\n\n添加完毕后，遍历结果格式如下：\n\n江苏省 = 南京市，扬州市，苏州市，无锡市，常州市\n\n湖北省 = 武汉市，孝感市，十堰市，宜昌市，鄂州市\n\n河北省 = 石家庄市，唐山市，邢台市，保定市，张家口市\n\n\n代码实现：\n\npublic class test4 {\n    public static void main(string[] args) {\n        /* 需求\n        定义一个map集合，键用表示省份名称province，值表示市city，但是市会有多个。\n        添加完毕后，遍历结果格式如下：\n                江苏省 = 南京市，扬州市，苏州市，无锡市，常州市\n                湖北省 = 武汉市，孝感市，十堰市，宜昌市，鄂州市\n                河北省 = 石家庄市，唐山市，邢台市，保定市，张家口市*/\n\n\n        //1.创建map集合\n        hashmap<string, arraylist<string>> hm = new hashmap<>();\n\n        //2.创建单列集合存储市\n        arraylist<string> city1 = new arraylist<>();\n        city1.add("南京市");\n        city1.add("扬州市");\n        city1.add("苏州市");\n        city1.add("无锡市");\n        city1.add("常州市");\n\n        arraylist<string> city2 = new arraylist<>();\n        city2.add("武汉市");\n        city2.add("孝感市");\n        city2.add("十堰市");\n        city2.add("宜昌市");\n        city2.add("鄂州市");\n\n        arraylist<string> city3 = new arraylist<>();\n        city3.add("石家庄市");\n        city3.add("唐山市");\n        city3.add("邢台市");\n        city3.add("保定市");\n        city3.add("张家口市");\n\n        //3.把省份和多个市添加到map集合\n        hm.put("江苏省",city1);\n        hm.put("湖北省",city2);\n        hm.put("河北省",city3);\n\n        set<map.entry<string, arraylist<string>>> entries = hm.entryset();\n        for (map.entry<string, arraylist<string>> entry : entries) {\n            //entry依次表示每一个键值对对象\n            string key = entry.getkey();\n            arraylist<string> value = entry.getvalue();\n            stringjoiner sj = new stringjoiner(", ","","");\n            for (string city : value) {\n                sj.add(city);\n            }\n            system.out.println(key + " = " + sj);\n\n        }\n    }\n}\n',charsets:{cjk:!0}},{title:"Collections类",frontmatter:{autoSort:96,title:"Collections类",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/9f2f57/",categories:["后端","Java","集合"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/50.%E9%9B%86%E5%90%88/30.%E5%8F%AF%E5%8F%98%E5%8F%82%E6%95%B0.html",relativePath:"01.后端/10.Java/50.集合/30.可变参数.md",key:"v-122e0ca4",path:"/pages/9f2f57/",headers:[{level:2,title:"可变参数",slug:"可变参数",normalizedTitle:"可变参数",charIndex:19},{level:2,title:"Collections常用功能",slug:"collections常用功能",normalizedTitle:"collections常用功能",charIndex:1023},{level:2,title:"Comparator比较器",slug:"comparator比较器",normalizedTitle:"comparator比较器",charIndex:1692}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"可变参数 Collections常用功能 Comparator比较器",content:"# Collections类\n\n\n# 可变参数\n\n在JDK1.5之后，如果我们定义一个方法需要接受多个参数，并且多个参数类型一致，我们可以对其简化.\n\n格式：\n\n修饰符 返回值类型 方法名(参数类型... 形参名){  }\n\n\n底层：\n\n其实就是一个数组\n\n好处：\n\n在传递数据的时候，省的我们自己创建数组并添加元素了，JDK底层帮我们自动创建数组并添加元素了\n\n代码演示:\n\n  public class ChangeArgs {\n    public static void main(String[] args) {\n        int sum = getSum(6, 7, 2, 12, 2121);\n        System.out.println(sum);\n    }\n    \n    public static int getSum(int... arr) {\n   \t\tint sum = 0;\n   \t     for (int a : arr) {\n         sum += a;\n        }\n   \t\t return sum;\n    }\n}\n\n\n注意：\n\n1.一个方法只能有一个可变参数\n\n2.如果方法中有多个参数，可变参数要放到最后。\n\n应用场景: Collections\n\n在Collections中也提供了添加一些元素方法：\n\npublic static <T> boolean addAll(Collection<T> c, T... elements):往集合中添加一些元素。\n\n代码演示:\n\npublic class CollectionsDemo {\n\tpublic static void main(String[] args) {\n      ArrayList<Integer> list = new ArrayList<Integer>();\n      //原来写法\n      //list.add(12);\n      //list.add(14);\n      //list.add(15);\n      //list.add(1000);\n      //采用工具类 完成 往集合中添加元素  \n      Collections.addAll(list, 5, 222, 1，2);\n      System.out.println(list);\n}\n\n\n\n# Collections常用功能\n\n * java.utils.Collections是集合工具类，用来对集合进行操作。\n   \n   常用方法如下：\n\n * public static void shuffle(List<?> list):打乱集合顺序。\n\n * public static <T> void sort(List<T> list):将集合中元素按照默认规则排序。\n\n * public static <T> void sort(List<T> list，Comparator<? super T> ):将集合中元素按照指定规则排序。\n\n代码演示：\n\npublic class CollectionsDemo {\n    public static void main(String[] args) {\n        ArrayList<Integer> list = new ArrayList<Integer>();\n   \n        list.add(100);\n        list.add(300);\n        list.add(200);\n        list.add(50);\n        //排序方法 \n        Collections.sort(list);\n        System.out.println(list);\n    }\n}\n结果：\n[50,100, 200, 300]\n\n\n我们的集合按照默认的自然顺序进行了排列，如果想要指定顺序那该怎么办呢？\n\n\n# Comparator比较器\n\n创建一个学生类，存储到ArrayList集合中完成指定排序操作。\n\nStudent 类\n\npublic class Student{\n    private String name;\n    private int age;\n\t//构造方法\n    //get/set\n \t//toString\n}\n\n\n测试类：\n\npublic class Demo {\n    public static void main(String[] args) {\n        // 创建四个学生对象 存储到集合中\n        ArrayList<Student> list = new ArrayList<Student>();\n\n        list.add(new Student(\"rose\",18));\n        list.add(new Student(\"jack\",16));\n        list.add(new Student(\"abc\",20));\n\t\tCollections.sort(list, new Comparator<Student>() {\n  \t\t  @Override\n    \t\tpublic int compare(Student o1, Student o2) {\n        \treturn o1.getAge()-o2.getAge();//以学生的年龄升序\n   \t\t }\n\t\t});\n\n\n        for (Student student : list) {\n            System.out.println(student);\n        }\n    }\n}\nStudent{name='jack', age=16}\nStudent{name='rose', age=18}\nStudent{name='abc', age=20}\n",normalizedContent:"# collections类\n\n\n# 可变参数\n\n在jdk1.5之后，如果我们定义一个方法需要接受多个参数，并且多个参数类型一致，我们可以对其简化.\n\n格式：\n\n修饰符 返回值类型 方法名(参数类型... 形参名){  }\n\n\n底层：\n\n其实就是一个数组\n\n好处：\n\n在传递数据的时候，省的我们自己创建数组并添加元素了，jdk底层帮我们自动创建数组并添加元素了\n\n代码演示:\n\n  public class changeargs {\n    public static void main(string[] args) {\n        int sum = getsum(6, 7, 2, 12, 2121);\n        system.out.println(sum);\n    }\n    \n    public static int getsum(int... arr) {\n   \t\tint sum = 0;\n   \t     for (int a : arr) {\n         sum += a;\n        }\n   \t\t return sum;\n    }\n}\n\n\n注意：\n\n1.一个方法只能有一个可变参数\n\n2.如果方法中有多个参数，可变参数要放到最后。\n\n应用场景: collections\n\n在collections中也提供了添加一些元素方法：\n\npublic static <t> boolean addall(collection<t> c, t... elements):往集合中添加一些元素。\n\n代码演示:\n\npublic class collectionsdemo {\n\tpublic static void main(string[] args) {\n      arraylist<integer> list = new arraylist<integer>();\n      //原来写法\n      //list.add(12);\n      //list.add(14);\n      //list.add(15);\n      //list.add(1000);\n      //采用工具类 完成 往集合中添加元素  \n      collections.addall(list, 5, 222, 1，2);\n      system.out.println(list);\n}\n\n\n\n# collections常用功能\n\n * java.utils.collections是集合工具类，用来对集合进行操作。\n   \n   常用方法如下：\n\n * public static void shuffle(list<?> list):打乱集合顺序。\n\n * public static <t> void sort(list<t> list):将集合中元素按照默认规则排序。\n\n * public static <t> void sort(list<t> list，comparator<? super t> ):将集合中元素按照指定规则排序。\n\n代码演示：\n\npublic class collectionsdemo {\n    public static void main(string[] args) {\n        arraylist<integer> list = new arraylist<integer>();\n   \n        list.add(100);\n        list.add(300);\n        list.add(200);\n        list.add(50);\n        //排序方法 \n        collections.sort(list);\n        system.out.println(list);\n    }\n}\n结果：\n[50,100, 200, 300]\n\n\n我们的集合按照默认的自然顺序进行了排列，如果想要指定顺序那该怎么办呢？\n\n\n# comparator比较器\n\n创建一个学生类，存储到arraylist集合中完成指定排序操作。\n\nstudent 类\n\npublic class student{\n    private string name;\n    private int age;\n\t//构造方法\n    //get/set\n \t//tostring\n}\n\n\n测试类：\n\npublic class demo {\n    public static void main(string[] args) {\n        // 创建四个学生对象 存储到集合中\n        arraylist<student> list = new arraylist<student>();\n\n        list.add(new student(\"rose\",18));\n        list.add(new student(\"jack\",16));\n        list.add(new student(\"abc\",20));\n\t\tcollections.sort(list, new comparator<student>() {\n  \t\t  @override\n    \t\tpublic int compare(student o1, student o2) {\n        \treturn o1.getage()-o2.getage();//以学生的年龄升序\n   \t\t }\n\t\t});\n\n\n        for (student student : list) {\n            system.out.println(student);\n        }\n    }\n}\nstudent{name='jack', age=16}\nstudent{name='rose', age=18}\nstudent{name='abc', age=20}\n",charsets:{cjk:!0}},{title:"TreeMap",frontmatter:{autoSort:97,title:"TreeMap",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/bb8a3e/",categories:["后端","Java","集合"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/50.%E9%9B%86%E5%90%88/25.TreeMap.html",relativePath:"01.后端/10.Java/50.集合/25.TreeMap.md",key:"v-cf325208",path:"/pages/bb8a3e/",headers:[{level:2,title:"TreeMap集合概述和特点【理解】",slug:"treemap集合概述和特点【理解】",normalizedTitle:"treemap集合概述和特点【理解】",charIndex:14},{level:2,title:"TreeMap集合应用案例【应用】",slug:"treemap集合应用案例【应用】",normalizedTitle:"treemap集合应用案例【应用】",charIndex:141}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688392125e3,headersStr:"TreeMap集合概述和特点【理解】 TreeMap集合应用案例【应用】",content:'# TreeMap\n\n\n# TreeMap集合概述和特点【理解】\n\n * TreeMap底层是红黑树结构\n\n * 依赖自然排序或者比较器排序,对键进行排序\n\n * 如果键存储的是自定义对象,需要实现Comparable接口或者在创建TreeMap对象时候给出比较器排序规则\n\n\n# TreeMap集合应用案例【应用】\n\n * 案例需求\n   \n   * 创建一个TreeMap集合,键是学生对象(Student),值是籍贯(String),学生属性姓名和年龄,按照年龄进行排序并遍历\n   \n   * 要求按照学生的年龄进行排序,如果年龄相同则按照姓名进行排序\n\n * 代码实现\n   \n   学生类\n   \n   public class Student implements Comparable<Student>{\n       private String name;\n       private int age;\n   \n       public Student() {\n       }\n   \n       public Student(String name, int age) {\n           this.name = name;\n           this.age = age;\n       }\n   \n       public String getName() {\n           return name;\n       }\n   \n       public void setName(String name) {\n           this.name = name;\n       }\n   \n       public int getAge() {\n           return age;\n       }\n   \n       public void setAge(int age) {\n           this.age = age;\n       }\n   \n       @Override\n       public String toString() {\n           return "Student{" +\n                   "name=\'" + name + \'\\\'\' +\n                   ", age=" + age +\n                   \'}\';\n       }\n   \n       @Override\n       public int compareTo(Student o) {\n           //按照年龄进行排序\n           int result = o.getAge() - this.getAge();\n           //次要条件，按照姓名排序。\n           result = result == 0 ? o.getName().compareTo(this.getName()) : result;\n           return result;\n       }\n   }\n   \n   \n   测试类\n   \n   public class Test1 {\n       public static void main(String[] args) {\n         \t// 创建TreeMap集合对象\n           TreeMap<Student,String> tm = new TreeMap<>();\n         \n   \t\t// 创建学生对象\n           Student s1 = new Student("xiaohei",23);\n           Student s2 = new Student("dapang",22);\n           Student s3 = new Student("xiaomei",22);\n         \n   \t\t// 将学生对象添加到TreeMap集合中\n           tm.put(s1,"江苏");\n           tm.put(s2,"北京");\n           tm.put(s3,"天津");\n         \n   \t\t// 遍历TreeMap集合,打印每个学生的信息\n           tm.forEach(\n                   (Student key, String value)->{\n                       System.out.println(key + "---" + value);\n                   }\n           );\n       }\n   }\n   \n\n新的统计思想： 利用map进行统计\n\n * 如果题目没有要求对键进行排序，优先使用 HashMap； HashMap 效率最高\n * 如果题目要求对键进行排序，推荐使用 TreeMap\n\n',normalizedContent:'# treemap\n\n\n# treemap集合概述和特点【理解】\n\n * treemap底层是红黑树结构\n\n * 依赖自然排序或者比较器排序,对键进行排序\n\n * 如果键存储的是自定义对象,需要实现comparable接口或者在创建treemap对象时候给出比较器排序规则\n\n\n# treemap集合应用案例【应用】\n\n * 案例需求\n   \n   * 创建一个treemap集合,键是学生对象(student),值是籍贯(string),学生属性姓名和年龄,按照年龄进行排序并遍历\n   \n   * 要求按照学生的年龄进行排序,如果年龄相同则按照姓名进行排序\n\n * 代码实现\n   \n   学生类\n   \n   public class student implements comparable<student>{\n       private string name;\n       private int age;\n   \n       public student() {\n       }\n   \n       public student(string name, int age) {\n           this.name = name;\n           this.age = age;\n       }\n   \n       public string getname() {\n           return name;\n       }\n   \n       public void setname(string name) {\n           this.name = name;\n       }\n   \n       public int getage() {\n           return age;\n       }\n   \n       public void setage(int age) {\n           this.age = age;\n       }\n   \n       @override\n       public string tostring() {\n           return "student{" +\n                   "name=\'" + name + \'\\\'\' +\n                   ", age=" + age +\n                   \'}\';\n       }\n   \n       @override\n       public int compareto(student o) {\n           //按照年龄进行排序\n           int result = o.getage() - this.getage();\n           //次要条件，按照姓名排序。\n           result = result == 0 ? o.getname().compareto(this.getname()) : result;\n           return result;\n       }\n   }\n   \n   \n   测试类\n   \n   public class test1 {\n       public static void main(string[] args) {\n         \t// 创建treemap集合对象\n           treemap<student,string> tm = new treemap<>();\n         \n   \t\t// 创建学生对象\n           student s1 = new student("xiaohei",23);\n           student s2 = new student("dapang",22);\n           student s3 = new student("xiaomei",22);\n         \n   \t\t// 将学生对象添加到treemap集合中\n           tm.put(s1,"江苏");\n           tm.put(s2,"北京");\n           tm.put(s3,"天津");\n         \n   \t\t// 遍历treemap集合,打印每个学生的信息\n           tm.foreach(\n                   (student key, string value)->{\n                       system.out.println(key + "---" + value);\n                   }\n           );\n       }\n   }\n   \n\n新的统计思想： 利用map进行统计\n\n * 如果题目没有要求对键进行排序，优先使用 hashmap； hashmap 效率最高\n * 如果题目要求对键进行排序，推荐使用 treemap\n\n',charsets:{cjk:!0}},{title:"HashMap",frontmatter:{autoSort:98,title:"HashMap",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/cfbd66/",categories:["后端","Java","集合"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/50.%E9%9B%86%E5%90%88/20.HashMap.html",relativePath:"01.后端/10.Java/50.集合/20.HashMap.md",key:"v-9db0ef34",path:"/pages/cfbd66/",headers:[{level:2,title:"HashMap集合概述和特点【理解】",slug:"hashmap集合概述和特点【理解】",normalizedTitle:"hashmap集合概述和特点【理解】",charIndex:14},{level:2,title:"HashMap集合应用案例【应用】",slug:"hashmap集合应用案例【应用】",normalizedTitle:"hashmap集合应用案例【应用】",charIndex:227}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"HashMap集合概述和特点【理解】 HashMap集合应用案例【应用】",content:'# HashMap\n\n\n# HashMap集合概述和特点【理解】\n\n * HashMap底层是==哈希表==结构的\n\n * 依赖==hashCode方法和equals==方法保证键的唯一\n\n * 如果键要存储的是自定义对象，需要重写hashCode和equals方法\n\n * jdk8以后，当存在哈希冲突时，会直接在原来的结点处，挂上新的entry，形成一条链表\n   \n   * 当链表长度超过8 && 数组长度超过64时，链表会转换成红黑树\n\n\n# HashMap集合应用案例【应用】\n\n * 案例需求\n   \n   * 创建一个HashMap集合，键是学生对象(Student)，值是居住地 (String)。存储多个元素，并遍历。\n   \n   * 要求保证键的唯一性：如果学生对象的成员变量值相同，我们就认为是同一个对象\n\n * 代码实现\n   \n   学生类\n   \n   public class Student {\n       private String name;\n       private int age;\n   \n       public Student() {\n       }\n   \n       public Student(String name, int age) {\n           this.name = name;\n           this.age = age;\n       }\n   \n       public String getName() {\n           return name;\n       }\n   \n       public void setName(String name) {\n           this.name = name;\n       }\n   \n       public int getAge() {\n           return age;\n       }\n   \n       public void setAge(int age) {\n           this.age = age;\n       }\n   \n       @Override\n       public boolean equals(Object o) {\n           if (this == o) return true;\n           if (o == null || getClass() != o.getClass()) return false;\n   \n           Student student = (Student) o;\n   \n           if (age != student.age) return false;\n           return name != null ? name.equals(student.name) : student.name == null;\n       }\n   \n       @Override\n       public int hashCode() {\n           int result = name != null ? name.hashCode() : 0;\n           result = 31 * result + age;\n           return result;\n       }\n   }\n   \n   \n   测试类\n   \n   public class HashMapDemo {\n       public static void main(String[] args) {\n           //创建HashMap集合对象\n           HashMap<Student, String> hm = new HashMap<Student, String>();\n   \n           //创建学生对象\n           Student s1 = new Student("林青霞", 30);\n           Student s2 = new Student("张曼玉", 35);\n           Student s3 = new Student("王祖贤", 33);\n           Student s4 = new Student("王祖贤", 33);\n   \n           //把学生添加到集合\n           hm.put(s1, "西安");\n           hm.put(s2, "武汉");\n           hm.put(s3, "郑州");\n           hm.put(s4, "北京");\n   \n           //遍历集合\n           Set<Student> keySet = hm.keySet();\n           for (Student key : keySet) {\n               String value = hm.get(key);\n               System.out.println(key.getName() + "," + key.getAge() + "," + value);\n           }\n       }\n   }\n   ',normalizedContent:'# hashmap\n\n\n# hashmap集合概述和特点【理解】\n\n * hashmap底层是==哈希表==结构的\n\n * 依赖==hashcode方法和equals==方法保证键的唯一\n\n * 如果键要存储的是自定义对象，需要重写hashcode和equals方法\n\n * jdk8以后，当存在哈希冲突时，会直接在原来的结点处，挂上新的entry，形成一条链表\n   \n   * 当链表长度超过8 && 数组长度超过64时，链表会转换成红黑树\n\n\n# hashmap集合应用案例【应用】\n\n * 案例需求\n   \n   * 创建一个hashmap集合，键是学生对象(student)，值是居住地 (string)。存储多个元素，并遍历。\n   \n   * 要求保证键的唯一性：如果学生对象的成员变量值相同，我们就认为是同一个对象\n\n * 代码实现\n   \n   学生类\n   \n   public class student {\n       private string name;\n       private int age;\n   \n       public student() {\n       }\n   \n       public student(string name, int age) {\n           this.name = name;\n           this.age = age;\n       }\n   \n       public string getname() {\n           return name;\n       }\n   \n       public void setname(string name) {\n           this.name = name;\n       }\n   \n       public int getage() {\n           return age;\n       }\n   \n       public void setage(int age) {\n           this.age = age;\n       }\n   \n       @override\n       public boolean equals(object o) {\n           if (this == o) return true;\n           if (o == null || getclass() != o.getclass()) return false;\n   \n           student student = (student) o;\n   \n           if (age != student.age) return false;\n           return name != null ? name.equals(student.name) : student.name == null;\n       }\n   \n       @override\n       public int hashcode() {\n           int result = name != null ? name.hashcode() : 0;\n           result = 31 * result + age;\n           return result;\n       }\n   }\n   \n   \n   测试类\n   \n   public class hashmapdemo {\n       public static void main(string[] args) {\n           //创建hashmap集合对象\n           hashmap<student, string> hm = new hashmap<student, string>();\n   \n           //创建学生对象\n           student s1 = new student("林青霞", 30);\n           student s2 = new student("张曼玉", 35);\n           student s3 = new student("王祖贤", 33);\n           student s4 = new student("王祖贤", 33);\n   \n           //把学生添加到集合\n           hm.put(s1, "西安");\n           hm.put(s2, "武汉");\n           hm.put(s3, "郑州");\n           hm.put(s4, "北京");\n   \n           //遍历集合\n           set<student> keyset = hm.keyset();\n           for (student key : keyset) {\n               string value = hm.get(key);\n               system.out.println(key.getname() + "," + key.getage() + "," + value);\n           }\n       }\n   }\n   ',charsets:{cjk:!0}},{title:"不可变集合",frontmatter:{autoSort:93,title:"不可变集合",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/d8ef9d/",categories:["后端","Java","集合"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/50.%E9%9B%86%E5%90%88/60.%E4%B8%8D%E5%8F%AF%E5%8F%98%E9%9B%86%E5%90%88.html",relativePath:"01.后端/10.Java/50.集合/60.不可变集合.md",key:"v-166a22fe",path:"/pages/d8ef9d/",headers:[{level:2,title:"什么是不可变集合",slug:"什么是不可变集合",normalizedTitle:"什么是不可变集合",charIndex:12},{level:2,title:"使用场景",slug:"使用场景",normalizedTitle:"使用场景",charIndex:46},{level:2,title:"不可变的list集合",slug:"不可变的list集合",normalizedTitle:"不可变的list集合",charIndex:245},{level:2,title:"不可变的Set集合",slug:"不可变的set集合",normalizedTitle:"不可变的set集合",charIndex:1427},{level:2,title:"不可变的Map集合",slug:"不可变的map集合",normalizedTitle:"不可变的map集合",charIndex:2153},{level:3,title:"键值对个数小于等于10",slug:"键值对个数小于等于10",normalizedTitle:"键值对个数小于等于10",charIndex:2167},{level:3,title:"键值对个数大于10",slug:"键值对个数大于10",normalizedTitle:"键值对个数大于10",charIndex:3313}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"什么是不可变集合 使用场景 不可变的list集合 不可变的Set集合 不可变的Map集合 键值对个数小于等于10 键值对个数大于10",content:'# 不可变集合\n\n\n# 什么是不可变集合\n\n是一个长度不可变，内容也无法修改的集合\n\n\n# 使用场景\n\n如果某个数据不能被修改，把它防御性地拷贝到不可变集合中是个很好的实践。\n\n当集合对象被不可信的库调用时，不可变形式是安全的。\n\n简单理解：\n\n不想让别人修改集合中的内容\n\n比如说：\n\n1，斗地主的54张牌，是不能添加，不能删除，不能修改的\n\n2，斗地主的打牌规则：单张，对子，三张，顺子等，也是不能修改的\n\n3，用代码获取的操作系统硬件信息，也是不能被修改的\n\n不可变集合分类\n\n * 不可变的list集合\n * 不可变的set集合\n * 不可变的map集合\n\n\n# 不可变的list集合\n\npublic class ImmutableDemo1 {\n    public static void main(String[] args) {\n        /*\n            创建不可变的List集合\n            "张三", "李四", "王五", "赵六"\n        */\n\n        //一旦创建完毕之后，是无法进行修改的，在下面的代码中，只能进行查询操作\n        List<String> list = List.of("张三", "李四", "王五", "赵六");\n\n        System.out.println(list.get(0));\n        System.out.println(list.get(1));\n        System.out.println(list.get(2));\n        System.out.println(list.get(3));\n\n        System.out.println("---------------------------");\n\n        for (String s : list) {\n            System.out.println(s);\n        }\n\n        System.out.println("---------------------------");\n\n\n        Iterator<String> it = list.iterator();\n        while(it.hasNext()){\n            String s = it.next();\n            System.out.println(s);\n        }\n        System.out.println("---------------------------");\n\n        for (int i = 0; i < list.size(); i++) {\n            String s = list.get(i);\n            System.out.println(s);\n        }\n        System.out.println("---------------------------");\n\n        //list.remove("李四");\n        //list.add("aaa");\n        list.set(0,"aaa");\n    }\n}\n\n\n\n# 不可变的Set集合\n\npublic class ImmutableDemo2 {\n    public static void main(String[] args) {\n        /*\n           创建不可变的Set集合\n           "张三", "李四", "王五", "赵六"\n\n\n           细节：\n                当我们要获取一个不可变的Set集合时，里面的参数一定要保证唯一性\n        */\n\n        //一旦创建完毕之后，是无法进行修改的，在下面的代码中，只能进行查询操作\n        Set<String> set = Set.of("张三", "张三", "李四", "王五", "赵六");\n\n        for (String s : set) {\n            System.out.println(s);\n        }\n\n        System.out.println("-----------------------");\n\n        Iterator<String> it = set.iterator();\n        while(it.hasNext()){\n            String s = it.next();\n            System.out.println(s);\n        }\n\n        System.out.println("-----------------------");\n        //set.remove("王五");\n    }\n}\n\n\n\n# 不可变的Map集合\n\n\n# 键值对个数小于等于10\n\npublic class ImmutableDemo3 {\n    public static void main(String[] args) {\n       /*\n        创建Map的不可变集合\n            细节1：\n                键是不能重复的\n            细节2：\n                Map里面的of方法，参数是有上限的，最多只能传递20个参数，10个键值对\n            细节3：\n                如果我们要传递多个键值对对象，数量大于10个，在Map接口中还有一个方法\n        */\n\n        //一旦创建完毕之后，是无法进行修改的，在下面的代码中，只能进行查询操作\n        Map<String, String> map = Map.of("张三", "南京", "张三", "北京", "王五", "上海",\n                "赵六", "广州", "孙七", "深圳", "周八", "杭州",\n                "吴九", "宁波", "郑十", "苏州", "刘一", "无锡",\n                "陈二", "嘉兴");\n\n        Set<String> keys = map.keySet();\n        for (String key : keys) {\n            String value = map.get(key);\n            System.out.println(key + "=" + value);\n        }\n\n        System.out.println("--------------------------");\n\n        Set<Map.Entry<String, String>> entries = map.entrySet();\n        for (Map.Entry<String, String> entry : entries) {\n            String key = entry.getKey();\n            String value = entry.getValue();\n            System.out.println(key + "=" + value);\n        }\n        System.out.println("--------------------------");\n    }\n}\n\n\n\n# 键值对个数大于10\n\npublic class ImmutableDemo4 {\n    public static void main(String[] args) {\n\n        /*\n            创建Map的不可变集合,键值对的数量超过10个\n        */\n\n        //1.创建一个普通的Map集合\n        HashMap<String, String> hm = new HashMap<>();\n        hm.put("张三", "南京");\n        hm.put("李四", "北京");\n        hm.put("王五", "上海");\n        hm.put("赵六", "北京");\n        hm.put("孙七", "深圳");\n        hm.put("周八", "杭州");\n        hm.put("吴九", "宁波");\n        hm.put("郑十", "苏州");\n        hm.put("刘一", "无锡");\n        hm.put("陈二", "嘉兴");\n        hm.put("aaa", "111");\n\n        //2.利用上面的数据来获取一个不可变的集合\n/*\n        //获取到所有的键值对对象（Entry对象）\n        Set<Map.Entry<String, String>> entries = hm.entrySet();\n        //把entries变成一个数组\n        Map.Entry[] arr1 = new Map.Entry[0];\n        //toArray方法在底层会比较集合的长度跟数组的长度两者的大小\n        //如果集合的长度 > 数组的长度 ：数据在数组中放不下，此时会根据实际数据的个数，重新创建数组\n        //如果集合的长度 <= 数组的长度：数据在数组中放的下，此时不会创建新的数组，而是直接用\n        Map.Entry[] arr2 = entries.toArray(arr1);\n        //不可变的map集合\n        Map map = Map.ofEntries(arr2);\n        map.put("bbb","222");*/\n\n\n        //Map<Object, Object> map = Map.ofEntries(hm.entrySet().toArray(new Map.Entry[0]));\n\n        Map<String, String> map = Map.copyOf(hm);\n        map.put("bbb","222");\n    }\n}\n',normalizedContent:'# 不可变集合\n\n\n# 什么是不可变集合\n\n是一个长度不可变，内容也无法修改的集合\n\n\n# 使用场景\n\n如果某个数据不能被修改，把它防御性地拷贝到不可变集合中是个很好的实践。\n\n当集合对象被不可信的库调用时，不可变形式是安全的。\n\n简单理解：\n\n不想让别人修改集合中的内容\n\n比如说：\n\n1，斗地主的54张牌，是不能添加，不能删除，不能修改的\n\n2，斗地主的打牌规则：单张，对子，三张，顺子等，也是不能修改的\n\n3，用代码获取的操作系统硬件信息，也是不能被修改的\n\n不可变集合分类\n\n * 不可变的list集合\n * 不可变的set集合\n * 不可变的map集合\n\n\n# 不可变的list集合\n\npublic class immutabledemo1 {\n    public static void main(string[] args) {\n        /*\n            创建不可变的list集合\n            "张三", "李四", "王五", "赵六"\n        */\n\n        //一旦创建完毕之后，是无法进行修改的，在下面的代码中，只能进行查询操作\n        list<string> list = list.of("张三", "李四", "王五", "赵六");\n\n        system.out.println(list.get(0));\n        system.out.println(list.get(1));\n        system.out.println(list.get(2));\n        system.out.println(list.get(3));\n\n        system.out.println("---------------------------");\n\n        for (string s : list) {\n            system.out.println(s);\n        }\n\n        system.out.println("---------------------------");\n\n\n        iterator<string> it = list.iterator();\n        while(it.hasnext()){\n            string s = it.next();\n            system.out.println(s);\n        }\n        system.out.println("---------------------------");\n\n        for (int i = 0; i < list.size(); i++) {\n            string s = list.get(i);\n            system.out.println(s);\n        }\n        system.out.println("---------------------------");\n\n        //list.remove("李四");\n        //list.add("aaa");\n        list.set(0,"aaa");\n    }\n}\n\n\n\n# 不可变的set集合\n\npublic class immutabledemo2 {\n    public static void main(string[] args) {\n        /*\n           创建不可变的set集合\n           "张三", "李四", "王五", "赵六"\n\n\n           细节：\n                当我们要获取一个不可变的set集合时，里面的参数一定要保证唯一性\n        */\n\n        //一旦创建完毕之后，是无法进行修改的，在下面的代码中，只能进行查询操作\n        set<string> set = set.of("张三", "张三", "李四", "王五", "赵六");\n\n        for (string s : set) {\n            system.out.println(s);\n        }\n\n        system.out.println("-----------------------");\n\n        iterator<string> it = set.iterator();\n        while(it.hasnext()){\n            string s = it.next();\n            system.out.println(s);\n        }\n\n        system.out.println("-----------------------");\n        //set.remove("王五");\n    }\n}\n\n\n\n# 不可变的map集合\n\n\n# 键值对个数小于等于10\n\npublic class immutabledemo3 {\n    public static void main(string[] args) {\n       /*\n        创建map的不可变集合\n            细节1：\n                键是不能重复的\n            细节2：\n                map里面的of方法，参数是有上限的，最多只能传递20个参数，10个键值对\n            细节3：\n                如果我们要传递多个键值对对象，数量大于10个，在map接口中还有一个方法\n        */\n\n        //一旦创建完毕之后，是无法进行修改的，在下面的代码中，只能进行查询操作\n        map<string, string> map = map.of("张三", "南京", "张三", "北京", "王五", "上海",\n                "赵六", "广州", "孙七", "深圳", "周八", "杭州",\n                "吴九", "宁波", "郑十", "苏州", "刘一", "无锡",\n                "陈二", "嘉兴");\n\n        set<string> keys = map.keyset();\n        for (string key : keys) {\n            string value = map.get(key);\n            system.out.println(key + "=" + value);\n        }\n\n        system.out.println("--------------------------");\n\n        set<map.entry<string, string>> entries = map.entryset();\n        for (map.entry<string, string> entry : entries) {\n            string key = entry.getkey();\n            string value = entry.getvalue();\n            system.out.println(key + "=" + value);\n        }\n        system.out.println("--------------------------");\n    }\n}\n\n\n\n# 键值对个数大于10\n\npublic class immutabledemo4 {\n    public static void main(string[] args) {\n\n        /*\n            创建map的不可变集合,键值对的数量超过10个\n        */\n\n        //1.创建一个普通的map集合\n        hashmap<string, string> hm = new hashmap<>();\n        hm.put("张三", "南京");\n        hm.put("李四", "北京");\n        hm.put("王五", "上海");\n        hm.put("赵六", "北京");\n        hm.put("孙七", "深圳");\n        hm.put("周八", "杭州");\n        hm.put("吴九", "宁波");\n        hm.put("郑十", "苏州");\n        hm.put("刘一", "无锡");\n        hm.put("陈二", "嘉兴");\n        hm.put("aaa", "111");\n\n        //2.利用上面的数据来获取一个不可变的集合\n/*\n        //获取到所有的键值对对象（entry对象）\n        set<map.entry<string, string>> entries = hm.entryset();\n        //把entries变成一个数组\n        map.entry[] arr1 = new map.entry[0];\n        //toarray方法在底层会比较集合的长度跟数组的长度两者的大小\n        //如果集合的长度 > 数组的长度 ：数据在数组中放不下，此时会根据实际数据的个数，重新创建数组\n        //如果集合的长度 <= 数组的长度：数据在数组中放的下，此时不会创建新的数组，而是直接用\n        map.entry[] arr2 = entries.toarray(arr1);\n        //不可变的map集合\n        map map = map.ofentries(arr2);\n        map.put("bbb","222");*/\n\n\n        //map<object, object> map = map.ofentries(hm.entryset().toarray(new map.entry[0]));\n\n        map<string, string> map = map.copyof(hm);\n        map.put("bbb","222");\n    }\n}\n',charsets:{cjk:!0}},{title:"斗地主练习",frontmatter:{autoSort:94,title:"斗地主练习",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/5208ee/",categories:["后端","Java","集合"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/50.%E9%9B%86%E5%90%88/50.%E6%96%97%E5%9C%B0%E4%B8%BB%E7%BB%83%E4%B9%A0.html",relativePath:"01.后端/10.Java/50.集合/50.斗地主练习.md",key:"v-a76b5b9a",path:"/pages/5208ee/",headers:[{level:2,title:"案例介绍",slug:"案例介绍",normalizedTitle:"案例介绍",charIndex:12},{level:2,title:"案例分析",slug:"案例分析",normalizedTitle:"案例分析",charIndex:94},{level:2,title:"代码实现",slug:"代码实现",normalizedTitle:"代码实现",charIndex:341},{level:2,title:"排序（第一种排序方式）",slug:"排序-第一种排序方式",normalizedTitle:"排序（第一种排序方式）",charIndex:2409},{level:2,title:"排序（第二种排序方式）",slug:"排序-第二种排序方式",normalizedTitle:"排序（第二种排序方式）",charIndex:4807}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"案例介绍 案例分析 代码实现 排序（第一种排序方式） 排序（第二种排序方式）",content:'# 斗地主发牌\n\n\n# 案例介绍\n\n按照斗地主的规则，完成洗牌发牌的动作。\n\n具体规则：\n\n使用54张牌打乱顺序,三个玩家参与游戏，三人交替摸牌，每人17张牌，最后三张留作底牌。\n\n\n# 案例分析\n\n * 准备牌：\n   \n   牌可以设计为一个ArrayList<String>,每个字符串为一张牌。\n   \n   每张牌由花色数字两部分组成，我们可以使用花色集合与数字集合嵌套迭代完成每张牌的组装。\n   \n   牌由Collections类的shuffle方法进行随机排序。\n\n * 发牌\n   \n   将每个人以及底牌设计为ArrayList<String>,将最后3张牌直接存放于底牌，剩余牌通过对3取模依次发牌。\n\n * 看牌\n   \n   直接打印每个集合。\n\n\n# 代码实现\n\npublic class App {\n    public static void main(String[] args) {\n      /*\n        完成控制台版的三步：\n          准备牌\n          洗牌\n          发牌\n       */\n        //从程序的主入口开启斗地主游戏\n        new PokerGame();\n    }\n}\n\npublic class PokerGame {\n    //牌盒\n    //♥3 ♣3\n    static ArrayList<String> list = new ArrayList<>();\n\n    //静态代码块\n    //特点：随着类的加载而在加载的，而且只执行一次。\n    static {\n        //准备牌\n        // "♦", "♣", "♥", "♠"\n        // "3", "4", "5", "6", "7", "8", "9", "10", "J", "Q", "K", "A", "2"\n        String[] color = {"♦", "♣", "♥", "♠" };\n        String[] number = {"3", "4", "5", "6", "7", "8", "9", "10", "J", "Q", "K", "A", "2"};\n\n        for (String c : color) {\n            //c依次表示每一种花色\n            for (String n : number) {\n                //n 依次表示每一个数字\n                list.add(c + n);\n            }\n        }\n        list.add("小王");\n        list.add("大王");\n    }\n\n    public PokerGame(){\n        //洗牌\n        Collections.shuffle(list);\n\n        //发牌\n        ArrayList<String> lord = new ArrayList<>();\n        ArrayList<String> player1 = new ArrayList<>();\n        ArrayList<String> player2 = new ArrayList<>();\n        ArrayList<String> player3 = new ArrayList<>();\n\n        //遍历牌盒得到每一张牌\n        for (int i = 0; i < list.size(); i++) {\n            //i：索引\n            String poker = list.get(i);\n            if(i <= 2){\n                lord.add(poker);\n                continue;\n            }\n\n            //给三个玩家轮流发牌\n            if(i % 3 == 0){\n                player1.add(poker);\n            }else if(i % 3 == 1){\n                player2.add(poker);\n            }else{\n                player3.add(poker);\n            }\n        }\n        //看牌\n        lookPoker("底牌",lord);\n        lookPoker("钢脑壳",player1);\n        lookPoker("大帅比",player2);\n        lookPoker("蛋筒",player3);\n\n    }\n\n    /*\n    * 参数一：玩家的名字\n    * 参数二：每位玩家的牌\n    * */\n    public void lookPoker(String name, ArrayList<String> list){\n        System.out.print(name + ": ");\n        for (String poker : list) {\n            System.out.print(poker + " ");\n        }\n        System.out.println();\n    }\n}\n\n\n\n# 排序（第一种排序方式）\n\npublic class App {\n    public static void main(String[] args) {\n      /*\n        完成控制台版的四步：\n          准备牌\n          洗牌\n          发牌\n          排序\n\n       */\n\n\t\t//从程序的主入口开启斗地主游戏\n        new PokerGame();\n    }\n}\n\n\npublic class PokerGame {\n    //牌盒 Map\n    //此时我们只要把牌跟序号产生对应关系就可以了，不需要按照序号进行排序，所以只要HashMap就可以了\n    static HashMap<Integer, String> hm = new HashMap<>();\n    static ArrayList<Integer> list = new ArrayList<>();\n\n    static {\n        String[] color = {"♦", "♣", "♥", "♠"};\n        String[] number = {"3", "4", "5", "6", "7", "8", "9", "10", "J", "Q", "K", "A", "2"};\n\n        //序号\n        int serialNumber = 1;\n        //细节\n        for (String n : number) {\n            //依次表示每一个数字\n            for (String c : color) {\n                //依次表示每一个花色\n                hm.put(serialNumber, c + n);\n                list.add(serialNumber);\n                serialNumber++;\n            }\n        }\n\n        hm.put(serialNumber, "小王");\n        list.add(serialNumber);\n        serialNumber++;\n        hm.put(serialNumber, "大王");\n        list.add(serialNumber);\n\n    }\n\n    public PokerGame() {\n        //洗牌\n        Collections.shuffle(list);\n\n        //发牌\n        TreeSet<Integer> lord = new TreeSet<>();\n        TreeSet<Integer> player1 = new TreeSet<>();\n        TreeSet<Integer> player2 = new TreeSet<>();\n        TreeSet<Integer> player3 = new TreeSet<>();\n\n        for (int i = 0; i < list.size(); i++) {\n            //i :依次表示集合中的每一个索引\n            //list.get(i)元素：牌的序号\n            int serialNumber = list.get(i);\n\n            if(i <= 2){\n                lord.add(serialNumber);\n                continue;\n            }\n\n            if(i % 3 == 0){\n                player1.add(serialNumber);\n            }else if(i % 3 == 1){\n                player2.add(serialNumber);\n            }else{\n                player3.add(serialNumber);\n            }\n        }\n\n\n        //看牌\n        lookPoker("底牌",lord);\n        lookPoker("钢脑壳",player1);\n        lookPoker("大帅比",player2);\n        lookPoker("蛋筒",player3);\n\n    }\n\n    /*\n    * 参数一：玩家的名字\n    * 参数二：牌的序号\n    * */\n    public void lookPoker(String name, TreeSet<Integer> ts){\n        System.out.print(name + ": ");\n        //遍历TreeSet集合得到每一个序号，再拿着序号到Map集合中去找真正的牌\n        for (int serialNumber : ts) {\n            String poker = hm.get(serialNumber);\n            System.out.print(poker + " ");\n        }\n        System.out.println();\n    }\n}\n\n\n\n# 排序（第二种排序方式）\n\npublic class App {\n    public static void main(String[] args) {\n        new PokerGame();\n    }\n}\n\n\npublic class PokerGame {\n    //牌盒\n    static ArrayList<String> list = new ArrayList<>();\n\n    //创建一个集合，用来添加牌的价值\n    static HashMap<String, Integer> hm = new HashMap<>();\n\n    static {\n        //准备牌\n        String[] color = {"♦", "♣", "♥", "♠"};\n        String[] number = {"3", "4", "5", "6", "7", "8", "9", "10", "J", "Q", "K", "A", "2"};\n\n        for (String c : color) {\n            for (String n : number) {\n                list.add(c + n);\n            }\n        }\n        list.add(" 小王");\n        list.add(" 大王");\n\n\n        //指定牌的价值\n        //牌上的数字到Map集合中判断是否存在\n        //存在，获取价值\n        //不存在，本身的数字就是价值\n        hm.put("J", 11);\n        hm.put("Q", 12);\n        hm.put("K", 13);\n        hm.put("A", 14);\n        hm.put("2", 15);\n        hm.put("小王", 50);\n        hm.put("大王", 100);\n\n\n    }\n\n    public PokerGame() {\n        //洗牌\n        Collections.shuffle(list);\n\n        //发牌\n        ArrayList<String> lord = new ArrayList<>();\n        ArrayList<String> player1 = new ArrayList<>();\n        ArrayList<String> player2 = new ArrayList<>();\n        ArrayList<String> player3 = new ArrayList<>();\n\n        for (int i = 0; i < list.size(); i++) {\n            String poker = list.get(i);\n            //发底牌\n            if (i <= 2) {\n                lord.add(poker);\n                continue;\n            }\n\n            //给三个玩家轮流发牌\n            if (i % 3 == 0) {\n                player1.add(poker);\n            } else if (i % 3 == 1) {\n                player2.add(poker);\n            } else {\n                player3.add(poker);\n            }\n        }\n\n\n        //排序\n        order(lord);\n        order(player1);\n        order(player2);\n        order(player3);\n\n\n        //看牌\n        lookPoker("底牌",lord);\n        lookPoker("钢脑壳",player1);\n        lookPoker("大帅比",player2);\n        lookPoker("蛋筒",player3);\n    }\n\n\n    /*\n     * 参数一：玩家的名字\n     * 参数二：每位玩家的牌\n     * */\n    public void lookPoker(String name, ArrayList<String> list){\n        System.out.print(name + ": ");\n        for (String poker : list) {\n            System.out.print(poker + " ");\n        }\n        System.out.println();\n\n    }\n\n\n    //利用牌的价值进行排序\n    //参数：集合\n    //♥5 ♥3 ♥6 ♥7 ♥9\n    public void order(ArrayList<String> list){\n        Collections.sort(list, new Comparator<String>() {\n            //Array.sort （插入排序 + 二分查找）\n            @Override\n            public int compare(String o1, String o2) {\n                //o1：表示当前要插入到有序序列中的牌\n                //o2：表示已经在有序序列中存在的牌\n\n                //负数：o1小 插入到前面\n                //正数：o1大 插入到后面\n                //0：o1的数字跟o2的数字是一样的，需要按照花色再次排序\n\n                //1.计算o1的花色和价值   大王\n                String color1 = o1.substring(0, 1);\n                int value1 = getValue(o1);\n\n                //2.计算o2的花色和价值\n                String color2 = o2.substring(0, 1);\n                int value2 = getValue(o2);\n\n                //3.比较o1和o2的价值    ♥3  ♠3\n                int i = value1 - value2;\n                return i == 0 ? color1.compareTo(color2) : i;\n\n            }\n        });\n    }\n\n    //计算牌的价值\n    //参数：牌\n    //返回值：价值\n    public int getValue(String poker){//♥3\n        //获取牌上的数字\n        String number = poker.substring(1);//把这里截取出来的结果，让这个结果再Map集合中存在 “ 大王”\n        //拿着数字到map集合中判断是否存在\n        if(hm.containsKey(number)){\n            //存在，获取价值\n            return hm.get(number);\n        }else{\n            //不存在，类型转换\n            return Integer.parseInt(number);\n        }\n    }\n}\n',normalizedContent:'# 斗地主发牌\n\n\n# 案例介绍\n\n按照斗地主的规则，完成洗牌发牌的动作。\n\n具体规则：\n\n使用54张牌打乱顺序,三个玩家参与游戏，三人交替摸牌，每人17张牌，最后三张留作底牌。\n\n\n# 案例分析\n\n * 准备牌：\n   \n   牌可以设计为一个arraylist<string>,每个字符串为一张牌。\n   \n   每张牌由花色数字两部分组成，我们可以使用花色集合与数字集合嵌套迭代完成每张牌的组装。\n   \n   牌由collections类的shuffle方法进行随机排序。\n\n * 发牌\n   \n   将每个人以及底牌设计为arraylist<string>,将最后3张牌直接存放于底牌，剩余牌通过对3取模依次发牌。\n\n * 看牌\n   \n   直接打印每个集合。\n\n\n# 代码实现\n\npublic class app {\n    public static void main(string[] args) {\n      /*\n        完成控制台版的三步：\n          准备牌\n          洗牌\n          发牌\n       */\n        //从程序的主入口开启斗地主游戏\n        new pokergame();\n    }\n}\n\npublic class pokergame {\n    //牌盒\n    //♥3 ♣3\n    static arraylist<string> list = new arraylist<>();\n\n    //静态代码块\n    //特点：随着类的加载而在加载的，而且只执行一次。\n    static {\n        //准备牌\n        // "♦", "♣", "♥", "♠"\n        // "3", "4", "5", "6", "7", "8", "9", "10", "j", "q", "k", "a", "2"\n        string[] color = {"♦", "♣", "♥", "♠" };\n        string[] number = {"3", "4", "5", "6", "7", "8", "9", "10", "j", "q", "k", "a", "2"};\n\n        for (string c : color) {\n            //c依次表示每一种花色\n            for (string n : number) {\n                //n 依次表示每一个数字\n                list.add(c + n);\n            }\n        }\n        list.add("小王");\n        list.add("大王");\n    }\n\n    public pokergame(){\n        //洗牌\n        collections.shuffle(list);\n\n        //发牌\n        arraylist<string> lord = new arraylist<>();\n        arraylist<string> player1 = new arraylist<>();\n        arraylist<string> player2 = new arraylist<>();\n        arraylist<string> player3 = new arraylist<>();\n\n        //遍历牌盒得到每一张牌\n        for (int i = 0; i < list.size(); i++) {\n            //i：索引\n            string poker = list.get(i);\n            if(i <= 2){\n                lord.add(poker);\n                continue;\n            }\n\n            //给三个玩家轮流发牌\n            if(i % 3 == 0){\n                player1.add(poker);\n            }else if(i % 3 == 1){\n                player2.add(poker);\n            }else{\n                player3.add(poker);\n            }\n        }\n        //看牌\n        lookpoker("底牌",lord);\n        lookpoker("钢脑壳",player1);\n        lookpoker("大帅比",player2);\n        lookpoker("蛋筒",player3);\n\n    }\n\n    /*\n    * 参数一：玩家的名字\n    * 参数二：每位玩家的牌\n    * */\n    public void lookpoker(string name, arraylist<string> list){\n        system.out.print(name + ": ");\n        for (string poker : list) {\n            system.out.print(poker + " ");\n        }\n        system.out.println();\n    }\n}\n\n\n\n# 排序（第一种排序方式）\n\npublic class app {\n    public static void main(string[] args) {\n      /*\n        完成控制台版的四步：\n          准备牌\n          洗牌\n          发牌\n          排序\n\n       */\n\n\t\t//从程序的主入口开启斗地主游戏\n        new pokergame();\n    }\n}\n\n\npublic class pokergame {\n    //牌盒 map\n    //此时我们只要把牌跟序号产生对应关系就可以了，不需要按照序号进行排序，所以只要hashmap就可以了\n    static hashmap<integer, string> hm = new hashmap<>();\n    static arraylist<integer> list = new arraylist<>();\n\n    static {\n        string[] color = {"♦", "♣", "♥", "♠"};\n        string[] number = {"3", "4", "5", "6", "7", "8", "9", "10", "j", "q", "k", "a", "2"};\n\n        //序号\n        int serialnumber = 1;\n        //细节\n        for (string n : number) {\n            //依次表示每一个数字\n            for (string c : color) {\n                //依次表示每一个花色\n                hm.put(serialnumber, c + n);\n                list.add(serialnumber);\n                serialnumber++;\n            }\n        }\n\n        hm.put(serialnumber, "小王");\n        list.add(serialnumber);\n        serialnumber++;\n        hm.put(serialnumber, "大王");\n        list.add(serialnumber);\n\n    }\n\n    public pokergame() {\n        //洗牌\n        collections.shuffle(list);\n\n        //发牌\n        treeset<integer> lord = new treeset<>();\n        treeset<integer> player1 = new treeset<>();\n        treeset<integer> player2 = new treeset<>();\n        treeset<integer> player3 = new treeset<>();\n\n        for (int i = 0; i < list.size(); i++) {\n            //i :依次表示集合中的每一个索引\n            //list.get(i)元素：牌的序号\n            int serialnumber = list.get(i);\n\n            if(i <= 2){\n                lord.add(serialnumber);\n                continue;\n            }\n\n            if(i % 3 == 0){\n                player1.add(serialnumber);\n            }else if(i % 3 == 1){\n                player2.add(serialnumber);\n            }else{\n                player3.add(serialnumber);\n            }\n        }\n\n\n        //看牌\n        lookpoker("底牌",lord);\n        lookpoker("钢脑壳",player1);\n        lookpoker("大帅比",player2);\n        lookpoker("蛋筒",player3);\n\n    }\n\n    /*\n    * 参数一：玩家的名字\n    * 参数二：牌的序号\n    * */\n    public void lookpoker(string name, treeset<integer> ts){\n        system.out.print(name + ": ");\n        //遍历treeset集合得到每一个序号，再拿着序号到map集合中去找真正的牌\n        for (int serialnumber : ts) {\n            string poker = hm.get(serialnumber);\n            system.out.print(poker + " ");\n        }\n        system.out.println();\n    }\n}\n\n\n\n# 排序（第二种排序方式）\n\npublic class app {\n    public static void main(string[] args) {\n        new pokergame();\n    }\n}\n\n\npublic class pokergame {\n    //牌盒\n    static arraylist<string> list = new arraylist<>();\n\n    //创建一个集合，用来添加牌的价值\n    static hashmap<string, integer> hm = new hashmap<>();\n\n    static {\n        //准备牌\n        string[] color = {"♦", "♣", "♥", "♠"};\n        string[] number = {"3", "4", "5", "6", "7", "8", "9", "10", "j", "q", "k", "a", "2"};\n\n        for (string c : color) {\n            for (string n : number) {\n                list.add(c + n);\n            }\n        }\n        list.add(" 小王");\n        list.add(" 大王");\n\n\n        //指定牌的价值\n        //牌上的数字到map集合中判断是否存在\n        //存在，获取价值\n        //不存在，本身的数字就是价值\n        hm.put("j", 11);\n        hm.put("q", 12);\n        hm.put("k", 13);\n        hm.put("a", 14);\n        hm.put("2", 15);\n        hm.put("小王", 50);\n        hm.put("大王", 100);\n\n\n    }\n\n    public pokergame() {\n        //洗牌\n        collections.shuffle(list);\n\n        //发牌\n        arraylist<string> lord = new arraylist<>();\n        arraylist<string> player1 = new arraylist<>();\n        arraylist<string> player2 = new arraylist<>();\n        arraylist<string> player3 = new arraylist<>();\n\n        for (int i = 0; i < list.size(); i++) {\n            string poker = list.get(i);\n            //发底牌\n            if (i <= 2) {\n                lord.add(poker);\n                continue;\n            }\n\n            //给三个玩家轮流发牌\n            if (i % 3 == 0) {\n                player1.add(poker);\n            } else if (i % 3 == 1) {\n                player2.add(poker);\n            } else {\n                player3.add(poker);\n            }\n        }\n\n\n        //排序\n        order(lord);\n        order(player1);\n        order(player2);\n        order(player3);\n\n\n        //看牌\n        lookpoker("底牌",lord);\n        lookpoker("钢脑壳",player1);\n        lookpoker("大帅比",player2);\n        lookpoker("蛋筒",player3);\n    }\n\n\n    /*\n     * 参数一：玩家的名字\n     * 参数二：每位玩家的牌\n     * */\n    public void lookpoker(string name, arraylist<string> list){\n        system.out.print(name + ": ");\n        for (string poker : list) {\n            system.out.print(poker + " ");\n        }\n        system.out.println();\n\n    }\n\n\n    //利用牌的价值进行排序\n    //参数：集合\n    //♥5 ♥3 ♥6 ♥7 ♥9\n    public void order(arraylist<string> list){\n        collections.sort(list, new comparator<string>() {\n            //array.sort （插入排序 + 二分查找）\n            @override\n            public int compare(string o1, string o2) {\n                //o1：表示当前要插入到有序序列中的牌\n                //o2：表示已经在有序序列中存在的牌\n\n                //负数：o1小 插入到前面\n                //正数：o1大 插入到后面\n                //0：o1的数字跟o2的数字是一样的，需要按照花色再次排序\n\n                //1.计算o1的花色和价值   大王\n                string color1 = o1.substring(0, 1);\n                int value1 = getvalue(o1);\n\n                //2.计算o2的花色和价值\n                string color2 = o2.substring(0, 1);\n                int value2 = getvalue(o2);\n\n                //3.比较o1和o2的价值    ♥3  ♠3\n                int i = value1 - value2;\n                return i == 0 ? color1.compareto(color2) : i;\n\n            }\n        });\n    }\n\n    //计算牌的价值\n    //参数：牌\n    //返回值：价值\n    public int getvalue(string poker){//♥3\n        //获取牌上的数字\n        string number = poker.substring(1);//把这里截取出来的结果，让这个结果再map集合中存在 “ 大王”\n        //拿着数字到map集合中判断是否存在\n        if(hm.containskey(number)){\n            //存在，获取价值\n            return hm.get(number);\n        }else{\n            //不存在，类型转换\n            return integer.parseint(number);\n        }\n    }\n}\n',charsets:{cjk:!0}},{title:"方法引用",frontmatter:{autoSort:91,title:"方法引用",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/eeb1b8/",categories:["后端","Java","集合"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/50.%E9%9B%86%E5%90%88/80.%E6%96%B9%E6%B3%95%E5%BC%95%E7%94%A8.html",relativePath:"01.后端/10.Java/50.集合/80.方法引用.md",key:"v-2a8eeee9",path:"/pages/eeb1b8/",headers:[{level:2,title:"体验方法引用",slug:"体验方法引用",normalizedTitle:"体验方法引用",charIndex:211},{level:2,title:"方法引用符",slug:"方法引用符",normalizedTitle:"方法引用符",charIndex:1006},{level:2,title:"基本引用方法",slug:"基本引用方法",normalizedTitle:"基本引用方法",charIndex:1207},{level:3,title:"引用静态方法",slug:"引用静态方法",normalizedTitle:"引用静态方法",charIndex:1363},{level:3,title:"引用对象的实例方法",slug:"引用对象的实例方法",normalizedTitle:"引用对象的实例方法",charIndex:2814},{level:3,title:"引用构造方法",slug:"引用构造方法",normalizedTitle:"引用构造方法",charIndex:1322},{level:2,title:"额外引用方法",slug:"额外引用方法",normalizedTitle:"额外引用方法",charIndex:5861},{level:3,title:"引用类的实例方法",slug:"引用类的实例方法",normalizedTitle:"引用类的实例方法",charIndex:5908},{level:3,title:"引用数组的构造方法",slug:"引用数组的构造方法",normalizedTitle:"引用数组的构造方法",charIndex:8197},{level:2,title:"方法引用总结",slug:"方法引用总结",normalizedTitle:"方法引用总结",charIndex:9261}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"体验方法引用 方法引用符 基本引用方法 引用静态方法 引用对象的实例方法 引用构造方法 额外引用方法 引用类的实例方法 引用数组的构造方法 方法引用总结",content:'# 方法引用\n\n函数式接口\n\n * 函数式接口在 Java 中是指： 有且仅有一个抽象方法的接口 。\n\n * @FunctionalInterface 。该注解可用于一个接口的定义上：\n   \n   一旦使用该注解来定义接口，编译器将会强制检查该接口是否确实有且仅有一个抽象方法，否则将会报错。\n   \n   需要 注意 的是，即使不使用该注解，只要满足函数式接口的定义，这仍然是一个函数式接口，使用起来都一样。\n\n\n# 体验方法引用\n\n * 方法引用的出现原因\n   \n   在使用Lambda表达式的时候，我们实际上传递进去的代码就是一种解决方案：拿参数做操作\n   \n   那么考虑一种情况：如果我们在Lambda中所指定的操作方案，已经有地方存在相同方案，那是否还有必要再写重复逻辑呢？答案肯定是没有必要\n   \n   那我们又是如何使用已经存在的方案的呢？\n   \n   这就是我们要讲解的方法引用，我们是通过方法引用来使用已经存在的方案\n\n * 代码演示\n   \n   public interface Printable {\n       void printString(String s);\n   }\n   \n   public class PrintableDemo {\n       public static void main(String[] args) {\n           //在主方法中调用usePrintable方法\n   //        usePrintable((String s) -> {\n   //            System.out.println(s);\n   //        });\n   \t    //Lambda简化写法\n           usePrintable(s -> System.out.println(s));\n   \n           //方法引用\n           usePrintable(System.out::println);\n   \n       }\n   \n       private static void usePrintable(Printable p) {\n           p.printString("爱生活爱Java");\n       }\n   }\n   \n   \n\n\n# 方法引用符\n\n * 方法引用符\n   \n   :: 该符号为引用运算符，而它所在的表达式被称为方法引用\n\n * 推导与省略\n   \n   * 如果使用Lambda，那么根据“可推导就是可省略”的原则，无需指定参数类型，也无需指定的重载形式，它们都将被自动推导\n   * 如果使用方法引用，也是同样可以根据上下文进行推导\n   * 方法引用是Lambda的孪生兄弟\n\n * 分类\n   \n   \n\n\n# 基本引用方法\n\n把已经有的方法拿过来用，当作函数式接口中抽象方法的方法体。\n\n要求：\n\n * 引用处必须是 函数式接口\n * 被引用的方法必须已经存在\n * 被引用的方法**-形参和返回值**，需要跟抽象方法保持一致。\n   * 引用构造方法，不需要考虑返回值\n * 被引用方法的功能，需要满足当前需求\n\n\n# 引用静态方法\n\n引用类方法，其实就是引用类的静态方法\n\n * 格式\n   \n   ==类名::静态方法==\n\n * 范例\n   \n   ==Integer::parseInt==\n   \n   Integer类的方法：public static int parseInt(String s) 将此String转换为int类型数据\n   \n   public static void main(String[] args) {\n   \n       ArrayList<String> list = new ArrayList<>();\n       Collections.addAll(list, "1", "2", "3", "4");\n   \n       // 将字符串类型转换成 int类型 保存起来\n   \n       // 1. lambda 表达式\n       List<Integer> collect = list.stream().map(s -> Integer.parseInt(s)).collect(Collectors.toList());\n       System.out.println(collect);\n   \n       // 2. 方法引用\n       List<Integer> collect1 = list.stream()\n               .map(Integer::parseInt)\n               .collect(Collectors.toList());\n       System.out.println(collect1);\n   \n   \n   }\n   \n\n * 练习描述\n   \n   * 定义一个接口(Converter)，里面定义一个抽象方法 int convert(String s);\n   * 定义一个测试类(ConverterDemo)，在测试类中提供两个方法\n     * 一个方法是：useConverter(Converter c)\n     * 一个方法是主方法，在主方法中调用useConverter方法\n\n * 代码演示\n   \n   public interface Converter {\n       int convert(String s);\n   }\n   \n   public class ConverterDemo {\n       public static void main(String[] args) {\n   \n   \t\t//Lambda写法\n           useConverter(s -> Integer.parseInt(s));\n   \n           //引用类方法\n           useConverter(Integer::parseInt);\n   \n       }\n   \n       private static void useConverter(Converter c) {\n           int number = c.convert("666");\n           System.out.println(number);\n       }\n   }\n   \n\n * 使用说明\n   \n   Lambda表达式被类方法替代的时候，它的形式参数全部传递给静态方法作为参数\n\n\n# 引用对象的实例方法\n\n引用对象的实例方法，其实就引用类中的成员方法\n\n * 类别\n   * 其他类----\x3e 其他类对象对象::方法名\n   * 本类----\x3e this::方法名 或者 本类对象::方法名 ----\x3e 注意: ==静态方法没有this 对象==\n   * 父类----\x3e super::方法名\n\n * 格式\n   \n   ==对象::成员方法==\n\n * 范例\n   \n   =="HelloWorld"::toUpperCase==\n   \n   String类中的方法：public String toUpperCase() 将此String所有字符转换为大写\n\n * 练习描述\n   \n   * 定义一个类(PrintString)，里面定义一个方法\n     \n     public void printUpper(String s)：把字符串参数变成大写的数据，然后在控制台输出\n   \n   * 定义一个接口(Printer)，里面定义一个抽象方法\n     \n     void printUpperCase(String s)\n   \n   * 定义一个测试类(PrinterDemo)，在测试类中提供两个方法\n     \n     * 一个方法是：usePrinter(Printer p)\n     * 一个方法是主方法，在主方法中调用usePrinter方法\n\n * 代码演示\n   \n   public class PrintString {\n       //把字符串参数变成大写的数据，然后在控制台输出\n       public void printUpper(String s) {\n           String result = s.toUpperCase();\n           System.out.println(result);\n       }\n   }\n   \n   public interface Printer {\n       void printUpperCase(String s);\n   }\n   \n   public class PrinterDemo {\n       public static void main(String[] args) {\n   \n   \t\t//Lambda简化写法\n           usePrinter(s -> System.out.println(s.toUpperCase()));\n   \n           //引用对象的实例方法\n           PrintString ps = new PrintString();\n           usePrinter(ps::printUpper);\n   \n       }\n   \n       private static void usePrinter(Printer p) {\n           p.printUpperCase("HelloWorld");\n       }\n   }\n   \n   \n\n * 使用说明\n   \n   Lambda表达式被对象的实例方法替代的时候，它的形式参数全部传递给该方法作为参数\n\n\n# 引用构造方法\n\n==构造方法，不用关心返回值, 但是仍然要关心传入参数==\n\n引用构造器，其实就是引用构造方法\n\n * 格式\n   \n   ==类名::new==\n\n * 范例\n   \n   ==Student::new==\n\n * 练习描述\n   \n   * 定义一个类(Student)，里面有两个成员变量(name,age)\n     \n     并提供无参构造方法和带参构造方法，以及成员变量对应的get和set方法\n\n * 代码演示\n   \n   /**\n    * @ClassName FunctionDemo1\n    * @Date 2023/3/1 14:00\n    * @Author diane\n    * @Description 方法引用：引用构造方法\n    * @Version 1.0\n    */\n   public class FunctionDemo4 {\n     public static void main(String[] args) {\n   \n       ArrayList<String> list = new ArrayList<String>();\n       list.add("周润发,23");\n       list.add("成龙,24");\n       list.add("刘德华,25");\n       list.add("吴京,26");\n       list.add("周星驰,17");\n       list.add("李连杰,18");\n   \n       // 将String 转换成 Student对象\n   \n       // 1. lambda 表达式\n       List<Student> collect = list.stream()\n               .map(s -> new Student(s.split(",")[0], Integer.parseInt(s.split(",")[1])))\n               .collect(Collectors.toList());\n       System.out.println(collect);\n   \n       // 2. 方法引用\n       // 构造方法比较特殊，不用考虑返回值；只需要关心传入参数是否一致\n       List<Student> collect1 = list.stream()\n               .map(Student::new)\n               .collect(Collectors.toList());\n       System.out.println(collect1);\n   \n   \n     }\n   }\n   \n   \n   \n   @Data\n   @AllArgsConstructor\n   class Student {\n     private String name;\n     private int age;\n   \n     /**\n      * 这个构造函数 是为了  可以使用 Student::new  作为方法调用\n      *    因为 流中的数据 参数是String类型, 所以要创建一个参数是String类型的构造方法\n      * @param s 参数--- 流中的数据\n      */\n     public Student(String s) {\n       String[] split = s.split(",");\n       this.name = split[0];\n       this.age = Integer.parseInt(split[1]);\n     }\n   }\n   \n\n * 使用说明\n   \n   Lambda表达式被构造器替代的时候，它的形式参数全部传递给构造器作为参数\n\n\n# 额外引用方法\n\n这里面的方法引用的规则 与基本引用方法的规则 不一致，是受到限制的。\n\n\n# 引用类的实例方法\n\n引用规则\n\n\n\n * 被引用方法的形参，需要跟抽象方法的==第二个形参到最后一个形参保持一致==，==返回值需要保持一致==。\n\n * 抽象方法的形参：==第一个形参的类型==，==作为方法的调用者==，决定了==可以引用那些类的成员方法==。\n\n * 特别的，当抽象方法只有一个参数时，如\n   \n   list.stream().map(s -> s.toUpperCase()).forEach(System.out::println);\n   list.stream().map(new Function<String, String>() {\n         @Override\n         public String apply(String s) {\n             // 第一个形参s  调用了这个方法\n           return s.toUpperCase();\n         }\n       }).forEach(System.out::println);\n   \n   \n   map中的抽象方法，只有一个String类型的参数，没有第二个形参到最后一个形参;且返回值是String类型的\n   \n   若想引用方法，则要求\n   \n   * 1.被引用的方法必须是属于String类的，或者是系统类\n     * 第一个参数作为方法的调用者，可以调用本类的，也可以调用一些系统类的方法\n   * 2.被引用的方法必须是==无参的==。\n   * 3.返回值必须是String类型的\n   \n   \n   \n   如上图所示，toUpperCase()是属于String类的，且它是无参的，且返回值是String类型的；满足要求，所以可以使用方法引用\n   \n   list.stream().map(String::toUpperCase).forEach(System.out::println);\n   \n\n * 当抽象方法只有一个参数，没有返回值时,跟上述所说只有一个区别，\n   \n   * 就是引用的方法必须是无返回值的\n   * 无参+系统类方法\n   \n   list.stream().map(String::toUpperCase)\n       .forEach(new Consumer<String>() {\n             @Override\n             public void accept(String s) {\n                 // 第一个参数s，调用了这个方法\n               System.out.println(s);\n             }\n   });\n   list.stream().map(String::toUpperCase).forEach(System.out::println);\n   \n\n引用类的实例方法，其实就是引用类中的成员方法\n\n * 格式\n   \n   ==类名::成员方法==\n\n * 范例\n   \n   ==String::substring==\n   \n   public String substring(int beginIndex,int endIndex)\n   \n   从beginIndex开始到endIndex结束，截取字符串。返回一个子串，子串的长度为endIndex-beginIndex\n   \n   ==System.out::println==\n\n * 练习描述\n   \n   * 定义一个接口(MyString)，里面定义一个抽象方法：\n     \n     String mySubString(String s,int x,int y);\n   \n   * 定义一个测试类(MyStringDemo)，在测试类中提供两个方法\n     \n     * 一个方法是：useMyString(MyString my)\n     * 一个方法是主方法，在主方法中调用useMyString方法\n\n * 代码演示\n   \n   public interface MyString {\n       String mySubString(String s,int x,int y);\n   }\n   \n   public class MyStringDemo {\n       public static void main(String[] args) {\n   \t\t//Lambda简化写法\n           useMyString((s,x,y) -> s.substring(x,y));\n   \n           //引用类的实例方法\n           useMyString(String::substring);\n   \n       }\n   \n       private static void useMyString(MyString my) {\n           String s = my.mySubString("HelloWorld", 2, 5);\n           System.out.println(s);\n       }\n   }\n   \n\n * 使用说明\n   \n   Lambda表达式被类的实例方法替代的时候 第一个参数作为调用者 后面的参数全部传递给该方法作为参数\n\n\n# 引用数组的构造方法\n\n * 格式\n   \n   数据类型[]::new\n\n * 细节\n   \n   数组的类型，需要跟流中的数据类型保持一致。\n   \n   /**\n    * @ClassName FunctionDemo1\n    * @Date 2023/3/1 14:00\n    * @Author diane\n    * @Description 方法引用：使用数组的构造方法\n    *    数据类型[]::new\n    * @Version 1.0\n    */\n   public class FunctionDemo6 {\n     public static void main(String[] args) {\n   \n       ArrayList<String> list = new ArrayList<>();\n       Collections.addAll(list, "aaa", "bbb", "ccc", "ddd");\n   \n       // 匿名内部类\n       String[] strings = list.stream().toArray(new IntFunction<String[]>() {\n         @Override\n         public String[] apply(int value) {\n           return new String[value];\n         }\n       });\n       System.out.println(Arrays.toString(strings));\n   \n   \n       // lambda 表达式\n       String[] strings1 = list.stream().toArray(value -> new String[value]);\n       System.out.println(Arrays.toString(strings1));\n   \n       // 方法引用\n       // 创建数组类型 必须与流中的类型一致\n       String[] strings2 = list.stream().toArray(String[]::new);\n       System.out.println(Arrays.toString(strings2));\n   \n     }\n   \n   }\n   \n\n\n# 方法引用总结\n\n\n\n',normalizedContent:'# 方法引用\n\n函数式接口\n\n * 函数式接口在 java 中是指： 有且仅有一个抽象方法的接口 。\n\n * @functionalinterface 。该注解可用于一个接口的定义上：\n   \n   一旦使用该注解来定义接口，编译器将会强制检查该接口是否确实有且仅有一个抽象方法，否则将会报错。\n   \n   需要 注意 的是，即使不使用该注解，只要满足函数式接口的定义，这仍然是一个函数式接口，使用起来都一样。\n\n\n# 体验方法引用\n\n * 方法引用的出现原因\n   \n   在使用lambda表达式的时候，我们实际上传递进去的代码就是一种解决方案：拿参数做操作\n   \n   那么考虑一种情况：如果我们在lambda中所指定的操作方案，已经有地方存在相同方案，那是否还有必要再写重复逻辑呢？答案肯定是没有必要\n   \n   那我们又是如何使用已经存在的方案的呢？\n   \n   这就是我们要讲解的方法引用，我们是通过方法引用来使用已经存在的方案\n\n * 代码演示\n   \n   public interface printable {\n       void printstring(string s);\n   }\n   \n   public class printabledemo {\n       public static void main(string[] args) {\n           //在主方法中调用useprintable方法\n   //        useprintable((string s) -> {\n   //            system.out.println(s);\n   //        });\n   \t    //lambda简化写法\n           useprintable(s -> system.out.println(s));\n   \n           //方法引用\n           useprintable(system.out::println);\n   \n       }\n   \n       private static void useprintable(printable p) {\n           p.printstring("爱生活爱java");\n       }\n   }\n   \n   \n\n\n# 方法引用符\n\n * 方法引用符\n   \n   :: 该符号为引用运算符，而它所在的表达式被称为方法引用\n\n * 推导与省略\n   \n   * 如果使用lambda，那么根据“可推导就是可省略”的原则，无需指定参数类型，也无需指定的重载形式，它们都将被自动推导\n   * 如果使用方法引用，也是同样可以根据上下文进行推导\n   * 方法引用是lambda的孪生兄弟\n\n * 分类\n   \n   \n\n\n# 基本引用方法\n\n把已经有的方法拿过来用，当作函数式接口中抽象方法的方法体。\n\n要求：\n\n * 引用处必须是 函数式接口\n * 被引用的方法必须已经存在\n * 被引用的方法**-形参和返回值**，需要跟抽象方法保持一致。\n   * 引用构造方法，不需要考虑返回值\n * 被引用方法的功能，需要满足当前需求\n\n\n# 引用静态方法\n\n引用类方法，其实就是引用类的静态方法\n\n * 格式\n   \n   ==类名::静态方法==\n\n * 范例\n   \n   ==integer::parseint==\n   \n   integer类的方法：public static int parseint(string s) 将此string转换为int类型数据\n   \n   public static void main(string[] args) {\n   \n       arraylist<string> list = new arraylist<>();\n       collections.addall(list, "1", "2", "3", "4");\n   \n       // 将字符串类型转换成 int类型 保存起来\n   \n       // 1. lambda 表达式\n       list<integer> collect = list.stream().map(s -> integer.parseint(s)).collect(collectors.tolist());\n       system.out.println(collect);\n   \n       // 2. 方法引用\n       list<integer> collect1 = list.stream()\n               .map(integer::parseint)\n               .collect(collectors.tolist());\n       system.out.println(collect1);\n   \n   \n   }\n   \n\n * 练习描述\n   \n   * 定义一个接口(converter)，里面定义一个抽象方法 int convert(string s);\n   * 定义一个测试类(converterdemo)，在测试类中提供两个方法\n     * 一个方法是：useconverter(converter c)\n     * 一个方法是主方法，在主方法中调用useconverter方法\n\n * 代码演示\n   \n   public interface converter {\n       int convert(string s);\n   }\n   \n   public class converterdemo {\n       public static void main(string[] args) {\n   \n   \t\t//lambda写法\n           useconverter(s -> integer.parseint(s));\n   \n           //引用类方法\n           useconverter(integer::parseint);\n   \n       }\n   \n       private static void useconverter(converter c) {\n           int number = c.convert("666");\n           system.out.println(number);\n       }\n   }\n   \n\n * 使用说明\n   \n   lambda表达式被类方法替代的时候，它的形式参数全部传递给静态方法作为参数\n\n\n# 引用对象的实例方法\n\n引用对象的实例方法，其实就引用类中的成员方法\n\n * 类别\n   * 其他类----\x3e 其他类对象对象::方法名\n   * 本类----\x3e this::方法名 或者 本类对象::方法名 ----\x3e 注意: ==静态方法没有this 对象==\n   * 父类----\x3e super::方法名\n\n * 格式\n   \n   ==对象::成员方法==\n\n * 范例\n   \n   =="helloworld"::touppercase==\n   \n   string类中的方法：public string touppercase() 将此string所有字符转换为大写\n\n * 练习描述\n   \n   * 定义一个类(printstring)，里面定义一个方法\n     \n     public void printupper(string s)：把字符串参数变成大写的数据，然后在控制台输出\n   \n   * 定义一个接口(printer)，里面定义一个抽象方法\n     \n     void printuppercase(string s)\n   \n   * 定义一个测试类(printerdemo)，在测试类中提供两个方法\n     \n     * 一个方法是：useprinter(printer p)\n     * 一个方法是主方法，在主方法中调用useprinter方法\n\n * 代码演示\n   \n   public class printstring {\n       //把字符串参数变成大写的数据，然后在控制台输出\n       public void printupper(string s) {\n           string result = s.touppercase();\n           system.out.println(result);\n       }\n   }\n   \n   public interface printer {\n       void printuppercase(string s);\n   }\n   \n   public class printerdemo {\n       public static void main(string[] args) {\n   \n   \t\t//lambda简化写法\n           useprinter(s -> system.out.println(s.touppercase()));\n   \n           //引用对象的实例方法\n           printstring ps = new printstring();\n           useprinter(ps::printupper);\n   \n       }\n   \n       private static void useprinter(printer p) {\n           p.printuppercase("helloworld");\n       }\n   }\n   \n   \n\n * 使用说明\n   \n   lambda表达式被对象的实例方法替代的时候，它的形式参数全部传递给该方法作为参数\n\n\n# 引用构造方法\n\n==构造方法，不用关心返回值, 但是仍然要关心传入参数==\n\n引用构造器，其实就是引用构造方法\n\n * 格式\n   \n   ==类名::new==\n\n * 范例\n   \n   ==student::new==\n\n * 练习描述\n   \n   * 定义一个类(student)，里面有两个成员变量(name,age)\n     \n     并提供无参构造方法和带参构造方法，以及成员变量对应的get和set方法\n\n * 代码演示\n   \n   /**\n    * @classname functiondemo1\n    * @date 2023/3/1 14:00\n    * @author diane\n    * @description 方法引用：引用构造方法\n    * @version 1.0\n    */\n   public class functiondemo4 {\n     public static void main(string[] args) {\n   \n       arraylist<string> list = new arraylist<string>();\n       list.add("周润发,23");\n       list.add("成龙,24");\n       list.add("刘德华,25");\n       list.add("吴京,26");\n       list.add("周星驰,17");\n       list.add("李连杰,18");\n   \n       // 将string 转换成 student对象\n   \n       // 1. lambda 表达式\n       list<student> collect = list.stream()\n               .map(s -> new student(s.split(",")[0], integer.parseint(s.split(",")[1])))\n               .collect(collectors.tolist());\n       system.out.println(collect);\n   \n       // 2. 方法引用\n       // 构造方法比较特殊，不用考虑返回值；只需要关心传入参数是否一致\n       list<student> collect1 = list.stream()\n               .map(student::new)\n               .collect(collectors.tolist());\n       system.out.println(collect1);\n   \n   \n     }\n   }\n   \n   \n   \n   @data\n   @allargsconstructor\n   class student {\n     private string name;\n     private int age;\n   \n     /**\n      * 这个构造函数 是为了  可以使用 student::new  作为方法调用\n      *    因为 流中的数据 参数是string类型, 所以要创建一个参数是string类型的构造方法\n      * @param s 参数--- 流中的数据\n      */\n     public student(string s) {\n       string[] split = s.split(",");\n       this.name = split[0];\n       this.age = integer.parseint(split[1]);\n     }\n   }\n   \n\n * 使用说明\n   \n   lambda表达式被构造器替代的时候，它的形式参数全部传递给构造器作为参数\n\n\n# 额外引用方法\n\n这里面的方法引用的规则 与基本引用方法的规则 不一致，是受到限制的。\n\n\n# 引用类的实例方法\n\n引用规则\n\n\n\n * 被引用方法的形参，需要跟抽象方法的==第二个形参到最后一个形参保持一致==，==返回值需要保持一致==。\n\n * 抽象方法的形参：==第一个形参的类型==，==作为方法的调用者==，决定了==可以引用那些类的成员方法==。\n\n * 特别的，当抽象方法只有一个参数时，如\n   \n   list.stream().map(s -> s.touppercase()).foreach(system.out::println);\n   list.stream().map(new function<string, string>() {\n         @override\n         public string apply(string s) {\n             // 第一个形参s  调用了这个方法\n           return s.touppercase();\n         }\n       }).foreach(system.out::println);\n   \n   \n   map中的抽象方法，只有一个string类型的参数，没有第二个形参到最后一个形参;且返回值是string类型的\n   \n   若想引用方法，则要求\n   \n   * 1.被引用的方法必须是属于string类的，或者是系统类\n     * 第一个参数作为方法的调用者，可以调用本类的，也可以调用一些系统类的方法\n   * 2.被引用的方法必须是==无参的==。\n   * 3.返回值必须是string类型的\n   \n   \n   \n   如上图所示，touppercase()是属于string类的，且它是无参的，且返回值是string类型的；满足要求，所以可以使用方法引用\n   \n   list.stream().map(string::touppercase).foreach(system.out::println);\n   \n\n * 当抽象方法只有一个参数，没有返回值时,跟上述所说只有一个区别，\n   \n   * 就是引用的方法必须是无返回值的\n   * 无参+系统类方法\n   \n   list.stream().map(string::touppercase)\n       .foreach(new consumer<string>() {\n             @override\n             public void accept(string s) {\n                 // 第一个参数s，调用了这个方法\n               system.out.println(s);\n             }\n   });\n   list.stream().map(string::touppercase).foreach(system.out::println);\n   \n\n引用类的实例方法，其实就是引用类中的成员方法\n\n * 格式\n   \n   ==类名::成员方法==\n\n * 范例\n   \n   ==string::substring==\n   \n   public string substring(int beginindex,int endindex)\n   \n   从beginindex开始到endindex结束，截取字符串。返回一个子串，子串的长度为endindex-beginindex\n   \n   ==system.out::println==\n\n * 练习描述\n   \n   * 定义一个接口(mystring)，里面定义一个抽象方法：\n     \n     string mysubstring(string s,int x,int y);\n   \n   * 定义一个测试类(mystringdemo)，在测试类中提供两个方法\n     \n     * 一个方法是：usemystring(mystring my)\n     * 一个方法是主方法，在主方法中调用usemystring方法\n\n * 代码演示\n   \n   public interface mystring {\n       string mysubstring(string s,int x,int y);\n   }\n   \n   public class mystringdemo {\n       public static void main(string[] args) {\n   \t\t//lambda简化写法\n           usemystring((s,x,y) -> s.substring(x,y));\n   \n           //引用类的实例方法\n           usemystring(string::substring);\n   \n       }\n   \n       private static void usemystring(mystring my) {\n           string s = my.mysubstring("helloworld", 2, 5);\n           system.out.println(s);\n       }\n   }\n   \n\n * 使用说明\n   \n   lambda表达式被类的实例方法替代的时候 第一个参数作为调用者 后面的参数全部传递给该方法作为参数\n\n\n# 引用数组的构造方法\n\n * 格式\n   \n   数据类型[]::new\n\n * 细节\n   \n   数组的类型，需要跟流中的数据类型保持一致。\n   \n   /**\n    * @classname functiondemo1\n    * @date 2023/3/1 14:00\n    * @author diane\n    * @description 方法引用：使用数组的构造方法\n    *    数据类型[]::new\n    * @version 1.0\n    */\n   public class functiondemo6 {\n     public static void main(string[] args) {\n   \n       arraylist<string> list = new arraylist<>();\n       collections.addall(list, "aaa", "bbb", "ccc", "ddd");\n   \n       // 匿名内部类\n       string[] strings = list.stream().toarray(new intfunction<string[]>() {\n         @override\n         public string[] apply(int value) {\n           return new string[value];\n         }\n       });\n       system.out.println(arrays.tostring(strings));\n   \n   \n       // lambda 表达式\n       string[] strings1 = list.stream().toarray(value -> new string[value]);\n       system.out.println(arrays.tostring(strings1));\n   \n       // 方法引用\n       // 创建数组类型 必须与流中的类型一致\n       string[] strings2 = list.stream().toarray(string[]::new);\n       system.out.println(arrays.tostring(strings2));\n   \n     }\n   \n   }\n   \n\n\n# 方法引用总结\n\n\n\n',charsets:{cjk:!0}},{title:"异常",frontmatter:{autoSort:100,title:"异常",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/2ff7b9/",categories:["后端","Java","流"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/60.%E6%B5%81/05.%E5%BC%82%E5%B8%B8.html",relativePath:"01.后端/10.Java/60.流/05.异常.md",key:"v-1c9e69dc",path:"/pages/2ff7b9/",headers:[{level:2,title:"异常概念",slug:"异常概念",normalizedTitle:"异常概念",charIndex:9},{level:2,title:"异常体系",slug:"异常体系",normalizedTitle:"异常体系",charIndex:245},{level:2,title:"异常分类",slug:"异常分类",normalizedTitle:"异常分类",charIndex:774},{level:2,title:"异常的产生过程解析",slug:"异常的产生过程解析",normalizedTitle:"异常的产生过程解析",charIndex:1089},{level:2,title:"抛出异常throw",slug:"抛出异常throw",normalizedTitle:"抛出异常throw",charIndex:1622},{level:2,title:"声明异常throws",slug:"声明异常throws",normalizedTitle:"声明异常throws",charIndex:3014},{level:2,title:"捕获异常try…catch",slug:"捕获异常try-catch",normalizedTitle:"捕获异常try…catch",charIndex:4261},{level:2,title:"finally 代码块",slug:"finally-代码块",normalizedTitle:"finally 代码块",charIndex:5958},{level:2,title:"异常注意事项",slug:"异常注意事项",normalizedTitle:"异常注意事项",charIndex:7040},{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:7265},{level:2,title:"自定义异常的练习",slug:"自定义异常的练习",normalizedTitle:"自定义异常的练习",charIndex:7664}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"异常概念 异常体系 异常分类 异常的产生过程解析 抛出异常throw 声明异常throws 捕获异常try…catch finally 代码块 异常注意事项 概述 自定义异常的练习",content:'# 异常\n\n\n# 异常概念\n\n异常，就是不正常的意思。在生活中:医生说,你的身体某个部位有异常,该部位和正常相比有点不同,该部位的功能将受影响.在程序中的意思就是：\n\n * 异常 ：指的是程序在执行过程中，出现的非正常的情况，最终会导致JVM的非正常停止。\n\n在Java等面向对象的编程语言中，异常本身是一个类，产生异常就是创建异常对象并抛出了一个异常对象。Java处理异常的方式是中断处理。\n\n> 异常指的并不是语法错误,语法错了,编译不通过,不会产生字节码文件,根本不能运行.\n\n\n# 异常体系\n\n\n\n异常机制其实是帮助我们找到程序中的问题，异常的根类是java.lang.Throwable，其下有两个子类：java.lang.Error与java.lang.Exception，平常所说的异常指java.lang.Exception。\n\n\n\nThrowable体系：\n\n * Error:严重错误Error，无法通过处理的错误，只能事先避免，好比绝症。\n * Exception:表示异常，异常产生后程序员可以通过代码的方式纠正，使程序继续运行，是必须要处理的。好比感冒、阑尾炎。\n\nThrowable中的常用方法：\n\n * public void printStackTrace():打印异常的详细信息。\n   \n   包含了异常的类型,异常的原因,还包括异常出现的位置,在开发和调试阶段,都得使用printStackTrace。\n\n * public String getMessage():获取发生异常的原因。\n   \n   提示给用户的时候,就提示错误原因。\n\n * public String toString():获取异常的类型和异常描述信息(不用)。\n\n出现异常,不要紧张,把异常的简单类名,拷贝到API中去查。\n\n\n\n\n# 异常分类\n\n我们平常说的异常就是指Exception，因为这类异常一旦出现，我们就要对代码进行更正，修复程序。\n\n异常(Exception)的分类:根据在编译时期还是运行时期去检查异常?\n\n * 编译时期异常:checked异常。在编译时期,就会检查,如果没有处理异常,则编译失败。(如日期格式化异常)\n   \n   * 除了 RuntimeException 和它的子类 都是编译时期异常\n   * 作用在于提醒程序员 注意\n\n * 运行时期异常:runtime异常。在运行时期,检查异常.在编译时期,运行异常不会编译器检测(不报错)。(如数学异常)\n   \n   * RuntimeException 和它的子类\n\n\n\n\n# 异常的产生过程解析\n\n先运行下面的程序，程序会产生一个数组索引越界异常ArrayIndexOfBoundsException。我们通过图解来解析下异常产生的过程。\n\n工具类\n\npublic class ArrayTools {\n    // 对给定的数组通过给定的角标获取元素。\n    public static int getElement(int[] arr, int index) {\n        int element = arr[index];\n        return element;\n    }\n}\n\n\n测试类\n\npublic class ExceptionDemo {\n    public static void main(String[] args) {\n        int[] arr = { 34, 12, 67 };\n        intnum = ArrayTools.getElement(arr, 4)\n        System.out.println("num=" + num);\n        System.out.println("over");\n    }\n}\n\n\n上述程序执行过程图解：\n\n\n\n\n# 抛出异常throw\n\n在编写程序时，我们必须要考虑程序出现问题的情况。比如，在定义方法时，方法需要接受参数。那么，当调用方法使用接受到的参数时，首先需要先对参数数据进行合法的判断，数据若不合法，就应该告诉调用者，传递合法的数据进来。这时需要使用抛出异常的方式来告诉调用者。\n\n在java中，提供了一个throw关键字，它用来抛出一个指定的异常对象。那么，抛出一个异常具体如何操作呢？\n\n 1. 创建一个异常对象。封装一些提示信息(信息可以自己编写)。\n\n 2. 需要将这个异常对象告知给调用者。怎么告知呢？怎么将这个异常对象传递到调用者处呢？通过关键字throw就可以完成。throw 异常对象。\n    \n    throw用在方法内，用来抛出一个异常对象，将这个异常对象传递到调用者处，并结束当前方法的执行。\n\n使用格式：\n\nthrow new 异常类名(参数);\n\n\n例如：\n\nthrow new NullPointerException("要访问的arr数组不存在");\n\nthrow new ArrayIndexOutOfBoundsException("该索引在数组中不存在，已超出范围");\n\n\n学习完抛出异常的格式后，我们通过下面程序演示下throw的使用。\n\npublic class ThrowDemo {\n    public static void main(String[] args) {\n        //创建一个数组 \n        int[] arr = {2,4,52,2};\n        //根据索引找对应的元素 \n        int index = 4;\n        int element = getElement(arr, index);\n\n        System.out.println(element);\n        System.out.println("over");\n    }\n    /*\n     * 根据 索引找到数组中对应的元素\n     */\n    public static int getElement(int[] arr,int index){ \n       \t//判断  索引是否越界\n        if(index<0 || index>arr.length-1){\n             /*\n             判断条件如果满足，当执行完throw抛出异常对象后，方法已经无法继续运算。\n             这时就会结束当前方法的执行，并将异常告知给调用者。这时就需要通过异常来解决。 \n              */\n             throw new ArrayIndexOutOfBoundsException("哥们，角标越界了```");\n        }\n        int element = arr[index];\n        return element;\n    }\n}\n\n\n> 注意：如果产生了问题，我们就会throw将问题描述类即异常进行抛出，也就是将问题返回给该方法的调用者。\n> \n> 那么对于调用者来说，该怎么处理呢？一种是进行捕获处理，另一种就是继续讲问题声明出去，使用throws声明处理。\n\n\n# 声明异常throws\n\n声明异常：将问题标识出来，报告给调用者。如果方法内通过throw抛出了编译时异常，而没有捕获处理（稍后讲解该方式），那么必须通过throws进行声明，让调用者去处理。\n\n关键字throws运用于方法声明之上,用于表示当前方法不处理异常,而是提醒该方法的调用者来处理异常(抛出异常).\n\n * 编译异常必须抛出-throws\n * 运行时异常，可以不抛出-throws\n\n声明异常格式：\n\n修饰符 返回值类型 方法名(参数) throws 异常类名1,异常类名2…{   }\t\n\n\n声明异常的代码演示：\n\npublic class ThrowsDemo {\n    public static void main(String[] args) throws FileNotFoundException {\n        read("a.txt");\n    }\n\n    // 如果定义功能时有问题发生需要报告给调用者。可以通过在方法上使用throws关键字进行声明\n    public static void read(String path) throws FileNotFoundException {\n        if (!path.equals("a.txt")) {//如果不是 a.txt这个文件 \n            // 我假设  如果不是 a.txt 认为 该文件不存在 是一个错误 也就是异常  throw\n            throw new FileNotFoundException("文件不存在");\n        }\n    }\n}\n\n\nthrows用于进行异常类的声明，若该方法可能有多种异常情况产生，那么在throws后面可以写多个异常类，用逗号隔开。\n\npublic class ThrowsDemo2 {\n    public static void main(String[] args) throws IOException {\n        read("a.txt");\n    }\n\n    public static void read(String path)throws FileNotFoundException, IOException {\n        if (!path.equals("a.txt")) {//如果不是 a.txt这个文件 \n            // 我假设  如果不是 a.txt 认为 该文件不存在 是一个错误 也就是异常  throw\n            throw new FileNotFoundException("文件不存在");\n        }\n        if (!path.equals("b.txt")) {\n            throw new IOException();\n        }\n    }\n}\n\n\n\n# 捕获异常try…catch\n\n\n\n如果异常出现的话,会立刻终止程序,所以我们得处理异常:\n\n 1. 该方法不处理,而是声明抛出,由该方法的调用者来处理(throws)。\n 2. 在方法中使用try-catch的语句块来处理异常。\n\ntry-catch的方式就是捕获异常。\n\n * 捕获异常：Java中对异常有针对性的语句进行捕获，可以对出现的异常进行指定方式的处理。\n\n捕获异常语法如下：\n\ntry{\n     编写可能会出现异常的代码\n}catch(异常类型  e){\n     处理异常的代码\n     //记录日志/打印异常信息/继续抛出异常\n}\n\n\n**try：**该代码块中编写可能产生异常的代码。\n\n**catch：**用来进行某种异常的捕获，实现对捕获到的异常进行处理。\n\n> 注意:try和catch都不能单独使用,必须连用。\n\n演示如下：\n\npublic class TryCatchDemo {\n    public static void main(String[] args) {\n        try {// 当产生异常时，必须有处理方式。要么捕获，要么声明。\n            read("b.txt");\n        } catch (FileNotFoundException e) {// 括号中需要定义什么呢？\n          \t//try中抛出的是什么异常，在括号中就定义什么异常类型\n            System.out.println(e);\n        }\n        System.out.println("over");\n    }\n    /*\n     *\n     * 我们 当前的这个方法中 有异常  有编译期异常\n     */\n    public static void read(String path) throws FileNotFoundException {\n        if (!path.equals("a.txt")) {//如果不是 a.txt这个文件 \n            // 我假设  如果不是 a.txt 认为 该文件不存在 是一个错误 也就是异常  throw\n            throw new FileNotFoundException("文件不存在");\n        }\n    }\n}\n\n\n如何获取异常信息：\n\nThrowable类中定义了一些查看方法:\n\n * public String getMessage():获取异常的描述信息,原因(提示给用户的时候,就提示错误原因。\n\n * public String toString():获取异常的类型和异常描述信息(不用)。\n\n * public void printStackTrace():打印异常的跟踪栈信息并输出到控制台。\n\n包含了异常的类型,异常的原因,还包括异常出现的位置,在开发和调试阶段,都得使用printStackTrace。\n\n在开发中呢也可以在catch将编译期异常转换成运行期异常处理。\n\n多个异常使用捕获又该如何处理呢？\n\n 1. 多个异常分别处理。\n 2. 多个异常一次捕获，多次处理。\n 3. 多个异常一次捕获一次处理。\n\n一般我们是使用一次捕获多次处理方式，格式如下：\n\ntry{\n     编写可能会出现异常的代码\n}catch(异常类型A  e){  当try中出现A类型异常,就用该catch来捕获.\n     处理异常的代码\n     //记录日志/打印异常信息/继续抛出异常\n}catch(异常类型B  e){  当try中出现B类型异常,就用该catch来捕获.\n     处理异常的代码\n     //记录日志/打印异常信息/继续抛出异常\n}\n\n\n> 注意:这种异常处理方式，要求多个catch中的异常不能相同，并且若catch中的多个异常之间有子父类异常的关系，那么子类异常要求在上面的catch处理，父类异常在下面的catch处理。\n\n\n# finally 代码块\n\nfinally：有一些特定的代码无论异常是否发生，都需要执行。另外，因为异常会引发程序跳转，导致有些语句执行不到。而finally就是解决这个问题的，在finally代码块中存放的代码都是一定会被执行的。\n\n什么时候的代码必须最终执行？\n\n当我们在try语句块中打开了一些物理资源(磁盘文件/网络连接/数据库连接等),我们都得在使用完之后,最终关闭打开的资源。\n\nfinally的语法:\n\ntry...catch....finally:自身需要处理异常,最终还得关闭资源。\n\n> 注意:finally不能单独使用。\n\n比如在我们之后学习的IO流中，当打开了一个关联文件的资源，最后程序不管结果如何，都需要把这个资源关闭掉。\n\nfinally代码参考如下：\n\npublic class TryCatchDemo4 {\n    public static void main(String[] args) {\n        try {\n            read("a.txt");\n        } catch (FileNotFoundException e) {\n            //抓取到的是编译期异常  抛出去的是运行期 \n            throw new RuntimeException(e);\n        } finally {\n            System.out.println("不管程序怎样，这里都将会被执行。");\n        }\n        System.out.println("over");\n    }\n    /*\n     *\n     * 我们 当前的这个方法中 有异常  有编译期异常\n     */\n    public static void read(String path) throws FileNotFoundException {\n        if (!path.equals("a.txt")) {//如果不是 a.txt这个文件 \n            // 我假设  如果不是 a.txt 认为 该文件不存在 是一个错误 也就是异常  throw\n            throw new FileNotFoundException("文件不存在");\n        }\n    }\n}\n\n\n> 当只有在try或者catch中调用退出JVM的相关方法,此时finally才不会执行,否则finally永远会执行。\n\n\n# 异常注意事项\n\n * 运行时异常被抛出可以不处理。即不捕获也不声明抛出。\n * 如果父类抛出了多个异常,子类覆盖父类方法时,只能抛出相同的异常或者是他的子集。\n * **父类方法没有抛出异常，子类覆盖父类该方法时也不可抛出异常。**此时子类产生该异常，只能捕获处理，不能声明抛出\n * 当多异常处理时，捕获处理，前边的类不能是后边类的父类\n * 在try/catch后可以追加finally代码块，其中的代码一定会被执行，通常用于资源回收。\n\n\n# 概述\n\n为什么需要自定义异常类:\n\n我们说了Java中不同的异常类,分别表示着某一种具体的异常情况,那么在开发中总是有些异常情况是SUN没有定义好的,此时我们根据自己业务的异常情况来定义异常类。,例如年龄负数问题,考试成绩负数问题。\n\n在上述代码中，发现这些异常都是JDK内部定义好的，但是实际开发中也会出现很多异常,这些异常很可能在JDK中没有定义过,例如年龄负数问题,考试成绩负数问题.那么能不能自己定义异常呢？\n\n什么是自定义异常类:\n\n在开发中根据自己业务的异常情况来定义异常类.\n\n自定义一个业务逻辑异常: LoginException。一个登陆异常类。\n\n异常类如何定义:\n\n 1. 自定义一个编译期异常: 自定义类 并继承于java.lang.Exception。\n 2. 自定义一个运行时期的异常类:自定义类 并继承于java.lang.RuntimeException。\n\n\n# 自定义异常的练习\n\n要求：我们模拟登陆操作，如果用户名已存在，则抛出异常并提示：亲，该用户名已经被注册。\n\n首先定义一个登陆异常类LoginException：\n\n// 业务逻辑异常\npublic class LoginException extends Exception {\n    /**\n     * 空参构造\n     */\n    public LoginException() {\n    }\n\n    /**\n     *\n     * @param message 表示异常提示\n     */\n    public LoginException(String message) {\n        super(message);\n    }\n}\n\n\n模拟登陆操作，使用数组模拟数据库中存储的数据，并提供当前注册账号是否存在方法用于判断。\n\npublic class Demo {\n    // 模拟数据库中已存在账号\n    private static String[] names = {"bill","hill","jill"};\n   \n    public static void main(String[] args) {     \n        //调用方法\n        try{\n            // 可能出现异常的代码\n            checkUsername("nill");\n            System.out.println("注册成功");//如果没有异常就是注册成功\n        } catch(LoginException e) {\n            //处理异常\n            e.printStackTrace();\n        }\n    }\n\n    //判断当前注册账号是否存在\n    //因为是编译期异常，又想调用者去处理 所以声明该异常\n    public static boolean checkUsername(String uname) throws LoginException {\n        for (String name : names) {\n            if(name.equals(uname)){//如果名字在这里面 就抛出登陆异常\n                throw new LoginException("亲"+name+"已经被注册了！");\n            }\n        }\n        return true;\n    }\n}\n',normalizedContent:'# 异常\n\n\n# 异常概念\n\n异常，就是不正常的意思。在生活中:医生说,你的身体某个部位有异常,该部位和正常相比有点不同,该部位的功能将受影响.在程序中的意思就是：\n\n * 异常 ：指的是程序在执行过程中，出现的非正常的情况，最终会导致jvm的非正常停止。\n\n在java等面向对象的编程语言中，异常本身是一个类，产生异常就是创建异常对象并抛出了一个异常对象。java处理异常的方式是中断处理。\n\n> 异常指的并不是语法错误,语法错了,编译不通过,不会产生字节码文件,根本不能运行.\n\n\n# 异常体系\n\n\n\n异常机制其实是帮助我们找到程序中的问题，异常的根类是java.lang.throwable，其下有两个子类：java.lang.error与java.lang.exception，平常所说的异常指java.lang.exception。\n\n\n\nthrowable体系：\n\n * error:严重错误error，无法通过处理的错误，只能事先避免，好比绝症。\n * exception:表示异常，异常产生后程序员可以通过代码的方式纠正，使程序继续运行，是必须要处理的。好比感冒、阑尾炎。\n\nthrowable中的常用方法：\n\n * public void printstacktrace():打印异常的详细信息。\n   \n   包含了异常的类型,异常的原因,还包括异常出现的位置,在开发和调试阶段,都得使用printstacktrace。\n\n * public string getmessage():获取发生异常的原因。\n   \n   提示给用户的时候,就提示错误原因。\n\n * public string tostring():获取异常的类型和异常描述信息(不用)。\n\n出现异常,不要紧张,把异常的简单类名,拷贝到api中去查。\n\n\n\n\n# 异常分类\n\n我们平常说的异常就是指exception，因为这类异常一旦出现，我们就要对代码进行更正，修复程序。\n\n异常(exception)的分类:根据在编译时期还是运行时期去检查异常?\n\n * 编译时期异常:checked异常。在编译时期,就会检查,如果没有处理异常,则编译失败。(如日期格式化异常)\n   \n   * 除了 runtimeexception 和它的子类 都是编译时期异常\n   * 作用在于提醒程序员 注意\n\n * 运行时期异常:runtime异常。在运行时期,检查异常.在编译时期,运行异常不会编译器检测(不报错)。(如数学异常)\n   \n   * runtimeexception 和它的子类\n\n\n\n\n# 异常的产生过程解析\n\n先运行下面的程序，程序会产生一个数组索引越界异常arrayindexofboundsexception。我们通过图解来解析下异常产生的过程。\n\n工具类\n\npublic class arraytools {\n    // 对给定的数组通过给定的角标获取元素。\n    public static int getelement(int[] arr, int index) {\n        int element = arr[index];\n        return element;\n    }\n}\n\n\n测试类\n\npublic class exceptiondemo {\n    public static void main(string[] args) {\n        int[] arr = { 34, 12, 67 };\n        intnum = arraytools.getelement(arr, 4)\n        system.out.println("num=" + num);\n        system.out.println("over");\n    }\n}\n\n\n上述程序执行过程图解：\n\n\n\n\n# 抛出异常throw\n\n在编写程序时，我们必须要考虑程序出现问题的情况。比如，在定义方法时，方法需要接受参数。那么，当调用方法使用接受到的参数时，首先需要先对参数数据进行合法的判断，数据若不合法，就应该告诉调用者，传递合法的数据进来。这时需要使用抛出异常的方式来告诉调用者。\n\n在java中，提供了一个throw关键字，它用来抛出一个指定的异常对象。那么，抛出一个异常具体如何操作呢？\n\n 1. 创建一个异常对象。封装一些提示信息(信息可以自己编写)。\n\n 2. 需要将这个异常对象告知给调用者。怎么告知呢？怎么将这个异常对象传递到调用者处呢？通过关键字throw就可以完成。throw 异常对象。\n    \n    throw用在方法内，用来抛出一个异常对象，将这个异常对象传递到调用者处，并结束当前方法的执行。\n\n使用格式：\n\nthrow new 异常类名(参数);\n\n\n例如：\n\nthrow new nullpointerexception("要访问的arr数组不存在");\n\nthrow new arrayindexoutofboundsexception("该索引在数组中不存在，已超出范围");\n\n\n学习完抛出异常的格式后，我们通过下面程序演示下throw的使用。\n\npublic class throwdemo {\n    public static void main(string[] args) {\n        //创建一个数组 \n        int[] arr = {2,4,52,2};\n        //根据索引找对应的元素 \n        int index = 4;\n        int element = getelement(arr, index);\n\n        system.out.println(element);\n        system.out.println("over");\n    }\n    /*\n     * 根据 索引找到数组中对应的元素\n     */\n    public static int getelement(int[] arr,int index){ \n       \t//判断  索引是否越界\n        if(index<0 || index>arr.length-1){\n             /*\n             判断条件如果满足，当执行完throw抛出异常对象后，方法已经无法继续运算。\n             这时就会结束当前方法的执行，并将异常告知给调用者。这时就需要通过异常来解决。 \n              */\n             throw new arrayindexoutofboundsexception("哥们，角标越界了```");\n        }\n        int element = arr[index];\n        return element;\n    }\n}\n\n\n> 注意：如果产生了问题，我们就会throw将问题描述类即异常进行抛出，也就是将问题返回给该方法的调用者。\n> \n> 那么对于调用者来说，该怎么处理呢？一种是进行捕获处理，另一种就是继续讲问题声明出去，使用throws声明处理。\n\n\n# 声明异常throws\n\n声明异常：将问题标识出来，报告给调用者。如果方法内通过throw抛出了编译时异常，而没有捕获处理（稍后讲解该方式），那么必须通过throws进行声明，让调用者去处理。\n\n关键字throws运用于方法声明之上,用于表示当前方法不处理异常,而是提醒该方法的调用者来处理异常(抛出异常).\n\n * 编译异常必须抛出-throws\n * 运行时异常，可以不抛出-throws\n\n声明异常格式：\n\n修饰符 返回值类型 方法名(参数) throws 异常类名1,异常类名2…{   }\t\n\n\n声明异常的代码演示：\n\npublic class throwsdemo {\n    public static void main(string[] args) throws filenotfoundexception {\n        read("a.txt");\n    }\n\n    // 如果定义功能时有问题发生需要报告给调用者。可以通过在方法上使用throws关键字进行声明\n    public static void read(string path) throws filenotfoundexception {\n        if (!path.equals("a.txt")) {//如果不是 a.txt这个文件 \n            // 我假设  如果不是 a.txt 认为 该文件不存在 是一个错误 也就是异常  throw\n            throw new filenotfoundexception("文件不存在");\n        }\n    }\n}\n\n\nthrows用于进行异常类的声明，若该方法可能有多种异常情况产生，那么在throws后面可以写多个异常类，用逗号隔开。\n\npublic class throwsdemo2 {\n    public static void main(string[] args) throws ioexception {\n        read("a.txt");\n    }\n\n    public static void read(string path)throws filenotfoundexception, ioexception {\n        if (!path.equals("a.txt")) {//如果不是 a.txt这个文件 \n            // 我假设  如果不是 a.txt 认为 该文件不存在 是一个错误 也就是异常  throw\n            throw new filenotfoundexception("文件不存在");\n        }\n        if (!path.equals("b.txt")) {\n            throw new ioexception();\n        }\n    }\n}\n\n\n\n# 捕获异常try…catch\n\n\n\n如果异常出现的话,会立刻终止程序,所以我们得处理异常:\n\n 1. 该方法不处理,而是声明抛出,由该方法的调用者来处理(throws)。\n 2. 在方法中使用try-catch的语句块来处理异常。\n\ntry-catch的方式就是捕获异常。\n\n * 捕获异常：java中对异常有针对性的语句进行捕获，可以对出现的异常进行指定方式的处理。\n\n捕获异常语法如下：\n\ntry{\n     编写可能会出现异常的代码\n}catch(异常类型  e){\n     处理异常的代码\n     //记录日志/打印异常信息/继续抛出异常\n}\n\n\n**try：**该代码块中编写可能产生异常的代码。\n\n**catch：**用来进行某种异常的捕获，实现对捕获到的异常进行处理。\n\n> 注意:try和catch都不能单独使用,必须连用。\n\n演示如下：\n\npublic class trycatchdemo {\n    public static void main(string[] args) {\n        try {// 当产生异常时，必须有处理方式。要么捕获，要么声明。\n            read("b.txt");\n        } catch (filenotfoundexception e) {// 括号中需要定义什么呢？\n          \t//try中抛出的是什么异常，在括号中就定义什么异常类型\n            system.out.println(e);\n        }\n        system.out.println("over");\n    }\n    /*\n     *\n     * 我们 当前的这个方法中 有异常  有编译期异常\n     */\n    public static void read(string path) throws filenotfoundexception {\n        if (!path.equals("a.txt")) {//如果不是 a.txt这个文件 \n            // 我假设  如果不是 a.txt 认为 该文件不存在 是一个错误 也就是异常  throw\n            throw new filenotfoundexception("文件不存在");\n        }\n    }\n}\n\n\n如何获取异常信息：\n\nthrowable类中定义了一些查看方法:\n\n * public string getmessage():获取异常的描述信息,原因(提示给用户的时候,就提示错误原因。\n\n * public string tostring():获取异常的类型和异常描述信息(不用)。\n\n * public void printstacktrace():打印异常的跟踪栈信息并输出到控制台。\n\n包含了异常的类型,异常的原因,还包括异常出现的位置,在开发和调试阶段,都得使用printstacktrace。\n\n在开发中呢也可以在catch将编译期异常转换成运行期异常处理。\n\n多个异常使用捕获又该如何处理呢？\n\n 1. 多个异常分别处理。\n 2. 多个异常一次捕获，多次处理。\n 3. 多个异常一次捕获一次处理。\n\n一般我们是使用一次捕获多次处理方式，格式如下：\n\ntry{\n     编写可能会出现异常的代码\n}catch(异常类型a  e){  当try中出现a类型异常,就用该catch来捕获.\n     处理异常的代码\n     //记录日志/打印异常信息/继续抛出异常\n}catch(异常类型b  e){  当try中出现b类型异常,就用该catch来捕获.\n     处理异常的代码\n     //记录日志/打印异常信息/继续抛出异常\n}\n\n\n> 注意:这种异常处理方式，要求多个catch中的异常不能相同，并且若catch中的多个异常之间有子父类异常的关系，那么子类异常要求在上面的catch处理，父类异常在下面的catch处理。\n\n\n# finally 代码块\n\nfinally：有一些特定的代码无论异常是否发生，都需要执行。另外，因为异常会引发程序跳转，导致有些语句执行不到。而finally就是解决这个问题的，在finally代码块中存放的代码都是一定会被执行的。\n\n什么时候的代码必须最终执行？\n\n当我们在try语句块中打开了一些物理资源(磁盘文件/网络连接/数据库连接等),我们都得在使用完之后,最终关闭打开的资源。\n\nfinally的语法:\n\ntry...catch....finally:自身需要处理异常,最终还得关闭资源。\n\n> 注意:finally不能单独使用。\n\n比如在我们之后学习的io流中，当打开了一个关联文件的资源，最后程序不管结果如何，都需要把这个资源关闭掉。\n\nfinally代码参考如下：\n\npublic class trycatchdemo4 {\n    public static void main(string[] args) {\n        try {\n            read("a.txt");\n        } catch (filenotfoundexception e) {\n            //抓取到的是编译期异常  抛出去的是运行期 \n            throw new runtimeexception(e);\n        } finally {\n            system.out.println("不管程序怎样，这里都将会被执行。");\n        }\n        system.out.println("over");\n    }\n    /*\n     *\n     * 我们 当前的这个方法中 有异常  有编译期异常\n     */\n    public static void read(string path) throws filenotfoundexception {\n        if (!path.equals("a.txt")) {//如果不是 a.txt这个文件 \n            // 我假设  如果不是 a.txt 认为 该文件不存在 是一个错误 也就是异常  throw\n            throw new filenotfoundexception("文件不存在");\n        }\n    }\n}\n\n\n> 当只有在try或者catch中调用退出jvm的相关方法,此时finally才不会执行,否则finally永远会执行。\n\n\n# 异常注意事项\n\n * 运行时异常被抛出可以不处理。即不捕获也不声明抛出。\n * 如果父类抛出了多个异常,子类覆盖父类方法时,只能抛出相同的异常或者是他的子集。\n * **父类方法没有抛出异常，子类覆盖父类该方法时也不可抛出异常。**此时子类产生该异常，只能捕获处理，不能声明抛出\n * 当多异常处理时，捕获处理，前边的类不能是后边类的父类\n * 在try/catch后可以追加finally代码块，其中的代码一定会被执行，通常用于资源回收。\n\n\n# 概述\n\n为什么需要自定义异常类:\n\n我们说了java中不同的异常类,分别表示着某一种具体的异常情况,那么在开发中总是有些异常情况是sun没有定义好的,此时我们根据自己业务的异常情况来定义异常类。,例如年龄负数问题,考试成绩负数问题。\n\n在上述代码中，发现这些异常都是jdk内部定义好的，但是实际开发中也会出现很多异常,这些异常很可能在jdk中没有定义过,例如年龄负数问题,考试成绩负数问题.那么能不能自己定义异常呢？\n\n什么是自定义异常类:\n\n在开发中根据自己业务的异常情况来定义异常类.\n\n自定义一个业务逻辑异常: loginexception。一个登陆异常类。\n\n异常类如何定义:\n\n 1. 自定义一个编译期异常: 自定义类 并继承于java.lang.exception。\n 2. 自定义一个运行时期的异常类:自定义类 并继承于java.lang.runtimeexception。\n\n\n# 自定义异常的练习\n\n要求：我们模拟登陆操作，如果用户名已存在，则抛出异常并提示：亲，该用户名已经被注册。\n\n首先定义一个登陆异常类loginexception：\n\n// 业务逻辑异常\npublic class loginexception extends exception {\n    /**\n     * 空参构造\n     */\n    public loginexception() {\n    }\n\n    /**\n     *\n     * @param message 表示异常提示\n     */\n    public loginexception(string message) {\n        super(message);\n    }\n}\n\n\n模拟登陆操作，使用数组模拟数据库中存储的数据，并提供当前注册账号是否存在方法用于判断。\n\npublic class demo {\n    // 模拟数据库中已存在账号\n    private static string[] names = {"bill","hill","jill"};\n   \n    public static void main(string[] args) {     \n        //调用方法\n        try{\n            // 可能出现异常的代码\n            checkusername("nill");\n            system.out.println("注册成功");//如果没有异常就是注册成功\n        } catch(loginexception e) {\n            //处理异常\n            e.printstacktrace();\n        }\n    }\n\n    //判断当前注册账号是否存在\n    //因为是编译期异常，又想调用者去处理 所以声明该异常\n    public static boolean checkusername(string uname) throws loginexception {\n        for (string name : names) {\n            if(name.equals(uname)){//如果名字在这里面 就抛出登陆异常\n                throw new loginexception("亲"+name+"已经被注册了！");\n            }\n        }\n        return true;\n    }\n}\n',charsets:{cjk:!0}},{title:"stream流",frontmatter:{autoSort:92,title:"stream流",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/bc84f5/",categories:["后端","Java","集合"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/50.%E9%9B%86%E5%90%88/70.stream%E6%B5%81.html",relativePath:"01.后端/10.Java/50.集合/70.stream流.md",key:"v-539d06af",path:"/pages/bc84f5/",headers:[{level:2,title:"体验Stream流",slug:"体验stream流",normalizedTitle:"体验stream流",charIndex:16},{level:2,title:"Stream流的常见生成方式",slug:"stream流的常见生成方式",normalizedTitle:"stream流的常见生成方式",charIndex:1526},{level:2,title:"生成Stream流的方式",slug:"生成stream流的方式",normalizedTitle:"生成stream流的方式",charIndex:1745},{level:2,title:"Stream流中间操作方法",slug:"stream流中间操作方法",normalizedTitle:"stream流中间操作方法",charIndex:4575},{level:2,title:"Stream流终结操作方法",slug:"stream流终结操作方法",normalizedTitle:"stream流终结操作方法",charIndex:8751},{level:2,title:"Stream流的收集操作",slug:"stream流的收集操作",normalizedTitle:"stream流的收集操作",charIndex:10617},{level:2,title:"Stream流综合练习",slug:"stream流综合练习",normalizedTitle:"stream流综合练习",charIndex:14246}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"体验Stream流 Stream流的常见生成方式 生成Stream流的方式 Stream流中间操作方法 Stream流终结操作方法 Stream流的收集操作 Stream流综合练习",content:'# Stream流\n\n\n\n\n# 体验Stream流\n\n * 案例需求\n   \n   按照下面的要求完成集合的创建和遍历\n   \n   * 创建一个集合，存储多个字符串元素\n   * 把集合中所有以"张"开头的元素存储到一个新的集合\n   * 把"张"开头的集合中的长度为3的元素存储到一个新的集合\n   * 遍历上一步得到的集合\n\n * 原始方式示例代码\n   \n   public class MyStream1 {\n       public static void main(String[] args) {\n           //集合的批量添加\n           ArrayList<String> list1 = new ArrayList<>(List.of("张三丰","张无忌","张翠山","王二麻子","张良","谢广坤"));\n           //list.add()\n   \n           //遍历list1把以张开头的元素添加到list2中。\n           ArrayList<String> list2 = new ArrayList<>();\n           for (String s : list1) {\n               if(s.startsWith("张")){\n                   list2.add(s);\n               }\n           }\n           //遍历list2集合，把其中长度为3的元素，再添加到list3中。\n           ArrayList<String> list3 = new ArrayList<>();\n           for (String s : list2) {\n               if(s.length() == 3){\n                   list3.add(s);\n               }\n           }\n           for (String s : list3) {\n               System.out.println(s);\n           }      \n       }\n   }\n   \n\n * 使用Stream流示例代码\n   \n   public class StreamDemo {\n       public static void main(String[] args) {\n           //集合的批量添加\n           ArrayList<String> list1 = new ArrayList<>(List.of("张三丰","张无忌","张翠山","王二麻子","张良","谢广坤"));\n   \n           //Stream流\n           list1.stream().filter(s->s.startsWith("张"))\n                   .filter(s->s.length() == 3)\n                   .forEach(s-> System.out.println(s));\n       }\n   }\n   \n\n * Stream流的好处\n   \n   * 直接阅读代码的字面意思即可完美展示无关逻辑方式的语义：获取流、过滤姓张、过滤长度为3、逐一打印\n   * Stream流把真正的函数式编程风格引入到Java中\n   * 代码简洁\n\n\n# Stream流的常见生成方式\n\n * Stream流的思想\n   \n   \n\n * Stream流的三类方法\n   \n   * 获取Stream流\n     * 创建一条流水线,并把数据放到流水线上准备进行操作\n   * 中间方法\n     * 流水线上的操作\n     * 一次操作完毕之后,还可以继续进行其他操作\n   * 终结方法\n     * 一个Stream流只能有一个终结方法\n     * 是流水线上的最后一个操作\n\n\n# 生成Stream流的方式\n\n * Collection体系集合\n   \n   使用默认方法stream()生成流\n   \n   public static void singleCollection() {\n       List<Integer> list = new ArrayList<>();\n       Collections.addAll(list, 1, 2, 3, 4, 5);\n       // lambda 表达式\n       // list.stream().forEach(num -> System.out.println(num));\n   \n       // 方法引用\n       list.stream().forEach(System.out::println);\n   }\n   \n\n * Map体系集合\n   \n   把Map转成Set集合，间接的生成流\n   \n   private static void doubleCollection() {\n       Map<Integer, String> map = new HashMap<>();\n       map.put(1, "a");\n       map.put(2, "b");\n       map.put(3, "c");\n   \n       // keySet 获取 stream流  -- 就是set集合获取stream流\n       map.keySet().stream().forEach(System.out::println);\n       // entrySet 获取 stream 流  -- 就是set集合获取stream流\n       map.entrySet().stream().forEach(System.out::println);\n   }\n   \n\n * 数组\n   \n   最好通过Arrays中的静态方法stream生成流\n   \n   引用类型的数组也可以用 Stream.of()方法\n   \n   private static void arrayStream() {\n       int[] arr = {1, 2, 3, 4};\n       Arrays.stream(arr).forEach(System.out::println);\n       String[] arrString = {"a", "b", "c"};\n       Arrays.stream(arrString).forEach(System.out::println);\n   }\n   \n\n * 同种数据类型的多个数据\n   \n   通过Stream接口的静态方法of(T... values)生成流\n   \n   // 4. 零散数据\n   // key Stream.of() 小细节\n   /**\n    * 方法的参数是可变参数，可以传递一堆零散的参数，也可以传递数组\n    * 但是数组的类型必须是引用类型。如果传递基本类型的数组，会将数组作为一个元素，放到stream中\n    */\n   Stream.of(1, 2, 3, 4).forEach(System.out::println);\n   Stream.of("a", "b", "c", "d").forEach(System.out::println);\n   int[] arr = {1, 2, 3, 4};\n   Integer[] arrInt = {1, 2, 3, 4};\n   String[] arrString = {"a", "b", "c"};\n   // [I@58372a00\n   Stream.of(arr).forEach(System.out::println);\n   // 1,2,3,4\n   Stream.of(arrInt).forEach(System.out::println);\n   // a,b,c\n   Stream.of(arrString).forEach(System.out::println);\n   \n\n * 代码演示\n   \n   public class StreamDemo {\n       public static void main(String[] args) {\n           //Collection体系的集合可以使用默认方法stream()生成流\n           List<String> list = new ArrayList<String>();\n           Stream<String> listStream = list.stream();\n   \n           Set<String> set = new HashSet<String>();\n           Stream<String> setStream = set.stream();\n   \n           //Map体系的集合间接的生成流\n           Map<String,Integer> map = new HashMap<String, Integer>();\n           Stream<String> keyStream = map.keySet().stream();\n           Stream<Integer> valueStream = map.values().stream();\n           Stream<Map.Entry<String, Integer>> entryStream = map.entrySet().stream();\n   \n           //数组可以通过Arrays中的静态方法stream生成流\n           String[] strArray = {"hello","world","java"};\n           Stream<String> strArrayStream = Arrays.stream(strArray);\n         \n         \t//同种数据类型的多个数据可以通过Stream接口的静态方法of(T... values)生成流\n           Stream<String> strArrayStream2 = Stream.of("hello", "world", "java");\n           Stream<Integer> intStream = Stream.of(10, 20, 30);\n       }\n   }\n   \n\n\n# Stream流中间操作方法\n\n * 概念\n   \n   中间操作的意思是,执行完此方法之后,Stream流依然可以继续执行其他操作\n\n * 注意\n   \n   * 中间方法，返回新的Stream流，用过一次就关闭了，建议使用链式编程。\n   * 修改流中的数据，不影响集合原来的数据。\n\n * 常见方法\n   \n   方法名                                              说明\n   Stream<T> filter(Predicate predicate)            用于对流中的数据进行过滤\n   Stream<T> limit(long maxSize)                    返回此流中的元素组成的流，截取前指定参数个数的数据\n   Stream<T> skip(long n)                           跳过指定参数个数的数据，返回由该流的剩余元素组成的流\n   static<T> Stream<T> concat(Stream a, Stream b)   合并a和b两个流为一个流；当a,b类型不一致时，类的提升为父类\n   Stream<T> distinct()                             返回由该流的不同元素（根据Object.equals(Object) ）组成的流\n   Stream<T> map(Function<T,R> mapper)              转换流中的数据类型\n\n * filter代码演示\n   \n   public class MyStream3 {\n       public static void main(String[] args) {\n   //        Stream<T> filter(Predicate predicate)：过滤\n   //        Predicate接口中的方法\tboolean test(T t)：对给定的参数进行判断，返回一个布尔值\n   \n           ArrayList<String> list = new ArrayList<>();\n           list.add("张三丰");\n           list.add("张无忌");\n           list.add("张翠山");\n           list.add("王二麻子");\n           list.add("张良");\n           list.add("谢广坤");\n   \n           //filter方法获取流中的 每一个数据.\n           //而test方法中的s,就依次表示流中的每一个数据.\n           //我们只要在test方法中对s进行判断就可以了.\n           //如果判断的结果为true,则当前的数据留下\n           //如果判断的结果为false,则当前数据就不要.\n   //        list.stream().filter(\n   //                new Predicate<String>() {\n   //                    @Override\n   //                    public boolean test(String s) {\n   //                        boolean result = s.startsWith("张");\n   //                        return result;\n   //                    }\n   //                }\n   //        ).forEach(s-> System.out.println(s));\n   \n           //因为Predicate接口中只有一个抽象方法test\n           //所以我们可以使用lambda表达式来简化\n   //        list.stream().filter(\n   //                (String s)->{\n   //                    boolean result = s.startsWith("张");\n   //                        return result;\n   //                }\n   //        ).forEach(s-> System.out.println(s));\n   \n           list.stream().filter(s ->s.startsWith("张")).forEach(s-> System.out.println(s));\n   \n       }\n   }\n   \n\n * limit&skip代码演示\n   \n   public class StreamDemo02 {\n       public static void main(String[] args) {\n           //创建一个集合，存储多个字符串元素\n           ArrayList<String> list = new ArrayList<String>();\n   \n           list.add("林青霞");\n           list.add("张曼玉");\n           list.add("王祖贤");\n           list.add("柳岩");\n           list.add("张敏");\n           list.add("张无忌");\n   \n           //需求1：取前3个数据在控制台输出\n           list.stream().limit(3).forEach(s-> System.out.println(s));\n           System.out.println("--------");\n   \n           //需求2：跳过3个元素，把剩下的元素在控制台输出\n           list.stream().skip(3).forEach(s-> System.out.println(s));\n           System.out.println("--------");\n   \n           //需求3：跳过2个元素，把剩下的元素中前2个在控制台输出\n           list.stream().skip(2).limit(2).forEach(s-> System.out.println(s));\n       }\n   }\n   \n\n * concat&distinct代码演示\n   \n   public class StreamDemo03 {\n       public static void main(String[] args) {\n           //创建一个集合，存储多个字符串元素\n           ArrayList<String> list = new ArrayList<String>();\n   \n           list.add("林青霞");\n           list.add("张曼玉");\n           list.add("王祖贤");\n           list.add("柳岩");\n           list.add("张敏");\n           list.add("张无忌");\n   \n           //需求1：取前4个数据组成一个流\n           Stream<String> s1 = list.stream().limit(4);\n   \n           //需求2：跳过2个数据组成一个流\n           Stream<String> s2 = list.stream().skip(2);\n   \n           //需求3：合并需求1和需求2得到的流，并把结果在控制台输出\n   //        Stream.concat(s1,s2).forEach(s-> System.out.println(s));\n   \n           //需求4：合并需求1和需求2得到的流，并把结果在控制台输出，要求字符串元素不能重复\n           Stream.concat(s1,s2).distinct().forEach(s-> System.out.println(s));\n       }\n   }\n   \n\n * map\n   \n   // 6. map --- 转换流中的数据\n   ArrayList<String> list2 = new ArrayList<>();\n   Collections.addAll(list2, "张无忌-15","周芷若-14", "赵敏-10", "张三-100", "张翠山-80", "张良-90");\n   // 需求-- 获取年龄打印\n   // s 表示流中的每一个数据 返回值表示 转换之后的数据\n   list2.stream().map(s -> Integer.valueOf(s.substring(s.indexOf("-") + 1))).forEach(System.out::println);\n   list2.stream().map(s -> Integer.parseInt(s.split("-")[1])).forEach(System.out::println);\n   \n\n\n# Stream流终结操作方法\n\n * 概念\n   \n   终结操作的意思是,执行完此方法之后,Stream流将不能再执行其他操作\n\n * 常见方法\n   \n   方法名                             说明\n   void forEach(Consumer action)   对此流的每个元素执行操作\n   long count()                    返回此流中的元素数\n   toArray()                       收集流中的数据，放到数组中\n   collect(Collector)              收集流中的数据，放到集合中\n\n * 代码演示\n   \n   public class MyStream5 {\n       public static void main(String[] args) {\n           ArrayList<String> list = new ArrayList<>();\n           list.add("张三丰");\n           list.add("张无忌");\n           list.add("张翠山");\n           list.add("王二麻子");\n           list.add("张良");\n           list.add("谢广坤");\n   \n           //method1(list);\n           \n   //        long count()：返回此流中的元素数\n           long count = list.stream().count();\n           System.out.println(count);\n       }\n   \n       private static void method1(ArrayList<String> list) {\n           //  void forEach(Consumer action)：对此流的每个元素执行操作\n           //  Consumer接口中的方法void accept(T t)：对给定的参数执行此操作\n           //在forEach方法的底层,会循环获取到流中的每一个数据.\n           //并循环调用accept方法,并把每一个数据传递给accept方法\n           //s就依次表示了流中的每一个数据.\n           //所以,我们只要在accept方法中,写上处理的业务逻辑就可以了.\n           list.stream().forEach(\n                   new Consumer<String>() {\n                       @Override\n                       public void accept(String s) {\n                           System.out.println(s);\n                       }\n                   }\n           );\n         \n           System.out.println("====================");\n           //lambda表达式的简化格式\n           //是因为Consumer接口中,只有一个accept方法\n           list.stream().forEach(\n                   (String s)->{\n                       System.out.println(s);\n                   }\n           );\n           System.out.println("====================");\n           //lambda表达式还是可以进一步简化的.\n           list.stream().forEach(s->System.out.println(s));\n       }\n   }\n   \n\n\n# Stream流的收集操作\n\n * 概念\n   \n   对数据使用Stream流的方式操作完毕后,可以把流中的数据收集到集合中\n\n * 常用方法\n   \n   方法名                              说明\n   R collect(Collector collector)   把结果收集到集合中\n\n * 工具类Collectors提供了具体的收集方式\n   \n   方法名                                                         说明\n   public static<T> Collector toList()                         把元素收集到List集合中\n   public static<T> Collector toSet()                          把元素收集到Set集合中\n   public static Collector toMap(Function keyMapper,Function   把元素收集到Map集合中\n   valueMapper)\n   public static Collector toMap(Function keyMapper,Function   把元素收集到Map集合中\n   valueMapper,BinaryOperator mergeFunction)\n\n * 代码演示\n   \n   public class EndStream2 {\n       public static void main(String[] args) {\n           ArrayList<String> list = new ArrayList<>();\n           Collections.addAll(list, "张无忌-男-15", "张无忌-男-16", "周芷若-女-14", "赵敏-女-13", "张三-男-20", "张三丰-男-100", "张翠山-男-40", "张良-男-45");\n   \n           // 1.1 将所有男性收集到 list 集合中\n           /*List<String> collect = list.stream()\n                   .filter(s -> "男".equals(s.split("-")[1]))\n                   .collect(Collectors.toList());\n           System.out.println(collect);*/\n   \n          // 1.2 将所有男性收集到 set 集合中\n         /*  Set<String> collectSet = list.stream()\n                   .filter(s -> "男".equals(s.split("-")[1]))\n                   .collect(Collectors.toSet());\n           System.out.println(collectSet);*/\n   \n           // 1.3.1 将所有男性收集到 map 集合中 --- 匿名内部类\n           // "张无忌-男-15"\n           // 键为-姓名，值为-年龄  张无忌:15\n           Map<String, Integer> collectMap = list.stream()\n                   .filter(s -> "男".equals(s.split("-")[1]))\n                   .collect(Collectors.toMap(\n                           new Function<String, String>() {\n                               // 两个泛型，第一个泛型是 流中数据的类型；第二个是map中键的类型\n                               @Override\n                               // s 表示流中的数据, 返回值为map中的键\n                               public String apply(String s) {\n                                   return s.split("-")[0];\n                               }\n                           }, new Function<String, Integer>() {\n                               // 两个泛型，第一个泛型是 流中数据的类型；第二个是map中值的类型\n                               @Override\n                               // s 表示流中的数据, 返回值为map中的值\n                               public Integer apply(String s) {\n                                   return Integer.valueOf(s.split("-")[2]);\n                               }\n                           }, new BinaryOperator<Integer>() {\n                               // 键重复的覆盖规则，integer先放入map集合中的数据；integer2表示后放入map集合中的数据\n                               // 如果不加这个 方法，则map中的键不能重复\n                               @Override\n                               public Integer apply(Integer integer, Integer integer2) {\n                                   return integer2;\n                               }\n                           }\n                   ));\n           // System.out.println(collectMap);\n   \n           // 1.3.2 将所有男性收集到 map 集合中 --- lambda值\n           // "张无忌-男-15"\n           // 键为-姓名，值为-年龄  张无忌:15\n           Map<String, Integer> collectMap1 = list.stream()\n                   .filter(s -> "男".equals(s.split("-")[1]))\n                   .collect(Collectors.toMap(\n                               // 生成键\n                               s -> s.split("-")[0],\n                               // 生成值\n                               s -> Integer.valueOf(s.split("-")[2]),\n                               // 重复键规则  a表示保留原来的，不覆盖；b表示覆盖\n                               (a , b) -> b));\n   \n           System.out.println(collectMap1);\n   \n       }\n   \n   }\n   \n   \n\n\n# Stream流综合练习\n\n * 案例需求\n   \n   现在有两个ArrayList集合，分别存储6名男演员名称和6名女演员名称，要求完成如下的操作\n   \n   * 男演员只要名字为3个字的前三人\n   * 女演员只要姓林的，并且不要第一个\n   * 把过滤后的男演员姓名和女演员姓名合并到一起\n   * 把上一步操作后的元素作为构造方法的参数创建演员对象,遍历数据\n   \n   演员类Actor已经提供，里面有一个成员变量，一个带参构造方法，以及成员变量对应的get/set方法\n\n * 代码实现\n   \n   演员类\n   \n   @Data\n   @AllArgsConstructor\n   class Actor {\n       private String name;\n       private int age;\n   }\n   \n   \n   测试类\n   \n   /**\n    * @ClassName Test1\n    * @Date 2023/3/1 11:21\n    * @Author diane\n    * @Description 自定义对象并收集\n    *\n    *现在有两个ArrayList集合，分别存储6名男演员名称和6名女演员名称，要求完成如下的操作 -- 张三,23\n    *      1.男演员只要名字为3个字的前两人\n    *      2.女演员只要姓林的，并且不要第一个\n    *      3.把过滤后的男演员姓名和女演员姓名合并到一起\n    *      4.把上一步操作后的元素作为构造方法的参数创建演员对象,遍历数据\n    *      5.将所有的演员对象保存的List集合中\n    * @Version 1.0\n    */\n   public class Test3 {\n       public static void main(String[] args) {\n           //创建集合\n           ArrayList<String> manList = new ArrayList<String>();\n           manList.add("周润发,23");\n           manList.add("成龙,24");\n           manList.add("刘德华,25");\n           manList.add("吴京,26");\n           manList.add("周星驰,17");\n           manList.add("李连杰,18");\n   \n           ArrayList<String> womanList = new ArrayList<String>();\n           womanList.add("林心如,16");\n           womanList.add("张曼玉,15");\n           womanList.add("林青霞,20");\n           womanList.add("柳岩,18");\n           womanList.add("林志玲,3");\n           womanList.add("王祖贤,10");\n   \n   \n           //  1.男演员只要名字为3个字的前两人\n           Stream<String> streamMan = manList.stream()\n                   .filter(s -> s.split(",")[0].length() == 3)\n                   .limit(2);\n           //  2.女演员只要姓林的，并且不要第一个\n           Stream<String> streamWoman = womanList.stream()\n                   .filter(s -> s.split(",")[0].startsWith("林"))\n                   .skip(1);\n           //  3.把过滤后的男演员姓名和女演员姓名合并到一起\n           // Stream.concat(streamMan, streamWoman).forEach(System.out::println);\n           //  4.把上一步操作后的元素作为构造方法的参数创建演员对象,遍历数据\n           // Stream.concat(streamMan, streamWoman)\n           //         .forEach(s -> System.out.println(new Actor(s.split(",")[0], Integer.parseInt(s.split(",")[1]))));\n           //  5.将所有的演员对象保存的List集合中\n           List<Actor> collect = Stream.concat(streamMan, streamWoman)\n                   .map(s -> new Actor(s.split(",")[0], Integer.parseInt(s.split(",")[1])))\n                   .collect(Collectors.toList());\n           System.out.println(collect);\n       }\n   }\n   \n   \n   \n   ',normalizedContent:'# stream流\n\n\n\n\n# 体验stream流\n\n * 案例需求\n   \n   按照下面的要求完成集合的创建和遍历\n   \n   * 创建一个集合，存储多个字符串元素\n   * 把集合中所有以"张"开头的元素存储到一个新的集合\n   * 把"张"开头的集合中的长度为3的元素存储到一个新的集合\n   * 遍历上一步得到的集合\n\n * 原始方式示例代码\n   \n   public class mystream1 {\n       public static void main(string[] args) {\n           //集合的批量添加\n           arraylist<string> list1 = new arraylist<>(list.of("张三丰","张无忌","张翠山","王二麻子","张良","谢广坤"));\n           //list.add()\n   \n           //遍历list1把以张开头的元素添加到list2中。\n           arraylist<string> list2 = new arraylist<>();\n           for (string s : list1) {\n               if(s.startswith("张")){\n                   list2.add(s);\n               }\n           }\n           //遍历list2集合，把其中长度为3的元素，再添加到list3中。\n           arraylist<string> list3 = new arraylist<>();\n           for (string s : list2) {\n               if(s.length() == 3){\n                   list3.add(s);\n               }\n           }\n           for (string s : list3) {\n               system.out.println(s);\n           }      \n       }\n   }\n   \n\n * 使用stream流示例代码\n   \n   public class streamdemo {\n       public static void main(string[] args) {\n           //集合的批量添加\n           arraylist<string> list1 = new arraylist<>(list.of("张三丰","张无忌","张翠山","王二麻子","张良","谢广坤"));\n   \n           //stream流\n           list1.stream().filter(s->s.startswith("张"))\n                   .filter(s->s.length() == 3)\n                   .foreach(s-> system.out.println(s));\n       }\n   }\n   \n\n * stream流的好处\n   \n   * 直接阅读代码的字面意思即可完美展示无关逻辑方式的语义：获取流、过滤姓张、过滤长度为3、逐一打印\n   * stream流把真正的函数式编程风格引入到java中\n   * 代码简洁\n\n\n# stream流的常见生成方式\n\n * stream流的思想\n   \n   \n\n * stream流的三类方法\n   \n   * 获取stream流\n     * 创建一条流水线,并把数据放到流水线上准备进行操作\n   * 中间方法\n     * 流水线上的操作\n     * 一次操作完毕之后,还可以继续进行其他操作\n   * 终结方法\n     * 一个stream流只能有一个终结方法\n     * 是流水线上的最后一个操作\n\n\n# 生成stream流的方式\n\n * collection体系集合\n   \n   使用默认方法stream()生成流\n   \n   public static void singlecollection() {\n       list<integer> list = new arraylist<>();\n       collections.addall(list, 1, 2, 3, 4, 5);\n       // lambda 表达式\n       // list.stream().foreach(num -> system.out.println(num));\n   \n       // 方法引用\n       list.stream().foreach(system.out::println);\n   }\n   \n\n * map体系集合\n   \n   把map转成set集合，间接的生成流\n   \n   private static void doublecollection() {\n       map<integer, string> map = new hashmap<>();\n       map.put(1, "a");\n       map.put(2, "b");\n       map.put(3, "c");\n   \n       // keyset 获取 stream流  -- 就是set集合获取stream流\n       map.keyset().stream().foreach(system.out::println);\n       // entryset 获取 stream 流  -- 就是set集合获取stream流\n       map.entryset().stream().foreach(system.out::println);\n   }\n   \n\n * 数组\n   \n   最好通过arrays中的静态方法stream生成流\n   \n   引用类型的数组也可以用 stream.of()方法\n   \n   private static void arraystream() {\n       int[] arr = {1, 2, 3, 4};\n       arrays.stream(arr).foreach(system.out::println);\n       string[] arrstring = {"a", "b", "c"};\n       arrays.stream(arrstring).foreach(system.out::println);\n   }\n   \n\n * 同种数据类型的多个数据\n   \n   通过stream接口的静态方法of(t... values)生成流\n   \n   // 4. 零散数据\n   // key stream.of() 小细节\n   /**\n    * 方法的参数是可变参数，可以传递一堆零散的参数，也可以传递数组\n    * 但是数组的类型必须是引用类型。如果传递基本类型的数组，会将数组作为一个元素，放到stream中\n    */\n   stream.of(1, 2, 3, 4).foreach(system.out::println);\n   stream.of("a", "b", "c", "d").foreach(system.out::println);\n   int[] arr = {1, 2, 3, 4};\n   integer[] arrint = {1, 2, 3, 4};\n   string[] arrstring = {"a", "b", "c"};\n   // [i@58372a00\n   stream.of(arr).foreach(system.out::println);\n   // 1,2,3,4\n   stream.of(arrint).foreach(system.out::println);\n   // a,b,c\n   stream.of(arrstring).foreach(system.out::println);\n   \n\n * 代码演示\n   \n   public class streamdemo {\n       public static void main(string[] args) {\n           //collection体系的集合可以使用默认方法stream()生成流\n           list<string> list = new arraylist<string>();\n           stream<string> liststream = list.stream();\n   \n           set<string> set = new hashset<string>();\n           stream<string> setstream = set.stream();\n   \n           //map体系的集合间接的生成流\n           map<string,integer> map = new hashmap<string, integer>();\n           stream<string> keystream = map.keyset().stream();\n           stream<integer> valuestream = map.values().stream();\n           stream<map.entry<string, integer>> entrystream = map.entryset().stream();\n   \n           //数组可以通过arrays中的静态方法stream生成流\n           string[] strarray = {"hello","world","java"};\n           stream<string> strarraystream = arrays.stream(strarray);\n         \n         \t//同种数据类型的多个数据可以通过stream接口的静态方法of(t... values)生成流\n           stream<string> strarraystream2 = stream.of("hello", "world", "java");\n           stream<integer> intstream = stream.of(10, 20, 30);\n       }\n   }\n   \n\n\n# stream流中间操作方法\n\n * 概念\n   \n   中间操作的意思是,执行完此方法之后,stream流依然可以继续执行其他操作\n\n * 注意\n   \n   * 中间方法，返回新的stream流，用过一次就关闭了，建议使用链式编程。\n   * 修改流中的数据，不影响集合原来的数据。\n\n * 常见方法\n   \n   方法名                                              说明\n   stream<t> filter(predicate predicate)            用于对流中的数据进行过滤\n   stream<t> limit(long maxsize)                    返回此流中的元素组成的流，截取前指定参数个数的数据\n   stream<t> skip(long n)                           跳过指定参数个数的数据，返回由该流的剩余元素组成的流\n   static<t> stream<t> concat(stream a, stream b)   合并a和b两个流为一个流；当a,b类型不一致时，类的提升为父类\n   stream<t> distinct()                             返回由该流的不同元素（根据object.equals(object) ）组成的流\n   stream<t> map(function<t,r> mapper)              转换流中的数据类型\n\n * filter代码演示\n   \n   public class mystream3 {\n       public static void main(string[] args) {\n   //        stream<t> filter(predicate predicate)：过滤\n   //        predicate接口中的方法\tboolean test(t t)：对给定的参数进行判断，返回一个布尔值\n   \n           arraylist<string> list = new arraylist<>();\n           list.add("张三丰");\n           list.add("张无忌");\n           list.add("张翠山");\n           list.add("王二麻子");\n           list.add("张良");\n           list.add("谢广坤");\n   \n           //filter方法获取流中的 每一个数据.\n           //而test方法中的s,就依次表示流中的每一个数据.\n           //我们只要在test方法中对s进行判断就可以了.\n           //如果判断的结果为true,则当前的数据留下\n           //如果判断的结果为false,则当前数据就不要.\n   //        list.stream().filter(\n   //                new predicate<string>() {\n   //                    @override\n   //                    public boolean test(string s) {\n   //                        boolean result = s.startswith("张");\n   //                        return result;\n   //                    }\n   //                }\n   //        ).foreach(s-> system.out.println(s));\n   \n           //因为predicate接口中只有一个抽象方法test\n           //所以我们可以使用lambda表达式来简化\n   //        list.stream().filter(\n   //                (string s)->{\n   //                    boolean result = s.startswith("张");\n   //                        return result;\n   //                }\n   //        ).foreach(s-> system.out.println(s));\n   \n           list.stream().filter(s ->s.startswith("张")).foreach(s-> system.out.println(s));\n   \n       }\n   }\n   \n\n * limit&skip代码演示\n   \n   public class streamdemo02 {\n       public static void main(string[] args) {\n           //创建一个集合，存储多个字符串元素\n           arraylist<string> list = new arraylist<string>();\n   \n           list.add("林青霞");\n           list.add("张曼玉");\n           list.add("王祖贤");\n           list.add("柳岩");\n           list.add("张敏");\n           list.add("张无忌");\n   \n           //需求1：取前3个数据在控制台输出\n           list.stream().limit(3).foreach(s-> system.out.println(s));\n           system.out.println("--------");\n   \n           //需求2：跳过3个元素，把剩下的元素在控制台输出\n           list.stream().skip(3).foreach(s-> system.out.println(s));\n           system.out.println("--------");\n   \n           //需求3：跳过2个元素，把剩下的元素中前2个在控制台输出\n           list.stream().skip(2).limit(2).foreach(s-> system.out.println(s));\n       }\n   }\n   \n\n * concat&distinct代码演示\n   \n   public class streamdemo03 {\n       public static void main(string[] args) {\n           //创建一个集合，存储多个字符串元素\n           arraylist<string> list = new arraylist<string>();\n   \n           list.add("林青霞");\n           list.add("张曼玉");\n           list.add("王祖贤");\n           list.add("柳岩");\n           list.add("张敏");\n           list.add("张无忌");\n   \n           //需求1：取前4个数据组成一个流\n           stream<string> s1 = list.stream().limit(4);\n   \n           //需求2：跳过2个数据组成一个流\n           stream<string> s2 = list.stream().skip(2);\n   \n           //需求3：合并需求1和需求2得到的流，并把结果在控制台输出\n   //        stream.concat(s1,s2).foreach(s-> system.out.println(s));\n   \n           //需求4：合并需求1和需求2得到的流，并把结果在控制台输出，要求字符串元素不能重复\n           stream.concat(s1,s2).distinct().foreach(s-> system.out.println(s));\n       }\n   }\n   \n\n * map\n   \n   // 6. map --- 转换流中的数据\n   arraylist<string> list2 = new arraylist<>();\n   collections.addall(list2, "张无忌-15","周芷若-14", "赵敏-10", "张三-100", "张翠山-80", "张良-90");\n   // 需求-- 获取年龄打印\n   // s 表示流中的每一个数据 返回值表示 转换之后的数据\n   list2.stream().map(s -> integer.valueof(s.substring(s.indexof("-") + 1))).foreach(system.out::println);\n   list2.stream().map(s -> integer.parseint(s.split("-")[1])).foreach(system.out::println);\n   \n\n\n# stream流终结操作方法\n\n * 概念\n   \n   终结操作的意思是,执行完此方法之后,stream流将不能再执行其他操作\n\n * 常见方法\n   \n   方法名                             说明\n   void foreach(consumer action)   对此流的每个元素执行操作\n   long count()                    返回此流中的元素数\n   toarray()                       收集流中的数据，放到数组中\n   collect(collector)              收集流中的数据，放到集合中\n\n * 代码演示\n   \n   public class mystream5 {\n       public static void main(string[] args) {\n           arraylist<string> list = new arraylist<>();\n           list.add("张三丰");\n           list.add("张无忌");\n           list.add("张翠山");\n           list.add("王二麻子");\n           list.add("张良");\n           list.add("谢广坤");\n   \n           //method1(list);\n           \n   //        long count()：返回此流中的元素数\n           long count = list.stream().count();\n           system.out.println(count);\n       }\n   \n       private static void method1(arraylist<string> list) {\n           //  void foreach(consumer action)：对此流的每个元素执行操作\n           //  consumer接口中的方法void accept(t t)：对给定的参数执行此操作\n           //在foreach方法的底层,会循环获取到流中的每一个数据.\n           //并循环调用accept方法,并把每一个数据传递给accept方法\n           //s就依次表示了流中的每一个数据.\n           //所以,我们只要在accept方法中,写上处理的业务逻辑就可以了.\n           list.stream().foreach(\n                   new consumer<string>() {\n                       @override\n                       public void accept(string s) {\n                           system.out.println(s);\n                       }\n                   }\n           );\n         \n           system.out.println("====================");\n           //lambda表达式的简化格式\n           //是因为consumer接口中,只有一个accept方法\n           list.stream().foreach(\n                   (string s)->{\n                       system.out.println(s);\n                   }\n           );\n           system.out.println("====================");\n           //lambda表达式还是可以进一步简化的.\n           list.stream().foreach(s->system.out.println(s));\n       }\n   }\n   \n\n\n# stream流的收集操作\n\n * 概念\n   \n   对数据使用stream流的方式操作完毕后,可以把流中的数据收集到集合中\n\n * 常用方法\n   \n   方法名                              说明\n   r collect(collector collector)   把结果收集到集合中\n\n * 工具类collectors提供了具体的收集方式\n   \n   方法名                                                         说明\n   public static<t> collector tolist()                         把元素收集到list集合中\n   public static<t> collector toset()                          把元素收集到set集合中\n   public static collector tomap(function keymapper,function   把元素收集到map集合中\n   valuemapper)\n   public static collector tomap(function keymapper,function   把元素收集到map集合中\n   valuemapper,binaryoperator mergefunction)\n\n * 代码演示\n   \n   public class endstream2 {\n       public static void main(string[] args) {\n           arraylist<string> list = new arraylist<>();\n           collections.addall(list, "张无忌-男-15", "张无忌-男-16", "周芷若-女-14", "赵敏-女-13", "张三-男-20", "张三丰-男-100", "张翠山-男-40", "张良-男-45");\n   \n           // 1.1 将所有男性收集到 list 集合中\n           /*list<string> collect = list.stream()\n                   .filter(s -> "男".equals(s.split("-")[1]))\n                   .collect(collectors.tolist());\n           system.out.println(collect);*/\n   \n          // 1.2 将所有男性收集到 set 集合中\n         /*  set<string> collectset = list.stream()\n                   .filter(s -> "男".equals(s.split("-")[1]))\n                   .collect(collectors.toset());\n           system.out.println(collectset);*/\n   \n           // 1.3.1 将所有男性收集到 map 集合中 --- 匿名内部类\n           // "张无忌-男-15"\n           // 键为-姓名，值为-年龄  张无忌:15\n           map<string, integer> collectmap = list.stream()\n                   .filter(s -> "男".equals(s.split("-")[1]))\n                   .collect(collectors.tomap(\n                           new function<string, string>() {\n                               // 两个泛型，第一个泛型是 流中数据的类型；第二个是map中键的类型\n                               @override\n                               // s 表示流中的数据, 返回值为map中的键\n                               public string apply(string s) {\n                                   return s.split("-")[0];\n                               }\n                           }, new function<string, integer>() {\n                               // 两个泛型，第一个泛型是 流中数据的类型；第二个是map中值的类型\n                               @override\n                               // s 表示流中的数据, 返回值为map中的值\n                               public integer apply(string s) {\n                                   return integer.valueof(s.split("-")[2]);\n                               }\n                           }, new binaryoperator<integer>() {\n                               // 键重复的覆盖规则，integer先放入map集合中的数据；integer2表示后放入map集合中的数据\n                               // 如果不加这个 方法，则map中的键不能重复\n                               @override\n                               public integer apply(integer integer, integer integer2) {\n                                   return integer2;\n                               }\n                           }\n                   ));\n           // system.out.println(collectmap);\n   \n           // 1.3.2 将所有男性收集到 map 集合中 --- lambda值\n           // "张无忌-男-15"\n           // 键为-姓名，值为-年龄  张无忌:15\n           map<string, integer> collectmap1 = list.stream()\n                   .filter(s -> "男".equals(s.split("-")[1]))\n                   .collect(collectors.tomap(\n                               // 生成键\n                               s -> s.split("-")[0],\n                               // 生成值\n                               s -> integer.valueof(s.split("-")[2]),\n                               // 重复键规则  a表示保留原来的，不覆盖；b表示覆盖\n                               (a , b) -> b));\n   \n           system.out.println(collectmap1);\n   \n       }\n   \n   }\n   \n   \n\n\n# stream流综合练习\n\n * 案例需求\n   \n   现在有两个arraylist集合，分别存储6名男演员名称和6名女演员名称，要求完成如下的操作\n   \n   * 男演员只要名字为3个字的前三人\n   * 女演员只要姓林的，并且不要第一个\n   * 把过滤后的男演员姓名和女演员姓名合并到一起\n   * 把上一步操作后的元素作为构造方法的参数创建演员对象,遍历数据\n   \n   演员类actor已经提供，里面有一个成员变量，一个带参构造方法，以及成员变量对应的get/set方法\n\n * 代码实现\n   \n   演员类\n   \n   @data\n   @allargsconstructor\n   class actor {\n       private string name;\n       private int age;\n   }\n   \n   \n   测试类\n   \n   /**\n    * @classname test1\n    * @date 2023/3/1 11:21\n    * @author diane\n    * @description 自定义对象并收集\n    *\n    *现在有两个arraylist集合，分别存储6名男演员名称和6名女演员名称，要求完成如下的操作 -- 张三,23\n    *      1.男演员只要名字为3个字的前两人\n    *      2.女演员只要姓林的，并且不要第一个\n    *      3.把过滤后的男演员姓名和女演员姓名合并到一起\n    *      4.把上一步操作后的元素作为构造方法的参数创建演员对象,遍历数据\n    *      5.将所有的演员对象保存的list集合中\n    * @version 1.0\n    */\n   public class test3 {\n       public static void main(string[] args) {\n           //创建集合\n           arraylist<string> manlist = new arraylist<string>();\n           manlist.add("周润发,23");\n           manlist.add("成龙,24");\n           manlist.add("刘德华,25");\n           manlist.add("吴京,26");\n           manlist.add("周星驰,17");\n           manlist.add("李连杰,18");\n   \n           arraylist<string> womanlist = new arraylist<string>();\n           womanlist.add("林心如,16");\n           womanlist.add("张曼玉,15");\n           womanlist.add("林青霞,20");\n           womanlist.add("柳岩,18");\n           womanlist.add("林志玲,3");\n           womanlist.add("王祖贤,10");\n   \n   \n           //  1.男演员只要名字为3个字的前两人\n           stream<string> streamman = manlist.stream()\n                   .filter(s -> s.split(",")[0].length() == 3)\n                   .limit(2);\n           //  2.女演员只要姓林的，并且不要第一个\n           stream<string> streamwoman = womanlist.stream()\n                   .filter(s -> s.split(",")[0].startswith("林"))\n                   .skip(1);\n           //  3.把过滤后的男演员姓名和女演员姓名合并到一起\n           // stream.concat(streamman, streamwoman).foreach(system.out::println);\n           //  4.把上一步操作后的元素作为构造方法的参数创建演员对象,遍历数据\n           // stream.concat(streamman, streamwoman)\n           //         .foreach(s -> system.out.println(new actor(s.split(",")[0], integer.parseint(s.split(",")[1]))));\n           //  5.将所有的演员对象保存的list集合中\n           list<actor> collect = stream.concat(streamman, streamwoman)\n                   .map(s -> new actor(s.split(",")[0], integer.parseint(s.split(",")[1])))\n                   .collect(collectors.tolist());\n           system.out.println(collect);\n       }\n   }\n   \n   \n   \n   ',charsets:{cjk:!0}},{title:"File练习",frontmatter:{autoSort:98,title:"File练习",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/cb6708/",categories:["后端","Java","流"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/60.%E6%B5%81/15.File%E7%BB%BC%E5%90%88%E7%BB%83%E4%B9%A0.html",relativePath:"01.后端/10.Java/60.流/15.File综合练习.md",key:"v-a41da114",path:"/pages/cb6708/",headers:[{level:2,title:"创建文件夹",slug:"创建文件夹",normalizedTitle:"创建文件夹",charIndex:13},{level:2,title:"查找文件（不考虑子文件夹）",slug:"查找文件-不考虑子文件夹",normalizedTitle:"查找文件（不考虑子文件夹）",charIndex:595},{level:2,title:"查找文件（考虑子文件夹）",slug:"查找文件-考虑子文件夹",normalizedTitle:"查找文件（考虑子文件夹）",charIndex:1449},{level:2,title:"删除多级文件夹",slug:"删除多级文件夹",normalizedTitle:"删除多级文件夹",charIndex:3946},{level:2,title:"统计大小",slug:"统计大小",normalizedTitle:"统计大小",charIndex:5370},{level:2,title:"统计文件个数",slug:"统计文件个数",normalizedTitle:"统计文件个数",charIndex:7065}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"创建文件夹 查找文件（不考虑子文件夹） 查找文件（考虑子文件夹） 删除多级文件夹 统计大小 统计文件个数",content:'# File练习\n\n\n# 创建文件夹\n\n在当前模块下的aaa文件夹中创建一个a.txt文件\n\n代码实现：\n\npublic class Test1 {\n    public static void main(String[] args) throws IOException {\n        //需求：在当前模块下的aaa文件夹中创建一个a.txt文件\n\n        //1.创建a.txt的父级路径\n        File file = new File("myfile\\\\aaa");\n        //2.创建父级路径\n        //如果aaa是存在的，那么此时创建失败的。\n        //如果aaa是不存在的，那么此时创建成功的。\n        file.mkdirs();\n        //3.拼接父级路径和子级路径\n        File src = new File(file,"a.txt");\n        boolean b = src.createNewFile();\n        if(b){\n            System.out.println("创建成功");\n        }else{\n            System.out.println("创建失败");\n        }\n    }\n}\n\n\n\n# 查找文件（不考虑子文件夹）\n\n定义一个方法找某一个文件夹中，是否有以avi结尾的电影（暂时不需要考虑子文件夹）\n\n代码示例：\n\npublic class Test2 {\n    public static void main(String[] args) {\n        /*需求：\n             定义一个方法找某一个文件夹中，是否有以avi结尾的电影。\n\t        （暂时不需要考虑子文件夹）\n        */\n\n        File file = new File("D:\\\\aaa\\\\bbb");\n        boolean b = haveAVI(file);\n        System.out.println(b);\n    }\n    /*\n    * 作用：用来找某一个文件夹中，是否有以avi结尾的电影\n    * 形参：要查找的文件夹\n    * 返回值：查找的结果  存在true  不存在false\n    * */\n    public static boolean haveAVI(File file){// D:\\\\aaa\n        //1.进入aaa文件夹，而且要获取里面所有的内容\n        File[] files = file.listFiles();\n        //2.遍历数组获取里面的每一个元素\n        for (File f : files) {\n            //f：依次表示aaa文件夹里面每一个文件或者文件夹的路径\n            if(f.isFile() && f.getName().endsWith(".avi")){\n                return true;\n            }\n        }\n        //3.如果循环结束之后还没有找到，直接返回false\n        return false;\n    }\n}\n\n\n\n# 查找文件（考虑子文件夹）\n\n找到电脑中所有以avi结尾的电影。（需要考虑子文件夹）\n\n代码示例：\n\npublic class Test3 {\n    public static void main(String[] args) {\n        /* 需求：\n        找到电脑中所有以avi结尾的电影。（需要考虑子文件夹）\n\n\n        套路：\n            1，进入文件夹\n            2，遍历数组\n            3，判断\n            4，判断\n\n        */\n\n        findAVI();\n\n    }\n\n    public static void findAVI(){\n        //获取本地所有的盘符\n        File[] arr = File.listRoots();\n        for (File f : arr) {\n            findAVI(f);\n        }\n    }\n\n    public static void findAVI(File src){//"C:\\\\\n        //1.进入文件夹src\n        File[] files = src.listFiles();\n        //2.遍历数组,依次得到src里面每一个文件或者文件夹\n        if(files != null){\n            for (File file : files) {\n                if(file.isFile()){\n                    //3，判断，如果是文件，就可以执行题目的业务逻辑\n                    String name = file.getName();\n                    if(name.endsWith(".avi")){\n                        System.out.println(file);\n                    }\n                }else{\n                    //4，判断，如果是文件夹，就可以递归\n                    //细节：再次调用本方法的时候，参数一定要是src的次一级路径\n                    findAVI(file);\n                }\n            }\n        }\n    }\n}\n\n\n\n\n/**\n * @ClassName test2\n * @Date 2023/3/2 11:04\n * @Author diane\n * @Description 查询当前文件夹下 是否有.txt结尾的文件   ---- 考虑子文件夹\n *      递归调用-深度优先遍历 + list集合保存文件\n * @Version 1.0\n */\npublic class test3 {\n    public static void main(String[] args) {\n        test3 test3 = new test3();\n        File file = new File("D:\\\\tmp");\n        List<File> fileList = new ArrayList<>();\n\n        // 获取当前文件夹及其子文件夹下的txt文件，存储到list集合中\n        test3.getTxt(file, fileList);\n        System.out.println(fileList);\n    }\n\n    /**\n     * 获取当前文件夹及子文件下的 txt文件，保存到list集合中\n     *      递归调用-深度优先遍历 + list集合保存文件\n     * @param file 当前文件夹\n     * @param list list集合\n     */\n    public void getTxt(File file, List<File> list) {\n        // 查询当前文件夹下 txt文件，保存到list集合中\n        File[] filePath = file.listFiles(new FileFilter() {\n            @Override\n            public boolean accept(File pathname) {\n                // 获取当前文件夹下的.txt文件\n                if (pathname.isFile() && pathname.getName().endsWith(".txt")) {\n                    list.add(pathname);\n                }\n                // 只保留文件夹\n                return pathname.isDirectory();\n            }\n        });\n\n        System.out.println(Arrays.toString(filePath));\n        // filePath 保护判断\n        if (filePath != null) {\n            // 递归调用\n            for (File file1 : filePath) {\n                getTxt(file1, list);\n            }\n        }\n    }\n}\n\n\n\n# 删除多级文件夹\n\n需求： 如果我们要删除一个有内容的文件夹 1.先删除文件夹里面所有的内容 2.再删除自己\n\n代码示例：\n\npublic class Test4 {\n    public static void main(String[] args) {\n        /*\n           删除一个多级文件夹\n           如果我们要删除一个有内容的文件夹\n           1.先删除文件夹里面所有的内容\n           2.再删除自己\n        */\n\n        File file = new File("D:\\\\aaa\\\\src");\n        delete(file);\n\n    }\n\n    /*\n    * 作用：删除src文件夹\n    * 参数：要删除的文件夹\n    * */\n    public static void delete(File src){\n        //1.先删除文件夹里面所有的内容\n        //进入src\n        File[] files = src.listFiles();\n        //遍历\n        for (File file : files) {\n            //判断,如果是文件，删除\n            if(file.isFile()){\n                file.delete();\n            }else {\n                //判断,如果是文件夹，就递归\n                delete(file);\n            }\n        }\n        //2.再删除自己\n        src.delete();\n    }\n    \n    \n    \n    /**\n     * 删除 file 文件夹\n     * @param file 要删除的文件夹\n     */\n    public static void deleteFile(File file) {\n        // 删除文件，返回文件夹目录\n        File[] files = file.listFiles(pathname -> {\n            if (pathname.isFile()) {\n                pathname.delete();\n            }\n            return pathname.isDirectory();\n        });\n        System.out.println(Arrays.toString(files));\n\n        if (files != null) {\n            for (File path : files) {\n                // 递归删除 本文件夹里的内容\n                deleteFile(path);\n            }\n        }\n\n        // 删除本文件夹\n        System.out.println(file);\n        file.delete();\n    }\n}\n\n\n\n# 统计大小\n\n需求：统计一个文件夹的总大小\n\n代码示例：\n\npublic class Test5 {\n    public static void main(String[] args) {\n       /*需求：\n            统计一个文件夹的总大小\n      */\n\n\n        File file = new File("D:\\\\aaa\\\\src");\n\n        long len = getLen(file);\n        System.out.println(len);//4919189\n    }\n\n    /*\n    * 作用：\n    *       统计一个文件夹的总大小\n    * 参数：\n    *       表示要统计的那个文件夹\n    * 返回值：\n    *       统计之后的结果\n    *\n    * 文件夹的总大小：\n    *       说白了，文件夹里面所有文件的大小\n    * */\n    public static long getLen(File src){\n        //1.定义变量进行累加\n        long len = 0;\n        //2.进入src文件夹\n        File[] files = src.listFiles();\n        //3.遍历数组\n        for (File file : files) {\n            //4.判断\n            if(file.isFile()){\n                //我们就把当前文件的大小累加到len当中\n                len = len + file.length();\n            }else{\n                //判断，如果是文件夹就递归\n                len = len + getLen(file);\n            }\n        }\n        return len;\n    }\n}\n\n\n\n\n/**\n * @ClassName test5\n * @Date 2023/3/2 15:13\n * @Author diane\n * @Description 统计一个文件的总大小\n * @Version 1.0\n */\npublic class test5 {\n    private static long len = 0;\n\n    public static void main(String[] args) {\n        File file = new File("D:\\\\tmp\\\\2");\n        getLen(file);\n        // 6,758,524 字节\n        System.out.println(len);\n        System.out.println((double)len / 1024 / 1024 + "Mb");\n\n    }\n    public static void getLen(File file) {\n        File[] files = file.listFiles(pathname -> {\n            if (pathname.isFile()) {\n                len += pathname.length();\n            }\n            return pathname.isDirectory();\n        });\n        if (files != null) {\n            for (File file1 : files) {\n                getLen(file1);\n            }\n        }\n    }\n}\n\n\n\n# 统计文件个数\n\n需求：统计一个文件夹中每种文件的个数并打印。（考虑子文件夹） 打印格式如下： txt:3个 doc:4个 jpg:6个\n\n代码示例：\n\n/**\n * @ClassName test5\n * @Date 2023/3/2 14:59\n * @Author diane\n * @Description 统计一个文件夹中每种文件的个数并打印---- 考虑子文件夹\n * @Version 1.0\n */\npublic class test6 {\n    public static void main(String[] args) {\n        File file = new File("D:\\\\tmp\\\\2");\n        Map<String, Integer> map = new HashMap<>();\n        getCount(file, map);\n        // 按照规则打印 结果\n        for (Map.Entry<String, Integer> entry : map.entrySet()) {\n            System.out.println(entry.getKey() + ":" + entry.getValue() + "个");\n        }\n    }\n\n    /**\n     * 统计文件夹中每种文件的个数\n     * @param file 要统计的文件夹\n     */\n    public static void getCount(File file, Map<String, Integer> map) {\n        File[] files = file.listFiles(pathname -> {\n            if (pathname.isFile()) {\n                String[] split = pathname.getName().split("\\\\.");\n                if (split.length >= 2) {\n                    String key = split[split.length - 1];\n                    map.merge(key, 1, Integer::sum);\n                }\n            }\n            return pathname.isDirectory();\n        });\n\n        if (files != null) {\n            for (File path : files) {\n                getCount(path, map);\n            }\n        }\n\n    }\n}\n\n',normalizedContent:'# file练习\n\n\n# 创建文件夹\n\n在当前模块下的aaa文件夹中创建一个a.txt文件\n\n代码实现：\n\npublic class test1 {\n    public static void main(string[] args) throws ioexception {\n        //需求：在当前模块下的aaa文件夹中创建一个a.txt文件\n\n        //1.创建a.txt的父级路径\n        file file = new file("myfile\\\\aaa");\n        //2.创建父级路径\n        //如果aaa是存在的，那么此时创建失败的。\n        //如果aaa是不存在的，那么此时创建成功的。\n        file.mkdirs();\n        //3.拼接父级路径和子级路径\n        file src = new file(file,"a.txt");\n        boolean b = src.createnewfile();\n        if(b){\n            system.out.println("创建成功");\n        }else{\n            system.out.println("创建失败");\n        }\n    }\n}\n\n\n\n# 查找文件（不考虑子文件夹）\n\n定义一个方法找某一个文件夹中，是否有以avi结尾的电影（暂时不需要考虑子文件夹）\n\n代码示例：\n\npublic class test2 {\n    public static void main(string[] args) {\n        /*需求：\n             定义一个方法找某一个文件夹中，是否有以avi结尾的电影。\n\t        （暂时不需要考虑子文件夹）\n        */\n\n        file file = new file("d:\\\\aaa\\\\bbb");\n        boolean b = haveavi(file);\n        system.out.println(b);\n    }\n    /*\n    * 作用：用来找某一个文件夹中，是否有以avi结尾的电影\n    * 形参：要查找的文件夹\n    * 返回值：查找的结果  存在true  不存在false\n    * */\n    public static boolean haveavi(file file){// d:\\\\aaa\n        //1.进入aaa文件夹，而且要获取里面所有的内容\n        file[] files = file.listfiles();\n        //2.遍历数组获取里面的每一个元素\n        for (file f : files) {\n            //f：依次表示aaa文件夹里面每一个文件或者文件夹的路径\n            if(f.isfile() && f.getname().endswith(".avi")){\n                return true;\n            }\n        }\n        //3.如果循环结束之后还没有找到，直接返回false\n        return false;\n    }\n}\n\n\n\n# 查找文件（考虑子文件夹）\n\n找到电脑中所有以avi结尾的电影。（需要考虑子文件夹）\n\n代码示例：\n\npublic class test3 {\n    public static void main(string[] args) {\n        /* 需求：\n        找到电脑中所有以avi结尾的电影。（需要考虑子文件夹）\n\n\n        套路：\n            1，进入文件夹\n            2，遍历数组\n            3，判断\n            4，判断\n\n        */\n\n        findavi();\n\n    }\n\n    public static void findavi(){\n        //获取本地所有的盘符\n        file[] arr = file.listroots();\n        for (file f : arr) {\n            findavi(f);\n        }\n    }\n\n    public static void findavi(file src){//"c:\\\\\n        //1.进入文件夹src\n        file[] files = src.listfiles();\n        //2.遍历数组,依次得到src里面每一个文件或者文件夹\n        if(files != null){\n            for (file file : files) {\n                if(file.isfile()){\n                    //3，判断，如果是文件，就可以执行题目的业务逻辑\n                    string name = file.getname();\n                    if(name.endswith(".avi")){\n                        system.out.println(file);\n                    }\n                }else{\n                    //4，判断，如果是文件夹，就可以递归\n                    //细节：再次调用本方法的时候，参数一定要是src的次一级路径\n                    findavi(file);\n                }\n            }\n        }\n    }\n}\n\n\n\n\n/**\n * @classname test2\n * @date 2023/3/2 11:04\n * @author diane\n * @description 查询当前文件夹下 是否有.txt结尾的文件   ---- 考虑子文件夹\n *      递归调用-深度优先遍历 + list集合保存文件\n * @version 1.0\n */\npublic class test3 {\n    public static void main(string[] args) {\n        test3 test3 = new test3();\n        file file = new file("d:\\\\tmp");\n        list<file> filelist = new arraylist<>();\n\n        // 获取当前文件夹及其子文件夹下的txt文件，存储到list集合中\n        test3.gettxt(file, filelist);\n        system.out.println(filelist);\n    }\n\n    /**\n     * 获取当前文件夹及子文件下的 txt文件，保存到list集合中\n     *      递归调用-深度优先遍历 + list集合保存文件\n     * @param file 当前文件夹\n     * @param list list集合\n     */\n    public void gettxt(file file, list<file> list) {\n        // 查询当前文件夹下 txt文件，保存到list集合中\n        file[] filepath = file.listfiles(new filefilter() {\n            @override\n            public boolean accept(file pathname) {\n                // 获取当前文件夹下的.txt文件\n                if (pathname.isfile() && pathname.getname().endswith(".txt")) {\n                    list.add(pathname);\n                }\n                // 只保留文件夹\n                return pathname.isdirectory();\n            }\n        });\n\n        system.out.println(arrays.tostring(filepath));\n        // filepath 保护判断\n        if (filepath != null) {\n            // 递归调用\n            for (file file1 : filepath) {\n                gettxt(file1, list);\n            }\n        }\n    }\n}\n\n\n\n# 删除多级文件夹\n\n需求： 如果我们要删除一个有内容的文件夹 1.先删除文件夹里面所有的内容 2.再删除自己\n\n代码示例：\n\npublic class test4 {\n    public static void main(string[] args) {\n        /*\n           删除一个多级文件夹\n           如果我们要删除一个有内容的文件夹\n           1.先删除文件夹里面所有的内容\n           2.再删除自己\n        */\n\n        file file = new file("d:\\\\aaa\\\\src");\n        delete(file);\n\n    }\n\n    /*\n    * 作用：删除src文件夹\n    * 参数：要删除的文件夹\n    * */\n    public static void delete(file src){\n        //1.先删除文件夹里面所有的内容\n        //进入src\n        file[] files = src.listfiles();\n        //遍历\n        for (file file : files) {\n            //判断,如果是文件，删除\n            if(file.isfile()){\n                file.delete();\n            }else {\n                //判断,如果是文件夹，就递归\n                delete(file);\n            }\n        }\n        //2.再删除自己\n        src.delete();\n    }\n    \n    \n    \n    /**\n     * 删除 file 文件夹\n     * @param file 要删除的文件夹\n     */\n    public static void deletefile(file file) {\n        // 删除文件，返回文件夹目录\n        file[] files = file.listfiles(pathname -> {\n            if (pathname.isfile()) {\n                pathname.delete();\n            }\n            return pathname.isdirectory();\n        });\n        system.out.println(arrays.tostring(files));\n\n        if (files != null) {\n            for (file path : files) {\n                // 递归删除 本文件夹里的内容\n                deletefile(path);\n            }\n        }\n\n        // 删除本文件夹\n        system.out.println(file);\n        file.delete();\n    }\n}\n\n\n\n# 统计大小\n\n需求：统计一个文件夹的总大小\n\n代码示例：\n\npublic class test5 {\n    public static void main(string[] args) {\n       /*需求：\n            统计一个文件夹的总大小\n      */\n\n\n        file file = new file("d:\\\\aaa\\\\src");\n\n        long len = getlen(file);\n        system.out.println(len);//4919189\n    }\n\n    /*\n    * 作用：\n    *       统计一个文件夹的总大小\n    * 参数：\n    *       表示要统计的那个文件夹\n    * 返回值：\n    *       统计之后的结果\n    *\n    * 文件夹的总大小：\n    *       说白了，文件夹里面所有文件的大小\n    * */\n    public static long getlen(file src){\n        //1.定义变量进行累加\n        long len = 0;\n        //2.进入src文件夹\n        file[] files = src.listfiles();\n        //3.遍历数组\n        for (file file : files) {\n            //4.判断\n            if(file.isfile()){\n                //我们就把当前文件的大小累加到len当中\n                len = len + file.length();\n            }else{\n                //判断，如果是文件夹就递归\n                len = len + getlen(file);\n            }\n        }\n        return len;\n    }\n}\n\n\n\n\n/**\n * @classname test5\n * @date 2023/3/2 15:13\n * @author diane\n * @description 统计一个文件的总大小\n * @version 1.0\n */\npublic class test5 {\n    private static long len = 0;\n\n    public static void main(string[] args) {\n        file file = new file("d:\\\\tmp\\\\2");\n        getlen(file);\n        // 6,758,524 字节\n        system.out.println(len);\n        system.out.println((double)len / 1024 / 1024 + "mb");\n\n    }\n    public static void getlen(file file) {\n        file[] files = file.listfiles(pathname -> {\n            if (pathname.isfile()) {\n                len += pathname.length();\n            }\n            return pathname.isdirectory();\n        });\n        if (files != null) {\n            for (file file1 : files) {\n                getlen(file1);\n            }\n        }\n    }\n}\n\n\n\n# 统计文件个数\n\n需求：统计一个文件夹中每种文件的个数并打印。（考虑子文件夹） 打印格式如下： txt:3个 doc:4个 jpg:6个\n\n代码示例：\n\n/**\n * @classname test5\n * @date 2023/3/2 14:59\n * @author diane\n * @description 统计一个文件夹中每种文件的个数并打印---- 考虑子文件夹\n * @version 1.0\n */\npublic class test6 {\n    public static void main(string[] args) {\n        file file = new file("d:\\\\tmp\\\\2");\n        map<string, integer> map = new hashmap<>();\n        getcount(file, map);\n        // 按照规则打印 结果\n        for (map.entry<string, integer> entry : map.entryset()) {\n            system.out.println(entry.getkey() + ":" + entry.getvalue() + "个");\n        }\n    }\n\n    /**\n     * 统计文件夹中每种文件的个数\n     * @param file 要统计的文件夹\n     */\n    public static void getcount(file file, map<string, integer> map) {\n        file[] files = file.listfiles(pathname -> {\n            if (pathname.isfile()) {\n                string[] split = pathname.getname().split("\\\\.");\n                if (split.length >= 2) {\n                    string key = split[split.length - 1];\n                    map.merge(key, 1, integer::sum);\n                }\n            }\n            return pathname.isdirectory();\n        });\n\n        if (files != null) {\n            for (file path : files) {\n                getcount(path, map);\n            }\n        }\n\n    }\n}\n\n',charsets:{cjk:!0}},{title:"IO流概述",frontmatter:{autoSort:97,title:"IO流概述",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/4fe65a/",categories:["后端","Java","流"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/60.%E6%B5%81/20.IO%E6%B5%81%E6%A6%82%E8%BF%B0.html",relativePath:"01.后端/10.Java/60.流/20.IO流概述.md",key:"v-75b1c1bd",path:"/pages/4fe65a/",headers:[{level:2,title:"什么是IO",slug:"什么是io",normalizedTitle:"什么是io",charIndex:11},{level:2,title:"IO的分类",slug:"io的分类",normalizedTitle:"io的分类",charIndex:277},{level:2,title:"IO的流向说明图解",slug:"io的流向说明图解",normalizedTitle:"io的流向说明图解",charIndex:526},{level:2,title:"顶级父类们",slug:"顶级父类们",normalizedTitle:"顶级父类们",charIndex:542}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"什么是IO IO的分类 IO的流向说明图解 顶级父类们",content:"# IO概述\n\n\n# 什么是IO\n\n生活中，你肯定经历过这样的场景。当你编辑一个文本文件，忘记了ctrl+s ，可能文件就白白编辑了。当你电脑上插入一个U盘，可以把一个视频，拷贝到你的电脑硬盘里。那么数据都是在哪些设备上的呢？键盘、内存、硬盘、外接设备等等。\n\n我们把这种数据的传输，可以看做是一种数据的流动，按照流动的方向，以内存为基准，分为输入input 和输出output ，即流向内存是输入流，流出内存的输出流。\n\nJava中I/O操作主要是指使用java.io包下的内容，进行输入、输出操作。输入也叫做读取数据，输出也叫做作写出数据。\n\n\n# IO的分类\n\n根据数据的流向分为：输入流和输出流。\n\n * 输入流 ：把数据从其他设备上读取到内存中的流。\n * 输出流 ：把数据从内存 中写出到其他设备上的流。\n\n格局数据的类型分为：字节流和字符流。\n\n * 字节流 ：以字节为单位，读写数据的流。\n   \n   * 可以操作所有类型的文件\n\n * 字符流 ：以字符为单位，读写数据的流。\n   \n   * 只能操作纯文本文件\n   * Window里自带的记事本能够 看懂的就是纯文本文件\n     * .txt\n     * .md\n\n\n# IO的流向说明图解\n\n\n\n\n# 顶级父类们\n\n      输入流           输出流\n字节流   字节输入流         字节输出流\n      InputStream   OutputStream\n                    \n字符流   字符输入流         字符输出流\n      Reader        Writer",normalizedContent:"# io概述\n\n\n# 什么是io\n\n生活中，你肯定经历过这样的场景。当你编辑一个文本文件，忘记了ctrl+s ，可能文件就白白编辑了。当你电脑上插入一个u盘，可以把一个视频，拷贝到你的电脑硬盘里。那么数据都是在哪些设备上的呢？键盘、内存、硬盘、外接设备等等。\n\n我们把这种数据的传输，可以看做是一种数据的流动，按照流动的方向，以内存为基准，分为输入input 和输出output ，即流向内存是输入流，流出内存的输出流。\n\njava中i/o操作主要是指使用java.io包下的内容，进行输入、输出操作。输入也叫做读取数据，输出也叫做作写出数据。\n\n\n# io的分类\n\n根据数据的流向分为：输入流和输出流。\n\n * 输入流 ：把数据从其他设备上读取到内存中的流。\n * 输出流 ：把数据从内存 中写出到其他设备上的流。\n\n格局数据的类型分为：字节流和字符流。\n\n * 字节流 ：以字节为单位，读写数据的流。\n   \n   * 可以操作所有类型的文件\n\n * 字符流 ：以字符为单位，读写数据的流。\n   \n   * 只能操作纯文本文件\n   * window里自带的记事本能够 看懂的就是纯文本文件\n     * .txt\n     * .md\n\n\n# io的流向说明图解\n\n\n\n\n# 顶级父类们\n\n      输入流           输出流\n字节流   字节输入流         字节输出流\n      inputstream   outputstream\n                    \n字符流   字符输入流         字符输出流\n      reader        writer",charsets:{cjk:!0}},{title:"File基础",frontmatter:{autoSort:99,title:"File基础",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/217f27/",categories:["后端","Java","流"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/60.%E6%B5%81/10.File%E5%9F%BA%E7%A1%80.html",relativePath:"01.后端/10.Java/60.流/10.File基础.md",key:"v-6ab9acf7",path:"/pages/217f27/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:13},{level:2,title:"构造方法",slug:"构造方法",normalizedTitle:"构造方法",charIndex:73},{level:2,title:"常用方法",slug:"常用方法",normalizedTitle:"常用方法",charIndex:1428},{level:3,title:"获取功能的方法",slug:"获取功能的方法",normalizedTitle:"获取功能的方法",charIndex:1437},{level:3,title:"绝对路径和相对路径",slug:"绝对路径和相对路径",normalizedTitle:"绝对路径和相对路径",charIndex:3750},{level:3,title:"判断功能的方法",slug:"判断功能的方法",normalizedTitle:"判断功能的方法",charIndex:4194},{level:3,title:"创建删除功能的方法",slug:"创建删除功能的方法",normalizedTitle:"创建删除功能的方法",charIndex:5254},{level:2,title:"目录的遍历",slug:"目录的遍历",normalizedTitle:"目录的遍历",charIndex:6941}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"概述 构造方法 常用方法 获取功能的方法 绝对路径和相对路径 判断功能的方法 创建删除功能的方法 目录的遍历",content:'# File基础\n\n\n# 概述\n\njava.io.File 类是文件和目录路径名的抽象表示，主要用于文件和目录的创建、查找和删除等操作。\n\n\n# 构造方法\n\n * public File(String pathname) ：通过将给定的路径名字符串转换为抽象路径名来创建新的 File实例。\n * public File(String parent, String child) ：从父路径名字符串和子路径名字符串创建新的 File实例。\n * public File(File parent, String child) ：从父抽象路径名和子路径名字符串创建新的 File实例。\n * 构造举例，代码如下：\n\n/**\n * @ClassName FileDemo1\n * @Date 2023/3/1 20:46\n * @Author diane\n * @Description  路径 File  -- 构造方法\n *      - `public File(String pathname) ` ：通过将给定的**路径名字符串**转换为抽象路径名来创建新的 File实例。\n *      - `public File(String parent, String child) ` ：从**父路径名字符串和子路径名字符串**创建新的 File实例。\n *      - `public File(File parent, String child)` ：从**父抽象路径名和子路径名字符串**创建新的 File实例。\n * @Version 1.0\n */\npublic class FileDemo1 {\n    public static void main(String[] args) {\n        // 1. 根据字符串表示的路径, 变成File对象\n        // 第一个\\ 代表转义字符\n        String str = "D:\\\\tmp\\\\1.txt";\n        File file = new File(str);\n        // D:\\tmp\\1.txt\n        System.out.println(file);\n\n\n        // 2. 父路径字符串+子路径字符串\n        // 这个是存储路径\n        String parent = "D:\\\\tmp";\n        // 这个是文件名\n        String son = "1.txt";\n        File file1 = new File(parent, son);\n        // D:\\tmp\\1.txt\n        System.out.println(file1);\n\n\n        // 3. 父路径+子路径字符串\n        File parentFile = new File(parent);\n        File file2 = new File(parentFile, son);\n        System.out.println(file2);\n\n    }\n}\n\n\n> 小贴士：\n> \n>  1. 一个File对象代表硬盘中实际存在的一个文件或者目录。\n>  2. 无论该路径下是否存在文件或者目录，都不影响File对象的创建。\n\n\n# 常用方法\n\n\n# 获取功能的方法\n\n * public String getAbsolutePath() ：返回此File的绝对路径名字符串。\n\n * public String getPath() ：获取定义文件时使用的路径。\n\n * public String getName() ：返回由此File表示的文件或目录的名称。\n\n * public long length() ：返回由此File表示的文件的长度。\n\n * public long lastModified(): 返回文件的最后修改时间-(时间 毫秒值)\n   \n   方法演示，代码如下：\n   \n   /**\n    * @ClassName FileDemo2\n    * @Date 2023/3/1 20:56\n    * @Author diane\n    * @Description  获取 的方法\n    *\n    * - `public long length()`  ：返回由此File表示的文件的大小\n    *\n    * - `public String getAbsolutePath() ` ：返回此File的绝对路径名字符串。\n    *\n    * - ` public String getPath() ` ：获取定义文件时使用的路径。\n    *\n    * - `public String getName()`  ：返回由此File表示的文件或目录的名称。\n    *\n    * - `public long lastModified()`: 返回文件的最后修改时间-(时间 毫秒值)\n    *\n    * @Version 1.0\n    */\n   public class FileDemo3 {\n       public static void main(String[] args) {\n   \n           // 1. 返回文件的大小\n           // 细节1: 这个方法只能获取 字节大小\n           //      如果单位要M,G, 可以不断的除 1024\n           // 细节2：这个方法无法获取文件夹的大小\n           String str = "D:\\\\tmp\\\\1.txt";\n           File file = new File(str);\n           // 3 个字节\n           System.out.println(file.length());\n   \n           String str1 = "D:\\\\tmp";\n           File file1 = new File(str1);\n           // 0\n           System.out.println(file1.length());\n   \n   \n           // 2. 获取绝对路径\n           System.out.println(file.getAbsolutePath());\n           // 使用相对路径去创建File 对象  默认的根是 项目的路径\n           File file2 = new File("a.txt");\n           // D:\\生活\\学习\\西电杭研院\\代码库\\java\\Java基础\\code\\a.txt\n           System.out.println(file2.getAbsolutePath());\n   \n           // 3. 获取File 定义时的路径\n           // D:\\tmp\\1.txt\n           System.out.println(file.getPath());\n           // a.txt\n           System.out.println(file2.getPath());\n   \n           // 4. 获取名字\n           // 1.txt -- 文件名称\n           System.out.println(file.getName());\n           // tmp  -- 文件夹名称\n           System.out.println(file1.getName());\n   \n           // 5. 获取文件最后修改时间 -- 返回 时间毫秒值\n           // 1677676136252\n           long time = file.lastModified();\n           System.out.println(time);\n           // 将毫秒值转换成字符串表示的时间\n           SimpleDateFormat simpleDateFormat = new SimpleDateFormat("yyyy年MM月dd日 HH:mm:ss");\n           String date = simpleDateFormat.format(time);\n           System.out.println(date);\n   \n   \n       }\n   }\n   \n\n> API中说明：length()，表示文件的长度。但是File对象表示目录，则返回值未指定。\n\n\n# 绝对路径和相对路径\n\n * 绝对路径：从盘符开始的路径，这是一个完整的路径。\n * 相对路径：相对于项目目录的路径，这是一个便捷的路径，开发中经常使用。\n\npublic class FilePath {\n    public static void main(String[] args) {\n      \t// D盘下的bbb.java文件\n        File f = new File("D:\\\\bbb.java");\n        System.out.println(f.getAbsolutePath());\n      \t\n\t\t// 项目下的bbb.java文件\n        File f2 = new File("bbb.java");\n        System.out.println(f2.getAbsolutePath());\n    }\n}\n输出结果：\nD:\\bbb.java\nD:\\idea_project_test4\\bbb.java\n\n\n\n# 判断功能的方法\n\n * public boolean exists() ：此File表示的文件或目录是否实际存在。\n * public boolean isDirectory() ：此File表示的是否为目录。\n * public boolean isFile() ：此File表示的是否为文件。\n\n方法演示，代码如下：\n\n/**\n * @ClassName FileDemo2\n * @Date 2023/3/1 20:56\n * @Author diane\n * @Description  判断的方法\n * @Version 1.0\n */\npublic class FileDemo2 {\n    public static void main(String[] args) {\n\n        // 1. 对文件进行判断\n        String str = "D:\\\\tmp\\\\1.txt";\n        File file = new File(str);\n        System.out.println(file.isFile());\n        System.out.println(file.isDirectory());\n        System.out.println(file.exists());\n\n        //2. 对文件夹进行判断\n        String str1 = "D:\\\\tmp";\n        File file1 = new File(str1);\n        System.out.println(file1.isFile());\n        System.out.println(file1.isDirectory());\n        System.out.println(file1.exists());\n\n        // 3. 对不存在的文件进行判断\n        String str2 = "D:\\\\tmp\\\\2.txt";\n        File file2 = new File(str2);\n        System.out.println(file2.isFile());\n        System.out.println(file2.isDirectory());\n        System.out.println(file2.exists());\n\n    }\n}\n\n\n\n# 创建删除功能的方法\n\n * public boolean createNewFile() ：当且仅当具有该名称的文件尚不存在时，创建一个新的空文件。\n * public boolean delete() ：删除由此File表示的文件或目录。\n * public boolean mkdir() ：创建由此File表示的目录。\n * public boolean mkdirs() ：创建由此File表示的目录，包括任何必需但不存在的父目录。\n\n方法演示，代码如下：\n\n/**\n * @ClassName FileDemo4\n * @Date 2023/3/1 21:33\n * @Author diane\n * @Description 创建与删除方法\n *  - `public boolean createNewFile()` ：当且仅当具有该名称的文件尚不存在时，创建一个新的空文件。\n * - `public boolean mkdir()` ：创建由此File表示的目录。\n * - `public boolean mkdirs()` ：创建由此File表示的目录，包括任何必需但不存在的父目录。\n * - `public boolean delete()` ：删除由此File表示的文件或目录。\n * @Version 1.0\n */\npublic class FileDemo4 {\n    public static void main(String[] args) throws IOException {\n\n        // 1. createNewFile 创建空文件\n        // 1.1 如果文件存在，则返回true；不存在，则返回false\n        // 1.2 如果父级路径不存在，那么方法会抛出异常 IOException\n        // 1.3 createNewFile 方法创建的一定是文件,如果路径不包含后缀名,则创建一个没有后缀名的文件\n       /* File file = new File("D:\\\\tmp\\\\2.txt");\n        System.out.println(file.createNewFile());*/\n\n\n        // 2. mkdir 创建文件夹\n        // 2.1 windows 中路径是唯一的,如果当前路径已经存在，则创建失败，返回false\n        // 2.2 mkdir 只能创建单级文件夹,无法创建多级文件夹； 所以后续我们都不会使用\n        /*File file = new File("D:\\\\tmp\\\\3\\\\1");\n        System.out.println(file.mkdir());*/\n\n\n        // 3. mkdirs 创建文件夹\n        // 3.1 mkdirs 可以创建多级文件夹，也可以创建单级文件夹\n        // key 所以，我们创建文件夹的话就用这个方法就好了\n        /*File file = new File("D:\\\\tmp\\\\3\\\\1");\n        System.out.println(file.mkdirs());*/\n\n\n        // 4. 删除文件 -- 删除不会放入回收站\n        // 4.1 有文件则删除，返回true\n        // 4.2 没有文件则不会删除，返回false\n        // 4.3 删除文件夹的时候，只能删除空文件夹； 当文件夹内有内容的时候，不能删除\n        File file1 = new File("D:\\\\tmp\\\\2\\\\1");\n        System.out.println(file1.delete());\n\n    }\n}\n\n\n\n> API中说明：delete方法，如果此File表示目录，则目录必须为空才能删除。\n\n\n# 目录的遍历\n\n * public File[] listFiles() ：返回一个File数组，表示该File目录中的所有的子文件或目录。\n\n * public File[] listFiles(FilenameFilter filter) ：返回一个File数组，中间加一层过滤器，表示该File目录中的所有的子文件或目录。\n\n * public File[] listFiles(FileFilter filter) ：返回一个File数组，中间加一层过滤器，表示该File目录中的所有的子文件或目录。\n   \n   \n   \n   * 只能获取当前文件夹下的所有文件和文件夹 ; 不能获取子文件夹的内容\n\n/**\n * @ClassName FileDemo4\n * @Date 2023/3/1 21:33\n * @Author diane\n * @Description 获取当前路径下的内容  -- 以 File数组的形式返回\n *  - `public File[] listFiles()` ：返回一个File数组，表示该File目录中的所有的子文件或目录。\n *  - `public File[] listFiles(FilenameFilter filter)` ：返回一个File数组，表示该File目录中的所有的子文件或目录。\n *  - `public File[] listFiles(FileFilter filter)` ：返回一个File数组，表示该File目录中的所有的子文件或目录。\n * @Version 1.0\n */\npublic class FileDemo5 {\n    public static void main(String[] args) throws IOException {\n        String str = "D:\\\\tmp";\n        File file = new File(str);\n\n        // 1. listFiles() --- 获取File对象\n        // 只能获取当前文件夹下的所有文件和文件夹 ; 不能获取子文件夹的内容\n        File[] files = file.listFiles();\n        for (File file1 : files) {\n            System.out.println(file1);\n        }\n\n\n        // 2. listFiles(FilenameFilter filter)\n        File[] files1 = file.listFiles(new FilenameFilter() {\n            @Override\n            // dir 为父级路径  name 为文件或文件夹 名称\n            // 返回 true 表示保留这个文件；false 表示不保留这个文件\n            public boolean accept(File dir, String name) {\n                // 直接这样是不好的，如果有文件命名为 4.txt  则其也会读取到对应的文件夹；不符合只读取文件的初衷\n                return name.endsWith(".txt");\n            }\n        });\n        System.out.println(Arrays.toString(files1));\n\n\n        // 3. listFiles(FileFilter filter)\n        File[] files2 = file.listFiles(new FileFilter() {\n            @Override\n            // pathname 为完整的路径\n            // 返回 true 表示保留这个文件；false 表示不保留这个文件\n            public boolean accept(File pathname) {\n                // 更推荐使用这一种\n                return pathname.isFile() && pathname.getName().endsWith(".txt");\n            }\n        });\n        System.out.println(Arrays.toString(files2));\n\n\n        // pathname 为完整的路径\n        // 返回 true 表示保留这个文件；false 表示不保留这个文件\n        // 更推荐使用这一种  先判断是否是文件，然后判断后缀是否是.txt\n        File[] files3 = file.listFiles(pathname -> pathname.isFile() && pathname.getName().endsWith(".txt"));\n        System.out.println(Arrays.toString(files3));\n\n    }\n}\n\n\n> 小贴士：\n> \n> 调用listFiles方法的File对象，表示的必须是实际存在的目录，否则返回null，无法进行遍历。\n\n * public String[] list() ：返回一个String数组，表示该File目录中的所有子文件或目录。\n * public String[] list(FilenameFilter filter) ：返回一个String数组，中间加一层过滤器，表示该File目录中的所有子文件或目录。\n\n/**\n * @ClassName FileDemo4\n * @Date 2023/3/1 21:33\n * @Author diane\n * @Description 获取当前路径下的内容 -- 以 String数组的形式返回\n *  - `public String[] list()` ：返回一个String数组，表示该File目录中的所有子文件或目录。\n *  - `public String[] list(FilenameFilter filter)` ：返回一个String数组，中间加一层过滤器，表示该File目录中的所有子文件或目录。\n * @Version 1.0\n */\npublic class FileDemo6 {\n    public static void main(String[] args) throws IOException {\n        String str = "D:\\\\tmp";\n        File file = new File(str);\n\n        // 1. list()  --- 获取String类型\n        String[] list = file.list();\n        for (String s : list) {\n            System.out.println(s);\n        }\n\n        // 2. list(FilenameFilter filter)\n        String[] list1 = file.list(new FilenameFilter() {\n            @Override\n            // dir 为父级路径  name 为文件或文件夹 名称\n            // 返回 true 表示保留这个文件；false 表示不保留这个文件\n            public boolean accept(File dir, String name) {\n                File file1 = new File(dir, name);\n                return file1.isFile() && name.endsWith(".txt");\n            }\n        });\n        System.out.println(Arrays.toString(list1));\n\n\n        String[] list2 = file.list((dir, name) -> {\n            File file1 = new File(dir, name);\n            return file1.isFile() && name.endsWith(".txt");\n        });\n        System.out.println(Arrays.toString(list2));\n\n\n        String[] list3 = file.list((dir, name) -> new File(dir, name).isFile() && name.endsWith(".txt"));\n        System.out.println(Arrays.toString(list3));\n\n    }\n}\n\n',normalizedContent:'# file基础\n\n\n# 概述\n\njava.io.file 类是文件和目录路径名的抽象表示，主要用于文件和目录的创建、查找和删除等操作。\n\n\n# 构造方法\n\n * public file(string pathname) ：通过将给定的路径名字符串转换为抽象路径名来创建新的 file实例。\n * public file(string parent, string child) ：从父路径名字符串和子路径名字符串创建新的 file实例。\n * public file(file parent, string child) ：从父抽象路径名和子路径名字符串创建新的 file实例。\n * 构造举例，代码如下：\n\n/**\n * @classname filedemo1\n * @date 2023/3/1 20:46\n * @author diane\n * @description  路径 file  -- 构造方法\n *      - `public file(string pathname) ` ：通过将给定的**路径名字符串**转换为抽象路径名来创建新的 file实例。\n *      - `public file(string parent, string child) ` ：从**父路径名字符串和子路径名字符串**创建新的 file实例。\n *      - `public file(file parent, string child)` ：从**父抽象路径名和子路径名字符串**创建新的 file实例。\n * @version 1.0\n */\npublic class filedemo1 {\n    public static void main(string[] args) {\n        // 1. 根据字符串表示的路径, 变成file对象\n        // 第一个\\ 代表转义字符\n        string str = "d:\\\\tmp\\\\1.txt";\n        file file = new file(str);\n        // d:\\tmp\\1.txt\n        system.out.println(file);\n\n\n        // 2. 父路径字符串+子路径字符串\n        // 这个是存储路径\n        string parent = "d:\\\\tmp";\n        // 这个是文件名\n        string son = "1.txt";\n        file file1 = new file(parent, son);\n        // d:\\tmp\\1.txt\n        system.out.println(file1);\n\n\n        // 3. 父路径+子路径字符串\n        file parentfile = new file(parent);\n        file file2 = new file(parentfile, son);\n        system.out.println(file2);\n\n    }\n}\n\n\n> 小贴士：\n> \n>  1. 一个file对象代表硬盘中实际存在的一个文件或者目录。\n>  2. 无论该路径下是否存在文件或者目录，都不影响file对象的创建。\n\n\n# 常用方法\n\n\n# 获取功能的方法\n\n * public string getabsolutepath() ：返回此file的绝对路径名字符串。\n\n * public string getpath() ：获取定义文件时使用的路径。\n\n * public string getname() ：返回由此file表示的文件或目录的名称。\n\n * public long length() ：返回由此file表示的文件的长度。\n\n * public long lastmodified(): 返回文件的最后修改时间-(时间 毫秒值)\n   \n   方法演示，代码如下：\n   \n   /**\n    * @classname filedemo2\n    * @date 2023/3/1 20:56\n    * @author diane\n    * @description  获取 的方法\n    *\n    * - `public long length()`  ：返回由此file表示的文件的大小\n    *\n    * - `public string getabsolutepath() ` ：返回此file的绝对路径名字符串。\n    *\n    * - ` public string getpath() ` ：获取定义文件时使用的路径。\n    *\n    * - `public string getname()`  ：返回由此file表示的文件或目录的名称。\n    *\n    * - `public long lastmodified()`: 返回文件的最后修改时间-(时间 毫秒值)\n    *\n    * @version 1.0\n    */\n   public class filedemo3 {\n       public static void main(string[] args) {\n   \n           // 1. 返回文件的大小\n           // 细节1: 这个方法只能获取 字节大小\n           //      如果单位要m,g, 可以不断的除 1024\n           // 细节2：这个方法无法获取文件夹的大小\n           string str = "d:\\\\tmp\\\\1.txt";\n           file file = new file(str);\n           // 3 个字节\n           system.out.println(file.length());\n   \n           string str1 = "d:\\\\tmp";\n           file file1 = new file(str1);\n           // 0\n           system.out.println(file1.length());\n   \n   \n           // 2. 获取绝对路径\n           system.out.println(file.getabsolutepath());\n           // 使用相对路径去创建file 对象  默认的根是 项目的路径\n           file file2 = new file("a.txt");\n           // d:\\生活\\学习\\西电杭研院\\代码库\\java\\java基础\\code\\a.txt\n           system.out.println(file2.getabsolutepath());\n   \n           // 3. 获取file 定义时的路径\n           // d:\\tmp\\1.txt\n           system.out.println(file.getpath());\n           // a.txt\n           system.out.println(file2.getpath());\n   \n           // 4. 获取名字\n           // 1.txt -- 文件名称\n           system.out.println(file.getname());\n           // tmp  -- 文件夹名称\n           system.out.println(file1.getname());\n   \n           // 5. 获取文件最后修改时间 -- 返回 时间毫秒值\n           // 1677676136252\n           long time = file.lastmodified();\n           system.out.println(time);\n           // 将毫秒值转换成字符串表示的时间\n           simpledateformat simpledateformat = new simpledateformat("yyyy年mm月dd日 hh:mm:ss");\n           string date = simpledateformat.format(time);\n           system.out.println(date);\n   \n   \n       }\n   }\n   \n\n> api中说明：length()，表示文件的长度。但是file对象表示目录，则返回值未指定。\n\n\n# 绝对路径和相对路径\n\n * 绝对路径：从盘符开始的路径，这是一个完整的路径。\n * 相对路径：相对于项目目录的路径，这是一个便捷的路径，开发中经常使用。\n\npublic class filepath {\n    public static void main(string[] args) {\n      \t// d盘下的bbb.java文件\n        file f = new file("d:\\\\bbb.java");\n        system.out.println(f.getabsolutepath());\n      \t\n\t\t// 项目下的bbb.java文件\n        file f2 = new file("bbb.java");\n        system.out.println(f2.getabsolutepath());\n    }\n}\n输出结果：\nd:\\bbb.java\nd:\\idea_project_test4\\bbb.java\n\n\n\n# 判断功能的方法\n\n * public boolean exists() ：此file表示的文件或目录是否实际存在。\n * public boolean isdirectory() ：此file表示的是否为目录。\n * public boolean isfile() ：此file表示的是否为文件。\n\n方法演示，代码如下：\n\n/**\n * @classname filedemo2\n * @date 2023/3/1 20:56\n * @author diane\n * @description  判断的方法\n * @version 1.0\n */\npublic class filedemo2 {\n    public static void main(string[] args) {\n\n        // 1. 对文件进行判断\n        string str = "d:\\\\tmp\\\\1.txt";\n        file file = new file(str);\n        system.out.println(file.isfile());\n        system.out.println(file.isdirectory());\n        system.out.println(file.exists());\n\n        //2. 对文件夹进行判断\n        string str1 = "d:\\\\tmp";\n        file file1 = new file(str1);\n        system.out.println(file1.isfile());\n        system.out.println(file1.isdirectory());\n        system.out.println(file1.exists());\n\n        // 3. 对不存在的文件进行判断\n        string str2 = "d:\\\\tmp\\\\2.txt";\n        file file2 = new file(str2);\n        system.out.println(file2.isfile());\n        system.out.println(file2.isdirectory());\n        system.out.println(file2.exists());\n\n    }\n}\n\n\n\n# 创建删除功能的方法\n\n * public boolean createnewfile() ：当且仅当具有该名称的文件尚不存在时，创建一个新的空文件。\n * public boolean delete() ：删除由此file表示的文件或目录。\n * public boolean mkdir() ：创建由此file表示的目录。\n * public boolean mkdirs() ：创建由此file表示的目录，包括任何必需但不存在的父目录。\n\n方法演示，代码如下：\n\n/**\n * @classname filedemo4\n * @date 2023/3/1 21:33\n * @author diane\n * @description 创建与删除方法\n *  - `public boolean createnewfile()` ：当且仅当具有该名称的文件尚不存在时，创建一个新的空文件。\n * - `public boolean mkdir()` ：创建由此file表示的目录。\n * - `public boolean mkdirs()` ：创建由此file表示的目录，包括任何必需但不存在的父目录。\n * - `public boolean delete()` ：删除由此file表示的文件或目录。\n * @version 1.0\n */\npublic class filedemo4 {\n    public static void main(string[] args) throws ioexception {\n\n        // 1. createnewfile 创建空文件\n        // 1.1 如果文件存在，则返回true；不存在，则返回false\n        // 1.2 如果父级路径不存在，那么方法会抛出异常 ioexception\n        // 1.3 createnewfile 方法创建的一定是文件,如果路径不包含后缀名,则创建一个没有后缀名的文件\n       /* file file = new file("d:\\\\tmp\\\\2.txt");\n        system.out.println(file.createnewfile());*/\n\n\n        // 2. mkdir 创建文件夹\n        // 2.1 windows 中路径是唯一的,如果当前路径已经存在，则创建失败，返回false\n        // 2.2 mkdir 只能创建单级文件夹,无法创建多级文件夹； 所以后续我们都不会使用\n        /*file file = new file("d:\\\\tmp\\\\3\\\\1");\n        system.out.println(file.mkdir());*/\n\n\n        // 3. mkdirs 创建文件夹\n        // 3.1 mkdirs 可以创建多级文件夹，也可以创建单级文件夹\n        // key 所以，我们创建文件夹的话就用这个方法就好了\n        /*file file = new file("d:\\\\tmp\\\\3\\\\1");\n        system.out.println(file.mkdirs());*/\n\n\n        // 4. 删除文件 -- 删除不会放入回收站\n        // 4.1 有文件则删除，返回true\n        // 4.2 没有文件则不会删除，返回false\n        // 4.3 删除文件夹的时候，只能删除空文件夹； 当文件夹内有内容的时候，不能删除\n        file file1 = new file("d:\\\\tmp\\\\2\\\\1");\n        system.out.println(file1.delete());\n\n    }\n}\n\n\n\n> api中说明：delete方法，如果此file表示目录，则目录必须为空才能删除。\n\n\n# 目录的遍历\n\n * public file[] listfiles() ：返回一个file数组，表示该file目录中的所有的子文件或目录。\n\n * public file[] listfiles(filenamefilter filter) ：返回一个file数组，中间加一层过滤器，表示该file目录中的所有的子文件或目录。\n\n * public file[] listfiles(filefilter filter) ：返回一个file数组，中间加一层过滤器，表示该file目录中的所有的子文件或目录。\n   \n   \n   \n   * 只能获取当前文件夹下的所有文件和文件夹 ; 不能获取子文件夹的内容\n\n/**\n * @classname filedemo4\n * @date 2023/3/1 21:33\n * @author diane\n * @description 获取当前路径下的内容  -- 以 file数组的形式返回\n *  - `public file[] listfiles()` ：返回一个file数组，表示该file目录中的所有的子文件或目录。\n *  - `public file[] listfiles(filenamefilter filter)` ：返回一个file数组，表示该file目录中的所有的子文件或目录。\n *  - `public file[] listfiles(filefilter filter)` ：返回一个file数组，表示该file目录中的所有的子文件或目录。\n * @version 1.0\n */\npublic class filedemo5 {\n    public static void main(string[] args) throws ioexception {\n        string str = "d:\\\\tmp";\n        file file = new file(str);\n\n        // 1. listfiles() --- 获取file对象\n        // 只能获取当前文件夹下的所有文件和文件夹 ; 不能获取子文件夹的内容\n        file[] files = file.listfiles();\n        for (file file1 : files) {\n            system.out.println(file1);\n        }\n\n\n        // 2. listfiles(filenamefilter filter)\n        file[] files1 = file.listfiles(new filenamefilter() {\n            @override\n            // dir 为父级路径  name 为文件或文件夹 名称\n            // 返回 true 表示保留这个文件；false 表示不保留这个文件\n            public boolean accept(file dir, string name) {\n                // 直接这样是不好的，如果有文件命名为 4.txt  则其也会读取到对应的文件夹；不符合只读取文件的初衷\n                return name.endswith(".txt");\n            }\n        });\n        system.out.println(arrays.tostring(files1));\n\n\n        // 3. listfiles(filefilter filter)\n        file[] files2 = file.listfiles(new filefilter() {\n            @override\n            // pathname 为完整的路径\n            // 返回 true 表示保留这个文件；false 表示不保留这个文件\n            public boolean accept(file pathname) {\n                // 更推荐使用这一种\n                return pathname.isfile() && pathname.getname().endswith(".txt");\n            }\n        });\n        system.out.println(arrays.tostring(files2));\n\n\n        // pathname 为完整的路径\n        // 返回 true 表示保留这个文件；false 表示不保留这个文件\n        // 更推荐使用这一种  先判断是否是文件，然后判断后缀是否是.txt\n        file[] files3 = file.listfiles(pathname -> pathname.isfile() && pathname.getname().endswith(".txt"));\n        system.out.println(arrays.tostring(files3));\n\n    }\n}\n\n\n> 小贴士：\n> \n> 调用listfiles方法的file对象，表示的必须是实际存在的目录，否则返回null，无法进行遍历。\n\n * public string[] list() ：返回一个string数组，表示该file目录中的所有子文件或目录。\n * public string[] list(filenamefilter filter) ：返回一个string数组，中间加一层过滤器，表示该file目录中的所有子文件或目录。\n\n/**\n * @classname filedemo4\n * @date 2023/3/1 21:33\n * @author diane\n * @description 获取当前路径下的内容 -- 以 string数组的形式返回\n *  - `public string[] list()` ：返回一个string数组，表示该file目录中的所有子文件或目录。\n *  - `public string[] list(filenamefilter filter)` ：返回一个string数组，中间加一层过滤器，表示该file目录中的所有子文件或目录。\n * @version 1.0\n */\npublic class filedemo6 {\n    public static void main(string[] args) throws ioexception {\n        string str = "d:\\\\tmp";\n        file file = new file(str);\n\n        // 1. list()  --- 获取string类型\n        string[] list = file.list();\n        for (string s : list) {\n            system.out.println(s);\n        }\n\n        // 2. list(filenamefilter filter)\n        string[] list1 = file.list(new filenamefilter() {\n            @override\n            // dir 为父级路径  name 为文件或文件夹 名称\n            // 返回 true 表示保留这个文件；false 表示不保留这个文件\n            public boolean accept(file dir, string name) {\n                file file1 = new file(dir, name);\n                return file1.isfile() && name.endswith(".txt");\n            }\n        });\n        system.out.println(arrays.tostring(list1));\n\n\n        string[] list2 = file.list((dir, name) -> {\n            file file1 = new file(dir, name);\n            return file1.isfile() && name.endswith(".txt");\n        });\n        system.out.println(arrays.tostring(list2));\n\n\n        string[] list3 = file.list((dir, name) -> new file(dir, name).isfile() && name.endswith(".txt"));\n        system.out.println(arrays.tostring(list3));\n\n    }\n}\n\n',charsets:{cjk:!0}},{title:"字节流",frontmatter:{autoSort:96,title:"字节流",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/674de2/",categories:["后端","Java","流"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/60.%E6%B5%81/25.%E5%AD%97%E8%8A%82%E6%B5%81.html",relativePath:"01.后端/10.Java/60.流/25.字节流.md",key:"v-b72fe238",path:"/pages/674de2/",headers:[{level:2,title:"一切皆为字节",slug:"一切皆为字节",normalizedTitle:"一切皆为字节",charIndex:10},{level:2,title:"字节输出流【OutputStream】",slug:"字节输出流【outputstream】",normalizedTitle:"字节输出流【outputstream】",charIndex:141},{level:2,title:"FileOutputStream类",slug:"fileoutputstream类",normalizedTitle:"fileoutputstream类",charIndex:576},{level:3,title:"构造方法",slug:"构造方法",normalizedTitle:"构造方法",charIndex:677},{level:3,title:"写出字节数据",slug:"写出字节数据",normalizedTitle:"写出字节数据",charIndex:1438},{level:3,title:"数据追加续写",slug:"数据追加续写",normalizedTitle:"数据追加续写",charIndex:2821},{level:3,title:"写出换行",slug:"写出换行",normalizedTitle:"写出换行",charIndex:3515},{level:2,title:"字节输入流【InputStream】",slug:"字节输入流【inputstream】",normalizedTitle:"字节输入流【inputstream】",charIndex:4616},{level:2,title:"FileInputStream类",slug:"fileinputstream类",normalizedTitle:"fileinputstream类",charIndex:4913},{level:3,title:"构造方法",slug:"构造方法-2",normalizedTitle:"构造方法",charIndex:677},{level:3,title:"读取字节数据",slug:"读取字节数据",normalizedTitle:"读取字节数据",charIndex:5570},{level:2,title:"字节流练习：图片复制",slug:"字节流练习-图片复制",normalizedTitle:"字节流练习：图片复制",charIndex:8435},{level:3,title:"复制原理图解",slug:"复制原理图解",normalizedTitle:"复制原理图解",charIndex:8450},{level:3,title:"案例实现",slug:"案例实现",normalizedTitle:"案例实现",charIndex:8463},{level:2,title:"try-catch-finally",slug:"try-catch-finally",normalizedTitle:"try-catch-finally",charIndex:9114}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"一切皆为字节 字节输出流【OutputStream】 FileOutputStream类 构造方法 写出字节数据 数据追加续写 写出换行 字节输入流【InputStream】 FileInputStream类 构造方法 读取字节数据 字节流练习：图片复制 复制原理图解 案例实现 try-catch-finally",content:'# 字节流\n\n\n# 一切皆为字节\n\n一切文件数据(文本、图片、视频等)在存储时，都是以二进制数字的形式保存，都一个一个的字节，那么传输时一样如此。所以，字节流可以传输任意文件数据。在操作流的时候，我们要时刻明确，无论使用什么样的流对象，底层传输的始终为二进制数据****。\n\n\n# 字节输出流【OutputStream】\n\njava.io.OutputStream抽象类是表示字节输出流的所有类的超类，将指定的字节信息写出到目的地。它定义了字节输出流的基本共性功能方法。\n\n * public void close() ：关闭此输出流并释放与此流相关联的任何系统资源。\n * public void flush() ：刷新此输出流并强制任何缓冲的输出字节被写出。\n * public void write(byte[] b)：将 b.length字节从指定的字节数组写入此输出流。\n * public void write(byte[] b, int off, int len) ：从指定的字节数组写入 len字节，从偏移量 off开始输出到此输出流。\n * public abstract void write(int b) ：将指定的字节输出流。\n\n> 小贴士：\n> \n> close方法，当完成流的操作时，必须调用此方法，释放系统资源。\n\n\n# FileOutputStream类\n\nOutputStream有很多子类，我们从最简单的一个子类开始。\n\njava.io.FileOutputStream类是文件输出流，用于将数据写出到文件。\n\n\n# 构造方法\n\n * public FileOutputStream(File file)：创建文件输出流以写入由指定的 File对象表示的文件，覆盖写\n\n * public FileOutputStream(File file, boolean append)：创建文件输出流以写入由指定的 File对象表示的文件\n   \n   * 不带 append 参数，默认是false，为覆盖写\n   * 带append参数，设为true为追加写\n\n * public FileOutputStream(String name)： 创建文件输出流以指定的名称写入文件，覆盖写\n\n * public FileOutputStream(String name, boolean append)： 创建文件输出流以指定的名称写入文件,true-追加写\n\n当你创建一个流对象时，必须传入一个文件路径。**该路径下，如果没有这个文件，会创建该文件。**构造举例，代码如下：\n\npublic class FileOutputStreamConstructor throws IOException {\n    public static void main(String[] args) {\n   \t \t// 使用File对象创建流对象\n        File file = new File("a.txt");\n        FileOutputStream fos = new FileOutputStream(file);\n      \n        // 使用文件名称创建流对象\n        FileOutputStream fos = new FileOutputStream("b.txt");\n    }\n}\n\n\n\n# 写出字节数据\n\n 1. 写出字节：write(int b) 方法，每次可以写出一个字节数据，代码使用演示：\n\npublic class FOSWrite {\n    public static void main(String[] args) throws IOException {\n        // 使用文件名称创建流对象\n        FileOutputStream fos = new FileOutputStream("fos.txt");     \n      \t// 写出数据\n        \n      \tfos.write(97); // 写出第1个字节 -a\n      \tfos.write(98); // 写出第2个字节 -b\n      \tfos.write(99); // 写出第3个字节 -c\n        fos.write(\'b\');// 写出第4个字节 -b\n      \t// 关闭资源\n        fos.close();\n    }\n}\n文件内容为：\nabcb\n\n\n> 小贴士：\n> \n>  1. 虽然参数为int类型四个字节，但是只会保留一个字节的信息写出。ASCII码值\n>  2. 流操作完毕后，必须释放系统资源，调用close方法，千万记得。\n\n 2. 写出字节数组：write(byte[] b)，每次可以写出数组中的数据，代码使用演示：\n\npublic class FOSWrite {\n    public static void main(String[] args) throws IOException {\n        // 使用文件名称创建流对象\n        FileOutputStream fos = new FileOutputStream("fos.txt");     \n      \t// 字符串转换为字节数组\n      \tbyte[] b = "黑马程序员".getBytes();\n      \t// 写出字节数组数据\n      \tfos.write(b);\n      \t// 关闭资源\n        fos.close();\n    }\n}\n文件内容为：\n黑马程序员\n\n\n 3. 写出指定长度字节数组：write(byte[] b, int off, int len) ,每次写出从off索引开始，len个字节，代码使用演示：\n\npublic class FOSWrite {\n    public static void main(String[] args) throws IOException {\n        // 使用文件名称创建流对象\n        FileOutputStream fos = new FileOutputStream("fos.txt");     \n      \t// 字符串转换为字节数组\n      \tbyte[] b = "abcde".getBytes();\n\t\t// 写出从索引2开始，2个字节。索引2是c，两个字节，也就是cd。\n        fos.write(b,2,2);\n      \t// 关闭资源\n        fos.close();\n    }\n}\n文件内容为：\ncd\n\n\n\n# 数据追加续写\n\n经过以上的演示，每次程序运行，创建输出流对象，都会清空目标文件中的数据。如何保留目标文件中数据，还能继续添加新数据呢？\n\n * public FileOutputStream(File file, boolean append)： 创建文件输出流以写入由指定的 File对象表示的文件。\n * public FileOutputStream(String name, boolean append)： 创建文件输出流以指定的名称写入文件。\n\n这两个构造方法，参数中都需要传入一个boolean类型的值，true 表示追加数据，false 表示清空原有数据。这样创建的输出流对象，就可以指定是否追加续写了，代码使用演示：\n\npublic class FOSWrite {\n    public static void main(String[] args) throws IOException {\n        // 使用文件名称创建流对象\n        FileOutputStream fos = new FileOutputStream("fos.txt"，true);     \n      \t// 字符串转换为字节数组\n      \tbyte[] b = "abcde".getBytes();\n\t\t// 写出从索引2开始，2个字节。索引2是c，两个字节，也就是cd。\n        fos.write(b);\n      \t// 关闭资源\n        fos.close();\n    }\n}\n文件操作前：cd\n文件操作后：cdabcde\n\n\n\n# 写出换行\n\n *      换行符\n        *          windows: `\\r\\n `\n                   *          回车`\\r`-将光标放到这一行的最开头\n                   *          换行`\\n`-将光标换到下一行\n        *          linux: `\\n`\n        *          mac: `\\r`\n   \n\n *      细节\n        *          虽然 window系统是 \\r\\n  但是 **写一个\\r 或者 \\n 都可以**\n        *          java在底层会进行补全\n   \n\n *      **建议写全**\n   \n\nWindows系统里，换行符号是\\r\\n 。把\n\n以指定是否追加续写了，代码使用演示：\n\npublic class FOSWrite {\n    public static void main(String[] args) throws IOException {\n        // 使用文件名称创建流对象\n        FileOutputStream fos = new FileOutputStream("fos.txt");  \n      \t// 定义字节数组\n      \tbyte[] words = {97,98,99,100,101};\n      \t// 遍历数组\n        for (int i = 0; i < words.length; i++) {\n          \t// 写出一个字节\n            fos.write(words[i]);\n          \t// 写出一个换行, 换行符号转成数组写出\n            fos.write("\\r\\n".getBytes());\n        }\n      \t// 关闭资源\n        fos.close();\n    }\n}\n\n输出结果：\na\nb\nc\nd\ne\n\n\n>  * 回车符\\r和换行符\\n ：\n>    * 回车符：回到一行的开头（return）。\n>    * 换行符：下一行（newline）。\n>  * 系统中的换行：\n>    * Windows系统里，每行结尾是 回车+换行 ，即\\r\\n；\n>    * Unix系统里，每行结尾只有 换行 ，即\\n；\n>    * Mac系统里，每行结尾是 回车 ，即\\r。从 Mac OS X开始与Linux统一。\n\n\n# 字节输入流【InputStream】\n\njava.io.InputStream抽象类是表示字节输入流的所有类的超类，可以读取字节信息到内存中。它定义了字节输入流的基本共性功能方法。\n\n * public void close() ：关闭此输入流并释放与此流相关联的任何系统资源。\n * public abstract int read()： 从输入流读取数据的下一个字节。\n * public int read(byte[] b)： 从输入流中读取一些字节数，并将它们存储到字节数组 b中 。\n\n> 小贴士：\n> \n> close方法，当完成流的操作时，必须调用此方法，释放系统资源。\n\n\n# FileInputStream类\n\njava.io.FileInputStream类是文件输入流，从文件中读取字节。\n\n\n# 构造方法\n\n * FileInputStream(File file)： 通过打开与实际文件的连接来创建一个 FileInputStream ，该文件由文件系统中的 File对象 file命名。\n * FileInputStream(String name)： 通过打开与实际文件的连接来创建一个 FileInputStream ，该文件由文件系统中的路径名 name命名。\n\n当你创建一个流对象时，必须传入一个文件路径。该路径下，如果没有该文件,会抛出FileNotFoundException 。\n\n * 构造举例，代码如下：\n\npublic class FileInputStreamConstructor throws IOException{\n    public static void main(String[] args) {\n   \t \t// 使用File对象创建流对象\n        File file = new File("a.txt");\n        FileInputStream fos = new FileInputStream(file);\n      \n        // 使用文件名称创建流对象\n        FileInputStream fos = new FileInputStream("b.txt");\n    }\n}\n\n\n\n# 读取字节数据\n\n * 1.创建输入流对象\n   \n   * 如果文件不存在，就会直接报错\n\n * 2.读取数据\n   \n   * 一次读取一个字节，读出来的数据是在ASCII码上对应的数字\n   \n   * 读到文件尾了,read方法会返回-1\n   \n   * 当文件中有“-1”时，也是先读-，在读1；是无法读取到-1的\n\n * 3.关闭流\n\n 1. 读取字节：read方法，每次可以读取一个字节的数据，提升为int类型，读取到文件末尾，返回-1，代码使用演示：\n\npublic class FISRead {\n    public static void main(String[] args) throws IOException{\n      \t// 使用文件名称创建流对象\n       \tFileInputStream fis = new FileInputStream("read.txt");\n      \t// 读取数据，返回一个字节\n        int read = fis.read();\n        System.out.println((char) read);\n        read = fis.read();\n        System.out.println((char) read);\n        read = fis.read();\n        System.out.println((char) read);\n        read = fis.read();\n        System.out.println((char) read);\n        read = fis.read();\n        System.out.println((char) read);\n      \t// 读取到末尾,返回-1\n       \tread = fis.read();\n        System.out.println( read);\n\t\t// 关闭资源\n        fis.close();\n    }\n}\n输出结果：\na\nb\nc\nd\ne\n-1\n\n\n循环改进读取方式，代码使用演示：\n\npublic class FISRead {\n    public static void main(String[] args) throws IOException{\n      \t// 使用文件名称创建流对象\n       \tFileInputStream fis = new FileInputStream("read.txt");\n      \t// 定义变量，保存数据\n        int b;\n        // 循环读取 --- 优雅\n        while ((b = fis.read())!=-1) {\n            System.out.println((char)b);\n        }\n        \n        // 2.2  以前的写法，，不优雅---- 不推荐\n        int b;\n        while (true) {\n            b = fis.read();\n            if (b ** -1) {\n                break;\n            }\n            System.out.print((char) b);\n        }\n        \n        \n\t\t// 关闭资源\n        fis.close();\n    }\n}\n输出结果：\na\nb\nc\nd\ne\n\n\n> 小贴士：\n> \n>  1. 虽然读取了一个字节，但是会自动提升为int类型。\n>  2. 流操作完毕后，必须释放系统资源，调用close方法，千万记得。\n\n 2. 使用字节数组读取：read(byte[] b)，每次读取b的长度个字节到数组中，返回读取到的有效字节个数，读取到末尾时，返回-1 ，代码使用演示：\n\npublic class FISRead {\n    public static void main(String[] args) throws IOException{\n      \t// 使用文件名称创建流对象.\n       \tFileInputStream fis = new FileInputStream("read.txt"); // 文件中为abcde\n      \t// 定义变量，作为有效个数\n        int len;\n        // 定义字节数组，作为装字节数据的容器   \n        byte[] b = new byte[2];\n        // 循环读取\n        while (( len= fis.read(b))!=-1) {\n           \t// 每次读取后,把数组变成字符串打印\n            System.out.println(new String(b));\n        }\n\t\t// 关闭资源\n        fis.close();\n    }\n}\n\n输出结果：\nab\ncd\ned\n\n\n错误数据d，是由于最后一次读取时，只读取一个字节e，数组中，上次读取的数据没有被完全替换，所以要通过len ，获取有效的字节，代码使用演示：\n\npublic class FISRead {\n    public static void main(String[] args) throws IOException{\n      \t// 使用文件名称创建流对象.\n       \tFileInputStream fis = new FileInputStream("read.txt"); // 文件中为abcde\n      \t// 定义变量，作为有效个数\n        int len ；\n        // 定义字节数组，作为装字节数据的容器   \n        byte[] b = new byte[2];\n        // 循环读取\n        while (( len= fis.read(b))!=-1) {\n           \t// 每次读取后,把数组的有效字节部分，变成字符串打印\n            System.out.println(new String(b，0，len));//  len 每次读取的有效字节个数\n        }\n\t\t// 关闭资源\n        fis.close();\n    }\n}\n\n输出结果：\nab\ncd\ne\n\n\n> 小贴士：\n> \n> 使用数组读取，每次读取多个字节，减少了系统间的IO操作次数，从而提高了读写的效率，建议开发中使用。\n\n\n# 字节流练习：图片复制\n\n\n# 复制原理图解\n\n\n\n\n# 案例实现\n\n复制图片文件，代码使用演示：\n\npublic class Copy {\n    public static void main(String[] args) throws IOException {\n        // 1.创建流对象\n        // 1.1 指定数据源\n        FileInputStream fis = new FileInputStream("D:\\\\test.jpg");\n        // 1.2 指定目的地\n        FileOutputStream fos = new FileOutputStream("test_copy.jpg");\n\n        // 2.读写数据\n        // 2.1 定义数组\n        byte[] b = new byte[1024];\n        // 2.2 定义长度\n        int len;\n        // 2.3 循环读取\n        while ((len = fis.read(b))!=-1) {\n            // 2.4 写出数据\n            fos.write(b, 0 , len);\n        }\n\n        // 3.关闭资源\n        fos.close();\n        fis.close();\n    }\n}\n\n\n> 小贴士：\n> \n> 流的关闭原则：先开后关，后开先关。\n\n\n# try-catch-finally\n\n自动释放资源———— 实现了AutoCloseable接口的资源可以自动释放\n\n\n\n * 显示释放资源\n   \n   /**\n    * 使用try-catch-finally 手动关闭流\n    */\n   private static void tryFinally() {\n       // 1. 创建输入流对象 和 输出流对象\n       FileInputStream fis = null;\n       FileOutputStream fos = null;\n       long start = 0;\n   \n       try {\n           fis = new FileInputStream("D:\\\\tmp\\\\1.mp4");\n           fos = new FileOutputStream("2.mp4");\n   \n           // 当前 时间\n           start = System.currentTimeMillis();\n   \n           // 2.1 使用字节数组读取\n           // read(bytes[]) --- 一次读取 byte[] 数组大小的字节数据\n           // 返回值：读取到字节数组的字节数,或者-1\n   \n           // 这个 一般是 1024 的整数倍   --- 1M\n           byte[] bytes = new byte[1024 * 1024];\n           int len;\n           // 每次读取，都会覆盖byte数组里面的值；但是最后读不满数组的时候，只能覆盖一部分，后面的数据是上一轮读取的数据\n           // 所以 要借助 len 读取到byte数组的 字节长度 来 读取内容\n           while ((len = fis.read(bytes)) != -1) {\n               // 将数据输出\n               fos.write(bytes, 0, len);\n           }\n       } catch (IOException e) {\n           e.printStackTrace();\n       } finally {\n           // 3. 关闭流  当 流不是null的时候在 关闭\n           // 规则： 先开的流最后关闭\n   \n           // 3.1 关闭输出流\n           if (fos != null) {\n               try {\n                   fos.close();\n               } catch (IOException e) {\n                   e.printStackTrace();\n               }\n           }\n           // 3.2 关闭输入流\n           if (fis != null) {\n               try {\n                   fis.close();\n               } catch (IOException e) {\n                   e.printStackTrace();\n               }\n           }\n           long end = System.currentTimeMillis();\n           System.out.println("copy花费时间：" + (end - start) + "ms");\n       }\n   }\n   \n\n * 资源自动释放\n   \n   /**\n    * 使用try-catch-finally 让JVM自动关闭流\n    * 语法糖\n    *\n    * 在try的括号里创建流,就不用显示的去关闭流了；jvm在编译的时候会自动补上finally中关闭流的代码\n    */\n   private static void tryFinallyEasy() {\n       // 开始 时间\n       long start = System.currentTimeMillis();\n   \n       // 1. 创建输入流对象 和 输出流对象\n       try(\n               FileInputStream fis = new FileInputStream("D:\\\\tmp\\\\1.mp4");\n               FileOutputStream fos = new FileOutputStream("2.mp4")\n       ) {\n   \n           // 2 使用字节数组读取\n           // read(bytes[]) --- 一次读取 byte[] 数组大小的字节数据\n           // 返回值：读取到字节数组的字节数,或者-1\n   \n           // 这个 一般是 1024 的整数倍   --- 1M\n           byte[] bytes = new byte[1024 * 1024];\n           int len;\n           // 每次读取，都会覆盖byte数组里面的值；但是最后读不满数组的时候，只能覆盖一部分，后面的数据是上一轮读取的数据\n           // 所以 要借助 len 读取到byte数组的 字节长度 来 读取内容\n           while ((len = fis.read(bytes)) != -1) {\n               // 将数据输出\n               fos.write(bytes, 0, len);\n           }\n       } catch (IOException e) {\n           e.printStackTrace();\n       }\n   \n       long end = System.currentTimeMillis();\n       System.out.println("copy花费时间：" + (end - start) + "ms");\n   }\n   ',normalizedContent:'# 字节流\n\n\n# 一切皆为字节\n\n一切文件数据(文本、图片、视频等)在存储时，都是以二进制数字的形式保存，都一个一个的字节，那么传输时一样如此。所以，字节流可以传输任意文件数据。在操作流的时候，我们要时刻明确，无论使用什么样的流对象，底层传输的始终为二进制数据****。\n\n\n# 字节输出流【outputstream】\n\njava.io.outputstream抽象类是表示字节输出流的所有类的超类，将指定的字节信息写出到目的地。它定义了字节输出流的基本共性功能方法。\n\n * public void close() ：关闭此输出流并释放与此流相关联的任何系统资源。\n * public void flush() ：刷新此输出流并强制任何缓冲的输出字节被写出。\n * public void write(byte[] b)：将 b.length字节从指定的字节数组写入此输出流。\n * public void write(byte[] b, int off, int len) ：从指定的字节数组写入 len字节，从偏移量 off开始输出到此输出流。\n * public abstract void write(int b) ：将指定的字节输出流。\n\n> 小贴士：\n> \n> close方法，当完成流的操作时，必须调用此方法，释放系统资源。\n\n\n# fileoutputstream类\n\noutputstream有很多子类，我们从最简单的一个子类开始。\n\njava.io.fileoutputstream类是文件输出流，用于将数据写出到文件。\n\n\n# 构造方法\n\n * public fileoutputstream(file file)：创建文件输出流以写入由指定的 file对象表示的文件，覆盖写\n\n * public fileoutputstream(file file, boolean append)：创建文件输出流以写入由指定的 file对象表示的文件\n   \n   * 不带 append 参数，默认是false，为覆盖写\n   * 带append参数，设为true为追加写\n\n * public fileoutputstream(string name)： 创建文件输出流以指定的名称写入文件，覆盖写\n\n * public fileoutputstream(string name, boolean append)： 创建文件输出流以指定的名称写入文件,true-追加写\n\n当你创建一个流对象时，必须传入一个文件路径。**该路径下，如果没有这个文件，会创建该文件。**构造举例，代码如下：\n\npublic class fileoutputstreamconstructor throws ioexception {\n    public static void main(string[] args) {\n   \t \t// 使用file对象创建流对象\n        file file = new file("a.txt");\n        fileoutputstream fos = new fileoutputstream(file);\n      \n        // 使用文件名称创建流对象\n        fileoutputstream fos = new fileoutputstream("b.txt");\n    }\n}\n\n\n\n# 写出字节数据\n\n 1. 写出字节：write(int b) 方法，每次可以写出一个字节数据，代码使用演示：\n\npublic class foswrite {\n    public static void main(string[] args) throws ioexception {\n        // 使用文件名称创建流对象\n        fileoutputstream fos = new fileoutputstream("fos.txt");     \n      \t// 写出数据\n        \n      \tfos.write(97); // 写出第1个字节 -a\n      \tfos.write(98); // 写出第2个字节 -b\n      \tfos.write(99); // 写出第3个字节 -c\n        fos.write(\'b\');// 写出第4个字节 -b\n      \t// 关闭资源\n        fos.close();\n    }\n}\n文件内容为：\nabcb\n\n\n> 小贴士：\n> \n>  1. 虽然参数为int类型四个字节，但是只会保留一个字节的信息写出。ascii码值\n>  2. 流操作完毕后，必须释放系统资源，调用close方法，千万记得。\n\n 2. 写出字节数组：write(byte[] b)，每次可以写出数组中的数据，代码使用演示：\n\npublic class foswrite {\n    public static void main(string[] args) throws ioexception {\n        // 使用文件名称创建流对象\n        fileoutputstream fos = new fileoutputstream("fos.txt");     \n      \t// 字符串转换为字节数组\n      \tbyte[] b = "黑马程序员".getbytes();\n      \t// 写出字节数组数据\n      \tfos.write(b);\n      \t// 关闭资源\n        fos.close();\n    }\n}\n文件内容为：\n黑马程序员\n\n\n 3. 写出指定长度字节数组：write(byte[] b, int off, int len) ,每次写出从off索引开始，len个字节，代码使用演示：\n\npublic class foswrite {\n    public static void main(string[] args) throws ioexception {\n        // 使用文件名称创建流对象\n        fileoutputstream fos = new fileoutputstream("fos.txt");     \n      \t// 字符串转换为字节数组\n      \tbyte[] b = "abcde".getbytes();\n\t\t// 写出从索引2开始，2个字节。索引2是c，两个字节，也就是cd。\n        fos.write(b,2,2);\n      \t// 关闭资源\n        fos.close();\n    }\n}\n文件内容为：\ncd\n\n\n\n# 数据追加续写\n\n经过以上的演示，每次程序运行，创建输出流对象，都会清空目标文件中的数据。如何保留目标文件中数据，还能继续添加新数据呢？\n\n * public fileoutputstream(file file, boolean append)： 创建文件输出流以写入由指定的 file对象表示的文件。\n * public fileoutputstream(string name, boolean append)： 创建文件输出流以指定的名称写入文件。\n\n这两个构造方法，参数中都需要传入一个boolean类型的值，true 表示追加数据，false 表示清空原有数据。这样创建的输出流对象，就可以指定是否追加续写了，代码使用演示：\n\npublic class foswrite {\n    public static void main(string[] args) throws ioexception {\n        // 使用文件名称创建流对象\n        fileoutputstream fos = new fileoutputstream("fos.txt"，true);     \n      \t// 字符串转换为字节数组\n      \tbyte[] b = "abcde".getbytes();\n\t\t// 写出从索引2开始，2个字节。索引2是c，两个字节，也就是cd。\n        fos.write(b);\n      \t// 关闭资源\n        fos.close();\n    }\n}\n文件操作前：cd\n文件操作后：cdabcde\n\n\n\n# 写出换行\n\n *      换行符\n        *          windows: `\\r\\n `\n                   *          回车`\\r`-将光标放到这一行的最开头\n                   *          换行`\\n`-将光标换到下一行\n        *          linux: `\\n`\n        *          mac: `\\r`\n   \n\n *      细节\n        *          虽然 window系统是 \\r\\n  但是 **写一个\\r 或者 \\n 都可以**\n        *          java在底层会进行补全\n   \n\n *      **建议写全**\n   \n\nwindows系统里，换行符号是\\r\\n 。把\n\n以指定是否追加续写了，代码使用演示：\n\npublic class foswrite {\n    public static void main(string[] args) throws ioexception {\n        // 使用文件名称创建流对象\n        fileoutputstream fos = new fileoutputstream("fos.txt");  \n      \t// 定义字节数组\n      \tbyte[] words = {97,98,99,100,101};\n      \t// 遍历数组\n        for (int i = 0; i < words.length; i++) {\n          \t// 写出一个字节\n            fos.write(words[i]);\n          \t// 写出一个换行, 换行符号转成数组写出\n            fos.write("\\r\\n".getbytes());\n        }\n      \t// 关闭资源\n        fos.close();\n    }\n}\n\n输出结果：\na\nb\nc\nd\ne\n\n\n>  * 回车符\\r和换行符\\n ：\n>    * 回车符：回到一行的开头（return）。\n>    * 换行符：下一行（newline）。\n>  * 系统中的换行：\n>    * windows系统里，每行结尾是 回车+换行 ，即\\r\\n；\n>    * unix系统里，每行结尾只有 换行 ，即\\n；\n>    * mac系统里，每行结尾是 回车 ，即\\r。从 mac os x开始与linux统一。\n\n\n# 字节输入流【inputstream】\n\njava.io.inputstream抽象类是表示字节输入流的所有类的超类，可以读取字节信息到内存中。它定义了字节输入流的基本共性功能方法。\n\n * public void close() ：关闭此输入流并释放与此流相关联的任何系统资源。\n * public abstract int read()： 从输入流读取数据的下一个字节。\n * public int read(byte[] b)： 从输入流中读取一些字节数，并将它们存储到字节数组 b中 。\n\n> 小贴士：\n> \n> close方法，当完成流的操作时，必须调用此方法，释放系统资源。\n\n\n# fileinputstream类\n\njava.io.fileinputstream类是文件输入流，从文件中读取字节。\n\n\n# 构造方法\n\n * fileinputstream(file file)： 通过打开与实际文件的连接来创建一个 fileinputstream ，该文件由文件系统中的 file对象 file命名。\n * fileinputstream(string name)： 通过打开与实际文件的连接来创建一个 fileinputstream ，该文件由文件系统中的路径名 name命名。\n\n当你创建一个流对象时，必须传入一个文件路径。该路径下，如果没有该文件,会抛出filenotfoundexception 。\n\n * 构造举例，代码如下：\n\npublic class fileinputstreamconstructor throws ioexception{\n    public static void main(string[] args) {\n   \t \t// 使用file对象创建流对象\n        file file = new file("a.txt");\n        fileinputstream fos = new fileinputstream(file);\n      \n        // 使用文件名称创建流对象\n        fileinputstream fos = new fileinputstream("b.txt");\n    }\n}\n\n\n\n# 读取字节数据\n\n * 1.创建输入流对象\n   \n   * 如果文件不存在，就会直接报错\n\n * 2.读取数据\n   \n   * 一次读取一个字节，读出来的数据是在ascii码上对应的数字\n   \n   * 读到文件尾了,read方法会返回-1\n   \n   * 当文件中有“-1”时，也是先读-，在读1；是无法读取到-1的\n\n * 3.关闭流\n\n 1. 读取字节：read方法，每次可以读取一个字节的数据，提升为int类型，读取到文件末尾，返回-1，代码使用演示：\n\npublic class fisread {\n    public static void main(string[] args) throws ioexception{\n      \t// 使用文件名称创建流对象\n       \tfileinputstream fis = new fileinputstream("read.txt");\n      \t// 读取数据，返回一个字节\n        int read = fis.read();\n        system.out.println((char) read);\n        read = fis.read();\n        system.out.println((char) read);\n        read = fis.read();\n        system.out.println((char) read);\n        read = fis.read();\n        system.out.println((char) read);\n        read = fis.read();\n        system.out.println((char) read);\n      \t// 读取到末尾,返回-1\n       \tread = fis.read();\n        system.out.println( read);\n\t\t// 关闭资源\n        fis.close();\n    }\n}\n输出结果：\na\nb\nc\nd\ne\n-1\n\n\n循环改进读取方式，代码使用演示：\n\npublic class fisread {\n    public static void main(string[] args) throws ioexception{\n      \t// 使用文件名称创建流对象\n       \tfileinputstream fis = new fileinputstream("read.txt");\n      \t// 定义变量，保存数据\n        int b;\n        // 循环读取 --- 优雅\n        while ((b = fis.read())!=-1) {\n            system.out.println((char)b);\n        }\n        \n        // 2.2  以前的写法，，不优雅---- 不推荐\n        int b;\n        while (true) {\n            b = fis.read();\n            if (b ** -1) {\n                break;\n            }\n            system.out.print((char) b);\n        }\n        \n        \n\t\t// 关闭资源\n        fis.close();\n    }\n}\n输出结果：\na\nb\nc\nd\ne\n\n\n> 小贴士：\n> \n>  1. 虽然读取了一个字节，但是会自动提升为int类型。\n>  2. 流操作完毕后，必须释放系统资源，调用close方法，千万记得。\n\n 2. 使用字节数组读取：read(byte[] b)，每次读取b的长度个字节到数组中，返回读取到的有效字节个数，读取到末尾时，返回-1 ，代码使用演示：\n\npublic class fisread {\n    public static void main(string[] args) throws ioexception{\n      \t// 使用文件名称创建流对象.\n       \tfileinputstream fis = new fileinputstream("read.txt"); // 文件中为abcde\n      \t// 定义变量，作为有效个数\n        int len;\n        // 定义字节数组，作为装字节数据的容器   \n        byte[] b = new byte[2];\n        // 循环读取\n        while (( len= fis.read(b))!=-1) {\n           \t// 每次读取后,把数组变成字符串打印\n            system.out.println(new string(b));\n        }\n\t\t// 关闭资源\n        fis.close();\n    }\n}\n\n输出结果：\nab\ncd\ned\n\n\n错误数据d，是由于最后一次读取时，只读取一个字节e，数组中，上次读取的数据没有被完全替换，所以要通过len ，获取有效的字节，代码使用演示：\n\npublic class fisread {\n    public static void main(string[] args) throws ioexception{\n      \t// 使用文件名称创建流对象.\n       \tfileinputstream fis = new fileinputstream("read.txt"); // 文件中为abcde\n      \t// 定义变量，作为有效个数\n        int len ；\n        // 定义字节数组，作为装字节数据的容器   \n        byte[] b = new byte[2];\n        // 循环读取\n        while (( len= fis.read(b))!=-1) {\n           \t// 每次读取后,把数组的有效字节部分，变成字符串打印\n            system.out.println(new string(b，0，len));//  len 每次读取的有效字节个数\n        }\n\t\t// 关闭资源\n        fis.close();\n    }\n}\n\n输出结果：\nab\ncd\ne\n\n\n> 小贴士：\n> \n> 使用数组读取，每次读取多个字节，减少了系统间的io操作次数，从而提高了读写的效率，建议开发中使用。\n\n\n# 字节流练习：图片复制\n\n\n# 复制原理图解\n\n\n\n\n# 案例实现\n\n复制图片文件，代码使用演示：\n\npublic class copy {\n    public static void main(string[] args) throws ioexception {\n        // 1.创建流对象\n        // 1.1 指定数据源\n        fileinputstream fis = new fileinputstream("d:\\\\test.jpg");\n        // 1.2 指定目的地\n        fileoutputstream fos = new fileoutputstream("test_copy.jpg");\n\n        // 2.读写数据\n        // 2.1 定义数组\n        byte[] b = new byte[1024];\n        // 2.2 定义长度\n        int len;\n        // 2.3 循环读取\n        while ((len = fis.read(b))!=-1) {\n            // 2.4 写出数据\n            fos.write(b, 0 , len);\n        }\n\n        // 3.关闭资源\n        fos.close();\n        fis.close();\n    }\n}\n\n\n> 小贴士：\n> \n> 流的关闭原则：先开后关，后开先关。\n\n\n# try-catch-finally\n\n自动释放资源———— 实现了autocloseable接口的资源可以自动释放\n\n\n\n * 显示释放资源\n   \n   /**\n    * 使用try-catch-finally 手动关闭流\n    */\n   private static void tryfinally() {\n       // 1. 创建输入流对象 和 输出流对象\n       fileinputstream fis = null;\n       fileoutputstream fos = null;\n       long start = 0;\n   \n       try {\n           fis = new fileinputstream("d:\\\\tmp\\\\1.mp4");\n           fos = new fileoutputstream("2.mp4");\n   \n           // 当前 时间\n           start = system.currenttimemillis();\n   \n           // 2.1 使用字节数组读取\n           // read(bytes[]) --- 一次读取 byte[] 数组大小的字节数据\n           // 返回值：读取到字节数组的字节数,或者-1\n   \n           // 这个 一般是 1024 的整数倍   --- 1m\n           byte[] bytes = new byte[1024 * 1024];\n           int len;\n           // 每次读取，都会覆盖byte数组里面的值；但是最后读不满数组的时候，只能覆盖一部分，后面的数据是上一轮读取的数据\n           // 所以 要借助 len 读取到byte数组的 字节长度 来 读取内容\n           while ((len = fis.read(bytes)) != -1) {\n               // 将数据输出\n               fos.write(bytes, 0, len);\n           }\n       } catch (ioexception e) {\n           e.printstacktrace();\n       } finally {\n           // 3. 关闭流  当 流不是null的时候在 关闭\n           // 规则： 先开的流最后关闭\n   \n           // 3.1 关闭输出流\n           if (fos != null) {\n               try {\n                   fos.close();\n               } catch (ioexception e) {\n                   e.printstacktrace();\n               }\n           }\n           // 3.2 关闭输入流\n           if (fis != null) {\n               try {\n                   fis.close();\n               } catch (ioexception e) {\n                   e.printstacktrace();\n               }\n           }\n           long end = system.currenttimemillis();\n           system.out.println("copy花费时间：" + (end - start) + "ms");\n       }\n   }\n   \n\n * 资源自动释放\n   \n   /**\n    * 使用try-catch-finally 让jvm自动关闭流\n    * 语法糖\n    *\n    * 在try的括号里创建流,就不用显示的去关闭流了；jvm在编译的时候会自动补上finally中关闭流的代码\n    */\n   private static void tryfinallyeasy() {\n       // 开始 时间\n       long start = system.currenttimemillis();\n   \n       // 1. 创建输入流对象 和 输出流对象\n       try(\n               fileinputstream fis = new fileinputstream("d:\\\\tmp\\\\1.mp4");\n               fileoutputstream fos = new fileoutputstream("2.mp4")\n       ) {\n   \n           // 2 使用字节数组读取\n           // read(bytes[]) --- 一次读取 byte[] 数组大小的字节数据\n           // 返回值：读取到字节数组的字节数,或者-1\n   \n           // 这个 一般是 1024 的整数倍   --- 1m\n           byte[] bytes = new byte[1024 * 1024];\n           int len;\n           // 每次读取，都会覆盖byte数组里面的值；但是最后读不满数组的时候，只能覆盖一部分，后面的数据是上一轮读取的数据\n           // 所以 要借助 len 读取到byte数组的 字节长度 来 读取内容\n           while ((len = fis.read(bytes)) != -1) {\n               // 将数据输出\n               fos.write(bytes, 0, len);\n           }\n       } catch (ioexception e) {\n           e.printstacktrace();\n       }\n   \n       long end = system.currenttimemillis();\n       system.out.println("copy花费时间：" + (end - start) + "ms");\n   }\n   ',charsets:{cjk:!0}},{title:"字符流",frontmatter:{autoSort:95,title:"字符流",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/f0c1fd/",categories:["后端","Java","流"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/60.%E6%B5%81/30.%E5%AD%97%E7%AC%A6%E6%B5%81.html",relativePath:"01.后端/10.Java/60.流/30.字符流.md",key:"v-4515ebe1",path:"/pages/f0c1fd/",headers:[{level:2,title:"字符集",slug:"字符集",normalizedTitle:"字符集",charIndex:10},{level:3,title:"ASCII",slug:"ascii",normalizedTitle:"ascii",charIndex:18},{level:3,title:"GBK",slug:"gbk",normalizedTitle:"gbk",charIndex:139},{level:3,title:"Unicode",slug:"unicode",normalizedTitle:"unicode",charIndex:516},{level:3,title:"乱码",slug:"乱码",normalizedTitle:"乱码",charIndex:801},{level:2,title:"字符输入流【Reader】",slug:"字符输入流【reader】",normalizedTitle:"字符输入流【reader】",charIndex:1030},{level:2,title:"字符输入流-底层原理",slug:"字符输入流-底层原理",normalizedTitle:"字符输入流-底层原理",charIndex:1263},{level:2,title:"FileReader类",slug:"filereader类",normalizedTitle:"filereader类",charIndex:3616},{level:3,title:"构造方法",slug:"构造方法",normalizedTitle:"构造方法",charIndex:3804},{level:3,title:"读取字符数据",slug:"读取字符数据",normalizedTitle:"读取字符数据",charIndex:4293},{level:2,title:"字符输出流【Writer】",slug:"字符输出流【writer】",normalizedTitle:"字符输出流【writer】",charIndex:5922},{level:2,title:"字符输出流-底层原理",slug:"字符输出流-底层原理",normalizedTitle:"字符输出流-底层原理",charIndex:6325},{level:2,title:"FileWriter类",slug:"filewriter类",normalizedTitle:"filewriter类",charIndex:7220},{level:3,title:"构造方法",slug:"构造方法-2",normalizedTitle:"构造方法",charIndex:3804},{level:3,title:"基本写出数据",slug:"基本写出数据",normalizedTitle:"基本写出数据",charIndex:7781},{level:3,title:"关闭和刷新",slug:"关闭和刷新",normalizedTitle:"关闭和刷新",charIndex:8400},{level:3,title:"写出其他数据",slug:"写出其他数据",normalizedTitle:"写出其他数据",charIndex:9109}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"字符集 ASCII GBK Unicode 乱码 字符输入流【Reader】 字符输入流-底层原理 FileReader类 构造方法 读取字符数据 字符输出流【Writer】 字符输出流-底层原理 FileWriter类 构造方法 基本写出数据 关闭和刷新 写出其他数据",content:'# 字符流\n\n\n# 字符集\n\n\n# ASCII\n\n\n\n * 这个字符集只能存储 英文和数字,以及一些常用字符；不能存储中文\n * 最大到127，一个字节就可以存储；\n * 不足8位的，前面补0；计算机存储的最小单位是字节****，就是8个bit，需要8位才能存储。\n\n\n\n\n# GBK\n\n * windows 系统默认就是 GBK 编码\n\n\n\n * GBK 编码 完全兼容 ASCII码\n * 英文在GBK字符集下的编码方式与ASCII字符集是一样的；都是补0\n\n\n\n * GBK 存储中文的时候\n   * 汉字要用两个字节存储\n     * --- 汉字太多了，一个字节存不下，三个字节太浪费了，两个刚刚好\n   * 高位字节二进制一定以1开头，转换成十进制之后是一个负数\n     * 就是为了和英文区分开；英文都是以0开头；中文都是以1开头\n\n\n\n * GBK字符集下 汉字的编码与解码\n   * 高位 补1\n\n\n\n * 中文与英文在GBK字符集下的对比\n\n\n\n * 编码练习 --- 区分中英文\n\n\n\n * 总结\n   * 中文占两个字节，二进制高位字节的第一位是1\n   * 英文占一个字节，二进制第一位是0\n\n\n\n\n# Unicode\n\n * Unicode字符集的编码方式\n   \n   * UTF-8\n     \n     * 1--4个可变字节\n   \n   * UTF-16\n     \n     * 用2--4个字节保存\n   \n   * UTF-32\n     \n     * 固定使用4个字节保存\n     \n     \n\n * UTF-8\n   \n   \n\n * 练习\n   \n   \n\n * 总结\n   \n   1. 一个英文 占 一个字节，二进制第一位是0，转换成十进制是正数\n   2. 一个中文 占 三个字节，二进制第一位是1，第一个字节转换成十进制是负数\n\n\n# 乱码\n\n * 读取数据时未读完整个汉字\n   \n   \n\n * 编码和解码的方式不统一\n   \n   \n\n * 如何解决？即如何不产生乱码\n   \n   * 不要用字节流读取文本文件\n   * 编码，解码使用一个码表，同一种编码方式\n\n当使用字节流读取文本文件时，可能会有一个小问题。就是遇到中文字符时，可能不会显示完整的字符，那是因为一个中文字符可能占用多个字节存储。所以Java提供一些字符流类，以字符为单位读写数据，专门用于处理文本文件。\n\n\n\n\n# 字符输入流【Reader】\n\njava.io.Reader抽象类是表示用于读取字符流的所有类的超类，可以读取字符信息到内存中。它定义了字符输入流的基本共性功能方法。\n\n * public void close() ：关闭此流并释放与此流相关联的任何系统资源。\n * public int read()： 从输入流读取一个字符。\n * public int read(char[] cbuf)： 从输入流中读取一些字符，并将它们存储到字符数组 cbuf中 。\n\n\n# 字符输入流-底层原理\n\n * 字符输入流在创建的时候，会创建一个缓冲区(在内存中)，大小为byte[8192]\n\n * 第一次调用read()方法时，会从文件读取 8192个字节的内容到缓存区\n\n * 后续读取数据的时候，会先在缓存区读，当缓冲区读到头时，会重新从文件加载数据到缓冲区。\n   \n   \n   \n   * 输入流与输出流交错\n   \n   /**\n    * @ClassName FileWriter\n    * @Date 2023/3/3 15:24\n    * @Author diane\n    * @Description\n    *      字符输入流，，在关联文件的同时，会创建一个缓冲区，大小为 bytes[8192]\n    *      字符输出流，，在创建输出流的时候，如果不指定append为true，会清空原文件内容\n    * @Version 1.0\n    */\n   public class CharStreamDemo {\n       public static void main(String[] args) throws IOException {\n           // 先读取一次，，， 会将内容装入缓冲区\n           FileReader fr = new FileReader("D:\\\\tmp\\\\1.txt");\n           fr.read();\n   \n           // 创建输出流对象，会清空原文件内容\n           FileWriter fw = new FileWriter("D:\\\\tmp\\\\1.txt");\n   \n           // 创建输出流对象，虽然会清空文件，，但是输入流fr还可以从缓冲区读取到数据\n           int ch;\n           while ((ch = fr.read()) != -1) {\n               System.out.print((char) ch);\n           }\n   \n   \n           fw.close();\n           fr.close();\n   \n       }\n   }\n   \n   \n\n * 输入流 缓冲区 详解\n   \n   * 创建8194个字符的文件 a(8192)bc\n   \n   * 创建字符输入流，创建一个byte[8192]，limit字段为0\n     \n     \n   \n   * 第一次read,缓冲区载入8192个a，limit字段为 8192\n     \n     \n   \n   * 循环结束后，第二次read，缓冲区中的数据已经读完了，需要重新读取文件数据到缓冲区。\n     \n     * 覆盖写，不会清空原来的数据。 缓冲区:bca(8190)\n     \n     * limit字段，为2，即限制读取2个，就结束了\n       \n       \n     \n     * 不用担心，采用覆盖写的方式，当文件本该结束的时候，再读到\'a\'\n   \n   * 在去read，就会输出-1\n   \n   /**\n    * @ClassName Demo\n    * @Date 2023/3/3 15:59\n    * @Author diane\n    * @Description 读取数据超过 8192 的文件情况\n    * @Version 1.0\n    */\n   public class Demo {\n     public static void main(String[] args) throws IOException {\n   \n       // 创建文件\n       FileWriter fw = new FileWriter("D:\\\\tmp\\\\1.txt");\n   \n       for (int i = 0; i < 8192; i++) {\n         fw.write(\'a\');\n       }\n       fw.write(\'b\');\n       fw.write(\'c\');\n   \n       fw.close();\n   \n   \n       // 读取文件\n       FileReader fr = new FileReader("D:\\\\tmp\\\\1.txt");\n       for (int i = 0; i < 8192; i++) {\n         System.out.println((char) fr.read());\n       }\n       // 缓冲区中的 数据读完了， 在次将文件数据读入缓冲区\n       // 此时文件只剩下 bc\n       // 读入缓冲区的时候，是覆盖写，不会删除原来的数据 ---- 缓冲区：bca(8190)\n       System.out.println((char) fr.read()); // b\n       System.out.println((char) fr.read()); // c\n   \n       // 这里读的时候 不会读取缓冲区的下一个字符 a\n       // 因为文件真实数据已经结束了\n       // 会有一个字段 limit = 2 来限制 可以在缓冲区中读取多少个字符\n       System.out.println(fr.read());// -1\n     }\n   }\n   \n\n\n# FileReader类\n\njava.io.FileReader类是读取字符文件的便利类。构造时使用系统默认的字符编码和默认字节缓冲区。\n\n> 小贴士：\n> \n>  1. 字符编码：字节与字符的对应规则。Windows系统的中文编码默认是GBK编码表。\n>     \n>     idea中UTF-8\n> \n>  2. 字节缓冲区：一个字节数组，用来临时存储字节数据。\n\n\n# 构造方法\n\n * FileReader(File file)： 创建一个新的 FileReader ，给定要读取的File对象。\n * FileReader(String fileName)： 创建一个新的 FileReader ，给定要读取的文件的名称。\n\n当你创建一个流对象时，必须传入一个文件路径。类似于FileInputStream 。\n\n * 构造举例，代码如下：\n\npublic class FileReaderConstructor throws IOException{\n    public static void main(String[] args) {\n   \t \t// 使用File对象创建流对象\n        File file = new File("a.txt");\n        FileReader fr = new FileReader(file);\n      \n        // 使用文件名称创建流对象\n        FileReader fr = new FileReader("b.txt");\n    }\n}\n\n\n\n# 读取字符数据\n\n 1. 读取字符：read方法，每次可以读取一个字符的数据，提升为int类型，读取到文件末尾，返回-1，循环读取，代码使用演示：\n    * 英文读取一个字节，汉字会读取多个字节-根据不同的编码方式来确定(UTF-8 三个字节；GBK-两个字节)\n    * 方法的返回值 是 该字符在 字符集上的数字，为10进制；要想转换成字符的话，需要用char 强转\n\npublic class FRRead {\n    public static void main(String[] args) throws IOException {\n      \t// 使用文件名称创建流对象\n       \tFileReader fr = new FileReader("read.txt");\n      \t// 定义变量，保存数据\n        int b ；\n        // 循环读取\n        while ((b = fr.read())!=-1) {\n            System.out.println((char)b);\n        }\n\t\t// 关闭资源\n        fr.close();\n    }\n}\n输出结果：\n黑\n马\n程\n序\n员\n\n\n> 小贴士：虽然读取了一个字符，但是会自动提升为int类型。--- 字符集上对应的十进制数字\n\n 2. 使用字符数组读取：read(char[] cbuf)，每次读取b的长度个字符到数组中，返回读取到的有效字符个数，读取到末尾时，返回-1 ，代码使用演示：\n    * read(chars) 读取数据，解码，强转，放到char数组中\n\npublic class FRRead {\n    public static void main(String[] args) throws IOException {\n      \t// 使用文件名称创建流对象\n       \tFileReader fr = new FileReader("read.txt");\n      \t// 定义变量，保存有效字符个数\n        int len ；\n        // 定义字符数组，作为装字符数据的容器\n         char[] cbuf = new char[2];\n        // 循环读取\n        while ((len = fr.read(cbuf))!=-1) {\n            System.out.println(new String(cbuf));\n        }\n\t\t// 关闭资源\n        fr.close();\n    }\n}\n输出结果：\n黑马\n程序\n员序\n\n\n获取有效的字符改进，代码使用演示：\n\npublic class FISRead {\n    public static void main(String[] args) throws IOException {\n      \t// 使用文件名称创建流对象\n       \tFileReader fr = new FileReader("read.txt");\n      \t// 定义变量，保存有效字符个数\n        int len ；\n        // 定义字符数组，作为装字符数据的容器\n        char[] cbuf = new char[2];\n        // 循环读取\n        while ((len = fr.read(cbuf))!=-1) {\n            System.out.println(new String(cbuf,0,len));\n        }\n    \t// 关闭资源\n        fr.close();\n    }\n}\n\n输出结果：\n黑马\n程序\n员\n\n\n\n# 字符输出流【Writer】\n\njava.io.Writer抽象类是表示用于写出字符流的所有类的超类，将指定的字符信息写出到目的地。它定义了字节输出流的基本共性功能方法。\n\n * void write(int c) 写入单个字符。\n * void write(char[] cbuf)写入字符数组。\n * abstract void write(char[] cbuf, int off, int len)写入字符数组的某一部分,off数组的开始索引,len写的字符个数。\n * void write(String str)写入字符串。\n * void write(String str, int off, int len) 写入字符串的某一部分,off字符串的开始索引,len写的字符个数。\n * void flush()刷新该流的缓冲。\n * void close() 关闭此流，但要先刷新它。\n\n\n# 字符输出流-底层原理\n\n * 字符输出流创建的时候，会在内存中创建一个byte[8192]的缓冲区。\n\n * 输出数据的时候，会先将数据写入缓冲区，后续在写入文件。\n   \n   * 当缓冲区满了，会自动写入文件。\n   * 调用flush()方法，刷新缓冲区，会将缓存区数据写入文件\n   * 调用close()方法，在断开连接前，会先刷新缓冲区，在关闭。\n   \n   /**\n    * @ClassName Demo\n    * @Date 2023/3/3 15:59\n    * @Author diane\n    * @Description 输出缓冲区\n    *  输出数据的时候，会先将数据写入缓冲区，后续在写入文件。\n    *      当缓冲区满了，会自动写入文件。\n    *      调用flush()方法，刷新缓冲区，会将缓存区数据写入文件\n    *      调用close()方法，在断开连接前，会先刷新缓冲区，在关闭。\n    *\n    *\n    * @Version 1.0\n    */\n   public class FileWriteBufferDemo {\n     public static void main(String[] args) throws IOException {\n   \n       // 创建文件\n       FileWriter fw = new FileWriter("D:\\\\tmp\\\\1.txt");\n   \n       for (int i = 0; i < 8192; i++) {\n         fw.write(\'a\');\n       }\n       // 缓冲区满了，会将缓冲区中的数据 写入缓冲区\n       fw.write(\'b\');\n      \t// 调用flush 方法，将缓冲区数据写入文件\n       fw.flush();\n       fw.write(\'c\');\n   \n       fw.close();\n     }\n   }\n   \n\n\n# FileWriter类\n\njava.io.FileWriter类是写出字符到文件的便利类。构造时使用系统默认的字符编码和默认字节缓冲区。\n\n\n# 构造方法\n\n * FileWriter(File file)： 创建一个新的 FileWriter，给定要读取的File对象。\n * FileWriter(String fileName)： 创建一个新的 FileWriter，给定要读取的文件的名称。\n\n当你创建一个流对象时，必须传入一个文件路径，类似于FileOutputStream。\n\n * 构造举例，代码如下：\n\npublic class FileWriterConstructor {\n    public static void main(String[] args) throws IOException {\n   \t \t// 使用File对象创建流对象\n        File file = new File("a.txt");\n        FileWriter fw = new FileWriter(file);\n      \n        // 使用文件名称创建流对象\n        FileWriter fw = new FileWriter("b.txt");\n    }\n}\n\n\n\n# 基本写出数据\n\n写出字符：write(int b) 方法，每次可以写出一个字符数据，代码使用演示：\n\npublic class FWWrite {\n    public static void main(String[] args) throws IOException {\n        // 使用文件名称创建流对象\n        FileWriter fw = new FileWriter("fw.txt");     \n      \t// 写出数据\n      \tfw.write(97); // 写出第1个字符\n      \tfw.write(\'b\'); // 写出第2个字符\n      \tfw.write(\'C\'); // 写出第3个字符\n      \tfw.write(30000); // 写出第4个字符，中文编码表中30000对应一个汉字。\n      \n      \t/*\n        【注意】关闭资源时,与FileOutputStream不同。\n      \t 如果不关闭,数据只是保存到缓冲区，并未保存到文件。\n        */\n        // fw.close();\n    }\n}\n输出结果：\nabC田\n\n\n> 小贴士：\n> \n>  1. 虽然参数为int类型四个字节，但是只会保留一个字符的信息写出。\n>  2. 未调用close方法，数据只是保存到了缓冲区，并未写出到文件中。\n\n\n# 关闭和刷新\n\n因为内置缓冲区的原因，如果不关闭输出流，无法写出字符到文件中。但是关闭的流对象，是无法继续写出数据的。如果我们既想写出数据，又想继续使用流，就需要flush 方法了。\n\n * flush ：刷新缓冲区，流对象可以继续使用。\n * close:先刷新缓冲区，然后通知系统释放资源。流对象不可以再被使用了。\n\n代码使用演示：\n\npublic class FWWrite {\n    public static void main(String[] args) throws IOException {\n        // 使用文件名称创建流对象\n        FileWriter fw = new FileWriter("fw.txt");\n        // 写出数据，通过flush\n        fw.write(\'刷\'); // 写出第1个字符\n        fw.flush();\n        fw.write(\'新\'); // 继续写出第2个字符，写出成功\n        fw.flush();\n      \n      \t// 写出数据，通过close\n        fw.write(\'关\'); // 写出第1个字符\n        fw.close();\n        fw.write(\'闭\'); // 继续写出第2个字符,【报错】java.io.IOException: Stream closed\n        fw.close();\n    }\n}\n\n\n> 小贴士：即便是flush方法写出了数据，操作的最后还是要调用close方法，释放系统资源。\n\n\n# 写出其他数据\n\n 1. 写出字符数组 ：write(char[] cbuf) 和 write(char[] cbuf, int off, int len) ，每次可以写出字符数组中的数据，用法类似FileOutputStream，代码使用演示：\n\npublic class FWWrite {\n    public static void main(String[] args) throws IOException {\n        // 使用文件名称创建流对象\n        FileWriter fw = new FileWriter("fw.txt");     \n      \t// 字符串转换为字节数组\n      \tchar[] chars = "黑马程序员".toCharArray();\n      \n      \t// 写出字符数组\n      \tfw.write(chars); // 黑马程序员\n        \n\t\t// 写出从索引2开始，2个字节。索引2是\'程\'，两个字节，也就是\'程序\'。\n        fw.write(b,2,2); // 程序\n      \n      \t// 关闭资源\n        fos.close();\n    }\n}\n\n\n 2. 写出字符串：write(String str) 和 write(String str, int off, int len) ，每次可以写出字符串中的数据，更为方便，代码使用演示：\n\npublic class FWWrite {\n    public static void main(String[] args) throws IOException {\n        // 使用文件名称创建流对象\n        FileWriter fw = new FileWriter("fw.txt");     \n      \t// 字符串\n      \tString msg = "黑马程序员";\n      \n      \t// 写出字符数组\n      \tfw.write(msg); //黑马程序员\n      \n\t\t// 写出从索引2开始，2个字节。索引2是\'程\'，两个字节，也就是\'程序\'。\n        fw.write(msg,2,2);\t// 程序\n      \t\n        // 关闭资源\n        fos.close();\n    }\n}\n\n\n 3. 续写和换行：操作类似于FileOutputStream。\n\npublic class FWWrite {\n    public static void main(String[] args) throws IOException {\n        // 使用文件名称创建流对象，可以续写数据\n        FileWriter fw = new FileWriter("fw.txt"，true);     \n      \t// 写出字符串\n        fw.write("黑马");\n      \t// 写出换行\n      \tfw.write("\\r\\n");\n      \t// 写出字符串\n  \t\tfw.write("程序员");\n      \t// 关闭资源\n        fw.close();\n    }\n}\n输出结果:\n黑马\n程序员\n\n\n> 小贴士：字符流，只能操作文本文件，不能操作图片，视频等非文本文件。\n> \n> 当我们单纯读或者写文本文件时 使用字符流 其他情况使用字节流',normalizedContent:'# 字符流\n\n\n# 字符集\n\n\n# ascii\n\n\n\n * 这个字符集只能存储 英文和数字,以及一些常用字符；不能存储中文\n * 最大到127，一个字节就可以存储；\n * 不足8位的，前面补0；计算机存储的最小单位是字节****，就是8个bit，需要8位才能存储。\n\n\n\n\n# gbk\n\n * windows 系统默认就是 gbk 编码\n\n\n\n * gbk 编码 完全兼容 ascii码\n * 英文在gbk字符集下的编码方式与ascii字符集是一样的；都是补0\n\n\n\n * gbk 存储中文的时候\n   * 汉字要用两个字节存储\n     * --- 汉字太多了，一个字节存不下，三个字节太浪费了，两个刚刚好\n   * 高位字节二进制一定以1开头，转换成十进制之后是一个负数\n     * 就是为了和英文区分开；英文都是以0开头；中文都是以1开头\n\n\n\n * gbk字符集下 汉字的编码与解码\n   * 高位 补1\n\n\n\n * 中文与英文在gbk字符集下的对比\n\n\n\n * 编码练习 --- 区分中英文\n\n\n\n * 总结\n   * 中文占两个字节，二进制高位字节的第一位是1\n   * 英文占一个字节，二进制第一位是0\n\n\n\n\n# unicode\n\n * unicode字符集的编码方式\n   \n   * utf-8\n     \n     * 1--4个可变字节\n   \n   * utf-16\n     \n     * 用2--4个字节保存\n   \n   * utf-32\n     \n     * 固定使用4个字节保存\n     \n     \n\n * utf-8\n   \n   \n\n * 练习\n   \n   \n\n * 总结\n   \n   1. 一个英文 占 一个字节，二进制第一位是0，转换成十进制是正数\n   2. 一个中文 占 三个字节，二进制第一位是1，第一个字节转换成十进制是负数\n\n\n# 乱码\n\n * 读取数据时未读完整个汉字\n   \n   \n\n * 编码和解码的方式不统一\n   \n   \n\n * 如何解决？即如何不产生乱码\n   \n   * 不要用字节流读取文本文件\n   * 编码，解码使用一个码表，同一种编码方式\n\n当使用字节流读取文本文件时，可能会有一个小问题。就是遇到中文字符时，可能不会显示完整的字符，那是因为一个中文字符可能占用多个字节存储。所以java提供一些字符流类，以字符为单位读写数据，专门用于处理文本文件。\n\n\n\n\n# 字符输入流【reader】\n\njava.io.reader抽象类是表示用于读取字符流的所有类的超类，可以读取字符信息到内存中。它定义了字符输入流的基本共性功能方法。\n\n * public void close() ：关闭此流并释放与此流相关联的任何系统资源。\n * public int read()： 从输入流读取一个字符。\n * public int read(char[] cbuf)： 从输入流中读取一些字符，并将它们存储到字符数组 cbuf中 。\n\n\n# 字符输入流-底层原理\n\n * 字符输入流在创建的时候，会创建一个缓冲区(在内存中)，大小为byte[8192]\n\n * 第一次调用read()方法时，会从文件读取 8192个字节的内容到缓存区\n\n * 后续读取数据的时候，会先在缓存区读，当缓冲区读到头时，会重新从文件加载数据到缓冲区。\n   \n   \n   \n   * 输入流与输出流交错\n   \n   /**\n    * @classname filewriter\n    * @date 2023/3/3 15:24\n    * @author diane\n    * @description\n    *      字符输入流，，在关联文件的同时，会创建一个缓冲区，大小为 bytes[8192]\n    *      字符输出流，，在创建输出流的时候，如果不指定append为true，会清空原文件内容\n    * @version 1.0\n    */\n   public class charstreamdemo {\n       public static void main(string[] args) throws ioexception {\n           // 先读取一次，，， 会将内容装入缓冲区\n           filereader fr = new filereader("d:\\\\tmp\\\\1.txt");\n           fr.read();\n   \n           // 创建输出流对象，会清空原文件内容\n           filewriter fw = new filewriter("d:\\\\tmp\\\\1.txt");\n   \n           // 创建输出流对象，虽然会清空文件，，但是输入流fr还可以从缓冲区读取到数据\n           int ch;\n           while ((ch = fr.read()) != -1) {\n               system.out.print((char) ch);\n           }\n   \n   \n           fw.close();\n           fr.close();\n   \n       }\n   }\n   \n   \n\n * 输入流 缓冲区 详解\n   \n   * 创建8194个字符的文件 a(8192)bc\n   \n   * 创建字符输入流，创建一个byte[8192]，limit字段为0\n     \n     \n   \n   * 第一次read,缓冲区载入8192个a，limit字段为 8192\n     \n     \n   \n   * 循环结束后，第二次read，缓冲区中的数据已经读完了，需要重新读取文件数据到缓冲区。\n     \n     * 覆盖写，不会清空原来的数据。 缓冲区:bca(8190)\n     \n     * limit字段，为2，即限制读取2个，就结束了\n       \n       \n     \n     * 不用担心，采用覆盖写的方式，当文件本该结束的时候，再读到\'a\'\n   \n   * 在去read，就会输出-1\n   \n   /**\n    * @classname demo\n    * @date 2023/3/3 15:59\n    * @author diane\n    * @description 读取数据超过 8192 的文件情况\n    * @version 1.0\n    */\n   public class demo {\n     public static void main(string[] args) throws ioexception {\n   \n       // 创建文件\n       filewriter fw = new filewriter("d:\\\\tmp\\\\1.txt");\n   \n       for (int i = 0; i < 8192; i++) {\n         fw.write(\'a\');\n       }\n       fw.write(\'b\');\n       fw.write(\'c\');\n   \n       fw.close();\n   \n   \n       // 读取文件\n       filereader fr = new filereader("d:\\\\tmp\\\\1.txt");\n       for (int i = 0; i < 8192; i++) {\n         system.out.println((char) fr.read());\n       }\n       // 缓冲区中的 数据读完了， 在次将文件数据读入缓冲区\n       // 此时文件只剩下 bc\n       // 读入缓冲区的时候，是覆盖写，不会删除原来的数据 ---- 缓冲区：bca(8190)\n       system.out.println((char) fr.read()); // b\n       system.out.println((char) fr.read()); // c\n   \n       // 这里读的时候 不会读取缓冲区的下一个字符 a\n       // 因为文件真实数据已经结束了\n       // 会有一个字段 limit = 2 来限制 可以在缓冲区中读取多少个字符\n       system.out.println(fr.read());// -1\n     }\n   }\n   \n\n\n# filereader类\n\njava.io.filereader类是读取字符文件的便利类。构造时使用系统默认的字符编码和默认字节缓冲区。\n\n> 小贴士：\n> \n>  1. 字符编码：字节与字符的对应规则。windows系统的中文编码默认是gbk编码表。\n>     \n>     idea中utf-8\n> \n>  2. 字节缓冲区：一个字节数组，用来临时存储字节数据。\n\n\n# 构造方法\n\n * filereader(file file)： 创建一个新的 filereader ，给定要读取的file对象。\n * filereader(string filename)： 创建一个新的 filereader ，给定要读取的文件的名称。\n\n当你创建一个流对象时，必须传入一个文件路径。类似于fileinputstream 。\n\n * 构造举例，代码如下：\n\npublic class filereaderconstructor throws ioexception{\n    public static void main(string[] args) {\n   \t \t// 使用file对象创建流对象\n        file file = new file("a.txt");\n        filereader fr = new filereader(file);\n      \n        // 使用文件名称创建流对象\n        filereader fr = new filereader("b.txt");\n    }\n}\n\n\n\n# 读取字符数据\n\n 1. 读取字符：read方法，每次可以读取一个字符的数据，提升为int类型，读取到文件末尾，返回-1，循环读取，代码使用演示：\n    * 英文读取一个字节，汉字会读取多个字节-根据不同的编码方式来确定(utf-8 三个字节；gbk-两个字节)\n    * 方法的返回值 是 该字符在 字符集上的数字，为10进制；要想转换成字符的话，需要用char 强转\n\npublic class frread {\n    public static void main(string[] args) throws ioexception {\n      \t// 使用文件名称创建流对象\n       \tfilereader fr = new filereader("read.txt");\n      \t// 定义变量，保存数据\n        int b ；\n        // 循环读取\n        while ((b = fr.read())!=-1) {\n            system.out.println((char)b);\n        }\n\t\t// 关闭资源\n        fr.close();\n    }\n}\n输出结果：\n黑\n马\n程\n序\n员\n\n\n> 小贴士：虽然读取了一个字符，但是会自动提升为int类型。--- 字符集上对应的十进制数字\n\n 2. 使用字符数组读取：read(char[] cbuf)，每次读取b的长度个字符到数组中，返回读取到的有效字符个数，读取到末尾时，返回-1 ，代码使用演示：\n    * read(chars) 读取数据，解码，强转，放到char数组中\n\npublic class frread {\n    public static void main(string[] args) throws ioexception {\n      \t// 使用文件名称创建流对象\n       \tfilereader fr = new filereader("read.txt");\n      \t// 定义变量，保存有效字符个数\n        int len ；\n        // 定义字符数组，作为装字符数据的容器\n         char[] cbuf = new char[2];\n        // 循环读取\n        while ((len = fr.read(cbuf))!=-1) {\n            system.out.println(new string(cbuf));\n        }\n\t\t// 关闭资源\n        fr.close();\n    }\n}\n输出结果：\n黑马\n程序\n员序\n\n\n获取有效的字符改进，代码使用演示：\n\npublic class fisread {\n    public static void main(string[] args) throws ioexception {\n      \t// 使用文件名称创建流对象\n       \tfilereader fr = new filereader("read.txt");\n      \t// 定义变量，保存有效字符个数\n        int len ；\n        // 定义字符数组，作为装字符数据的容器\n        char[] cbuf = new char[2];\n        // 循环读取\n        while ((len = fr.read(cbuf))!=-1) {\n            system.out.println(new string(cbuf,0,len));\n        }\n    \t// 关闭资源\n        fr.close();\n    }\n}\n\n输出结果：\n黑马\n程序\n员\n\n\n\n# 字符输出流【writer】\n\njava.io.writer抽象类是表示用于写出字符流的所有类的超类，将指定的字符信息写出到目的地。它定义了字节输出流的基本共性功能方法。\n\n * void write(int c) 写入单个字符。\n * void write(char[] cbuf)写入字符数组。\n * abstract void write(char[] cbuf, int off, int len)写入字符数组的某一部分,off数组的开始索引,len写的字符个数。\n * void write(string str)写入字符串。\n * void write(string str, int off, int len) 写入字符串的某一部分,off字符串的开始索引,len写的字符个数。\n * void flush()刷新该流的缓冲。\n * void close() 关闭此流，但要先刷新它。\n\n\n# 字符输出流-底层原理\n\n * 字符输出流创建的时候，会在内存中创建一个byte[8192]的缓冲区。\n\n * 输出数据的时候，会先将数据写入缓冲区，后续在写入文件。\n   \n   * 当缓冲区满了，会自动写入文件。\n   * 调用flush()方法，刷新缓冲区，会将缓存区数据写入文件\n   * 调用close()方法，在断开连接前，会先刷新缓冲区，在关闭。\n   \n   /**\n    * @classname demo\n    * @date 2023/3/3 15:59\n    * @author diane\n    * @description 输出缓冲区\n    *  输出数据的时候，会先将数据写入缓冲区，后续在写入文件。\n    *      当缓冲区满了，会自动写入文件。\n    *      调用flush()方法，刷新缓冲区，会将缓存区数据写入文件\n    *      调用close()方法，在断开连接前，会先刷新缓冲区，在关闭。\n    *\n    *\n    * @version 1.0\n    */\n   public class filewritebufferdemo {\n     public static void main(string[] args) throws ioexception {\n   \n       // 创建文件\n       filewriter fw = new filewriter("d:\\\\tmp\\\\1.txt");\n   \n       for (int i = 0; i < 8192; i++) {\n         fw.write(\'a\');\n       }\n       // 缓冲区满了，会将缓冲区中的数据 写入缓冲区\n       fw.write(\'b\');\n      \t// 调用flush 方法，将缓冲区数据写入文件\n       fw.flush();\n       fw.write(\'c\');\n   \n       fw.close();\n     }\n   }\n   \n\n\n# filewriter类\n\njava.io.filewriter类是写出字符到文件的便利类。构造时使用系统默认的字符编码和默认字节缓冲区。\n\n\n# 构造方法\n\n * filewriter(file file)： 创建一个新的 filewriter，给定要读取的file对象。\n * filewriter(string filename)： 创建一个新的 filewriter，给定要读取的文件的名称。\n\n当你创建一个流对象时，必须传入一个文件路径，类似于fileoutputstream。\n\n * 构造举例，代码如下：\n\npublic class filewriterconstructor {\n    public static void main(string[] args) throws ioexception {\n   \t \t// 使用file对象创建流对象\n        file file = new file("a.txt");\n        filewriter fw = new filewriter(file);\n      \n        // 使用文件名称创建流对象\n        filewriter fw = new filewriter("b.txt");\n    }\n}\n\n\n\n# 基本写出数据\n\n写出字符：write(int b) 方法，每次可以写出一个字符数据，代码使用演示：\n\npublic class fwwrite {\n    public static void main(string[] args) throws ioexception {\n        // 使用文件名称创建流对象\n        filewriter fw = new filewriter("fw.txt");     \n      \t// 写出数据\n      \tfw.write(97); // 写出第1个字符\n      \tfw.write(\'b\'); // 写出第2个字符\n      \tfw.write(\'c\'); // 写出第3个字符\n      \tfw.write(30000); // 写出第4个字符，中文编码表中30000对应一个汉字。\n      \n      \t/*\n        【注意】关闭资源时,与fileoutputstream不同。\n      \t 如果不关闭,数据只是保存到缓冲区，并未保存到文件。\n        */\n        // fw.close();\n    }\n}\n输出结果：\nabc田\n\n\n> 小贴士：\n> \n>  1. 虽然参数为int类型四个字节，但是只会保留一个字符的信息写出。\n>  2. 未调用close方法，数据只是保存到了缓冲区，并未写出到文件中。\n\n\n# 关闭和刷新\n\n因为内置缓冲区的原因，如果不关闭输出流，无法写出字符到文件中。但是关闭的流对象，是无法继续写出数据的。如果我们既想写出数据，又想继续使用流，就需要flush 方法了。\n\n * flush ：刷新缓冲区，流对象可以继续使用。\n * close:先刷新缓冲区，然后通知系统释放资源。流对象不可以再被使用了。\n\n代码使用演示：\n\npublic class fwwrite {\n    public static void main(string[] args) throws ioexception {\n        // 使用文件名称创建流对象\n        filewriter fw = new filewriter("fw.txt");\n        // 写出数据，通过flush\n        fw.write(\'刷\'); // 写出第1个字符\n        fw.flush();\n        fw.write(\'新\'); // 继续写出第2个字符，写出成功\n        fw.flush();\n      \n      \t// 写出数据，通过close\n        fw.write(\'关\'); // 写出第1个字符\n        fw.close();\n        fw.write(\'闭\'); // 继续写出第2个字符,【报错】java.io.ioexception: stream closed\n        fw.close();\n    }\n}\n\n\n> 小贴士：即便是flush方法写出了数据，操作的最后还是要调用close方法，释放系统资源。\n\n\n# 写出其他数据\n\n 1. 写出字符数组 ：write(char[] cbuf) 和 write(char[] cbuf, int off, int len) ，每次可以写出字符数组中的数据，用法类似fileoutputstream，代码使用演示：\n\npublic class fwwrite {\n    public static void main(string[] args) throws ioexception {\n        // 使用文件名称创建流对象\n        filewriter fw = new filewriter("fw.txt");     \n      \t// 字符串转换为字节数组\n      \tchar[] chars = "黑马程序员".tochararray();\n      \n      \t// 写出字符数组\n      \tfw.write(chars); // 黑马程序员\n        \n\t\t// 写出从索引2开始，2个字节。索引2是\'程\'，两个字节，也就是\'程序\'。\n        fw.write(b,2,2); // 程序\n      \n      \t// 关闭资源\n        fos.close();\n    }\n}\n\n\n 2. 写出字符串：write(string str) 和 write(string str, int off, int len) ，每次可以写出字符串中的数据，更为方便，代码使用演示：\n\npublic class fwwrite {\n    public static void main(string[] args) throws ioexception {\n        // 使用文件名称创建流对象\n        filewriter fw = new filewriter("fw.txt");     \n      \t// 字符串\n      \tstring msg = "黑马程序员";\n      \n      \t// 写出字符数组\n      \tfw.write(msg); //黑马程序员\n      \n\t\t// 写出从索引2开始，2个字节。索引2是\'程\'，两个字节，也就是\'程序\'。\n        fw.write(msg,2,2);\t// 程序\n      \t\n        // 关闭资源\n        fos.close();\n    }\n}\n\n\n 3. 续写和换行：操作类似于fileoutputstream。\n\npublic class fwwrite {\n    public static void main(string[] args) throws ioexception {\n        // 使用文件名称创建流对象，可以续写数据\n        filewriter fw = new filewriter("fw.txt"，true);     \n      \t// 写出字符串\n        fw.write("黑马");\n      \t// 写出换行\n      \tfw.write("\\r\\n");\n      \t// 写出字符串\n  \t\tfw.write("程序员");\n      \t// 关闭资源\n        fw.close();\n    }\n}\n输出结果:\n黑马\n程序员\n\n\n> 小贴士：字符流，只能操作文本文件，不能操作图片，视频等非文本文件。\n> \n> 当我们单纯读或者写文本文件时 使用字符流 其他情况使用字节流',charsets:{cjk:!0}},{title:"IO流练习",frontmatter:{autoSort:93,title:"IO流练习",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/6e842a/",categories:["后端","Java","流"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/60.%E6%B5%81/40.IO%E6%B5%81%E7%BB%BC%E5%90%88%E7%BB%83%E4%B9%A0.html",relativePath:"01.后端/10.Java/60.流/40.IO流综合练习.md",key:"v-00877d3f",path:"/pages/6e842a/",headers:[{level:2,title:"拷贝文件夹",slug:"拷贝文件夹",normalizedTitle:"拷贝文件夹",charIndex:11},{level:2,title:"文件加密",slug:"文件加密",normalizedTitle:"文件加密",charIndex:1220},{level:2,title:"数字排序",slug:"数字排序",normalizedTitle:"数字排序",charIndex:2250}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"拷贝文件夹 文件加密 数字排序",content:'# 综合练习\n\n\n# 拷贝文件夹\n\npublic class Test01 {\n    public static void main(String[] args) throws IOException {\n        //拷贝一个文件夹，考虑子文件夹\n\n        //1.创建对象表示数据源\n        File src = new File("D:\\\\aaa\\\\src");\n        //2.创建对象表示目的地\n        File dest = new File("D:\\\\aaa\\\\dest");\n\n        //3.调用方法开始拷贝\n        copydir(src,dest);\n\n\n\n    }\n\n    /*\n    * 作用：拷贝文件夹\n    * 参数一：数据源\n    * 参数二：目的地\n    *\n    * */\n    private static void copydir(File src, File dest) throws IOException {\n        dest.mkdirs();\n        //递归\n        //1.进入数据源\n        File[] files = src.listFiles();\n        //2.遍历数组\n        for (File file : files) {\n            if(file.isFile()){\n                //3.判断文件，拷贝\n                FileInputStream fis = new FileInputStream(file);\n                FileOutputStream fos = new FileOutputStream(new File(dest,file.getName()));\n                byte[] bytes = new byte[1024];\n                int len;\n                while((len = fis.read(bytes)) != -1){\n                    fos.write(bytes,0,len);\n                }\n                fos.close();\n                fis.close();\n            }else {\n                //4.判断文件夹，递归\n                copydir(file, new File(dest,file.getName()));\n            }\n        }\n    }\n}\n\n\n\n\n# 文件加密\n\npublic class Test02 {\n    public static void main(String[] args) throws IOException {\n        /*\n            为了保证文件的安全性，就需要对原始文件进行加密存储，再使用的时候再对其进行解密处理。\n            加密原理：\n                对原始文件中的每一个字节数据进行更改，然后将更改以后的数据存储到新的文件中。\n            解密原理：\n                读取加密之后的文件，按照加密的规则反向操作，变成原始文件。\n\n             ^ : 异或\n                 两边相同：false\n                 两边不同：true\n\n                 0：false\n                 1：true\n\n               100:1100100\n               10: 1010\n\n               1100100\n             ^ 0001010\n             __________\n               1101110\n             ^ 0001010\n             __________\n               1100100\n\n        */\n    }\n\n    public static void encryptionAndReduction(File src, File dest) throws IOException {\n        FileInputStream fis = new FileInputStream(src);\n        FileOutputStream fos = new FileOutputStream(dest);\n        int b;\n        while ((b = fis.read()) != -1) {\n            fos.write(b ^ 2);\n        }\n        //4.释放资源\n        fos.close();\n        fis.close();\n    }\n\n\n}\n\n\n\n\n# 数字排序\n\n文本文件中有以下的数据： 2-1-9-4-7-8 将文件中的数据进行排序，变成以下的数据： 1-2-4-7-8-9\n\n实现方式一：\n\npublic class Test03 {\n    public static void main(String[] args) throws IOException {\n        /*\n            文本文件中有以下的数据：\n                2-1-9-4-7-8\n            将文件中的数据进行排序，变成以下的数据：\n                1-2-4-7-8-9\n        */\n\n\n        //1.读取数据\n        FileReader fr = new FileReader("myio\\\\a.txt");\n        StringBuilder sb = new StringBuilder();\n        int ch;\n        while((ch = fr.read()) != -1){\n            sb.append((char)ch);\n        }\n        fr.close();\n        System.out.println(sb);\n        //2.排序\n        String str = sb.toString();\n        String[] arrStr = str.split("-");//2-1-9-4-7-8\n\n        ArrayList<Integer> list = new ArrayList<>();\n        for (String s : arrStr) {\n            int i = Integer.parseInt(s);\n            list.add(i);\n        }\n        Collections.sort(list);\n        System.out.println(list);\n        //3.写出\n        FileWriter fw = new FileWriter("myio\\\\a.txt");\n        for (int i = 0; i < list.size(); i++) {\n            if(i == list.size() - 1){\n                fw.write(list.get(i) + "");\n            }else{\n                fw.write(list.get(i) + "-");\n            }\n        }\n        fw.close();\n    }\n}\n\n\n实现方式二：\n\npublic class Test04 {\n    public static void main(String[] args) throws IOException {\n        /*\n            文本文件中有以下的数据：\n                2-1-9-4-7-8\n            将文件中的数据进行排序，变成以下的数据：\n                1-2-4-7-8-9\n\n           细节1：\n                文件中的数据不要换行\n\n            细节2:\n                bom头\n        */\n        //1.读取数据\n        FileReader fr = new FileReader("myio\\\\a.txt");\n        StringBuilder sb = new StringBuilder();\n        int ch;\n        while((ch = fr.read()) != -1){\n            sb.append((char)ch);\n        }\n        fr.close();\n        System.out.println(sb);\n        //2.排序\n        Integer[] arr = Arrays.stream(sb.toString()\n                                      .split("-"))\n            .map(Integer::parseInt)\n            .sorted()\n            .toArray(Integer[]::new);\n        //3.写出\n        FileWriter fw = new FileWriter("myio\\\\a.txt");\n        String s = Arrays.toString(arr).replace(", ","-");\n        String result = s.substring(1, s.length() - 1);\n        fw.write(result);\n        fw.close();\n    }\n}\n\n\n实现文件三\n\npublic static void main(String[] args) throws IOException {\n    // 读入数据\n    FileInputStream fis = new FileInputStream("D:\\\\tmp\\\\6.txt");\n    byte[] bytes = new byte[2];\n    int len;\n    StringBuilder str = new StringBuilder();\n    while ((len = fis.read(bytes)) != -1) {\n        str.append(new String(bytes, 0, len));\n    }\n    fis.close();\n\n    // 排序\n    String string = str.toString();\n    String[] split = string.split("-");\n    Arrays.sort(split);\n    //[1, 2, 4, 7, 8, 9]\n    String res = Arrays.toString(split);\n    // [1-2-4-7-8-9]\n    res = res.replace(", ", "-");\n    // 1-2-4-7-8-9\n    res = res.substring(1, res.length() - 1);\n\n\n    // 写出\n    FileOutputStream fos = new FileOutputStream("D:\\\\tmp\\\\6.txt");\n    fos.write(res.getBytes());\n    fos.close();\n}\n',normalizedContent:'# 综合练习\n\n\n# 拷贝文件夹\n\npublic class test01 {\n    public static void main(string[] args) throws ioexception {\n        //拷贝一个文件夹，考虑子文件夹\n\n        //1.创建对象表示数据源\n        file src = new file("d:\\\\aaa\\\\src");\n        //2.创建对象表示目的地\n        file dest = new file("d:\\\\aaa\\\\dest");\n\n        //3.调用方法开始拷贝\n        copydir(src,dest);\n\n\n\n    }\n\n    /*\n    * 作用：拷贝文件夹\n    * 参数一：数据源\n    * 参数二：目的地\n    *\n    * */\n    private static void copydir(file src, file dest) throws ioexception {\n        dest.mkdirs();\n        //递归\n        //1.进入数据源\n        file[] files = src.listfiles();\n        //2.遍历数组\n        for (file file : files) {\n            if(file.isfile()){\n                //3.判断文件，拷贝\n                fileinputstream fis = new fileinputstream(file);\n                fileoutputstream fos = new fileoutputstream(new file(dest,file.getname()));\n                byte[] bytes = new byte[1024];\n                int len;\n                while((len = fis.read(bytes)) != -1){\n                    fos.write(bytes,0,len);\n                }\n                fos.close();\n                fis.close();\n            }else {\n                //4.判断文件夹，递归\n                copydir(file, new file(dest,file.getname()));\n            }\n        }\n    }\n}\n\n\n\n\n# 文件加密\n\npublic class test02 {\n    public static void main(string[] args) throws ioexception {\n        /*\n            为了保证文件的安全性，就需要对原始文件进行加密存储，再使用的时候再对其进行解密处理。\n            加密原理：\n                对原始文件中的每一个字节数据进行更改，然后将更改以后的数据存储到新的文件中。\n            解密原理：\n                读取加密之后的文件，按照加密的规则反向操作，变成原始文件。\n\n             ^ : 异或\n                 两边相同：false\n                 两边不同：true\n\n                 0：false\n                 1：true\n\n               100:1100100\n               10: 1010\n\n               1100100\n             ^ 0001010\n             __________\n               1101110\n             ^ 0001010\n             __________\n               1100100\n\n        */\n    }\n\n    public static void encryptionandreduction(file src, file dest) throws ioexception {\n        fileinputstream fis = new fileinputstream(src);\n        fileoutputstream fos = new fileoutputstream(dest);\n        int b;\n        while ((b = fis.read()) != -1) {\n            fos.write(b ^ 2);\n        }\n        //4.释放资源\n        fos.close();\n        fis.close();\n    }\n\n\n}\n\n\n\n\n# 数字排序\n\n文本文件中有以下的数据： 2-1-9-4-7-8 将文件中的数据进行排序，变成以下的数据： 1-2-4-7-8-9\n\n实现方式一：\n\npublic class test03 {\n    public static void main(string[] args) throws ioexception {\n        /*\n            文本文件中有以下的数据：\n                2-1-9-4-7-8\n            将文件中的数据进行排序，变成以下的数据：\n                1-2-4-7-8-9\n        */\n\n\n        //1.读取数据\n        filereader fr = new filereader("myio\\\\a.txt");\n        stringbuilder sb = new stringbuilder();\n        int ch;\n        while((ch = fr.read()) != -1){\n            sb.append((char)ch);\n        }\n        fr.close();\n        system.out.println(sb);\n        //2.排序\n        string str = sb.tostring();\n        string[] arrstr = str.split("-");//2-1-9-4-7-8\n\n        arraylist<integer> list = new arraylist<>();\n        for (string s : arrstr) {\n            int i = integer.parseint(s);\n            list.add(i);\n        }\n        collections.sort(list);\n        system.out.println(list);\n        //3.写出\n        filewriter fw = new filewriter("myio\\\\a.txt");\n        for (int i = 0; i < list.size(); i++) {\n            if(i == list.size() - 1){\n                fw.write(list.get(i) + "");\n            }else{\n                fw.write(list.get(i) + "-");\n            }\n        }\n        fw.close();\n    }\n}\n\n\n实现方式二：\n\npublic class test04 {\n    public static void main(string[] args) throws ioexception {\n        /*\n            文本文件中有以下的数据：\n                2-1-9-4-7-8\n            将文件中的数据进行排序，变成以下的数据：\n                1-2-4-7-8-9\n\n           细节1：\n                文件中的数据不要换行\n\n            细节2:\n                bom头\n        */\n        //1.读取数据\n        filereader fr = new filereader("myio\\\\a.txt");\n        stringbuilder sb = new stringbuilder();\n        int ch;\n        while((ch = fr.read()) != -1){\n            sb.append((char)ch);\n        }\n        fr.close();\n        system.out.println(sb);\n        //2.排序\n        integer[] arr = arrays.stream(sb.tostring()\n                                      .split("-"))\n            .map(integer::parseint)\n            .sorted()\n            .toarray(integer[]::new);\n        //3.写出\n        filewriter fw = new filewriter("myio\\\\a.txt");\n        string s = arrays.tostring(arr).replace(", ","-");\n        string result = s.substring(1, s.length() - 1);\n        fw.write(result);\n        fw.close();\n    }\n}\n\n\n实现文件三\n\npublic static void main(string[] args) throws ioexception {\n    // 读入数据\n    fileinputstream fis = new fileinputstream("d:\\\\tmp\\\\6.txt");\n    byte[] bytes = new byte[2];\n    int len;\n    stringbuilder str = new stringbuilder();\n    while ((len = fis.read(bytes)) != -1) {\n        str.append(new string(bytes, 0, len));\n    }\n    fis.close();\n\n    // 排序\n    string string = str.tostring();\n    string[] split = string.split("-");\n    arrays.sort(split);\n    //[1, 2, 4, 7, 8, 9]\n    string res = arrays.tostring(split);\n    // [1-2-4-7-8-9]\n    res = res.replace(", ", "-");\n    // 1-2-4-7-8-9\n    res = res.substring(1, res.length() - 1);\n\n\n    // 写出\n    fileoutputstream fos = new fileoutputstream("d:\\\\tmp\\\\6.txt");\n    fos.write(res.getbytes());\n    fos.close();\n}\n',charsets:{cjk:!0}},{title:"缓冲流",frontmatter:{autoSort:92,title:"缓冲流",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/bce7f1/",categories:["后端","Java","流"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/60.%E6%B5%81/45.%E7%BC%93%E5%86%B2%E6%B5%81.html",relativePath:"01.后端/10.Java/60.流/45.缓冲流.md",key:"v-3ab071ee",path:"/pages/bce7f1/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:149},{level:2,title:"字节缓冲流",slug:"字节缓冲流",normalizedTitle:"字节缓冲流",charIndex:205},{level:3,title:"构造方法",slug:"构造方法",normalizedTitle:"构造方法",charIndex:453},{level:3,title:"效率测试",slug:"效率测试",normalizedTitle:"效率测试",charIndex:796},{level:2,title:"字符缓冲流",slug:"字符缓冲流",normalizedTitle:"字符缓冲流",charIndex:255},{level:3,title:"构造方法",slug:"构造方法-2",normalizedTitle:"构造方法",charIndex:453},{level:3,title:"特有方法",slug:"特有方法",normalizedTitle:"特有方法",charIndex:3696},{level:2,title:"练习:文本排序",slug:"练习-文本排序",normalizedTitle:"练习:文本排序",charIndex:4743},{level:3,title:"案例分析",slug:"案例分析",normalizedTitle:"案例分析",charIndex:5539},{level:3,title:"案例实现",slug:"案例实现",normalizedTitle:"案例实现",charIndex:5618},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:6936}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"概述 字节缓冲流 构造方法 效率测试 字符缓冲流 构造方法 特有方法 练习:文本排序 案例分析 案例实现 总结",content:'# 缓冲流\n\n昨天学习了基本的一些流，作为IO流的入门，今天我们要见识一些更强大的流。比如能够高效读写的缓冲流，能够转换编码的转换流，能够持久化存储对象的序列化流等等。这些功能更为强大的流，都是在基本的流对象基础之上创建而来的，就像穿上铠甲的武士一样，相当于是对基本流对象的一种增强。\n\n\n\n\n# 概述\n\n缓冲流,也叫高效流，是对4个基本的FileXxx 流的增强，所以也是4个流，按照数据类型分类：\n\n * 字节缓冲流：BufferedInputStream，BufferedOutputStream\n * 字符缓冲流：BufferedReader，BufferedWriter\n\n缓冲流的基本原理，是在创建流对象时，会创建一个内置的默认大小的缓冲区数组，通过缓冲区读写，减少系统IO次数，从而提高读写的效率。\n\n * 缓冲流--提高效率的原因\n\n内存中操作数据是很快的，但是内存与硬盘进行交互的，IO操作是非常耗时间的，因为缓冲区减少了 系统IO次数，所以可以提高读写效率\n\n\n# 字节缓冲流\n\n\n# 构造方法\n\n * public BufferedInputStream(InputStream in) ：创建一个 新的缓冲输入流。\n * public BufferedOutputStream(OutputStream out)： 创建一个新的缓冲输出流。\n\n构造举例，代码如下：\n\n// 创建字节缓冲输入流\nBufferedInputStream bis = new BufferedInputStream(new FileInputStream("bis.txt"));\n// 创建字节缓冲输出流\nBufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream("bos.txt"));\n\n\n\n# 效率测试\n\n查询API，缓冲流读写方法与基本的流是一致的，我们通过复制大文件（375MB），测试它的效率。\n\n 1. 基本流，代码如下：\n\npublic class BufferedDemo {\n    public static void main(String[] args) throws FileNotFoundException {\n        // 记录开始时间\n      \tlong start = System.currentTimeMillis();\n\t\t// 创建流对象\n        try (\n        \tFileInputStream fis = new FileInputStream("jdk9.exe");\n        \tFileOutputStream fos = new FileOutputStream("copy.exe")\n        ){\n        \t// 读写数据\n            int b;\n            while ((b = fis.read()) != -1) {\n                fos.write(b);\n            }\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n\t\t// 记录结束时间\n        long end = System.currentTimeMillis();\n        System.out.println("普通流复制时间:"+(end - start)+" 毫秒");\n    }\n}\n\n十几分钟过去了...\n\n\n 2. 缓冲流，代码如下：\n\npublic class BufferedDemo {\n    public static void main(String[] args) throws FileNotFoundException {\n        // 记录开始时间\n      \tlong start = System.currentTimeMillis();\n\t\t// 创建流对象\n        try (\n        \tBufferedInputStream bis = new BufferedInputStream(new FileInputStream("jdk9.exe"));\n\t     BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream("copy.exe"));\n        ){\n        // 读写数据\n            int b;\n            while ((b = bis.read()) != -1) {\n                bos.write(b);\n            }\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n\t\t// 记录结束时间\n        long end = System.currentTimeMillis();\n        System.out.println("缓冲流复制时间:"+(end - start)+" 毫秒");\n    }\n}\n\n缓冲流复制时间:8016 毫秒\n\n\n如何更快呢？\n\n * 定义的字节数组可以一次性在两个缓冲区直接传递更多的数据，进而提高效率。\n\n使用数组的方式，代码如下：\n\npublic class BufferedDemo {\n    public static void main(String[] args) throws FileNotFoundException {\n      \t// 记录开始时间\n        long start = System.currentTimeMillis();\n\t\t// 创建流对象\n        try (\n\t\t\tBufferedInputStream bis = new BufferedInputStream(new FileInputStream("jdk9.exe"));\n\t\t BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream("copy.exe"));\n        ){\n          \t// 读写数据\n            int len;\n            byte[] bytes = new byte[8*1024];\n            while ((len = bis.read(bytes)) != -1) {\n                bos.write(bytes, 0 , len);\n            }\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n\t\t// 记录结束时间\n        long end = System.currentTimeMillis();\n        System.out.println("缓冲流使用数组复制时间:"+(end - start)+" 毫秒");\n    }\n}\n缓冲流使用数组复制时间:666 毫秒\n\n\n\n# 字符缓冲流\n\n * 字符流 底层自带了 长度为 8192的缓冲区， 来提高性能，在字符输入流和字符输出流 的底层有过详细介绍\n * 所以，字符缓冲流 相比于 字符流 带来的性能 提升没有那么明显。主要学习特有的好用的方法。。\n * 但是有两个比较好用的方法\n   * BufferedReader：public String readLine(): 读一行文字。\n   * BufferedWriter：public void newLine(): 写一行行分隔符,由系统属性定义符号。\n\n\n# 构造方法\n\n * public BufferedReader(Reader in) ：创建一个 新的缓冲输入流。\n * public BufferedWriter(Writer out)： 创建一个新的缓冲输出流。\n\n构造举例，代码如下：\n\n// 创建字符缓冲输入流\nBufferedReader br = new BufferedReader(new FileReader("br.txt"));\n// 创建字符缓冲输出流\nBufferedWriter bw = new BufferedWriter(new FileWriter("bw.txt"));\n\n\n\n# 特有方法\n\n字符缓冲流的基本方法与普通字符流调用方式一致，不再阐述，我们来看它们具备的特有方法。\n\n * BufferedReader：public String readLine(): 读一行文字。\n * BufferedWriter：public void newLine(): 写一行行分隔符,由系统属性定义符号。\n\nreadLine方法演示，代码如下：\n\npublic class BufferedReaderDemo {\n    public static void main(String[] args) throws IOException {\n      \t // 创建流对象\n        BufferedReader br = new BufferedReader(new FileReader("in.txt"));\n\t\t// 定义字符串,保存读取的一行文字\n        String line  = null;\n      \t// 循环读取,读取到最后返回null\n        while ((line = br.readLine())!=null) {\n            System.out.print(line);\n            System.out.println("------");\n        }\n\t\t// 释放资源\n        br.close();\n    }\n}\n\n\nnewLine方法演示，代码如下：\n\npublic class BufferedWriterDemo throws IOException {\n  public static void main(String[] args) throws IOException  {\n    \t// 创建流对象\n  \tBufferedWriter bw = new BufferedWriter(new FileWriter("out.txt"));\n    \t// 写出数据\n      bw.write("黑马");\n    \t// 写出换行\n      bw.newLine();\n      bw.write("程序");\n      bw.newLine();\n      bw.write("员");\n      bw.newLine();\n  \t// 释放资源\n      bw.close();\n  }\n}\n输出效果:\n黑马\n程序\n员\n\n\n\n# 练习:文本排序\n\n请将文本信息恢复顺序。\n\n3.侍中、侍郎郭攸之、费祎、董允等，此皆良实，志虑忠纯，是以先帝简拔以遗陛下。愚以为宫中之事，事无大小，悉以咨之，然后施行，必得裨补阙漏，有所广益。\n8.愿陛下托臣以讨贼兴复之效，不效，则治臣之罪，以告先帝之灵。若无兴德之言，则责攸之、祎、允等之慢，以彰其咎；陛下亦宜自谋，以咨诹善道，察纳雅言，深追先帝遗诏，臣不胜受恩感激。\n4.将军向宠，性行淑均，晓畅军事，试用之于昔日，先帝称之曰能，是以众议举宠为督。愚以为营中之事，悉以咨之，必能使行阵和睦，优劣得所。\n2.宫中府中，俱为一体，陟罚臧否，不宜异同。若有作奸犯科及为忠善者，宜付有司论其刑赏，以昭陛下平明之理，不宜偏私，使内外异法也。\n1.先帝创业未半而中道崩殂，今天下三分，益州疲弊，此诚危急存亡之秋也。然侍卫之臣不懈于内，忠志之士忘身于外者，盖追先帝之殊遇，欲报之于陛下也。诚宜开张圣听，以光先帝遗德，恢弘志士之气，不宜妄自菲薄，引喻失义，以塞忠谏之路也。\n9.今当远离，临表涕零，不知所言。\n6.臣本布衣，躬耕于南阳，苟全性命于乱世，不求闻达于诸侯。先帝不以臣卑鄙，猥自枉屈，三顾臣于草庐之中，咨臣以当世之事，由是感激，遂许先帝以驱驰。后值倾覆，受任于败军之际，奉命于危难之间，尔来二十有一年矣。\n7.先帝知臣谨慎，故临崩寄臣以大事也。受命以来，夙夜忧叹，恐付托不效，以伤先帝之明，故五月渡泸，深入不毛。今南方已定，兵甲已足，当奖率三军，北定中原，庶竭驽钝，攘除奸凶，兴复汉室，还于旧都。此臣所以报先帝而忠陛下之职分也。至于斟酌损益，进尽忠言，则攸之、祎、允之任也。\n5.亲贤臣，远小人，此先汉所以兴隆也；亲小人，远贤臣，此后汉所以倾颓也。先帝在时，每与臣论此事，未尝不叹息痛恨于桓、灵也。侍中、尚书、长史、参军，此悉贞良死节之臣，愿陛下亲之信之，则汉室之隆，可计日而待也。\n\n\n\n# 案例分析\n\n 1. 逐行读取文本信息。\n 2. 把读取到的文本存储到集合中\n 3. 对集合中的文本进行排序\n 4. 遍历集合，按顺序，写出文本信息。\n\n\n# 案例实现\n\npublic class Demo05Test {\n    public static void main(String[] args) throws IOException {\n        //1.创建ArrayList集合,泛型使用String\n        ArrayList<String> list = new ArrayList<>();\n        //2.创建BufferedReader对象,构造方法中传递FileReader对象\n        BufferedReader br = new BufferedReader(new FileReader("10_IO\\\\in.txt"));\n        //3.创建BufferedWriter对象,构造方法中传递FileWriter对象\n        BufferedWriter bw = new BufferedWriter(new FileWriter("10_IO\\\\out.txt"));\n        //4.使用BufferedReader对象中的方法readLine,以行的方式读取文本\n        String line;\n        while((line = br.readLine())!=null){\n            //5.把读取到的文本存储到ArrayList集合中\n            list.add(line);\n        }\n        //6.使用Collections集合工具类中的方法sort,对集合中的元素按照自定义规则排序\n        Collections.sort(list, new Comparator<String>() {\n            /*\n                o1-o2:升序\n                o2-o1:降序\n             */\n            @Override\n            public int compare(String o1, String o2) {\n                //依次比较集合中两个元素的首字母,升序排序\n                return o1.charAt(0)-o2.charAt(0);\n            }\n        });\n        //7.遍历ArrayList集合,获取每一个元素\n        for (String s : list) {\n            //8.使用BufferedWriter对象中的方法wirte,把遍历得到的元素写入到文本中(内存缓冲区中)\n            bw.write(s);\n            //9.写换行\n            bw.newLine();\n        }\n        //10.释放资源\n        bw.close();\n        br.close();\n    }\n}\n\n\n\n# 总结\n\n * 字节缓冲流 缓冲区大小为 byte[8192]\n * 字符缓冲流 缓冲区大小为 char[8192]\n * 字节缓冲流可以提高效率，字符缓冲流提升不明显\n\n',normalizedContent:'# 缓冲流\n\n昨天学习了基本的一些流，作为io流的入门，今天我们要见识一些更强大的流。比如能够高效读写的缓冲流，能够转换编码的转换流，能够持久化存储对象的序列化流等等。这些功能更为强大的流，都是在基本的流对象基础之上创建而来的，就像穿上铠甲的武士一样，相当于是对基本流对象的一种增强。\n\n\n\n\n# 概述\n\n缓冲流,也叫高效流，是对4个基本的filexxx 流的增强，所以也是4个流，按照数据类型分类：\n\n * 字节缓冲流：bufferedinputstream，bufferedoutputstream\n * 字符缓冲流：bufferedreader，bufferedwriter\n\n缓冲流的基本原理，是在创建流对象时，会创建一个内置的默认大小的缓冲区数组，通过缓冲区读写，减少系统io次数，从而提高读写的效率。\n\n * 缓冲流--提高效率的原因\n\n内存中操作数据是很快的，但是内存与硬盘进行交互的，io操作是非常耗时间的，因为缓冲区减少了 系统io次数，所以可以提高读写效率\n\n\n# 字节缓冲流\n\n\n# 构造方法\n\n * public bufferedinputstream(inputstream in) ：创建一个 新的缓冲输入流。\n * public bufferedoutputstream(outputstream out)： 创建一个新的缓冲输出流。\n\n构造举例，代码如下：\n\n// 创建字节缓冲输入流\nbufferedinputstream bis = new bufferedinputstream(new fileinputstream("bis.txt"));\n// 创建字节缓冲输出流\nbufferedoutputstream bos = new bufferedoutputstream(new fileoutputstream("bos.txt"));\n\n\n\n# 效率测试\n\n查询api，缓冲流读写方法与基本的流是一致的，我们通过复制大文件（375mb），测试它的效率。\n\n 1. 基本流，代码如下：\n\npublic class buffereddemo {\n    public static void main(string[] args) throws filenotfoundexception {\n        // 记录开始时间\n      \tlong start = system.currenttimemillis();\n\t\t// 创建流对象\n        try (\n        \tfileinputstream fis = new fileinputstream("jdk9.exe");\n        \tfileoutputstream fos = new fileoutputstream("copy.exe")\n        ){\n        \t// 读写数据\n            int b;\n            while ((b = fis.read()) != -1) {\n                fos.write(b);\n            }\n        } catch (ioexception e) {\n            e.printstacktrace();\n        }\n\t\t// 记录结束时间\n        long end = system.currenttimemillis();\n        system.out.println("普通流复制时间:"+(end - start)+" 毫秒");\n    }\n}\n\n十几分钟过去了...\n\n\n 2. 缓冲流，代码如下：\n\npublic class buffereddemo {\n    public static void main(string[] args) throws filenotfoundexception {\n        // 记录开始时间\n      \tlong start = system.currenttimemillis();\n\t\t// 创建流对象\n        try (\n        \tbufferedinputstream bis = new bufferedinputstream(new fileinputstream("jdk9.exe"));\n\t     bufferedoutputstream bos = new bufferedoutputstream(new fileoutputstream("copy.exe"));\n        ){\n        // 读写数据\n            int b;\n            while ((b = bis.read()) != -1) {\n                bos.write(b);\n            }\n        } catch (ioexception e) {\n            e.printstacktrace();\n        }\n\t\t// 记录结束时间\n        long end = system.currenttimemillis();\n        system.out.println("缓冲流复制时间:"+(end - start)+" 毫秒");\n    }\n}\n\n缓冲流复制时间:8016 毫秒\n\n\n如何更快呢？\n\n * 定义的字节数组可以一次性在两个缓冲区直接传递更多的数据，进而提高效率。\n\n使用数组的方式，代码如下：\n\npublic class buffereddemo {\n    public static void main(string[] args) throws filenotfoundexception {\n      \t// 记录开始时间\n        long start = system.currenttimemillis();\n\t\t// 创建流对象\n        try (\n\t\t\tbufferedinputstream bis = new bufferedinputstream(new fileinputstream("jdk9.exe"));\n\t\t bufferedoutputstream bos = new bufferedoutputstream(new fileoutputstream("copy.exe"));\n        ){\n          \t// 读写数据\n            int len;\n            byte[] bytes = new byte[8*1024];\n            while ((len = bis.read(bytes)) != -1) {\n                bos.write(bytes, 0 , len);\n            }\n        } catch (ioexception e) {\n            e.printstacktrace();\n        }\n\t\t// 记录结束时间\n        long end = system.currenttimemillis();\n        system.out.println("缓冲流使用数组复制时间:"+(end - start)+" 毫秒");\n    }\n}\n缓冲流使用数组复制时间:666 毫秒\n\n\n\n# 字符缓冲流\n\n * 字符流 底层自带了 长度为 8192的缓冲区， 来提高性能，在字符输入流和字符输出流 的底层有过详细介绍\n * 所以，字符缓冲流 相比于 字符流 带来的性能 提升没有那么明显。主要学习特有的好用的方法。。\n * 但是有两个比较好用的方法\n   * bufferedreader：public string readline(): 读一行文字。\n   * bufferedwriter：public void newline(): 写一行行分隔符,由系统属性定义符号。\n\n\n# 构造方法\n\n * public bufferedreader(reader in) ：创建一个 新的缓冲输入流。\n * public bufferedwriter(writer out)： 创建一个新的缓冲输出流。\n\n构造举例，代码如下：\n\n// 创建字符缓冲输入流\nbufferedreader br = new bufferedreader(new filereader("br.txt"));\n// 创建字符缓冲输出流\nbufferedwriter bw = new bufferedwriter(new filewriter("bw.txt"));\n\n\n\n# 特有方法\n\n字符缓冲流的基本方法与普通字符流调用方式一致，不再阐述，我们来看它们具备的特有方法。\n\n * bufferedreader：public string readline(): 读一行文字。\n * bufferedwriter：public void newline(): 写一行行分隔符,由系统属性定义符号。\n\nreadline方法演示，代码如下：\n\npublic class bufferedreaderdemo {\n    public static void main(string[] args) throws ioexception {\n      \t // 创建流对象\n        bufferedreader br = new bufferedreader(new filereader("in.txt"));\n\t\t// 定义字符串,保存读取的一行文字\n        string line  = null;\n      \t// 循环读取,读取到最后返回null\n        while ((line = br.readline())!=null) {\n            system.out.print(line);\n            system.out.println("------");\n        }\n\t\t// 释放资源\n        br.close();\n    }\n}\n\n\nnewline方法演示，代码如下：\n\npublic class bufferedwriterdemo throws ioexception {\n  public static void main(string[] args) throws ioexception  {\n    \t// 创建流对象\n  \tbufferedwriter bw = new bufferedwriter(new filewriter("out.txt"));\n    \t// 写出数据\n      bw.write("黑马");\n    \t// 写出换行\n      bw.newline();\n      bw.write("程序");\n      bw.newline();\n      bw.write("员");\n      bw.newline();\n  \t// 释放资源\n      bw.close();\n  }\n}\n输出效果:\n黑马\n程序\n员\n\n\n\n# 练习:文本排序\n\n请将文本信息恢复顺序。\n\n3.侍中、侍郎郭攸之、费祎、董允等，此皆良实，志虑忠纯，是以先帝简拔以遗陛下。愚以为宫中之事，事无大小，悉以咨之，然后施行，必得裨补阙漏，有所广益。\n8.愿陛下托臣以讨贼兴复之效，不效，则治臣之罪，以告先帝之灵。若无兴德之言，则责攸之、祎、允等之慢，以彰其咎；陛下亦宜自谋，以咨诹善道，察纳雅言，深追先帝遗诏，臣不胜受恩感激。\n4.将军向宠，性行淑均，晓畅军事，试用之于昔日，先帝称之曰能，是以众议举宠为督。愚以为营中之事，悉以咨之，必能使行阵和睦，优劣得所。\n2.宫中府中，俱为一体，陟罚臧否，不宜异同。若有作奸犯科及为忠善者，宜付有司论其刑赏，以昭陛下平明之理，不宜偏私，使内外异法也。\n1.先帝创业未半而中道崩殂，今天下三分，益州疲弊，此诚危急存亡之秋也。然侍卫之臣不懈于内，忠志之士忘身于外者，盖追先帝之殊遇，欲报之于陛下也。诚宜开张圣听，以光先帝遗德，恢弘志士之气，不宜妄自菲薄，引喻失义，以塞忠谏之路也。\n9.今当远离，临表涕零，不知所言。\n6.臣本布衣，躬耕于南阳，苟全性命于乱世，不求闻达于诸侯。先帝不以臣卑鄙，猥自枉屈，三顾臣于草庐之中，咨臣以当世之事，由是感激，遂许先帝以驱驰。后值倾覆，受任于败军之际，奉命于危难之间，尔来二十有一年矣。\n7.先帝知臣谨慎，故临崩寄臣以大事也。受命以来，夙夜忧叹，恐付托不效，以伤先帝之明，故五月渡泸，深入不毛。今南方已定，兵甲已足，当奖率三军，北定中原，庶竭驽钝，攘除奸凶，兴复汉室，还于旧都。此臣所以报先帝而忠陛下之职分也。至于斟酌损益，进尽忠言，则攸之、祎、允之任也。\n5.亲贤臣，远小人，此先汉所以兴隆也；亲小人，远贤臣，此后汉所以倾颓也。先帝在时，每与臣论此事，未尝不叹息痛恨于桓、灵也。侍中、尚书、长史、参军，此悉贞良死节之臣，愿陛下亲之信之，则汉室之隆，可计日而待也。\n\n\n\n# 案例分析\n\n 1. 逐行读取文本信息。\n 2. 把读取到的文本存储到集合中\n 3. 对集合中的文本进行排序\n 4. 遍历集合，按顺序，写出文本信息。\n\n\n# 案例实现\n\npublic class demo05test {\n    public static void main(string[] args) throws ioexception {\n        //1.创建arraylist集合,泛型使用string\n        arraylist<string> list = new arraylist<>();\n        //2.创建bufferedreader对象,构造方法中传递filereader对象\n        bufferedreader br = new bufferedreader(new filereader("10_io\\\\in.txt"));\n        //3.创建bufferedwriter对象,构造方法中传递filewriter对象\n        bufferedwriter bw = new bufferedwriter(new filewriter("10_io\\\\out.txt"));\n        //4.使用bufferedreader对象中的方法readline,以行的方式读取文本\n        string line;\n        while((line = br.readline())!=null){\n            //5.把读取到的文本存储到arraylist集合中\n            list.add(line);\n        }\n        //6.使用collections集合工具类中的方法sort,对集合中的元素按照自定义规则排序\n        collections.sort(list, new comparator<string>() {\n            /*\n                o1-o2:升序\n                o2-o1:降序\n             */\n            @override\n            public int compare(string o1, string o2) {\n                //依次比较集合中两个元素的首字母,升序排序\n                return o1.charat(0)-o2.charat(0);\n            }\n        });\n        //7.遍历arraylist集合,获取每一个元素\n        for (string s : list) {\n            //8.使用bufferedwriter对象中的方法wirte,把遍历得到的元素写入到文本中(内存缓冲区中)\n            bw.write(s);\n            //9.写换行\n            bw.newline();\n        }\n        //10.释放资源\n        bw.close();\n        br.close();\n    }\n}\n\n\n\n# 总结\n\n * 字节缓冲流 缓冲区大小为 byte[8192]\n * 字符缓冲流 缓冲区大小为 char[8192]\n * 字节缓冲流可以提高效率，字符缓冲流提升不明显\n\n',charsets:{cjk:!0}},{title:"转换流",frontmatter:{autoSort:91,title:"转换流",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/d2a882/",categories:["后端","Java","流"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/60.%E6%B5%81/47.%E8%BD%AC%E6%8D%A2%E6%B5%81.html",relativePath:"01.后端/10.Java/60.流/47.转换流.md",key:"v-751012fa",path:"/pages/d2a882/",headers:[{level:2,title:"字符编码和字符集",slug:"字符编码和字符集",normalizedTitle:"字符编码和字符集",charIndex:166},{level:3,title:"字符编码",slug:"字符编码",normalizedTitle:"字符编码",charIndex:166},{level:3,title:"字符集",slug:"字符集",normalizedTitle:"字符集",charIndex:171},{level:2,title:"编码引出的问题",slug:"编码引出的问题",normalizedTitle:"编码引出的问题",charIndex:1889},{level:2,title:"InputStreamReader类",slug:"inputstreamreader类",normalizedTitle:"inputstreamreader类",charIndex:2364},{level:3,title:"构造方法",slug:"构造方法",normalizedTitle:"构造方法",charIndex:2492},{level:3,title:"指定编码读取",slug:"指定编码读取",normalizedTitle:"指定编码读取",charIndex:2809},{level:2,title:"OutputStreamWriter类",slug:"outputstreamwriter类",normalizedTitle:"outputstreamwriter类",charIndex:3565},{level:3,title:"构造方法",slug:"构造方法-2",normalizedTitle:"构造方法",charIndex:2492},{level:3,title:"指定编码写出",slug:"指定编码写出",normalizedTitle:"指定编码写出",charIndex:4019},{level:3,title:"转换流理解图解",slug:"转换流理解图解",normalizedTitle:"转换流理解图解",charIndex:4624},{level:2,title:"练习：转换文件编码",slug:"练习-转换文件编码",normalizedTitle:"练习：转换文件编码",charIndex:4652},{level:3,title:"练习1",slug:"练习1",normalizedTitle:"练习1",charIndex:4696},{level:3,title:"练习2",slug:"练习2",normalizedTitle:"练习2",charIndex:5470},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:6391}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"字符编码和字符集 字符编码 字符集 编码引出的问题 InputStreamReader类 构造方法 指定编码读取 OutputStreamWriter类 构造方法 指定编码写出 转换流理解图解 练习：转换文件编码 练习1 练习2 总结",content:'# 转换流\n\n * idea 默认是 UTF-8编码，使用FileReader 和 FileWriter 都是 UTF-8；JDK11以前不能指定文件编码；\n * 所以只能通过转换流，转换文件编码。\n * jdk11 以后，FileReader 和 FileWriter 都可以指定编码，转换流就没有太大的用处了。。。\n\n\n\n\n# 字符编码和字符集\n\n\n# 字符编码\n\n计算机中储存的信息都是用二进制数表示的，而我们在屏幕上看到的数字、英文、标点符号、汉字等字符是二进制数转换之后的结果。按照某种规则，将字符存储到计算机中，称为编码 。反之，将存储在计算机中的二进制数按照某种规则解析显示出来，称为解码 。比如说，按照A规则存储，同样按照A规则解析，那么就能显示正确的文本符号。反之，按照A规则存储，再按照B规则解析，就会导致乱码现象。\n\n编码:字符(能看懂的)--字节(看不懂的)\n\n解码:字节(看不懂的)--\x3e字符(能看懂的)\n\n * 字符编码Character Encoding : 就是一套自然语言的字符与二进制数之间的对应规则。\n   \n   编码表:生活中文字和计算机中二进制的对应规则\n\n\n# 字符集\n\n * 字符集 Charset：也叫编码表。是一个系统支持的所有字符的集合，包括各国家文字、标点符号、图形符号、数字等。\n\n计算机要准确的存储和识别各种字符集符号，需要进行字符编码，一套字符集必然至少有一套字符编码。常见字符集有ASCII字符集、GBK字符集、Unicode字符集等。\n\n可见，当指定了编码，它所对应的字符集自然就指定了，所以编码才是我们最终要关心的。\n\n * ASCII字符集 ：\n   * ASCII（American Standard Code for Information Interchange，美国信息交换标准代码）是基于拉丁字母的一套电脑编码系统，用于显示现代英语，主要包括控制字符（回车键、退格、换行键等）和可显示字符（英文大小写字符、阿拉伯数字和西文符号）。\n   * 基本的ASCII字符集，使用7位（bits）表示一个字符，共128字符。ASCII的扩展字符集使用8位（bits）表示一个字符，共256字符，方便支持欧洲常用字符。\n * ISO-8859-1字符集：\n   * 拉丁码表，别名Latin-1，用于显示欧洲使用的语言，包括荷兰、丹麦、德语、意大利语、西班牙语等。\n   * ISO-8859-1使用单字节编码，兼容ASCII编码。\n * GBxxx字符集：\n   * GB就是国标的意思，是为了显示中文而设计的一套字符集。\n   * GB2312：简体中文码表。一个小于127的字符的意义与原来相同。但两个大于127的字符连在一起时，就表示一个汉字，这样大约可以组合了包含7000多个简体汉字，此外数学符号、罗马希腊的字母、日文的假名们都编进去了，连在ASCII里本来就有的数字、标点、字母都统统重新编了两个字节长的编码，这就是常说的"全角"字符，而原来在127号以下的那些就叫"半角"字符了。\n   * GBK：最常用的中文码表。是在GB2312标准基础上的扩展规范，使用了双字节编码方案，共收录了21003个汉字，完全兼容GB2312标准，同时支持繁体汉字以及日韩汉字等。\n   * GB18030：最新的中文码表。收录汉字70244个，采用多字节编码，每个字可以由1个、2个或4个字节组成。支持中国国内少数民族的文字，同时支持繁体汉字以及日韩汉字等。\n * Unicode字符集 ：\n   * Unicode编码系统为表达任意语言的任意字符而设计，是业界的一种标准，也称为统一码、标准万国码。\n   * 它最多使用4个字节的数字来表达每个字母、符号，或者文字。有三种编码方案，UTF-8、UTF-16和UTF-32。最为常用的UTF-8编码。\n   * UTF-8编码，可以用来表示Unicode标准中任何字符，它是电子邮件、网页及其他存储或传送文字的应用中，优先采用的编码。互联网工程工作小组（IETF）要求所有互联网协议都必须支持UTF-8编码。所以，我们开发Web应用，也要使用UTF-8编码。它使用一至四个字节为每个字符编码，编码规则：\n     1. 128个US-ASCII字符，只需一个字节编码。\n     2. 拉丁文等字符，需要二个字节编码。\n     3. 大部分常用字（含中文），使用三个字节编码。\n     4. 其他极少使用的Unicode辅助字符，使用四字节编码。\n\n\n# 编码引出的问题\n\n在IDEA中，使用FileReader 读取项目中的文本文件。由于IDEA的设置，都是默认的UTF-8编码，所以没有任何问题。但是，当读取Windows系统中创建的文本文件时，由于Windows系统的默认是GBK编码，就会出现乱码。\n\npublic class ReaderDemo {\n    public static void main(String[] args) throws IOException {\n        FileReader fileReader = new FileReader("E:\\\\File_GBK.txt");\n        int read;\n        while ((read = fileReader.read()) != -1) {\n            System.out.print((char)read);\n        }\n        fileReader.close();\n    }\n}\n输出结果：\n���\n\n\n那么如何读取GBK编码的文件呢？\n\n\n# InputStreamReader类\n\n转换流java.io.InputStreamReader，是Reader的子类，是从字节流到字符流的桥梁。它读取字节，并使用指定的字符集将其解码为字符。它的字符集可以由名称指定，也可以接受平台的默认字符集。\n\n\n# 构造方法\n\n * InputStreamReader(InputStream in): 创建一个使用默认字符集的字符流。\n * InputStreamReader(InputStream in, String charsetName): 创建一个指定字符集的字符流。\n\n构造举例，代码如下：\n\nInputStreamReader isr = new InputStreamReader(new FileInputStream("in.txt"));\nInputStreamReader isr2 = new InputStreamReader(new FileInputStream("in.txt") , "GBK");\n\n\n\n# 指定编码读取\n\npublic class ReaderDemo2 {\n    public static void main(String[] args) throws IOException {\n      \t// 定义文件路径,文件为gbk编码\n        String FileName = "E:\\\\file_gbk.txt";\n      \t// 创建流对象,默认UTF8编码\n        InputStreamReader isr = new InputStreamReader(new FileInputStream(FileName));\n      \t// 创建流对象,指定GBK编码\n        InputStreamReader isr2 = new InputStreamReader(new FileInputStream(FileName) , "GBK");\n\t\t// 定义变量,保存字符\n        int read;\n      \t// 使用默认编码字符流读取,乱码\n        while ((read = isr.read()) != -1) {\n            System.out.print((char)read); // ��Һ�\n        }\n        isr.close();\n      \n      \t// 使用指定编码字符流读取,正常解析\n        while ((read = isr2.read()) != -1) {\n            System.out.print((char)read);// 大家好\n        }\n        isr2.close();\n    }\n}\n\n\n\n# OutputStreamWriter类\n\n转换流java.io.OutputStreamWriter ，是Writer的子类，是从字符流到字节流的桥梁。使用指定的字符集将字符编码为字节。它的字符集可以由名称指定，也可以接受平台的默认字符集。\n\n\n# 构造方法\n\n * OutputStreamWriter(OutputStream in): 创建一个使用默认字符集的字符流。\n * OutputStreamWriter(OutputStream in, String charsetName): 创建一个指定字符集的字符流。\n\n构造举例，代码如下：\n\nOutputStreamWriter isr = new OutputStreamWriter(new FileOutputStream("out.txt"));\nOutputStreamWriter isr2 = new OutputStreamWriter(new FileOutputStream("out.txt") , "GBK");\n\n\n\n# 指定编码写出\n\npublic class OutputDemo {\n    public static void main(String[] args) throws IOException {\n      \t// 定义文件路径\n        String FileName = "E:\\\\out.txt";\n      \t// 创建流对象,默认UTF8编码\n        OutputStreamWriter osw = new OutputStreamWriter(new FileOutputStream(FileName));\n        // 写出数据\n      \tosw.write("你好"); // 保存为6个字节\n        osw.close();\n      \t\n\t\t// 定义文件路径\n\t\tString FileName2 = "E:\\\\out2.txt";\n     \t// 创建流对象,指定GBK编码\n        OutputStreamWriter osw2 = new OutputStreamWriter(new FileOutputStream(FileName2),"GBK");\n        // 写出数据\n      \tosw2.write("你好");// 保存为4个字节\n        osw2.close();\n    }\n}\n\n\n\n# 转换流理解图解\n\n转换流是字节与字符间的桥梁！\n\n\n# 练习：转换文件编码\n\n将GBK编码的文本文件，转换为UTF-8编码的文本文件。\n\n\n# 练习1\n\n 1. 指定GBK编码的转换流，读取文本文件。\n 2. 使用UTF-8编码的转换流，写出文本文件。\n\npublic class TransDemo {\n   public static void main(String[] args) {      \n    \t// 1.定义文件路径\n     \tString srcFile = "file_gbk.txt";\n        String destFile = "file_utf8.txt";\n\t\t// 2.创建流对象\n    \t// 2.1 转换输入流,指定GBK编码\n        InputStreamReader isr = new InputStreamReader(new FileInputStream(srcFile) , "GBK");\n    \t// 2.2 转换输出流,默认utf8编码\n        OutputStreamWriter osw = new OutputStreamWriter(new FileOutputStream(destFile));\n\t\t// 3.读写数据\n    \t// 3.1 定义数组\n        char[] cbuf = new char[1024];\n    \t// 3.2 定义长度\n        int len;\n    \t// 3.3 循环读取\n        while ((len = isr.read(cbuf))!=-1) {\n            // 循环写出\n          \tosw.write(cbuf,0,len);\n        }\n    \t// 4.释放资源\n        osw.close();\n        isr.close();\n  \t}\n}\n\n\n\n# 练习2\n\n利用字节流读取文件中的数据，每次读取一行，而且不能出现乱码\n\n * 问题\n   \n   * 使用字节流读取文件会出现乱码。 -----------\x3e 使用转换流，将字节流转换成字符流\n   * 字节流没有读取一行的方法。 -----------\x3e 将字符流包装成 缓冲字符流，可以使用 里面的 ReadLine() 方法\n\n * java代码\n   \n   /**\n    * @ClassName ConvertStreamDemo1\n    * @Date 2023/3/6 10:26\n    * @Author diane\n    * @Description 转换流；将字节流，转换成字符流 ，使字节流可以使用字符流的方法\n    *\n    *     利用字节流读取文件中的数据，每次读取一行，而且不能出现乱码\n    *\n    * @Version 1.0\n    */\n   public class ConvertStreamDemo4 {\n       public static void main(String[] args) throws IOException {\n   \n           // 将字节流 转换成字符流，可以解决读取中文的乱码问题\n           InputStreamReader isr = new InputStreamReader(new FileInputStream("D:\\\\tmp\\\\csb.txt"));\n           // 将字符流转换成 缓冲字符流；  可以使用里面的读取一行的方法\n           BufferedReader bis = new BufferedReader(isr);\n           String s;\n           while ((s = bis.readLine()) != null) {\n               System.out.println(s);\n           }\n           bis.close();\n   \n       }\n   }\n   \n\n\n# 总结\n\n',normalizedContent:'# 转换流\n\n * idea 默认是 utf-8编码，使用filereader 和 filewriter 都是 utf-8；jdk11以前不能指定文件编码；\n * 所以只能通过转换流，转换文件编码。\n * jdk11 以后，filereader 和 filewriter 都可以指定编码，转换流就没有太大的用处了。。。\n\n\n\n\n# 字符编码和字符集\n\n\n# 字符编码\n\n计算机中储存的信息都是用二进制数表示的，而我们在屏幕上看到的数字、英文、标点符号、汉字等字符是二进制数转换之后的结果。按照某种规则，将字符存储到计算机中，称为编码 。反之，将存储在计算机中的二进制数按照某种规则解析显示出来，称为解码 。比如说，按照a规则存储，同样按照a规则解析，那么就能显示正确的文本符号。反之，按照a规则存储，再按照b规则解析，就会导致乱码现象。\n\n编码:字符(能看懂的)--字节(看不懂的)\n\n解码:字节(看不懂的)--\x3e字符(能看懂的)\n\n * 字符编码character encoding : 就是一套自然语言的字符与二进制数之间的对应规则。\n   \n   编码表:生活中文字和计算机中二进制的对应规则\n\n\n# 字符集\n\n * 字符集 charset：也叫编码表。是一个系统支持的所有字符的集合，包括各国家文字、标点符号、图形符号、数字等。\n\n计算机要准确的存储和识别各种字符集符号，需要进行字符编码，一套字符集必然至少有一套字符编码。常见字符集有ascii字符集、gbk字符集、unicode字符集等。\n\n可见，当指定了编码，它所对应的字符集自然就指定了，所以编码才是我们最终要关心的。\n\n * ascii字符集 ：\n   * ascii（american standard code for information interchange，美国信息交换标准代码）是基于拉丁字母的一套电脑编码系统，用于显示现代英语，主要包括控制字符（回车键、退格、换行键等）和可显示字符（英文大小写字符、阿拉伯数字和西文符号）。\n   * 基本的ascii字符集，使用7位（bits）表示一个字符，共128字符。ascii的扩展字符集使用8位（bits）表示一个字符，共256字符，方便支持欧洲常用字符。\n * iso-8859-1字符集：\n   * 拉丁码表，别名latin-1，用于显示欧洲使用的语言，包括荷兰、丹麦、德语、意大利语、西班牙语等。\n   * iso-8859-1使用单字节编码，兼容ascii编码。\n * gbxxx字符集：\n   * gb就是国标的意思，是为了显示中文而设计的一套字符集。\n   * gb2312：简体中文码表。一个小于127的字符的意义与原来相同。但两个大于127的字符连在一起时，就表示一个汉字，这样大约可以组合了包含7000多个简体汉字，此外数学符号、罗马希腊的字母、日文的假名们都编进去了，连在ascii里本来就有的数字、标点、字母都统统重新编了两个字节长的编码，这就是常说的"全角"字符，而原来在127号以下的那些就叫"半角"字符了。\n   * gbk：最常用的中文码表。是在gb2312标准基础上的扩展规范，使用了双字节编码方案，共收录了21003个汉字，完全兼容gb2312标准，同时支持繁体汉字以及日韩汉字等。\n   * gb18030：最新的中文码表。收录汉字70244个，采用多字节编码，每个字可以由1个、2个或4个字节组成。支持中国国内少数民族的文字，同时支持繁体汉字以及日韩汉字等。\n * unicode字符集 ：\n   * unicode编码系统为表达任意语言的任意字符而设计，是业界的一种标准，也称为统一码、标准万国码。\n   * 它最多使用4个字节的数字来表达每个字母、符号，或者文字。有三种编码方案，utf-8、utf-16和utf-32。最为常用的utf-8编码。\n   * utf-8编码，可以用来表示unicode标准中任何字符，它是电子邮件、网页及其他存储或传送文字的应用中，优先采用的编码。互联网工程工作小组（ietf）要求所有互联网协议都必须支持utf-8编码。所以，我们开发web应用，也要使用utf-8编码。它使用一至四个字节为每个字符编码，编码规则：\n     1. 128个us-ascii字符，只需一个字节编码。\n     2. 拉丁文等字符，需要二个字节编码。\n     3. 大部分常用字（含中文），使用三个字节编码。\n     4. 其他极少使用的unicode辅助字符，使用四字节编码。\n\n\n# 编码引出的问题\n\n在idea中，使用filereader 读取项目中的文本文件。由于idea的设置，都是默认的utf-8编码，所以没有任何问题。但是，当读取windows系统中创建的文本文件时，由于windows系统的默认是gbk编码，就会出现乱码。\n\npublic class readerdemo {\n    public static void main(string[] args) throws ioexception {\n        filereader filereader = new filereader("e:\\\\file_gbk.txt");\n        int read;\n        while ((read = filereader.read()) != -1) {\n            system.out.print((char)read);\n        }\n        filereader.close();\n    }\n}\n输出结果：\n���\n\n\n那么如何读取gbk编码的文件呢？\n\n\n# inputstreamreader类\n\n转换流java.io.inputstreamreader，是reader的子类，是从字节流到字符流的桥梁。它读取字节，并使用指定的字符集将其解码为字符。它的字符集可以由名称指定，也可以接受平台的默认字符集。\n\n\n# 构造方法\n\n * inputstreamreader(inputstream in): 创建一个使用默认字符集的字符流。\n * inputstreamreader(inputstream in, string charsetname): 创建一个指定字符集的字符流。\n\n构造举例，代码如下：\n\ninputstreamreader isr = new inputstreamreader(new fileinputstream("in.txt"));\ninputstreamreader isr2 = new inputstreamreader(new fileinputstream("in.txt") , "gbk");\n\n\n\n# 指定编码读取\n\npublic class readerdemo2 {\n    public static void main(string[] args) throws ioexception {\n      \t// 定义文件路径,文件为gbk编码\n        string filename = "e:\\\\file_gbk.txt";\n      \t// 创建流对象,默认utf8编码\n        inputstreamreader isr = new inputstreamreader(new fileinputstream(filename));\n      \t// 创建流对象,指定gbk编码\n        inputstreamreader isr2 = new inputstreamreader(new fileinputstream(filename) , "gbk");\n\t\t// 定义变量,保存字符\n        int read;\n      \t// 使用默认编码字符流读取,乱码\n        while ((read = isr.read()) != -1) {\n            system.out.print((char)read); // ��һ�\n        }\n        isr.close();\n      \n      \t// 使用指定编码字符流读取,正常解析\n        while ((read = isr2.read()) != -1) {\n            system.out.print((char)read);// 大家好\n        }\n        isr2.close();\n    }\n}\n\n\n\n# outputstreamwriter类\n\n转换流java.io.outputstreamwriter ，是writer的子类，是从字符流到字节流的桥梁。使用指定的字符集将字符编码为字节。它的字符集可以由名称指定，也可以接受平台的默认字符集。\n\n\n# 构造方法\n\n * outputstreamwriter(outputstream in): 创建一个使用默认字符集的字符流。\n * outputstreamwriter(outputstream in, string charsetname): 创建一个指定字符集的字符流。\n\n构造举例，代码如下：\n\noutputstreamwriter isr = new outputstreamwriter(new fileoutputstream("out.txt"));\noutputstreamwriter isr2 = new outputstreamwriter(new fileoutputstream("out.txt") , "gbk");\n\n\n\n# 指定编码写出\n\npublic class outputdemo {\n    public static void main(string[] args) throws ioexception {\n      \t// 定义文件路径\n        string filename = "e:\\\\out.txt";\n      \t// 创建流对象,默认utf8编码\n        outputstreamwriter osw = new outputstreamwriter(new fileoutputstream(filename));\n        // 写出数据\n      \tosw.write("你好"); // 保存为6个字节\n        osw.close();\n      \t\n\t\t// 定义文件路径\n\t\tstring filename2 = "e:\\\\out2.txt";\n     \t// 创建流对象,指定gbk编码\n        outputstreamwriter osw2 = new outputstreamwriter(new fileoutputstream(filename2),"gbk");\n        // 写出数据\n      \tosw2.write("你好");// 保存为4个字节\n        osw2.close();\n    }\n}\n\n\n\n# 转换流理解图解\n\n转换流是字节与字符间的桥梁！\n\n\n# 练习：转换文件编码\n\n将gbk编码的文本文件，转换为utf-8编码的文本文件。\n\n\n# 练习1\n\n 1. 指定gbk编码的转换流，读取文本文件。\n 2. 使用utf-8编码的转换流，写出文本文件。\n\npublic class transdemo {\n   public static void main(string[] args) {      \n    \t// 1.定义文件路径\n     \tstring srcfile = "file_gbk.txt";\n        string destfile = "file_utf8.txt";\n\t\t// 2.创建流对象\n    \t// 2.1 转换输入流,指定gbk编码\n        inputstreamreader isr = new inputstreamreader(new fileinputstream(srcfile) , "gbk");\n    \t// 2.2 转换输出流,默认utf8编码\n        outputstreamwriter osw = new outputstreamwriter(new fileoutputstream(destfile));\n\t\t// 3.读写数据\n    \t// 3.1 定义数组\n        char[] cbuf = new char[1024];\n    \t// 3.2 定义长度\n        int len;\n    \t// 3.3 循环读取\n        while ((len = isr.read(cbuf))!=-1) {\n            // 循环写出\n          \tosw.write(cbuf,0,len);\n        }\n    \t// 4.释放资源\n        osw.close();\n        isr.close();\n  \t}\n}\n\n\n\n# 练习2\n\n利用字节流读取文件中的数据，每次读取一行，而且不能出现乱码\n\n * 问题\n   \n   * 使用字节流读取文件会出现乱码。 -----------\x3e 使用转换流，将字节流转换成字符流\n   * 字节流没有读取一行的方法。 -----------\x3e 将字符流包装成 缓冲字符流，可以使用 里面的 readline() 方法\n\n * java代码\n   \n   /**\n    * @classname convertstreamdemo1\n    * @date 2023/3/6 10:26\n    * @author diane\n    * @description 转换流；将字节流，转换成字符流 ，使字节流可以使用字符流的方法\n    *\n    *     利用字节流读取文件中的数据，每次读取一行，而且不能出现乱码\n    *\n    * @version 1.0\n    */\n   public class convertstreamdemo4 {\n       public static void main(string[] args) throws ioexception {\n   \n           // 将字节流 转换成字符流，可以解决读取中文的乱码问题\n           inputstreamreader isr = new inputstreamreader(new fileinputstream("d:\\\\tmp\\\\csb.txt"));\n           // 将字符流转换成 缓冲字符流；  可以使用里面的读取一行的方法\n           bufferedreader bis = new bufferedreader(isr);\n           string s;\n           while ((s = bis.readline()) != null) {\n               system.out.println(s);\n           }\n           bis.close();\n   \n       }\n   }\n   \n\n\n# 总结\n\n',charsets:{cyrillic:!0,cjk:!0}},{title:"IO异常处理",frontmatter:{autoSort:94,title:"IO异常处理",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/2ca28f/",categories:["后端","Java","流"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/60.%E6%B5%81/35.IO%E6%B5%81%E5%BC%82%E5%B8%B8.html",relativePath:"01.后端/10.Java/60.流/35.IO流异常.md",key:"v-9882acb6",path:"/pages/2ca28f/",headers:[{level:2,title:"JDK7前处理",slug:"jdk7前处理",normalizedTitle:"jdk7前处理",charIndex:14},{level:2,title:"JDK7的处理(扩展知识点了解内容)",slug:"jdk7的处理-扩展知识点了解内容",normalizedTitle:"jdk7的处理(扩展知识点了解内容)",charIndex:660},{level:2,title:"JDK9的改进(扩展知识点了解内容)",slug:"jdk9的改进-扩展知识点了解内容",normalizedTitle:"jdk9的改进(扩展知识点了解内容)",charIndex:1173}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"JDK7前处理 JDK7的处理(扩展知识点了解内容) JDK9的改进(扩展知识点了解内容)",content:'# IO异常的处理\n\n\n# JDK7前处理\n\n之前的入门练习，我们一直把异常抛出，而实际开发中并不能这样处理，建议使用try...catch...finally 代码块，处理异常部分，代码使用演示：\n\npublic class HandleException1 {\n    public static void main(String[] args) {\n      \t// 声明变量\n        FileWriter fw = null;\n        try {\n            //创建流对象\n            fw = new FileWriter("fw.txt");\n            // 写出数据\n            fw.write("黑马程序员"); //黑马程序员\n        } catch (IOException e) {\n            e.printStackTrace();\n        } finally {\n            try {\n                if (fw != null) {\n                    fw.close();\n                }\n            } catch (IOException e) {\n                e.printStackTrace();\n            }\n        }\n    }\n}\n\n\n\n# JDK7的处理(扩展知识点了解内容)\n\n还可以使用JDK7优化后的try-with-resource 语句，该语句确保了每个资源在语句结束时关闭。所谓的资源（resource）是指在程序完成后，必须关闭的对象。\n\n格式：\n\ntry (创建流对象语句，如果多个,使用\';\'隔开) {\n\t// 读写数据\n} catch (IOException e) {\n\te.printStackTrace();\n}\n\n\n代码使用演示：\n\npublic class HandleException2 {\n    public static void main(String[] args) {\n      \t// 创建流对象\n        try ( FileWriter fw = new FileWriter("fw.txt"); ) {\n            // 写出数据\n            fw.write("黑马程序员"); //黑马程序员\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n}\n\n\n\n# JDK9的改进(扩展知识点了解内容)\n\nJDK9中try-with-resource 的改进，对于引入对象的方式，支持的更加简洁。被引入的对象，同样可以自动关闭，无需手动close，我们来了解一下格式。\n\n改进前格式：\n\n// 被final修饰的对象\nfinal Resource resource1 = new Resource("resource1");\n// 普通对象\nResource resource2 = new Resource("resource2");\n// 引入方式：创建新的变量保存\ntry (Resource r1 = resource1;\n     Resource r2 = resource2) {\n     // 使用对象\n}\n\n\n改进后格式：\n\n// 被final修饰的对象\nfinal Resource resource1 = new Resource("resource1");\n// 普通对象\nResource resource2 = new Resource("resource2");\n\n// 引入方式：直接引入\ntry (resource1; resource2) {\n     // 使用对象\n}\n\n\n改进后，代码使用演示：\n\npublic class TryDemo {\n    public static void main(String[] args) throws IOException {\n       \t// 创建流对象\n        final  FileReader fr  = new FileReader("in.txt");\n        FileWriter fw = new FileWriter("out.txt");\n       \t// 引入到try中\n        try (fr; fw) {\n          \t// 定义变量\n            int b;\n          \t// 读取数据\n          \twhile ((b = fr.read())!=-1) {\n            \t// 写出数据\n            \tfw.write(b);\n          \t}\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n}\n',normalizedContent:'# io异常的处理\n\n\n# jdk7前处理\n\n之前的入门练习，我们一直把异常抛出，而实际开发中并不能这样处理，建议使用try...catch...finally 代码块，处理异常部分，代码使用演示：\n\npublic class handleexception1 {\n    public static void main(string[] args) {\n      \t// 声明变量\n        filewriter fw = null;\n        try {\n            //创建流对象\n            fw = new filewriter("fw.txt");\n            // 写出数据\n            fw.write("黑马程序员"); //黑马程序员\n        } catch (ioexception e) {\n            e.printstacktrace();\n        } finally {\n            try {\n                if (fw != null) {\n                    fw.close();\n                }\n            } catch (ioexception e) {\n                e.printstacktrace();\n            }\n        }\n    }\n}\n\n\n\n# jdk7的处理(扩展知识点了解内容)\n\n还可以使用jdk7优化后的try-with-resource 语句，该语句确保了每个资源在语句结束时关闭。所谓的资源（resource）是指在程序完成后，必须关闭的对象。\n\n格式：\n\ntry (创建流对象语句，如果多个,使用\';\'隔开) {\n\t// 读写数据\n} catch (ioexception e) {\n\te.printstacktrace();\n}\n\n\n代码使用演示：\n\npublic class handleexception2 {\n    public static void main(string[] args) {\n      \t// 创建流对象\n        try ( filewriter fw = new filewriter("fw.txt"); ) {\n            // 写出数据\n            fw.write("黑马程序员"); //黑马程序员\n        } catch (ioexception e) {\n            e.printstacktrace();\n        }\n    }\n}\n\n\n\n# jdk9的改进(扩展知识点了解内容)\n\njdk9中try-with-resource 的改进，对于引入对象的方式，支持的更加简洁。被引入的对象，同样可以自动关闭，无需手动close，我们来了解一下格式。\n\n改进前格式：\n\n// 被final修饰的对象\nfinal resource resource1 = new resource("resource1");\n// 普通对象\nresource resource2 = new resource("resource2");\n// 引入方式：创建新的变量保存\ntry (resource r1 = resource1;\n     resource r2 = resource2) {\n     // 使用对象\n}\n\n\n改进后格式：\n\n// 被final修饰的对象\nfinal resource resource1 = new resource("resource1");\n// 普通对象\nresource resource2 = new resource("resource2");\n\n// 引入方式：直接引入\ntry (resource1; resource2) {\n     // 使用对象\n}\n\n\n改进后，代码使用演示：\n\npublic class trydemo {\n    public static void main(string[] args) throws ioexception {\n       \t// 创建流对象\n        final  filereader fr  = new filereader("in.txt");\n        filewriter fw = new filewriter("out.txt");\n       \t// 引入到try中\n        try (fr; fw) {\n          \t// 定义变量\n            int b;\n          \t// 读取数据\n          \twhile ((b = fr.read())!=-1) {\n            \t// 写出数据\n            \tfw.write(b);\n          \t}\n        } catch (ioexception e) {\n            e.printstacktrace();\n        }\n    }\n}\n',charsets:{cjk:!0}},{title:"序列流",frontmatter:{autoSort:90,title:"序列流",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/57821f/",categories:["后端","Java","流"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/60.%E6%B5%81/49.%E5%BA%8F%E5%88%97%E6%B5%81.html",relativePath:"01.后端/10.Java/60.流/49.序列流.md",key:"v-05b14bdd",path:"/pages/57821f/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:10},{level:2,title:"ObjectOutputStream类",slug:"objectoutputstream类",normalizedTitle:"objectoutputstream类",charIndex:202},{level:3,title:"构造方法",slug:"构造方法",normalizedTitle:"构造方法",charIndex:287},{level:3,title:"序列化操作",slug:"序列化操作",normalizedTitle:"序列化操作",charIndex:521},{level:2,title:"ObjectInputStream类",slug:"objectinputstream类",normalizedTitle:"objectinputstream类",charIndex:1704},{level:3,title:"构造方法",slug:"构造方法-2",normalizedTitle:"构造方法",charIndex:287},{level:3,title:"反序列化操作1",slug:"反序列化操作1",normalizedTitle:"反序列化操作1",charIndex:1881},{level:3,title:"反序列化操作2",slug:"反序列化操作2",normalizedTitle:"反序列化操作2",charIndex:3021},{level:2,title:"序列化流的细节",slug:"序列化流的细节",normalizedTitle:"序列化流的细节",charIndex:3860},{level:2,title:"练习：序列化集合",slug:"练习-序列化集合",normalizedTitle:"练习：序列化集合",charIndex:4082},{level:3,title:"案例分析",slug:"案例分析",normalizedTitle:"案例分析",charIndex:4267},{level:3,title:"案例实现",slug:"案例实现",normalizedTitle:"案例实现",charIndex:4360}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"概述 ObjectOutputStream类 构造方法 序列化操作 ObjectInputStream类 构造方法 反序列化操作1 反序列化操作2 序列化流的细节 练习：序列化集合 案例分析 案例实现",content:'# 序列流\n\n\n# 概述\n\nJava 提供了一种对象序列化****的机制。用一个字节序列可以表示一个对象，该字节序列包含该对象的数据、对象的类型和对象中存储的属性等信息。字节序列写出到文件之后，相当于文件中持久保存了一个对象的信息。\n\n反之，该字节序列还可以从文件中读取回来，重构对象，对它进行反序列化。对象的数据、对象的类型和对象中存储的数据信息，都可以用来在内存中创建对象。看图理解序列化：\n\n\n# ObjectOutputStream类\n\njava.io.ObjectOutputStream 类，将Java对象的原始数据类型写出到文件,实现对象的持久存储。\n\n\n# 构造方法\n\n * public ObjectOutputStream(OutputStream out)： 创建一个指定OutputStream的ObjectOutputStream。\n\n构造举例，代码如下：\n\nFileOutputStream fileOut = new FileOutputStream("employee.txt");\nObjectOutputStream out = new ObjectOutputStream(fileOut);\n\n\n\n# 序列化操作\n\n 1. 一个对象要想序列化，必须满足两个条件:\n\n * 该类必须实现java.io.Serializable 接口，Serializable 是一个标记接口，不实现此接口的类将不会使任何状态序列化或反序列化，会抛出NotSerializableException 。\n * 该类的所有属性必须是可序列化的。如果有一个属性不需要可序列化的，则该属性必须注明是瞬态的，使用transient 关键字修饰。\n\npublic class Employee implements java.io.Serializable {\n    public String name;\n    public String address;\n    public transient int age; // transient瞬态修饰成员,不会被序列化\n    public void addressCheck() {\n      \tSystem.out.println("Address  check : " + name + " -- " + address);\n    }\n}\n\n\n2.写出对象方法\n\n * public final void writeObject (Object obj) : 将指定的对象写出。\n\npublic class SerializeDemo{\n   \tpublic static void main(String [] args)   {\n    \tEmployee e = new Employee();\n    \te.name = "zhangsan";\n    \te.address = "beiqinglu";\n    \te.age = 20; \n    \ttry {\n      \t\t// 创建序列化流对象\n          ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream("employee.txt"));\n        \t// 写出对象\n        \tout.writeObject(e);\n        \t// 释放资源\n        \tout.close();\n        \tfileOut.close();\n        \tSystem.out.println("Serialized data is saved"); // 姓名，地址被序列化，年龄没有被序列化。\n        } catch(IOException i)   {\n            i.printStackTrace();\n        }\n   \t}\n}\n输出结果：\nSerialized data is saved\n\n\n\n# ObjectInputStream类\n\nObjectInputStream反序列化流，将之前使用ObjectOutputStream序列化的原始数据恢复为对象。\n\n\n# 构造方法\n\n * public ObjectInputStream(InputStream in)： 创建一个指定InputStream的ObjectInputStream。\n\n\n# 反序列化操作1\n\n如果能找到一个对象的class文件，我们可以进行反序列化操作，调用ObjectInputStream读取对象的方法：\n\n * public final Object readObject () : 读取一个对象。\n\npublic class DeserializeDemo {\n   public static void main(String [] args)   {\n        Employee e = null;\n        try {\t\t\n             // 创建反序列化流\n             FileInputStream fileIn = new FileInputStream("employee.txt");\n             ObjectInputStream in = new ObjectInputStream(fileIn);\n             // 读取一个对象\n             e = (Employee) in.readObject();\n             // 释放资源\n             in.close();\n             fileIn.close();\n        }catch(IOException i) {\n             // 捕获其他异常\n             i.printStackTrace();\n             return;\n        }catch(ClassNotFoundException c)  {\n        \t// 捕获类找不到异常\n             System.out.println("Employee class not found");\n             c.printStackTrace();\n             return;\n        }\n        // 无异常,直接打印输出\n        System.out.println("Name: " + e.name);\t// zhangsan\n        System.out.println("Address: " + e.address); // beiqinglu\n        System.out.println("age: " + e.age); // 0\n    }\n}\n\n\n对于JVM可以反序列化对象，它必须是能够找到class文件的类。如果找不到该类的class文件，则抛出一个 ClassNotFoundException 异常。\n\n\n# 反序列化操作2\n\n * 固定版本号，这样对象进行序列化存储的时候，不会生成新的版本号。\n * 这样做的好处是，当类的某些字段发生变化时，不会改动其版本号，也不会影响已经存储的对象的反序列读取。\n * 如果定义类的时候，只实现了 Serializable接口，没有定义版本号，那么每次对类的字段修改时，都会产生新的版本号，不利于后续的改动和维护。\n\n**另外，当JVM反序列化对象时，能找到class文件，但是class文件在序列化对象之后发生了修改，那么反序列化操作也会失败，抛出一个InvalidClassException异常。**发生这个异常的原因如下：\n\n * 该类的序列版本号与从流中读取的类描述符的版本号不匹配\n * 该类包含未知数据类型\n * 该类没有可访问的无参数构造方法\n\nSerializable 接口给需要序列化的类，提供了一个序列版本号。\n\nserialVersionUID 该版本号的目的在于验证序列化的对象和对应类是否版本匹配。\n\n我们可以通过 固定化 版本号的方式，来保证对类的修改，不会产生前后版本号不一致的情况\n\npublic class Employee implements java.io.Serializable {\n     // 加入序列版本号\n     private static final long serialVersionUID = 1L;\n     public String name;\n     public String address;\n     // 添加新的属性 ,重新编译, 可以反序列化,该属性赋为默认值.\n     public int eid; \n\n     public void addressCheck() {\n         System.out.println("Address  check : " + name + " -- " + address);\n     }\n}\n\n\n\n# 序列化流的细节\n\n * 实现 Serializable 接口，才可以使用序列化输出流，将对象序列化到文件中\n * 序列后的文件，不能修改，修改了就改不了了。\n * 添加固定的序列号，可以解决类修改后，读取不了的原先存储的对象的问题。\n   * 因为，对象字段修改后，如果不加固定的序列号，java会自动生成一个序列号；与不修改前的肯定不一样，这样会造成读取问题\n * 对象的成员变量，不想被序列化，需要加关键字 transient\n\n\n\n\n# 练习：序列化集合\n\n * 当一次性序列化多个对象时，不要随便写；要将其放到List集合中，然后在写\n * 这样方便对象的反序列化；\n * 反序列化的读取，读到文件末尾时，不会返回-1，null这种，而是直接会抛异常。\n\n 1. 将存有多个自定义对象的集合序列化操作，保存到list.txt文件中。\n 2. 反序列化list.txt ，并遍历集合，打印对象信息。\n\n\n# 案例分析\n\n 1. 把若干学生对象 ，保存到集合中。\n 2. 把集合序列化。\n 3. 反序列化读取时，只需要读取一次，转换为集合类型。\n 4. 遍历集合，可以打印所有的学生信息\n\n\n# 案例实现\n\npublic class SerTest {\n\tpublic static void main(String[] args) throws Exception {\n\t\t// 创建 学生对象\n\t\tStudent student = new Student("老王", "laow");\n\t\tStudent student2 = new Student("老张", "laoz");\n\t\tStudent student3 = new Student("老李", "laol");\n\n\t\tArrayList<Student> arrayList = new ArrayList<>();\n\t\tarrayList.add(student);\n\t\tarrayList.add(student2);\n\t\tarrayList.add(student3);\n\t\t// 序列化操作\n\t\t// serializ(arrayList);\n\t\t\n\t\t// 反序列化  \n\t\tObjectInputStream ois  = new ObjectInputStream(new FileInputStream("list.txt"));\n\t\t// 读取对象,强转为ArrayList类型\n\t\tArrayList<Student> list  = (ArrayList<Student>)ois.readObject();\n\t\t\n      \tfor (int i = 0; i < list.size(); i++ ){\n          \tStudent s = list.get(i);\n        \tSystem.out.println(s.getName()+"--"+ s.getPwd());\n      \t}\n\t}\n\n\tprivate static void serializ(ArrayList<Student> arrayList) throws Exception {\n\t\t// 创建 序列化流 \n\t\tObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream("list.txt"));\n\t\t// 写出对象\n\t\toos.writeObject(arrayList);\n\t\t// 释放资源\n\t\toos.close();\n\t}\n}\n',normalizedContent:'# 序列流\n\n\n# 概述\n\njava 提供了一种对象序列化****的机制。用一个字节序列可以表示一个对象，该字节序列包含该对象的数据、对象的类型和对象中存储的属性等信息。字节序列写出到文件之后，相当于文件中持久保存了一个对象的信息。\n\n反之，该字节序列还可以从文件中读取回来，重构对象，对它进行反序列化。对象的数据、对象的类型和对象中存储的数据信息，都可以用来在内存中创建对象。看图理解序列化：\n\n\n# objectoutputstream类\n\njava.io.objectoutputstream 类，将java对象的原始数据类型写出到文件,实现对象的持久存储。\n\n\n# 构造方法\n\n * public objectoutputstream(outputstream out)： 创建一个指定outputstream的objectoutputstream。\n\n构造举例，代码如下：\n\nfileoutputstream fileout = new fileoutputstream("employee.txt");\nobjectoutputstream out = new objectoutputstream(fileout);\n\n\n\n# 序列化操作\n\n 1. 一个对象要想序列化，必须满足两个条件:\n\n * 该类必须实现java.io.serializable 接口，serializable 是一个标记接口，不实现此接口的类将不会使任何状态序列化或反序列化，会抛出notserializableexception 。\n * 该类的所有属性必须是可序列化的。如果有一个属性不需要可序列化的，则该属性必须注明是瞬态的，使用transient 关键字修饰。\n\npublic class employee implements java.io.serializable {\n    public string name;\n    public string address;\n    public transient int age; // transient瞬态修饰成员,不会被序列化\n    public void addresscheck() {\n      \tsystem.out.println("address  check : " + name + " -- " + address);\n    }\n}\n\n\n2.写出对象方法\n\n * public final void writeobject (object obj) : 将指定的对象写出。\n\npublic class serializedemo{\n   \tpublic static void main(string [] args)   {\n    \temployee e = new employee();\n    \te.name = "zhangsan";\n    \te.address = "beiqinglu";\n    \te.age = 20; \n    \ttry {\n      \t\t// 创建序列化流对象\n          objectoutputstream out = new objectoutputstream(new fileoutputstream("employee.txt"));\n        \t// 写出对象\n        \tout.writeobject(e);\n        \t// 释放资源\n        \tout.close();\n        \tfileout.close();\n        \tsystem.out.println("serialized data is saved"); // 姓名，地址被序列化，年龄没有被序列化。\n        } catch(ioexception i)   {\n            i.printstacktrace();\n        }\n   \t}\n}\n输出结果：\nserialized data is saved\n\n\n\n# objectinputstream类\n\nobjectinputstream反序列化流，将之前使用objectoutputstream序列化的原始数据恢复为对象。\n\n\n# 构造方法\n\n * public objectinputstream(inputstream in)： 创建一个指定inputstream的objectinputstream。\n\n\n# 反序列化操作1\n\n如果能找到一个对象的class文件，我们可以进行反序列化操作，调用objectinputstream读取对象的方法：\n\n * public final object readobject () : 读取一个对象。\n\npublic class deserializedemo {\n   public static void main(string [] args)   {\n        employee e = null;\n        try {\t\t\n             // 创建反序列化流\n             fileinputstream filein = new fileinputstream("employee.txt");\n             objectinputstream in = new objectinputstream(filein);\n             // 读取一个对象\n             e = (employee) in.readobject();\n             // 释放资源\n             in.close();\n             filein.close();\n        }catch(ioexception i) {\n             // 捕获其他异常\n             i.printstacktrace();\n             return;\n        }catch(classnotfoundexception c)  {\n        \t// 捕获类找不到异常\n             system.out.println("employee class not found");\n             c.printstacktrace();\n             return;\n        }\n        // 无异常,直接打印输出\n        system.out.println("name: " + e.name);\t// zhangsan\n        system.out.println("address: " + e.address); // beiqinglu\n        system.out.println("age: " + e.age); // 0\n    }\n}\n\n\n对于jvm可以反序列化对象，它必须是能够找到class文件的类。如果找不到该类的class文件，则抛出一个 classnotfoundexception 异常。\n\n\n# 反序列化操作2\n\n * 固定版本号，这样对象进行序列化存储的时候，不会生成新的版本号。\n * 这样做的好处是，当类的某些字段发生变化时，不会改动其版本号，也不会影响已经存储的对象的反序列读取。\n * 如果定义类的时候，只实现了 serializable接口，没有定义版本号，那么每次对类的字段修改时，都会产生新的版本号，不利于后续的改动和维护。\n\n**另外，当jvm反序列化对象时，能找到class文件，但是class文件在序列化对象之后发生了修改，那么反序列化操作也会失败，抛出一个invalidclassexception异常。**发生这个异常的原因如下：\n\n * 该类的序列版本号与从流中读取的类描述符的版本号不匹配\n * 该类包含未知数据类型\n * 该类没有可访问的无参数构造方法\n\nserializable 接口给需要序列化的类，提供了一个序列版本号。\n\nserialversionuid 该版本号的目的在于验证序列化的对象和对应类是否版本匹配。\n\n我们可以通过 固定化 版本号的方式，来保证对类的修改，不会产生前后版本号不一致的情况\n\npublic class employee implements java.io.serializable {\n     // 加入序列版本号\n     private static final long serialversionuid = 1l;\n     public string name;\n     public string address;\n     // 添加新的属性 ,重新编译, 可以反序列化,该属性赋为默认值.\n     public int eid; \n\n     public void addresscheck() {\n         system.out.println("address  check : " + name + " -- " + address);\n     }\n}\n\n\n\n# 序列化流的细节\n\n * 实现 serializable 接口，才可以使用序列化输出流，将对象序列化到文件中\n * 序列后的文件，不能修改，修改了就改不了了。\n * 添加固定的序列号，可以解决类修改后，读取不了的原先存储的对象的问题。\n   * 因为，对象字段修改后，如果不加固定的序列号，java会自动生成一个序列号；与不修改前的肯定不一样，这样会造成读取问题\n * 对象的成员变量，不想被序列化，需要加关键字 transient\n\n\n\n\n# 练习：序列化集合\n\n * 当一次性序列化多个对象时，不要随便写；要将其放到list集合中，然后在写\n * 这样方便对象的反序列化；\n * 反序列化的读取，读到文件末尾时，不会返回-1，null这种，而是直接会抛异常。\n\n 1. 将存有多个自定义对象的集合序列化操作，保存到list.txt文件中。\n 2. 反序列化list.txt ，并遍历集合，打印对象信息。\n\n\n# 案例分析\n\n 1. 把若干学生对象 ，保存到集合中。\n 2. 把集合序列化。\n 3. 反序列化读取时，只需要读取一次，转换为集合类型。\n 4. 遍历集合，可以打印所有的学生信息\n\n\n# 案例实现\n\npublic class sertest {\n\tpublic static void main(string[] args) throws exception {\n\t\t// 创建 学生对象\n\t\tstudent student = new student("老王", "laow");\n\t\tstudent student2 = new student("老张", "laoz");\n\t\tstudent student3 = new student("老李", "laol");\n\n\t\tarraylist<student> arraylist = new arraylist<>();\n\t\tarraylist.add(student);\n\t\tarraylist.add(student2);\n\t\tarraylist.add(student3);\n\t\t// 序列化操作\n\t\t// serializ(arraylist);\n\t\t\n\t\t// 反序列化  \n\t\tobjectinputstream ois  = new objectinputstream(new fileinputstream("list.txt"));\n\t\t// 读取对象,强转为arraylist类型\n\t\tarraylist<student> list  = (arraylist<student>)ois.readobject();\n\t\t\n      \tfor (int i = 0; i < list.size(); i++ ){\n          \tstudent s = list.get(i);\n        \tsystem.out.println(s.getname()+"--"+ s.getpwd());\n      \t}\n\t}\n\n\tprivate static void serializ(arraylist<student> arraylist) throws exception {\n\t\t// 创建 序列化流 \n\t\tobjectoutputstream oos = new objectoutputstream(new fileoutputstream("list.txt"));\n\t\t// 写出对象\n\t\toos.writeobject(arraylist);\n\t\t// 释放资源\n\t\toos.close();\n\t}\n}\n',charsets:{cjk:!0}},{title:"打印流",frontmatter:{autoSort:89,title:"打印流",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/a9cd19/",categories:["后端","Java","流"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/60.%E6%B5%81/51.%E6%89%93%E5%8D%B0%E6%B5%81.html",relativePath:"01.后端/10.Java/60.流/51.打印流.md",key:"v-5d250c21",path:"/pages/a9cd19/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:17},{level:2,title:"PrintStream类",slug:"printstream类",normalizedTitle:"printstream类",charIndex:75},{level:3,title:"构造方法",slug:"构造方法",normalizedTitle:"构造方法",charIndex:148},{level:3,title:"成员方法",slug:"成员方法",normalizedTitle:"成员方法",charIndex:288},{level:3,title:"改变打印流向",slug:"改变打印流向",normalizedTitle:"改变打印流向",charIndex:299},{level:2,title:"PrintWriter",slug:"printwriter",normalizedTitle:"printwriter",charIndex:754},{level:3,title:"构造方法",slug:"构造方法-2",normalizedTitle:"构造方法",charIndex:148},{level:3,title:"成员方法",slug:"成员方法-2",normalizedTitle:"成员方法",charIndex:288}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"概述 PrintStream类 构造方法 成员方法 改变打印流向 PrintWriter 构造方法 成员方法",content:'# 打印流-- 单身哦！\n\n\n# 概述\n\n平时我们在控制台打印输出，是调用print方法和println方法完成的，这两个方法都来自于java.io.PrintStream类，该类能够方便地打印各种数据类型的值，是一种便捷的输出方式。\n\n\n\n\n# PrintStream类\n\n字节打印流\n\n\n# 构造方法\n\n * public PrintStream(String/File/OutputStream)： 使用指定的文件名创建一个新的打印流。\n * \n\n构造举例，代码如下：\n\nPrintStream ps = new PrintStream("ps.txt");\n\n\n\n# 成员方法\n\n\n\n\n# 改变打印流向\n\nSystem.out就是PrintStream类型的，只不过它的流向是系统规定的，打印在控制台上。不过，既然是流对象，我们就可以玩一个"小把戏"，改变它的流向。\n\npublic class PrintDemo {\n    public static void main(String[] args) throws IOException {\n\t\t// 调用系统的打印流,控制台直接输出97\n        System.out.println(97);\n      \n\t\t// 创建打印流,指定文件的名称\n        PrintStream ps = new PrintStream("ps.txt");\n      \t\n      \t// 设置系统的打印流流向,输出到ps.txt\n        System.setOut(ps);\n      \t// 调用系统的打印流,ps.txt中输出97\n        System.out.println(97);\n    }\n}\n\n\n\n# PrintWriter\n\n字符打印流\n\n\n# 构造方法\n\n\n\n\n# 成员方法\n\n和字节打印流 基本一样；\n\n就是多了一个 自动刷新。因为字符流底层是带缓冲区的，所以要考虑自动刷新问题',normalizedContent:'# 打印流-- 单身哦！\n\n\n# 概述\n\n平时我们在控制台打印输出，是调用print方法和println方法完成的，这两个方法都来自于java.io.printstream类，该类能够方便地打印各种数据类型的值，是一种便捷的输出方式。\n\n\n\n\n# printstream类\n\n字节打印流\n\n\n# 构造方法\n\n * public printstream(string/file/outputstream)： 使用指定的文件名创建一个新的打印流。\n * \n\n构造举例，代码如下：\n\nprintstream ps = new printstream("ps.txt");\n\n\n\n# 成员方法\n\n\n\n\n# 改变打印流向\n\nsystem.out就是printstream类型的，只不过它的流向是系统规定的，打印在控制台上。不过，既然是流对象，我们就可以玩一个"小把戏"，改变它的流向。\n\npublic class printdemo {\n    public static void main(string[] args) throws ioexception {\n\t\t// 调用系统的打印流,控制台直接输出97\n        system.out.println(97);\n      \n\t\t// 创建打印流,指定文件的名称\n        printstream ps = new printstream("ps.txt");\n      \t\n      \t// 设置系统的打印流流向,输出到ps.txt\n        system.setout(ps);\n      \t// 调用系统的打印流,ps.txt中输出97\n        system.out.println(97);\n    }\n}\n\n\n\n# printwriter\n\n字符打印流\n\n\n# 构造方法\n\n\n\n\n# 成员方法\n\n和字节打印流 基本一样；\n\n就是多了一个 自动刷新。因为字符流底层是带缓冲区的，所以要考虑自动刷新问题',charsets:{cjk:!0}},{title:"压缩流",frontmatter:{autoSort:88,title:"压缩流",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/6201c3/",categories:["后端","Java","流"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/60.%E6%B5%81/53.%E5%8E%8B%E7%BC%A9%E6%B5%81.html",relativePath:"01.后端/10.Java/60.流/53.压缩流.md",key:"v-d4cda7d8",path:"/pages/6201c3/",headers:[{level:2,title:"解压缩流",slug:"解压缩流",normalizedTitle:"解压缩流",charIndex:6},{level:2,title:"压缩流",slug:"压缩流",normalizedTitle:"压缩流",charIndex:2},{level:3,title:"压缩单个文件",slug:"压缩单个文件",normalizedTitle:"压缩单个文件",charIndex:1666},{level:3,title:"压缩整个文件夹--1",slug:"压缩整个文件夹-1",normalizedTitle:"压缩整个文件夹--1",charIndex:2745},{level:3,title:"压缩整个文件夹--2",slug:"压缩整个文件夹-2",normalizedTitle:"压缩整个文件夹--2",charIndex:4466}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"解压缩流 压缩流 压缩单个文件 压缩整个文件夹--1 压缩整个文件夹--2",content:'# 压缩流和解压缩流\n\n\n# 解压缩流\n\n负责把压缩包中的文件和文件夹解压出来\n\n * zis.read() 的流程\n   * 每次调用 zis.getNextEntry() 会给其内部属性 entry赋值\n   * 此时调用read()方法，读取的是entry的数据，读取到文件末尾，返回-1；如果是文件夹直接返回-1；\n   * 如果调用read()方法，读取完entry的数据，返回-1以后，entry 会置为空；\n   * 如果不调用read()方法，entry对象会一直延续直到读取下一个entry对象\n\n/*\n*   解压缩流\n*\n* */\npublic class ZipStreamDemo1 {\n    public static void main(String[] args) throws IOException {\n\n        //1.创建一个File表示要解压的压缩包\n        File src = new File("D:\\\\aaa.zip");\n        //2.创建一个File表示解压的目的地\n        File dest = new File("D:\\\\");\n\n        //调用方法\n        unzip(src,dest);\n\n    }\n\n    //定义一个方法用来解压\n    public static void unzip(File src,File dest) throws IOException {\n        //解压的本质：把压缩包里面的每一个文件或者文件夹读取出来，按照层级拷贝到目的地当中\n        //创建一个解压缩流用来读取压缩包中的数据\n        ZipInputStream zip = new ZipInputStream(new FileInputStream(src));\n        //要先获取到压缩包里面的每一个zipentry对象\n        //表示当前在压缩包中获取到的文件或者文件夹\n        ZipEntry entry;\n        while((entry = zip.getNextEntry()) != null){\n            System.out.println(entry);\n            if(entry.isDirectory()){\n                //文件夹：需要在目的地dest处创建一个同样的文件夹\n                File file = new File(dest,entry.toString());\n                file.mkdirs();\n            }else{\n                //文件：需要读取到压缩包中的文件，并把他存放到目的地dest文件夹中（按照层级目录进行存放）\n                FileOutputStream fos = new FileOutputStream(new File(dest,entry.toString()));\n                int b;\n                while((b = zip.read()) != -1){\n                    //写到目的地\n                    fos.write(b);\n                }\n                fos.close();\n                //表示在压缩包中的一个文件处理完毕了。\n                zip.closeEntry();\n            }\n        }\n        zip.close();\n    }\n}\n\n\n\n# 压缩流\n\n负责压缩文件或者文件夹\n\n\n# 压缩单个文件\n\npublic class ZipStreamDemo2 {\n    public static void main(String[] args) throws IOException {\n        /*\n         *   压缩流\n         *      需求：\n         *          把D:\\\\a.txt打包成一个压缩包\n         * */\n        //1.创建File对象表示要压缩的文件\n        File src = new File("D:\\\\a.txt");\n        //2.创建File对象表示压缩包的位置\n        File dest = new File("D:\\\\");\n        //3.调用方法用来压缩\n        toZip(src,dest);\n    }\n\n    /*\n    *   作用：压缩\n    *   参数一：表示要压缩的文件\n    *   参数二：表示压缩包的位置\n    * */\n    public static void toZip(File src,File dest) throws IOException {\n        //1.创建压缩流关联压缩包\n        ZipOutputStream zos = new ZipOutputStream(new FileOutputStream(new File(dest,"a.zip")));\n        //2.创建ZipEntry对象，表示压缩包里面的每一个文件和文件夹\n        //参数：压缩包里面的路径\n        ZipEntry entry = new ZipEntry("aaa\\\\bbb\\\\a.txt");\n        //3.把ZipEntry对象放到压缩包当中\n        zos.putNextEntry(entry);\n        //4.把src文件中的数据写到压缩包当中\n        FileInputStream fis = new FileInputStream(src);\n        int b;\n        while((b = fis.read()) != -1){\n            zos.write(b);\n        }\n        zos.closeEntry();\n        zos.close();\n    }\n}\n\n\n\n# 压缩整个文件夹--1\n\npublic class ZipStreamDemo3 {\n    public static void main(String[] args) throws IOException {\n        /*\n         *   压缩流\n         *      需求：\n         *          把D:\\\\aaa文件夹压缩成一个压缩包\n         * */\n        //1.创建File对象表示要压缩的文件夹\n        File src = new File("D:\\\\aaa");\n        //2.创建File对象表示压缩包放在哪里（压缩包的父级路径）\n        File destParent = src.getParentFile();//D:\\\\\n        //3.创建File对象表示压缩包的路径\n        File dest = new File(destParent,src.getName() + ".zip");\n        //4.创建压缩流关联压缩包\n        ZipOutputStream zos = new ZipOutputStream(new FileOutputStream(dest));\n        //5.获取src里面的每一个文件，变成ZipEntry对象，放入到压缩包当中\n        toZip(src,zos,src.getName());//aaa\n        //6.释放资源\n        zos.close();\n    }\n\n    /*\n    *   作用：获取src里面的每一个文件，变成ZipEntry对象，放入到压缩包当中\n    *   参数一：数据源\n    *   参数二：压缩流\n    *   参数三：压缩包内部的路径\n    * */\n    public static void toZip(File src,ZipOutputStream zos,String name) throws IOException {\n        //1.进入src文件夹\n        File[] files = src.listFiles();\n        //2.遍历数组\n        for (File file : files) {\n            if(file.isFile()){\n                //3.判断-文件，变成ZipEntry对象，放入到压缩包当中\n                ZipEntry entry = new ZipEntry(name + "\\\\" + file.getName());//aaa\\\\no1\\\\a.txt\n                zos.putNextEntry(entry);\n                //读取文件中的数据，写到压缩包\n                FileInputStream fis = new FileInputStream(file);\n                int b;\n                while((b = fis.read()) != -1){\n                    zos.write(b);\n                }\n                fis.close();\n                zos.closeEntry();\n            }else{\n                //4.判断-文件夹，递归\n                toZip(file,zos,name + "\\\\" + file.getName());\n                //     no1            aaa   \\\\   no1\n            }\n        }\n    }\n}\n\n\n\n# 压缩整个文件夹--2\n\n/**\n * @ClassName ZipStreamDemo1\n * @Date 2023/3/6 20:04\n * @Author diane\n * @Description 压缩流  --- 将文件夹打包成压缩包\n *\n * @Version 1.0\n */\npublic class ZipStreamDemo3 {\n\n  public static void main(String[] args) throws IOException {\n    // 要压缩的文件夹\n    File src = new File("D:\\\\tmp\\\\zip1");\n    // 压缩成的 zip文件\n    File dest = new File("D:\\\\tmp\\\\zip.zip");\n    // 创建压缩流对象\n    ZipOutputStream zos = new ZipOutputStream(new FileOutputStream(dest));\n\n    // 调用方法\n    toZip(src, zos, src.getName() + "\\\\");\n\n    // 关闭流\n    zos.close();\n  }\n\n\n  /**\n   * 压缩整个文件夹\n   * @param src 待压缩的文件夹\n   * @param zos 压缩流\n   * @param name 压缩包里面的文件夹名称  带路径的 aaa\\\\\n   * @throws IOException\n   */\n  public static void toZip(File src, ZipOutputStream zos, String name) throws IOException {\n    // 递归遍历文件夹\n    File[] files = src.listFiles();\n    if (files ** null) {\n      return;\n    }\n    for (File file : files) {\n      if (file.isFile()) {\n        // 是文件 就进行压缩\n        fileToZip(file, name + file.getName(), zos);\n      } else {\n        // 是文件夹，就递归调用 --- name 的调用 还涉及到了回溯的知识\n        toZip(file, zos, name + file.getName() + "\\\\");\n      }\n    }\n  }\n\n\n  /**\n   * 将单个文件 进行压缩\n   * @param src 要压缩的文件\n   * @param name 压缩包内部的 文件名称  带路径的   aaa\\\\1.txt\n   * @param zos 压缩流\n   * @throws IOException\n   */\n  private static void fileToZip(File src, String name, ZipOutputStream zos) throws IOException {\n    // 得到待压缩的文件名\n    System.out.println(name);\n    // 创建 ZipEntry对象\n    ZipEntry zipEntry = new ZipEntry(name);\n    // 把 ZipEntry对象 放入 压缩包中\n    zos.putNextEntry(zipEntry);\n    // 读取待压缩文件；；并把读取到的数据 写入 ZipEntry 对象\n    FileInputStream fis = new FileInputStream(src);\n    byte[] bytes = new byte[10];\n    int len;\n    while ((len = fis.read(bytes)) != -1) {\n      zos.write(bytes, 0, len);\n    }\n    // 关闭流\n    fis.close();\n    zos.closeEntry();\n  }\n}\n',normalizedContent:'# 压缩流和解压缩流\n\n\n# 解压缩流\n\n负责把压缩包中的文件和文件夹解压出来\n\n * zis.read() 的流程\n   * 每次调用 zis.getnextentry() 会给其内部属性 entry赋值\n   * 此时调用read()方法，读取的是entry的数据，读取到文件末尾，返回-1；如果是文件夹直接返回-1；\n   * 如果调用read()方法，读取完entry的数据，返回-1以后，entry 会置为空；\n   * 如果不调用read()方法，entry对象会一直延续直到读取下一个entry对象\n\n/*\n*   解压缩流\n*\n* */\npublic class zipstreamdemo1 {\n    public static void main(string[] args) throws ioexception {\n\n        //1.创建一个file表示要解压的压缩包\n        file src = new file("d:\\\\aaa.zip");\n        //2.创建一个file表示解压的目的地\n        file dest = new file("d:\\\\");\n\n        //调用方法\n        unzip(src,dest);\n\n    }\n\n    //定义一个方法用来解压\n    public static void unzip(file src,file dest) throws ioexception {\n        //解压的本质：把压缩包里面的每一个文件或者文件夹读取出来，按照层级拷贝到目的地当中\n        //创建一个解压缩流用来读取压缩包中的数据\n        zipinputstream zip = new zipinputstream(new fileinputstream(src));\n        //要先获取到压缩包里面的每一个zipentry对象\n        //表示当前在压缩包中获取到的文件或者文件夹\n        zipentry entry;\n        while((entry = zip.getnextentry()) != null){\n            system.out.println(entry);\n            if(entry.isdirectory()){\n                //文件夹：需要在目的地dest处创建一个同样的文件夹\n                file file = new file(dest,entry.tostring());\n                file.mkdirs();\n            }else{\n                //文件：需要读取到压缩包中的文件，并把他存放到目的地dest文件夹中（按照层级目录进行存放）\n                fileoutputstream fos = new fileoutputstream(new file(dest,entry.tostring()));\n                int b;\n                while((b = zip.read()) != -1){\n                    //写到目的地\n                    fos.write(b);\n                }\n                fos.close();\n                //表示在压缩包中的一个文件处理完毕了。\n                zip.closeentry();\n            }\n        }\n        zip.close();\n    }\n}\n\n\n\n# 压缩流\n\n负责压缩文件或者文件夹\n\n\n# 压缩单个文件\n\npublic class zipstreamdemo2 {\n    public static void main(string[] args) throws ioexception {\n        /*\n         *   压缩流\n         *      需求：\n         *          把d:\\\\a.txt打包成一个压缩包\n         * */\n        //1.创建file对象表示要压缩的文件\n        file src = new file("d:\\\\a.txt");\n        //2.创建file对象表示压缩包的位置\n        file dest = new file("d:\\\\");\n        //3.调用方法用来压缩\n        tozip(src,dest);\n    }\n\n    /*\n    *   作用：压缩\n    *   参数一：表示要压缩的文件\n    *   参数二：表示压缩包的位置\n    * */\n    public static void tozip(file src,file dest) throws ioexception {\n        //1.创建压缩流关联压缩包\n        zipoutputstream zos = new zipoutputstream(new fileoutputstream(new file(dest,"a.zip")));\n        //2.创建zipentry对象，表示压缩包里面的每一个文件和文件夹\n        //参数：压缩包里面的路径\n        zipentry entry = new zipentry("aaa\\\\bbb\\\\a.txt");\n        //3.把zipentry对象放到压缩包当中\n        zos.putnextentry(entry);\n        //4.把src文件中的数据写到压缩包当中\n        fileinputstream fis = new fileinputstream(src);\n        int b;\n        while((b = fis.read()) != -1){\n            zos.write(b);\n        }\n        zos.closeentry();\n        zos.close();\n    }\n}\n\n\n\n# 压缩整个文件夹--1\n\npublic class zipstreamdemo3 {\n    public static void main(string[] args) throws ioexception {\n        /*\n         *   压缩流\n         *      需求：\n         *          把d:\\\\aaa文件夹压缩成一个压缩包\n         * */\n        //1.创建file对象表示要压缩的文件夹\n        file src = new file("d:\\\\aaa");\n        //2.创建file对象表示压缩包放在哪里（压缩包的父级路径）\n        file destparent = src.getparentfile();//d:\\\\\n        //3.创建file对象表示压缩包的路径\n        file dest = new file(destparent,src.getname() + ".zip");\n        //4.创建压缩流关联压缩包\n        zipoutputstream zos = new zipoutputstream(new fileoutputstream(dest));\n        //5.获取src里面的每一个文件，变成zipentry对象，放入到压缩包当中\n        tozip(src,zos,src.getname());//aaa\n        //6.释放资源\n        zos.close();\n    }\n\n    /*\n    *   作用：获取src里面的每一个文件，变成zipentry对象，放入到压缩包当中\n    *   参数一：数据源\n    *   参数二：压缩流\n    *   参数三：压缩包内部的路径\n    * */\n    public static void tozip(file src,zipoutputstream zos,string name) throws ioexception {\n        //1.进入src文件夹\n        file[] files = src.listfiles();\n        //2.遍历数组\n        for (file file : files) {\n            if(file.isfile()){\n                //3.判断-文件，变成zipentry对象，放入到压缩包当中\n                zipentry entry = new zipentry(name + "\\\\" + file.getname());//aaa\\\\no1\\\\a.txt\n                zos.putnextentry(entry);\n                //读取文件中的数据，写到压缩包\n                fileinputstream fis = new fileinputstream(file);\n                int b;\n                while((b = fis.read()) != -1){\n                    zos.write(b);\n                }\n                fis.close();\n                zos.closeentry();\n            }else{\n                //4.判断-文件夹，递归\n                tozip(file,zos,name + "\\\\" + file.getname());\n                //     no1            aaa   \\\\   no1\n            }\n        }\n    }\n}\n\n\n\n# 压缩整个文件夹--2\n\n/**\n * @classname zipstreamdemo1\n * @date 2023/3/6 20:04\n * @author diane\n * @description 压缩流  --- 将文件夹打包成压缩包\n *\n * @version 1.0\n */\npublic class zipstreamdemo3 {\n\n  public static void main(string[] args) throws ioexception {\n    // 要压缩的文件夹\n    file src = new file("d:\\\\tmp\\\\zip1");\n    // 压缩成的 zip文件\n    file dest = new file("d:\\\\tmp\\\\zip.zip");\n    // 创建压缩流对象\n    zipoutputstream zos = new zipoutputstream(new fileoutputstream(dest));\n\n    // 调用方法\n    tozip(src, zos, src.getname() + "\\\\");\n\n    // 关闭流\n    zos.close();\n  }\n\n\n  /**\n   * 压缩整个文件夹\n   * @param src 待压缩的文件夹\n   * @param zos 压缩流\n   * @param name 压缩包里面的文件夹名称  带路径的 aaa\\\\\n   * @throws ioexception\n   */\n  public static void tozip(file src, zipoutputstream zos, string name) throws ioexception {\n    // 递归遍历文件夹\n    file[] files = src.listfiles();\n    if (files ** null) {\n      return;\n    }\n    for (file file : files) {\n      if (file.isfile()) {\n        // 是文件 就进行压缩\n        filetozip(file, name + file.getname(), zos);\n      } else {\n        // 是文件夹，就递归调用 --- name 的调用 还涉及到了回溯的知识\n        tozip(file, zos, name + file.getname() + "\\\\");\n      }\n    }\n  }\n\n\n  /**\n   * 将单个文件 进行压缩\n   * @param src 要压缩的文件\n   * @param name 压缩包内部的 文件名称  带路径的   aaa\\\\1.txt\n   * @param zos 压缩流\n   * @throws ioexception\n   */\n  private static void filetozip(file src, string name, zipoutputstream zos) throws ioexception {\n    // 得到待压缩的文件名\n    system.out.println(name);\n    // 创建 zipentry对象\n    zipentry zipentry = new zipentry(name);\n    // 把 zipentry对象 放入 压缩包中\n    zos.putnextentry(zipentry);\n    // 读取待压缩文件；；并把读取到的数据 写入 zipentry 对象\n    fileinputstream fis = new fileinputstream(src);\n    byte[] bytes = new byte[10];\n    int len;\n    while ((len = fis.read(bytes)) != -1) {\n      zos.write(bytes, 0, len);\n    }\n    // 关闭流\n    fis.close();\n    zos.closeentry();\n  }\n}\n',charsets:{cjk:!0}},{title:"《Java》",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"01.后端/10.Java",imgUrl:"/assets/img/java6.webp",description:"Java学习笔记--整理自黑马程序员，在原教程基础上添加学习笔记"}},title:"《Java》",date:"2023-06-30T20:30:40.000Z",permalink:"/back/java/",article:!1,comment:!1,editLink:!1,author:{name:"凉冰",link:"https://github.com/diana-devil"}},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/",relativePath:"01.后端/10.Java/README.md",key:"v-0186e5b2",path:"/back/java/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"Listener",frontmatter:{autoSort:99,title:"Listener",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/3b2939/",categories:["后端","JavaWeb"],tags:["知识","JavaWeb"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/20.JavaWeb/15.Listener.html",relativePath:"01.后端/20.JavaWeb/15.Listener.md",key:"v-2e0b9ea9",path:"/pages/3b2939/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:15},{level:2,title:"分类",slug:"分类",normalizedTitle:"分类",charIndex:306},{level:2,title:"代码演示",slug:"代码演示",normalizedTitle:"代码演示",charIndex:628}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"概述 分类 代码演示",content:'# Listener\n\n\n# 概述\n\n * Listener 表示监听器，是 JavaWeb 三大组件(Servlet、Filter、Listener)之一。\n\n * 监听器可以监听就是在 application，session，request 三个对象创建、销毁或者往其中添加修改删除属性时自动执行代码的功能组件。\n   \n   request 和 session 我们学习过。而 application 是 ServletContext 类型的对象。\n   \n   ServletContext 代表整个web应用，在服务器启动的时候，tomcat会自动创建该对象。在服务器关闭时会自动销毁该对象。\n\n\n# 分类\n\nJavaWeb 提供了8个监听器：\n\n这里面只有 ServletContextListener 这个监听器后期我们会接触到，ServletContextListener 是用来监听 ServletContext 对象的创建和销毁。\n\nServletContextListener 接口中有以下两个方法\n\n * void contextInitialized(ServletContextEvent sce)：ServletContext 对象被创建了会自动执行的方法\n * void contextDestroyed(ServletContextEvent sce)：ServletContext 对象被销毁时会自动执行的方法\n\n\n# 代码演示\n\n我们只演示一下 ServletContextListener 监听器\n\n * 定义一个类，实现ServletContextListener 接口\n * 重写所有的抽象方法\n * 使用 @WebListener 进行配置\n\n代码如下：\n\n@WebListener\npublic class ContextLoaderListener implements ServletContextListener {\n    @Override\n    public void contextInitialized(ServletContextEvent sce) {\n        //加载资源\n        System.out.println("ContextLoaderListener...");\n    }\n\n    @Override\n    public void contextDestroyed(ServletContextEvent sce) {\n        //释放资源\n    }\n}\n\n\n启动服务器，就可以在启动的日志信息中看到 contextInitialized() 方法输出的内容，同时也说明了 ServletContext 对象在服务器启动的时候被创建了。',normalizedContent:'# listener\n\n\n# 概述\n\n * listener 表示监听器，是 javaweb 三大组件(servlet、filter、listener)之一。\n\n * 监听器可以监听就是在 application，session，request 三个对象创建、销毁或者往其中添加修改删除属性时自动执行代码的功能组件。\n   \n   request 和 session 我们学习过。而 application 是 servletcontext 类型的对象。\n   \n   servletcontext 代表整个web应用，在服务器启动的时候，tomcat会自动创建该对象。在服务器关闭时会自动销毁该对象。\n\n\n# 分类\n\njavaweb 提供了8个监听器：\n\n这里面只有 servletcontextlistener 这个监听器后期我们会接触到，servletcontextlistener 是用来监听 servletcontext 对象的创建和销毁。\n\nservletcontextlistener 接口中有以下两个方法\n\n * void contextinitialized(servletcontextevent sce)：servletcontext 对象被创建了会自动执行的方法\n * void contextdestroyed(servletcontextevent sce)：servletcontext 对象被销毁时会自动执行的方法\n\n\n# 代码演示\n\n我们只演示一下 servletcontextlistener 监听器\n\n * 定义一个类，实现servletcontextlistener 接口\n * 重写所有的抽象方法\n * 使用 @weblistener 进行配置\n\n代码如下：\n\n@weblistener\npublic class contextloaderlistener implements servletcontextlistener {\n    @override\n    public void contextinitialized(servletcontextevent sce) {\n        //加载资源\n        system.out.println("contextloaderlistener...");\n    }\n\n    @override\n    public void contextdestroyed(servletcontextevent sce) {\n        //释放资源\n    }\n}\n\n\n启动服务器，就可以在启动的日志信息中看到 contextinitialized() 方法输出的内容，同时也说明了 servletcontext 对象在服务器启动的时候被创建了。',charsets:{cjk:!0}},{title:"Filter",frontmatter:{autoSort:100,title:"Filter",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/7a0933/",categories:["后端","JavaWeb"],tags:["知识","JavaWeb"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/20.JavaWeb/10.Filter.html",relativePath:"01.后端/20.JavaWeb/10.Filter.md",key:"v-552a3e17",path:"/pages/7a0933/",headers:[{level:2,title:"Filter概述",slug:"filter概述",normalizedTitle:"filter概述",charIndex:13},{level:2,title:"Filter快速入门",slug:"filter快速入门",normalizedTitle:"filter快速入门",charIndex:636},{level:2,title:"Filter执行流程",slug:"filter执行流程",normalizedTitle:"filter执行流程",charIndex:3722},{level:2,title:"Filter拦截路径配置",slug:"filter拦截路径配置",normalizedTitle:"filter拦截路径配置",charIndex:4195},{level:2,title:"过滤器链",slug:"过滤器链",normalizedTitle:"过滤器链",charIndex:4498},{level:2,title:"案例",slug:"案例",normalizedTitle:"案例",charIndex:379}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"Filter概述 Filter快速入门 Filter执行流程 Filter拦截路径配置 过滤器链 案例",content:'# Filter\n\n\n# Filter概述\n\nFilter 表示过滤器，是 JavaWeb 三大组件(Servlet、Filter、Listener)之一。Servlet 我们之前都已经学习过了，Filter和Listener 我们今天都会进行学习。\n\n过滤器可以把对资源的请求拦截下来，从而实现一些特殊的功能。\n\n如下图所示，浏览器可以访问服务器上的所有的资源（servlet、jsp、html等）\n\n而在访问到这些资源之前可以使过滤器拦截来下，也就是说在访问资源之前会先经过 Filter，如下图\n\n拦截器拦截到后可以做什么功能呢？\n\n**过滤器一般完成一些通用的操作。**比如每个资源都要写一些代码完成某个功能，我们总不能在每个资源中写这样的代码吧，而此时我们可以将这些代码写在过滤器中，因为请求每一个资源都要经过过滤器。\n\n我们之前做的品牌数据管理的案例中就已经做了登陆的功能，而如果我们不登录能不能访问到数据呢？我们可以在浏览器直接访问首页 ，可以看到 查询所有 的超链接\n\n当我点击该按钮，居然可以看到品牌的数据\n\n这显然和我们的要求不符。我们希望实现的效果是用户如果登陆过了就跳转到品牌数据展示的页面；如果没有登陆就跳转到登陆页面让用户进行登陆，要实现这个效果需要在每一个资源中都写上这段逻辑，而像这种通用的操作，我们就可以放在过滤器中进行实现。这个就是权限控制，以后我们还会进行细粒度权限控制。过滤器还可以做 统一编码处理、 敏感字符处理 等等…\n\n\n# Filter快速入门\n\n# 开发步骤\n\n进行 Filter 开发分成以下三步实现\n\n * 定义类，实现 Filter接口，并重写其所有方法\n\n * 配置Filter拦截资源的路径：在类上定义 @WebFilter 注解。而注解的 value 属性值 /* 表示拦截所有的资源\n\n * 在doFilter方法中输出一句话，并放行\n   \n   > 上述代码中的 chain.doFilter(request,response); 就是放行，也就是让其访问本该访问的资源。\n\n# 代码演示\n\n创建一个项目，项目下有一个 hello.jsp 页面，项目结构如下：\n\npom.xml 配置文件内容如下：\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>org.example</groupId>\n    <artifactId>filter-demo</artifactId>\n    <version>1.0-SNAPSHOT</version>\n    <packaging>war</packaging>\n\n    <properties>\n        <maven.compiler.source>8</maven.compiler.source>\n        <maven.compiler.target>8</maven.compiler.target>\n    </properties>\n\n    <dependencies>\n        <dependency>\n            <groupId>javax.servlet</groupId>\n            <artifactId>javax.servlet-api</artifactId>\n            <version>3.1.0</version>\n            <scope>provided</scope>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.tomcat.maven</groupId>\n                <artifactId>tomcat7-maven-plugin</artifactId>\n                <version>2.2</version>\n                <configuration>\n                    <port>80</port>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n\n\nhello.jsp 页面内容如下：\n\n<%@ page contentType="text/html;charset=UTF-8" language="java" %>\n<html>\n<head>\n    <title>Title</title>\n</head>\n<body>\n    <h1>hello JSP~</h1>\n</body>\n</html>\n\n\n我们现在在浏览器输入 http://localhost/filter-demo/hello.jsp 访问 hello.jsp 页面，这里是可以访问到 hello.jsp 页面内容的。\n\n接下来编写过滤器。过滤器是 Web 三大组件之一，所以我们将 filter 创建在 com.itheima.web.filter 包下，起名为 FilterDemo\n\n@WebFilter("/*")\npublic class FilterDemo implements Filter {\n\n    @Override\n    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {\n        System.out.println("FilterDemo...");\n    }\n\n    @Override\n    public void init(FilterConfig filterConfig) throws ServletException {\n    }\n\n    @Override\n    public void destroy() {\n    }\n}\n\n\n\n重启启动服务器，再次重新访问 hello.jsp 页面，这次发现页面没有任何效果，但是在 idea 的控制台可以看到如下内容\n\n上述效果说明 FilterDemo 这个过滤器的 doFilter() 方法执行了，但是为什么在浏览器上看不到 hello.jsp 页面的内容呢？这是因为在 doFilter() 方法中添加放行的方法才能访问到 hello.jsp 页面。那就在 doFilter() 方法中添加放行的代码\n\n//放行\n chain.doFilter(request,response);\n\n\n再次重启服务器并访问 hello.jsp 页面，发现这次就可以在浏览器上看到页面效果。\n\nFilterDemo 过滤器完整代码如下：\n\n@WebFilter("/*")\npublic class FilterDemo implements Filter {\n\n    @Override\n    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {\n        System.out.println("1.FilterDemo...");\n        //放行\n        chain.doFilter(request,response);\n    }\n\n    @Override\n    public void init(FilterConfig filterConfig) throws ServletException {\n    }\n\n    @Override\n    public void destroy() {\n    }\n}\n\n\n\n\n# Filter执行流程\n\n如上图是使用过滤器的流程，我们通过以下问题来研究过滤器的执行流程：\n\n * 放行后访问对应资源，资源访问完成后，还会回到Filter中吗？\n   \n   从上图就可以看出肯定 会 回到Filter中\n\n * 如果回到Filter中，是重头执行还是执行放行后的逻辑呢？\n   \n   如果是重头执行的话，就意味着 放行前逻辑 会被执行两次，肯定不会这样设计了；所以访问完资源后，会回到 放行后逻辑，执行该部分代码。\n\n通过上述的说明，我们就可以总结Filter的执行流程如下：\n\n接下来我们通过代码验证一下，在 doFilter() 方法前后都加上输出语句，如下\n\n同时在 hello.jsp 页面加上输出语句，如下\n\n执行访问该资源打印的顺序是按照我们标记的标号进行打印的话，说明我们上边总结出来的流程是没有问题的。启动服务器访问 hello.jsp 页面，在控制台打印的内容如下：\n\n以后我们可以将对请求进行处理的代码放在放行之前进行处理，而如果请求完资源后还要对响应的数据进行处理时可以在放行后进行逻辑处理。\n\n\n# Filter拦截路径配置\n\n拦截路径表示 Filter 会对请求的哪些资源进行拦截，使用 @WebFilter 注解进行配置。如：@WebFilter("拦截路径")\n\n拦截路径有如下四种配置方式：\n\n * 拦截具体的资源：/index.jsp：只有访问index.jsp时才会被拦截\n * 目录拦截：/user/*：访问/user下的所有资源，都会被拦截\n * 后缀名拦截：*.jsp：访问后缀名为jsp的资源，都会被拦截\n * 拦截所有：/*：访问所有资源，都会被拦截\n\n通过上面拦截路径的学习，大家会发现拦截路径的配置方式和 Servlet 的请求资源路径配置方式一样，但是表示的含义不同。\n\n\n# 过滤器链\n\n# 概述\n\n过滤器链是指在一个Web应用，可以配置多个过滤器，这多个过滤器称为过滤器链。\n\n如下图就是一个过滤器链，我们学习过滤器链主要是学习过滤器链执行的流程\n\n上图中的过滤器链执行是按照以下流程执行：\n\n 1. 执行 Filter1 的放行前逻辑代码\n 2. 执行 Filter1 的放行代码\n 3. 执行 Filter2 的放行前逻辑代码\n 4. 执行 Filter2 的放行代码\n 5. 访问到资源\n 6. 执行 Filter2 的放行后逻辑代码\n 7. 执行 Filter1 的放行后逻辑代码\n\n以上流程串起来就像一条链子，故称之为过滤器链。\n\n# 代码演示\n\n * 编写第一个过滤器 FilterDemo ，配置成拦截所有资源\n   \n   @WebFilter("/*")\n   public class FilterDemo implements Filter {\n   \n       @Override\n       public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {\n   \n           //1. 放行前，对 request数据进行处理\n           System.out.println("1.FilterDemo...");\n           //放行\n           chain.doFilter(request,response);\n           //2. 放行后，对Response 数据进行处理\n           System.out.println("3.FilterDemo...");\n       }\n   \n       @Override\n       public void init(FilterConfig filterConfig) throws ServletException {\n       }\n   \n       @Override\n       public void destroy() {\n       }\n   }\n   \n\n * 编写第二个过滤器 FilterDemo2 ，配置炒年糕拦截所有资源\n   \n   @WebFilter("/*")\n   public class FilterDemo2 implements Filter {\n   \n       @Override\n       public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {\n   \n           //1. 放行前，对 request数据进行处理\n           System.out.println("2.FilterDemo...");\n           //放行\n           chain.doFilter(request,response);\n           //2. 放行后，对Response 数据进行处理\n           System.out.println("4.FilterDemo...");\n       }\n   \n       @Override\n       public void init(FilterConfig filterConfig) throws ServletException {\n       }\n   \n       @Override\n       public void destroy() {\n       }\n   }\n   \n   \n\n * 修改 hello.jsp 页面中脚本的输出语句\n   \n   <%@ page contentType="text/html;charset=UTF-8" language="java" %>\n   <html>\n   <head>\n       <title>Title</title>\n   </head>\n   <body>\n       <h1>hello JSP~</h1>\n       <%\n           System.out.println("3.hello jsp");\n       %>\n   </body>\n   </html>\n   \n\n * 启动服务器，在浏览器输入 http://localhost/filter-demo/hello.jsp 进行测试，在控制台打印内容如下\n   \n   从结果可以看到确实是按照我们之前说的执行流程进行执行的。\n\n# 问题\n\n上面代码中为什么是先执行 FilterDemo ，后执行 FilterDemo2 呢？\n\n我们现在使用的是注解配置Filter，而这种配置方式的优先级是按照过滤器类名(字符串)的自然排序。\n\n比如有如下两个名称的过滤器 ： BFilterDemo 和 AFilterDemo 。那一定是 AFilterDemo 过滤器先执行。\n\n\n# 案例\n\n# 需求\n\n访问服务器资源时，需要先进行登录验证，如果没有登录，则自动跳转到登录页面\n\n# 分析\n\n我们要实现该功能是在每一个资源里加入登陆状态校验的代码吗？显然是不需要的，只需要写一个 Filter ，在该过滤器中进行登陆状态校验即可。而在该 Filter 中逻辑如下：\n\n# 代码实现\n\n# 创建Filter\n\n在 brand-demo 工程创建 com.itheima.web.filter 包，在该下创建名为 LoginFilter 的过滤器\n\n@WebFilter("/*")\npublic class LoginFilter implements Filter {\n    @Override\n    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws ServletException, IOException {\n      \n    }\n\n    public void init(FilterConfig config) throws ServletException {\n    }\n\n    public void destroy() {\n    }\n}\n\n\n# 编写逻辑代码\n\n在 doFilter() 方法中编写登陆状态校验的逻辑代码。\n\n我们首先需要从 session 对象中获取用户信息，但是 ServletRequest 类型的 requset 对象没有获取 session 对象的方法，所以此时需要将 request对象强转成 HttpServletRequest 对象。\n\nHttpServletRequest req = (HttpServletRequest) request;\n\n\n然后完成以下逻辑\n\n * 获取Session对象\n * 从Session对象中获取名为 user 的数据\n * 判断获取到的数据是否是 null\n   * 如果不是，说明已经登陆，放行\n   * 如果是，说明尚未登陆，将提示信息存储到域对象中并跳转到登陆页面\n\n代码如下：\n\n@WebFilter("/*")\npublic class LoginFilter implements Filter {\n    @Override\n    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws ServletException, IOException {\n        HttpServletRequest req = (HttpServletRequest) request;\n   \n        //1. 判断session中是否有user\n        HttpSession session = req.getSession();\n        Object user = session.getAttribute("user");\n\n        //2. 判断user是否为null\n        if(user != null){\n            // 登录过了\n            //放行\n            chain.doFilter(request, response);\n        }else {\n            // 没有登陆，存储提示信息，跳转到登录页面\n\n            req.setAttribute("login_msg","您尚未登陆！");\n            req.getRequestDispatcher("/login.jsp").forward(req,response);\n        }\n    }\n\n    public void init(FilterConfig config) throws ServletException {\n    }\n\n    public void destroy() {\n    }\n}\n\n\n# 测试并抛出问题\n\n在浏览器上输入 http://localhost:8080/brand-demo/ ，可以看到如下页面效果\n\n从上面效果可以看出没有登陆确实是跳转到登陆页面了，但是登陆页面为什么展示成这种效果了呢？\n\n# 问题分析及解决\n\n因为登陆页面需要 css/login.css 这个文件进行样式的渲染，下图是登陆页面引入的css文件图解\n\n而在请求这个css资源时被过滤器拦截，就相当于没有加载到样式文件导致的。解决这个问题，只需要对所以的登陆相关的资源进行放行即可。还有一种情况就是当我没有用户信息时需要进行注册，而注册时也希望被过滤器放行。\n\n综上，我们需要在判断session中是否包含用户信息之前，应该加上对登陆及注册相关资源放行的逻辑处理\n\n//判断访问资源路径是否和登录注册相关\n//1,在数组中存储登陆和注册相关的资源路径\nString[] urls = {"/login.jsp","/imgs/","/css/","/loginServlet","/register.jsp","/registerServlet","/checkCodeServlet"};\n//2,获取当前访问的资源路径\nString url = req.getRequestURL().toString(); \n\n//3,遍历数组，获取到每一个需要放行的资源路径\nfor (String u : urls) {\n    //4,判断当前访问的资源路径字符串是否包含要放行的的资源路径字符串\n    /*\n    \t比如当前访问的资源路径是  /brand-demo/login.jsp\n    \t而字符串 /brand-demo/login.jsp 包含了  字符串 /login.jsp ，所以这个字符串就需要放行\n    */\n    if(url.contains(u)){\n        //找到了，放行\n        chain.doFilter(request, response);\n        //break;\n        return;\n    }\n}\n\n\n# 过滤器完整代码\n\n@WebFilter("/*")\npublic class LoginFilter implements Filter {\n    @Override\n    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws ServletException, IOException {\n        HttpServletRequest req = (HttpServletRequest) request;\n        \n        //判断访问资源路径是否和登录注册相关\n        //1,在数组中存储登陆和注册相关的资源路径\n        String[] urls = {"/login.jsp","/imgs/","/css/","/loginServlet","/register.jsp","/registerServlet","/checkCodeServlet"};\n        //2,获取当前访问的资源路径\n        String url = req.getRequestURL().toString(); \n\n        //3,遍历数组，获取到每一个需要放行的资源路径\n        for (String u : urls) {\n            //4,判断当前访问的资源路径字符串是否包含要放行的的资源路径字符串\n            /*\n                比如当前访问的资源路径是  /brand-demo/login.jsp\n                而字符串 /brand-demo/login.jsp 包含了  字符串 /login.jsp ，所以这个字符串就需要放行\n            */\n            if(url.contains(u)){\n                //找到了，放行\n                chain.doFilter(request, response);\n                //break;\n                return;\n            }\n        }\n   \n        //1. 判断session中是否有user\n        HttpSession session = req.getSession();\n        Object user = session.getAttribute("user");\n\n        //2. 判断user是否为null\n        if(user != null){\n            // 登录过了\n            //放行\n            chain.doFilter(request, response);\n        }else {\n            // 没有登陆，存储提示信息，跳转到登录页面\n\n            req.setAttribute("login_msg","您尚未登陆！");\n            req.getRequestDispatcher("/login.jsp").forward(req,response);\n        }\n    }\n\n    public void init(FilterConfig config) throws ServletException {\n    }\n\n    public void destroy() {\n    }\n}\n',normalizedContent:'# filter\n\n\n# filter概述\n\nfilter 表示过滤器，是 javaweb 三大组件(servlet、filter、listener)之一。servlet 我们之前都已经学习过了，filter和listener 我们今天都会进行学习。\n\n过滤器可以把对资源的请求拦截下来，从而实现一些特殊的功能。\n\n如下图所示，浏览器可以访问服务器上的所有的资源（servlet、jsp、html等）\n\n而在访问到这些资源之前可以使过滤器拦截来下，也就是说在访问资源之前会先经过 filter，如下图\n\n拦截器拦截到后可以做什么功能呢？\n\n**过滤器一般完成一些通用的操作。**比如每个资源都要写一些代码完成某个功能，我们总不能在每个资源中写这样的代码吧，而此时我们可以将这些代码写在过滤器中，因为请求每一个资源都要经过过滤器。\n\n我们之前做的品牌数据管理的案例中就已经做了登陆的功能，而如果我们不登录能不能访问到数据呢？我们可以在浏览器直接访问首页 ，可以看到 查询所有 的超链接\n\n当我点击该按钮，居然可以看到品牌的数据\n\n这显然和我们的要求不符。我们希望实现的效果是用户如果登陆过了就跳转到品牌数据展示的页面；如果没有登陆就跳转到登陆页面让用户进行登陆，要实现这个效果需要在每一个资源中都写上这段逻辑，而像这种通用的操作，我们就可以放在过滤器中进行实现。这个就是权限控制，以后我们还会进行细粒度权限控制。过滤器还可以做 统一编码处理、 敏感字符处理 等等…\n\n\n# filter快速入门\n\n# 开发步骤\n\n进行 filter 开发分成以下三步实现\n\n * 定义类，实现 filter接口，并重写其所有方法\n\n * 配置filter拦截资源的路径：在类上定义 @webfilter 注解。而注解的 value 属性值 /* 表示拦截所有的资源\n\n * 在dofilter方法中输出一句话，并放行\n   \n   > 上述代码中的 chain.dofilter(request,response); 就是放行，也就是让其访问本该访问的资源。\n\n# 代码演示\n\n创建一个项目，项目下有一个 hello.jsp 页面，项目结构如下：\n\npom.xml 配置文件内容如下：\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n\n    <groupid>org.example</groupid>\n    <artifactid>filter-demo</artifactid>\n    <version>1.0-snapshot</version>\n    <packaging>war</packaging>\n\n    <properties>\n        <maven.compiler.source>8</maven.compiler.source>\n        <maven.compiler.target>8</maven.compiler.target>\n    </properties>\n\n    <dependencies>\n        <dependency>\n            <groupid>javax.servlet</groupid>\n            <artifactid>javax.servlet-api</artifactid>\n            <version>3.1.0</version>\n            <scope>provided</scope>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupid>org.apache.tomcat.maven</groupid>\n                <artifactid>tomcat7-maven-plugin</artifactid>\n                <version>2.2</version>\n                <configuration>\n                    <port>80</port>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n\n\nhello.jsp 页面内容如下：\n\n<%@ page contenttype="text/html;charset=utf-8" language="java" %>\n<html>\n<head>\n    <title>title</title>\n</head>\n<body>\n    <h1>hello jsp~</h1>\n</body>\n</html>\n\n\n我们现在在浏览器输入 http://localhost/filter-demo/hello.jsp 访问 hello.jsp 页面，这里是可以访问到 hello.jsp 页面内容的。\n\n接下来编写过滤器。过滤器是 web 三大组件之一，所以我们将 filter 创建在 com.itheima.web.filter 包下，起名为 filterdemo\n\n@webfilter("/*")\npublic class filterdemo implements filter {\n\n    @override\n    public void dofilter(servletrequest request, servletresponse response, filterchain chain) throws ioexception, servletexception {\n        system.out.println("filterdemo...");\n    }\n\n    @override\n    public void init(filterconfig filterconfig) throws servletexception {\n    }\n\n    @override\n    public void destroy() {\n    }\n}\n\n\n\n重启启动服务器，再次重新访问 hello.jsp 页面，这次发现页面没有任何效果，但是在 idea 的控制台可以看到如下内容\n\n上述效果说明 filterdemo 这个过滤器的 dofilter() 方法执行了，但是为什么在浏览器上看不到 hello.jsp 页面的内容呢？这是因为在 dofilter() 方法中添加放行的方法才能访问到 hello.jsp 页面。那就在 dofilter() 方法中添加放行的代码\n\n//放行\n chain.dofilter(request,response);\n\n\n再次重启服务器并访问 hello.jsp 页面，发现这次就可以在浏览器上看到页面效果。\n\nfilterdemo 过滤器完整代码如下：\n\n@webfilter("/*")\npublic class filterdemo implements filter {\n\n    @override\n    public void dofilter(servletrequest request, servletresponse response, filterchain chain) throws ioexception, servletexception {\n        system.out.println("1.filterdemo...");\n        //放行\n        chain.dofilter(request,response);\n    }\n\n    @override\n    public void init(filterconfig filterconfig) throws servletexception {\n    }\n\n    @override\n    public void destroy() {\n    }\n}\n\n\n\n\n# filter执行流程\n\n如上图是使用过滤器的流程，我们通过以下问题来研究过滤器的执行流程：\n\n * 放行后访问对应资源，资源访问完成后，还会回到filter中吗？\n   \n   从上图就可以看出肯定 会 回到filter中\n\n * 如果回到filter中，是重头执行还是执行放行后的逻辑呢？\n   \n   如果是重头执行的话，就意味着 放行前逻辑 会被执行两次，肯定不会这样设计了；所以访问完资源后，会回到 放行后逻辑，执行该部分代码。\n\n通过上述的说明，我们就可以总结filter的执行流程如下：\n\n接下来我们通过代码验证一下，在 dofilter() 方法前后都加上输出语句，如下\n\n同时在 hello.jsp 页面加上输出语句，如下\n\n执行访问该资源打印的顺序是按照我们标记的标号进行打印的话，说明我们上边总结出来的流程是没有问题的。启动服务器访问 hello.jsp 页面，在控制台打印的内容如下：\n\n以后我们可以将对请求进行处理的代码放在放行之前进行处理，而如果请求完资源后还要对响应的数据进行处理时可以在放行后进行逻辑处理。\n\n\n# filter拦截路径配置\n\n拦截路径表示 filter 会对请求的哪些资源进行拦截，使用 @webfilter 注解进行配置。如：@webfilter("拦截路径")\n\n拦截路径有如下四种配置方式：\n\n * 拦截具体的资源：/index.jsp：只有访问index.jsp时才会被拦截\n * 目录拦截：/user/*：访问/user下的所有资源，都会被拦截\n * 后缀名拦截：*.jsp：访问后缀名为jsp的资源，都会被拦截\n * 拦截所有：/*：访问所有资源，都会被拦截\n\n通过上面拦截路径的学习，大家会发现拦截路径的配置方式和 servlet 的请求资源路径配置方式一样，但是表示的含义不同。\n\n\n# 过滤器链\n\n# 概述\n\n过滤器链是指在一个web应用，可以配置多个过滤器，这多个过滤器称为过滤器链。\n\n如下图就是一个过滤器链，我们学习过滤器链主要是学习过滤器链执行的流程\n\n上图中的过滤器链执行是按照以下流程执行：\n\n 1. 执行 filter1 的放行前逻辑代码\n 2. 执行 filter1 的放行代码\n 3. 执行 filter2 的放行前逻辑代码\n 4. 执行 filter2 的放行代码\n 5. 访问到资源\n 6. 执行 filter2 的放行后逻辑代码\n 7. 执行 filter1 的放行后逻辑代码\n\n以上流程串起来就像一条链子，故称之为过滤器链。\n\n# 代码演示\n\n * 编写第一个过滤器 filterdemo ，配置成拦截所有资源\n   \n   @webfilter("/*")\n   public class filterdemo implements filter {\n   \n       @override\n       public void dofilter(servletrequest request, servletresponse response, filterchain chain) throws ioexception, servletexception {\n   \n           //1. 放行前，对 request数据进行处理\n           system.out.println("1.filterdemo...");\n           //放行\n           chain.dofilter(request,response);\n           //2. 放行后，对response 数据进行处理\n           system.out.println("3.filterdemo...");\n       }\n   \n       @override\n       public void init(filterconfig filterconfig) throws servletexception {\n       }\n   \n       @override\n       public void destroy() {\n       }\n   }\n   \n\n * 编写第二个过滤器 filterdemo2 ，配置炒年糕拦截所有资源\n   \n   @webfilter("/*")\n   public class filterdemo2 implements filter {\n   \n       @override\n       public void dofilter(servletrequest request, servletresponse response, filterchain chain) throws ioexception, servletexception {\n   \n           //1. 放行前，对 request数据进行处理\n           system.out.println("2.filterdemo...");\n           //放行\n           chain.dofilter(request,response);\n           //2. 放行后，对response 数据进行处理\n           system.out.println("4.filterdemo...");\n       }\n   \n       @override\n       public void init(filterconfig filterconfig) throws servletexception {\n       }\n   \n       @override\n       public void destroy() {\n       }\n   }\n   \n   \n\n * 修改 hello.jsp 页面中脚本的输出语句\n   \n   <%@ page contenttype="text/html;charset=utf-8" language="java" %>\n   <html>\n   <head>\n       <title>title</title>\n   </head>\n   <body>\n       <h1>hello jsp~</h1>\n       <%\n           system.out.println("3.hello jsp");\n       %>\n   </body>\n   </html>\n   \n\n * 启动服务器，在浏览器输入 http://localhost/filter-demo/hello.jsp 进行测试，在控制台打印内容如下\n   \n   从结果可以看到确实是按照我们之前说的执行流程进行执行的。\n\n# 问题\n\n上面代码中为什么是先执行 filterdemo ，后执行 filterdemo2 呢？\n\n我们现在使用的是注解配置filter，而这种配置方式的优先级是按照过滤器类名(字符串)的自然排序。\n\n比如有如下两个名称的过滤器 ： bfilterdemo 和 afilterdemo 。那一定是 afilterdemo 过滤器先执行。\n\n\n# 案例\n\n# 需求\n\n访问服务器资源时，需要先进行登录验证，如果没有登录，则自动跳转到登录页面\n\n# 分析\n\n我们要实现该功能是在每一个资源里加入登陆状态校验的代码吗？显然是不需要的，只需要写一个 filter ，在该过滤器中进行登陆状态校验即可。而在该 filter 中逻辑如下：\n\n# 代码实现\n\n# 创建filter\n\n在 brand-demo 工程创建 com.itheima.web.filter 包，在该下创建名为 loginfilter 的过滤器\n\n@webfilter("/*")\npublic class loginfilter implements filter {\n    @override\n    public void dofilter(servletrequest request, servletresponse response, filterchain chain) throws servletexception, ioexception {\n      \n    }\n\n    public void init(filterconfig config) throws servletexception {\n    }\n\n    public void destroy() {\n    }\n}\n\n\n# 编写逻辑代码\n\n在 dofilter() 方法中编写登陆状态校验的逻辑代码。\n\n我们首先需要从 session 对象中获取用户信息，但是 servletrequest 类型的 requset 对象没有获取 session 对象的方法，所以此时需要将 request对象强转成 httpservletrequest 对象。\n\nhttpservletrequest req = (httpservletrequest) request;\n\n\n然后完成以下逻辑\n\n * 获取session对象\n * 从session对象中获取名为 user 的数据\n * 判断获取到的数据是否是 null\n   * 如果不是，说明已经登陆，放行\n   * 如果是，说明尚未登陆，将提示信息存储到域对象中并跳转到登陆页面\n\n代码如下：\n\n@webfilter("/*")\npublic class loginfilter implements filter {\n    @override\n    public void dofilter(servletrequest request, servletresponse response, filterchain chain) throws servletexception, ioexception {\n        httpservletrequest req = (httpservletrequest) request;\n   \n        //1. 判断session中是否有user\n        httpsession session = req.getsession();\n        object user = session.getattribute("user");\n\n        //2. 判断user是否为null\n        if(user != null){\n            // 登录过了\n            //放行\n            chain.dofilter(request, response);\n        }else {\n            // 没有登陆，存储提示信息，跳转到登录页面\n\n            req.setattribute("login_msg","您尚未登陆！");\n            req.getrequestdispatcher("/login.jsp").forward(req,response);\n        }\n    }\n\n    public void init(filterconfig config) throws servletexception {\n    }\n\n    public void destroy() {\n    }\n}\n\n\n# 测试并抛出问题\n\n在浏览器上输入 http://localhost:8080/brand-demo/ ，可以看到如下页面效果\n\n从上面效果可以看出没有登陆确实是跳转到登陆页面了，但是登陆页面为什么展示成这种效果了呢？\n\n# 问题分析及解决\n\n因为登陆页面需要 css/login.css 这个文件进行样式的渲染，下图是登陆页面引入的css文件图解\n\n而在请求这个css资源时被过滤器拦截，就相当于没有加载到样式文件导致的。解决这个问题，只需要对所以的登陆相关的资源进行放行即可。还有一种情况就是当我没有用户信息时需要进行注册，而注册时也希望被过滤器放行。\n\n综上，我们需要在判断session中是否包含用户信息之前，应该加上对登陆及注册相关资源放行的逻辑处理\n\n//判断访问资源路径是否和登录注册相关\n//1,在数组中存储登陆和注册相关的资源路径\nstring[] urls = {"/login.jsp","/imgs/","/css/","/loginservlet","/register.jsp","/registerservlet","/checkcodeservlet"};\n//2,获取当前访问的资源路径\nstring url = req.getrequesturl().tostring(); \n\n//3,遍历数组，获取到每一个需要放行的资源路径\nfor (string u : urls) {\n    //4,判断当前访问的资源路径字符串是否包含要放行的的资源路径字符串\n    /*\n    \t比如当前访问的资源路径是  /brand-demo/login.jsp\n    \t而字符串 /brand-demo/login.jsp 包含了  字符串 /login.jsp ，所以这个字符串就需要放行\n    */\n    if(url.contains(u)){\n        //找到了，放行\n        chain.dofilter(request, response);\n        //break;\n        return;\n    }\n}\n\n\n# 过滤器完整代码\n\n@webfilter("/*")\npublic class loginfilter implements filter {\n    @override\n    public void dofilter(servletrequest request, servletresponse response, filterchain chain) throws servletexception, ioexception {\n        httpservletrequest req = (httpservletrequest) request;\n        \n        //判断访问资源路径是否和登录注册相关\n        //1,在数组中存储登陆和注册相关的资源路径\n        string[] urls = {"/login.jsp","/imgs/","/css/","/loginservlet","/register.jsp","/registerservlet","/checkcodeservlet"};\n        //2,获取当前访问的资源路径\n        string url = req.getrequesturl().tostring(); \n\n        //3,遍历数组，获取到每一个需要放行的资源路径\n        for (string u : urls) {\n            //4,判断当前访问的资源路径字符串是否包含要放行的的资源路径字符串\n            /*\n                比如当前访问的资源路径是  /brand-demo/login.jsp\n                而字符串 /brand-demo/login.jsp 包含了  字符串 /login.jsp ，所以这个字符串就需要放行\n            */\n            if(url.contains(u)){\n                //找到了，放行\n                chain.dofilter(request, response);\n                //break;\n                return;\n            }\n        }\n   \n        //1. 判断session中是否有user\n        httpsession session = req.getsession();\n        object user = session.getattribute("user");\n\n        //2. 判断user是否为null\n        if(user != null){\n            // 登录过了\n            //放行\n            chain.dofilter(request, response);\n        }else {\n            // 没有登陆，存储提示信息，跳转到登录页面\n\n            req.setattribute("login_msg","您尚未登陆！");\n            req.getrequestdispatcher("/login.jsp").forward(req,response);\n        }\n    }\n\n    public void init(filterconfig config) throws servletexception {\n    }\n\n    public void destroy() {\n    }\n}\n',charsets:{cjk:!0}},{title:"Ajax",frontmatter:{autoSort:98,title:"Ajax",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/36c3d1/",categories:["后端","JavaWeb"],tags:["知识","JavaWeb"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/20.JavaWeb/20.Ajax.html",relativePath:"01.后端/20.JavaWeb/20.Ajax.md",key:"v-66b680f6",path:"/pages/36c3d1/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:11},{level:2,title:"快速入门",slug:"快速入门",normalizedTitle:"快速入门",charIndex:922},{level:2,title:"案例",slug:"案例",normalizedTitle:"案例",charIndex:2962}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"概述 快速入门 案例",content:'# Ajax\n\n\n# 概述\n\nAJAX (Asynchronous JavaScript And XML)：异步的 JavaScript 和 XML。\n\n我们先来说概念中的 JavaScript 和 XML，JavaScript 表明该技术和前端相关；XML 是指以此进行数据交换。而这两个我们之前都学习过。\n\n# 作用\n\nAJAX 作用有以下两方面：\n\n 1. 与服务器进行数据交换：通过AJAX可以给服务器发送请求，服务器将数据直接响应回给浏览器。如下图\n\n我们先来看之前做功能的流程，如下图：\n\n如上图，Servlet 调用完业务逻辑层后将数据存储到域对象中，然后跳转到指定的 jsp 页面，在页面上使用 EL表达式 和 JSTL 标签库进行数据的展示。\n\n而我们学习了AJAX 后，就可以使用AJAX和服务器进行通信，以达到使用 HTML+AJAX来替换JSP页面了。如下图，浏览器发送请求servlet，servlet 调用完业务逻辑层后将数据直接响应回给浏览器页面，页面使用 HTML 来进行数据展示。\n\n 2. 异步交互：可以在不重新加载整个页面的情况下，与服务器交换数据并更新部分网页的技术，如：搜索联想、用户名是否可用校验，等等…\n\n上图所示的效果我们经常见到，在我们输入一些关键字（例如 奥运）后就会在下面联想出相关的内容，而联想出来的这部分数据肯定是存储在百度的服务器上，而我们并没有看出页面重新刷新，这就是 更新局部页面 的效果。再如下图：\n\n我们在用户名的输入框输入用户名，当输入框一失去焦点，如果用户名已经被占用就会在下方展示提示的信息；在这整个过程中也没有页面的刷新，只是在局部展示出了提示信息，这就是 更新局部页面 的效果。\n\n# 同步和异步\n\n知道了局部刷新后，接下来我们再聊聊同步和异步:\n\n * 同步发送请求过程如下\n\n浏览器页面在发送请求给服务器，在服务器处理请求的过程中，浏览器页面不能做其他的操作。只能等到服务器响应结束后才能，浏览器页面才能继续做其他的操作。\n\n * 异步发送请求过程如下\n   \n   浏览器页面发送请求给服务器，在服务器处理请求的过程中，浏览器页面还可以做其他的操作。\n\n\n# 快速入门\n\n# 服务端实现\n\n在项目的创建 com.itheima.web.servlet ，并在该包下创建名为 AjaxServlet 的servlet\n\n@WebServlet("/ajaxServlet")\npublic class AjaxServlet extends HttpServlet {\n    @Override\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        //1. 响应数据\n        response.getWriter().write("hello ajax~");\n    }\n\n    @Override\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        this.doGet(request, response);\n    }\n}\n\n\n# 客户端实现\n\n在 webapp 下创建名为 01-ajax-demo1.html 的页面，在该页面书写 ajax 代码\n\n * 创建核心对象，不同的浏览器创建的对象是不同的\n   \n    var xhttp;\n   if (window.XMLHttpRequest) {\n       xhttp = new XMLHttpRequest();\n   } else {\n       // code for IE6, IE5\n       xhttp = new ActiveXObject("Microsoft.XMLHTTP");\n   }\n   \n\n * 发送请求\n   \n   //建立连接\n   xhttp.open("GET", "http://localhost:8080/ajax-demo/ajaxServlet");\n   //发送请求\n   xhttp.send();\n   \n\n * 获取响应\n   \n   xhttp.onreadystatechange = function() {\n       if (this.readyState ** 4 && this.status ** 200) {\n           // 通过 this.responseText 可以获取到服务端响应的数据\n           alert(this.responseText);\n       }\n   };\n   \n\n完整代码如下：\n\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <title>Title</title>\n</head>\n<body>\n\n<script>\n    //1. 创建核心对象\n    var xhttp;\n    if (window.XMLHttpRequest) {\n        xhttp = new XMLHttpRequest();\n    } else {\n        // code for IE6, IE5\n        xhttp = new ActiveXObject("Microsoft.XMLHTTP");\n    }\n    //2. 发送请求\n    xhttp.open("GET", "http://localhost:8080/ajax-demo/ajaxServlet");\n    xhttp.send();\n\n    //3. 获取响应\n    xhttp.onreadystatechange = function() {\n        if (this.readyState ** 4 && this.status ** 200) {\n            alert(this.responseText);\n        }\n    };\n<\/script>\n</body>\n</html>\n\n\n# 测试\n\n在浏览器地址栏输入 http://localhost:8080/ajax-demo/01-ajax-demo1.html ，在 01-ajax-demo1.html加载的时候就会发送 ajax 请求，效果如下\n\n我们可以通过 开发者模式 查看发送的 AJAX 请求。在浏览器上按 F12 快捷键\n\n这个是查看所有的请求，如果我们只是想看 异步请求的话，点击上图中 All 旁边的 XHR，会发现只展示 Type 是 xhr 的请求。如下图：\n\n\n# 案例\n\n需求：在完成用户注册时，当用户名输入框失去焦点时，校验用户名是否在数据库已存在\n\n# 分析\n\n * 前端完成的逻辑\n   1. 给用户名输入框绑定光标失去焦点事件 onblur\n   2. 发送 ajax请求，携带username参数\n   3. 处理响应：是否显示提示信息\n * 后端完成的逻辑\n   1. 接收用户名\n   2. 调用service查询User。此案例是为了演示前后端异步交互，所以此处我们不做业务逻辑处理\n   3. 返回标记\n\n整体流程如下：\n\n# 后端实现\n\n在 com.ithiema.web.servlet 包中定义名为 SelectUserServlet 的servlet。代码如下：\n\n@WebServlet("/selectUserServlet")\npublic class SelectUserServlet extends HttpServlet {\n    @Override\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        //1. 接收用户名\n        String username = request.getParameter("username");\n        //2. 调用service查询User对象，此处不进行业务逻辑处理，直接给 flag 赋值为 true，表明用户名占用\n        boolean flag = true;\n        //3. 响应标记\n        response.getWriter().write("" + flag);\n    }\n\n    @Override\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        this.doGet(request, response);\n    }\n}\n\n\n# 前端实现\n\n将 04-资料\\1. 验证用户名案例\\1. 静态页面 下的文件整体拷贝到项目下 webapp 下。并在 register.html 页面的 body 结束标签前编写 script 标签，在该标签中实现如下逻辑\n\n第一步：给用户名输入框绑定光标失去焦点事件 onblur\n\n//1. 给用户名输入框绑定 失去焦点事件\ndocument.getElementById("username").onblur = function () {\n    \n}\n\n\n第二步：发送 ajax请求，携带username参数\n\n在 第一步 绑定的匿名函数中书写发送 ajax 请求的代码\n\n//2. 发送ajax请求\n//2.1. 创建核心对象\nvar xhttp;\nif (window.XMLHttpRequest) {\n    xhttp = new XMLHttpRequest();\n} else {\n    // code for IE6, IE5\n    xhttp = new ActiveXObject("Microsoft.XMLHTTP");\n}\n//2.2. 发送请求\nxhttp.open("GET", "http://localhost:8080/ajax-demo/selectUserServlet);\nxhttp.send();\n\n//2.3. 获取响应\nxhttp.onreadystatechange = function() {\n    if (this.readyState ** 4 && this.status ** 200) {\n        //处理响应的结果\n    }\n};\n\n\n由于我们发送的是 GET 请求，所以需要在 URL 后拼接从输入框获取的用户名数据。而我们在 第一步 绑定的匿名函数中通过以下代码可以获取用户名数据\n\n// 获取用户名的值\nvar username = this.value;  //this ： 给谁绑定的事件，this就代表谁\n\n\n而携带数据需要将 URL 修改为：\n\nxhttp.open("GET", "http://localhost:8080/ajax-demo/selectUserServlet?username="+username);\n\n\n第三步：处理响应：是否显示提示信息\n\n当 this.readyState ** 4 && this.status ** 200 条件满足时，说明已经成功响应数据了。\n\n此时需要判断响应的数据是否是 "true" 字符串，如果是说明用户名已经占用给出错误提示；如果不是说明用户名未被占用清除错误提示。代码如下\n\n//判断\nif(this.responseText ** "true"){\n    //用户名存在，显示提示信息\n    document.getElementById("username_err").style.display = \'\';\n}else {\n    //用户名不存在 ，清楚提示信息\n    document.getElementById("username_err").style.display = \'none\';\n}\n\n\n综上所述，前端完成代码如下：\n\n//1. 给用户名输入框绑定 失去焦点事件\ndocument.getElementById("username").onblur = function () {\n    //2. 发送ajax请求\n    // 获取用户名的值\n    var username = this.value;\n\n    //2.1. 创建核心对象\n    var xhttp;\n    if (window.XMLHttpRequest) {\n        xhttp = new XMLHttpRequest();\n    } else {\n        // code for IE6, IE5\n        xhttp = new ActiveXObject("Microsoft.XMLHTTP");\n    }\n    //2.2. 发送请求\n    xhttp.open("GET", "http://localhost:8080/ajax-demo/selectUserServlet?username="+username);\n    xhttp.send();\n\n    //2.3. 获取响应\n    xhttp.onreadystatechange = function() {\n        if (this.readyState ** 4 && this.status ** 200) {\n            //alert(this.responseText);\n            //判断\n            if(this.responseText ** "true"){\n                //用户名存在，显示提示信息\n                document.getElementById("username_err").style.display = \'\';\n            }else {\n                //用户名不存在 ，清楚提示信息\n                document.getElementById("username_err").style.display = \'none\';\n            }\n        }\n    };\n}\n',normalizedContent:'# ajax\n\n\n# 概述\n\najax (asynchronous javascript and xml)：异步的 javascript 和 xml。\n\n我们先来说概念中的 javascript 和 xml，javascript 表明该技术和前端相关；xml 是指以此进行数据交换。而这两个我们之前都学习过。\n\n# 作用\n\najax 作用有以下两方面：\n\n 1. 与服务器进行数据交换：通过ajax可以给服务器发送请求，服务器将数据直接响应回给浏览器。如下图\n\n我们先来看之前做功能的流程，如下图：\n\n如上图，servlet 调用完业务逻辑层后将数据存储到域对象中，然后跳转到指定的 jsp 页面，在页面上使用 el表达式 和 jstl 标签库进行数据的展示。\n\n而我们学习了ajax 后，就可以使用ajax和服务器进行通信，以达到使用 html+ajax来替换jsp页面了。如下图，浏览器发送请求servlet，servlet 调用完业务逻辑层后将数据直接响应回给浏览器页面，页面使用 html 来进行数据展示。\n\n 2. 异步交互：可以在不重新加载整个页面的情况下，与服务器交换数据并更新部分网页的技术，如：搜索联想、用户名是否可用校验，等等…\n\n上图所示的效果我们经常见到，在我们输入一些关键字（例如 奥运）后就会在下面联想出相关的内容，而联想出来的这部分数据肯定是存储在百度的服务器上，而我们并没有看出页面重新刷新，这就是 更新局部页面 的效果。再如下图：\n\n我们在用户名的输入框输入用户名，当输入框一失去焦点，如果用户名已经被占用就会在下方展示提示的信息；在这整个过程中也没有页面的刷新，只是在局部展示出了提示信息，这就是 更新局部页面 的效果。\n\n# 同步和异步\n\n知道了局部刷新后，接下来我们再聊聊同步和异步:\n\n * 同步发送请求过程如下\n\n浏览器页面在发送请求给服务器，在服务器处理请求的过程中，浏览器页面不能做其他的操作。只能等到服务器响应结束后才能，浏览器页面才能继续做其他的操作。\n\n * 异步发送请求过程如下\n   \n   浏览器页面发送请求给服务器，在服务器处理请求的过程中，浏览器页面还可以做其他的操作。\n\n\n# 快速入门\n\n# 服务端实现\n\n在项目的创建 com.itheima.web.servlet ，并在该包下创建名为 ajaxservlet 的servlet\n\n@webservlet("/ajaxservlet")\npublic class ajaxservlet extends httpservlet {\n    @override\n    protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        //1. 响应数据\n        response.getwriter().write("hello ajax~");\n    }\n\n    @override\n    protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        this.doget(request, response);\n    }\n}\n\n\n# 客户端实现\n\n在 webapp 下创建名为 01-ajax-demo1.html 的页面，在该页面书写 ajax 代码\n\n * 创建核心对象，不同的浏览器创建的对象是不同的\n   \n    var xhttp;\n   if (window.xmlhttprequest) {\n       xhttp = new xmlhttprequest();\n   } else {\n       // code for ie6, ie5\n       xhttp = new activexobject("microsoft.xmlhttp");\n   }\n   \n\n * 发送请求\n   \n   //建立连接\n   xhttp.open("get", "http://localhost:8080/ajax-demo/ajaxservlet");\n   //发送请求\n   xhttp.send();\n   \n\n * 获取响应\n   \n   xhttp.onreadystatechange = function() {\n       if (this.readystate ** 4 && this.status ** 200) {\n           // 通过 this.responsetext 可以获取到服务端响应的数据\n           alert(this.responsetext);\n       }\n   };\n   \n\n完整代码如下：\n\n<!doctype html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>title</title>\n</head>\n<body>\n\n<script>\n    //1. 创建核心对象\n    var xhttp;\n    if (window.xmlhttprequest) {\n        xhttp = new xmlhttprequest();\n    } else {\n        // code for ie6, ie5\n        xhttp = new activexobject("microsoft.xmlhttp");\n    }\n    //2. 发送请求\n    xhttp.open("get", "http://localhost:8080/ajax-demo/ajaxservlet");\n    xhttp.send();\n\n    //3. 获取响应\n    xhttp.onreadystatechange = function() {\n        if (this.readystate ** 4 && this.status ** 200) {\n            alert(this.responsetext);\n        }\n    };\n<\/script>\n</body>\n</html>\n\n\n# 测试\n\n在浏览器地址栏输入 http://localhost:8080/ajax-demo/01-ajax-demo1.html ，在 01-ajax-demo1.html加载的时候就会发送 ajax 请求，效果如下\n\n我们可以通过 开发者模式 查看发送的 ajax 请求。在浏览器上按 f12 快捷键\n\n这个是查看所有的请求，如果我们只是想看 异步请求的话，点击上图中 all 旁边的 xhr，会发现只展示 type 是 xhr 的请求。如下图：\n\n\n# 案例\n\n需求：在完成用户注册时，当用户名输入框失去焦点时，校验用户名是否在数据库已存在\n\n# 分析\n\n * 前端完成的逻辑\n   1. 给用户名输入框绑定光标失去焦点事件 onblur\n   2. 发送 ajax请求，携带username参数\n   3. 处理响应：是否显示提示信息\n * 后端完成的逻辑\n   1. 接收用户名\n   2. 调用service查询user。此案例是为了演示前后端异步交互，所以此处我们不做业务逻辑处理\n   3. 返回标记\n\n整体流程如下：\n\n# 后端实现\n\n在 com.ithiema.web.servlet 包中定义名为 selectuserservlet 的servlet。代码如下：\n\n@webservlet("/selectuserservlet")\npublic class selectuserservlet extends httpservlet {\n    @override\n    protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        //1. 接收用户名\n        string username = request.getparameter("username");\n        //2. 调用service查询user对象，此处不进行业务逻辑处理，直接给 flag 赋值为 true，表明用户名占用\n        boolean flag = true;\n        //3. 响应标记\n        response.getwriter().write("" + flag);\n    }\n\n    @override\n    protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        this.doget(request, response);\n    }\n}\n\n\n# 前端实现\n\n将 04-资料\\1. 验证用户名案例\\1. 静态页面 下的文件整体拷贝到项目下 webapp 下。并在 register.html 页面的 body 结束标签前编写 script 标签，在该标签中实现如下逻辑\n\n第一步：给用户名输入框绑定光标失去焦点事件 onblur\n\n//1. 给用户名输入框绑定 失去焦点事件\ndocument.getelementbyid("username").onblur = function () {\n    \n}\n\n\n第二步：发送 ajax请求，携带username参数\n\n在 第一步 绑定的匿名函数中书写发送 ajax 请求的代码\n\n//2. 发送ajax请求\n//2.1. 创建核心对象\nvar xhttp;\nif (window.xmlhttprequest) {\n    xhttp = new xmlhttprequest();\n} else {\n    // code for ie6, ie5\n    xhttp = new activexobject("microsoft.xmlhttp");\n}\n//2.2. 发送请求\nxhttp.open("get", "http://localhost:8080/ajax-demo/selectuserservlet);\nxhttp.send();\n\n//2.3. 获取响应\nxhttp.onreadystatechange = function() {\n    if (this.readystate ** 4 && this.status ** 200) {\n        //处理响应的结果\n    }\n};\n\n\n由于我们发送的是 get 请求，所以需要在 url 后拼接从输入框获取的用户名数据。而我们在 第一步 绑定的匿名函数中通过以下代码可以获取用户名数据\n\n// 获取用户名的值\nvar username = this.value;  //this ： 给谁绑定的事件，this就代表谁\n\n\n而携带数据需要将 url 修改为：\n\nxhttp.open("get", "http://localhost:8080/ajax-demo/selectuserservlet?username="+username);\n\n\n第三步：处理响应：是否显示提示信息\n\n当 this.readystate ** 4 && this.status ** 200 条件满足时，说明已经成功响应数据了。\n\n此时需要判断响应的数据是否是 "true" 字符串，如果是说明用户名已经占用给出错误提示；如果不是说明用户名未被占用清除错误提示。代码如下\n\n//判断\nif(this.responsetext ** "true"){\n    //用户名存在，显示提示信息\n    document.getelementbyid("username_err").style.display = \'\';\n}else {\n    //用户名不存在 ，清楚提示信息\n    document.getelementbyid("username_err").style.display = \'none\';\n}\n\n\n综上所述，前端完成代码如下：\n\n//1. 给用户名输入框绑定 失去焦点事件\ndocument.getelementbyid("username").onblur = function () {\n    //2. 发送ajax请求\n    // 获取用户名的值\n    var username = this.value;\n\n    //2.1. 创建核心对象\n    var xhttp;\n    if (window.xmlhttprequest) {\n        xhttp = new xmlhttprequest();\n    } else {\n        // code for ie6, ie5\n        xhttp = new activexobject("microsoft.xmlhttp");\n    }\n    //2.2. 发送请求\n    xhttp.open("get", "http://localhost:8080/ajax-demo/selectuserservlet?username="+username);\n    xhttp.send();\n\n    //2.3. 获取响应\n    xhttp.onreadystatechange = function() {\n        if (this.readystate ** 4 && this.status ** 200) {\n            //alert(this.responsetext);\n            //判断\n            if(this.responsetext ** "true"){\n                //用户名存在，显示提示信息\n                document.getelementbyid("username_err").style.display = \'\';\n            }else {\n                //用户名不存在 ，清楚提示信息\n                document.getelementbyid("username_err").style.display = \'none\';\n            }\n        }\n    };\n}\n',charsets:{cjk:!0}},{title:"Axios",frontmatter:{autoSort:96,title:"Axios",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/140b55/",categories:["后端","JavaWeb"],tags:["知识","JavaWeb"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/20.JavaWeb/25.Axios.html",relativePath:"01.后端/20.JavaWeb/25.Axios.md",key:"v-ce7d277a",path:"/pages/140b55/",headers:[{level:2,title:"基本使用",slug:"基本使用",normalizedTitle:"基本使用",charIndex:74},{level:2,title:"快速入门",slug:"快速入门",normalizedTitle:"快速入门",charIndex:999},{level:3,title:"后端实现",slug:"后端实现",normalizedTitle:"后端实现",charIndex:1008},{level:3,title:"前端实现",slug:"前端实现",normalizedTitle:"前端实现",charIndex:1714},{level:2,title:"请求方法别名",slug:"请求方法别名",normalizedTitle:"请求方法别名",charIndex:2867}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"基本使用 快速入门 后端实现 前端实现 请求方法别名",content:'# Axios\n\nAxios 对原生的AJAX进行封装，简化书写。\n\nAxios官网是：https://www.axios-http.cn\n\n\n# 基本使用\n\naxios 使用是比较简单的，分为以下两步：\n\n * 引入 axios 的 js 文件\n   \n   <script src="js/axios-0.18.0.js"><\/script>\n   \n\n * 使用axios 发送请求，并获取响应结果\n   \n   * 发送 get 请求\n     \n     axios({\n         method:"get",\n         url:"http://localhost:8080/ajax-demo1/aJAXDemo1?username=zhangsan"\n     }).then(function (resp){\n         alert(resp.data);\n     })\n     \n   \n   * 发送 post 请求\n     \n     axios({\n         method:"post",\n         url:"http://localhost:8080/ajax-demo1/aJAXDemo1",\n         data:"username=zhangsan"\n     }).then(function (resp){\n         alert(resp.data);\n     });\n     \n\naxios() 是用来发送异步请求的，小括号中使用 js 对象传递请求相关的参数：\n\n * method 属性：用来设置请求方式的。取值为 get 或者 post。\n * url 属性：用来书写请求的资源路径。如果是 get 请求，需要将请求参数拼接到路径的后面，格式为： url?参数名=参数值&参数名2=参数值2。\n * data 属性：作为请求体被发送的数据。也就是说如果是 post 请求的话，数据需要作为 data 属性的值。\n\nthen() 需要传递一个匿名函数。我们将 then() 中传递的匿名函数称为 回调函数，意思是该匿名函数在发送请求时不会被调用，而是在成功响应后调用的函数。而该回调函数中的 resp 参数是对响应的数据进行封装的对象，通过 resp.data 可以获取到响应的数据。\n\n\n# 快速入门\n\n\n# 后端实现\n\n定义一个用于接收请求的servlet，代码如下：\n\n@WebServlet("/axiosServlet")\npublic class AxiosServlet extends HttpServlet {\n    @Override\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        System.out.println("get...");\n        //1. 接收请求参数\n        String username = request.getParameter("username");\n        System.out.println(username);\n        //2. 响应数据\n        response.getWriter().write("hello Axios~");\n    }\n\n    @Override\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        System.out.println("post...");\n        this.doGet(request, response);\n    }\n}\n\n\n\n# 前端实现\n\n * 引入 js 文件\n   \n   <script src="js/axios-0.18.0.js"><\/script>\n   \n\n * 发送 ajax 请求\n   \n   * get 请求\n     \n     axios({\n         method:"get",\n         url:"http://localhost:8080/ajax-demo/axiosServlet?username=zhangsan"\n     }).then(function (resp) {\n         alert(resp.data);\n     })\n     \n   \n   * post 请求\n     \n     axios({\n         method:"post",\n         url:"http://localhost:8080/ajax-demo/axiosServlet",\n         data:"username=zhangsan"\n     }).then(function (resp) {\n         alert(resp.data);\n     })\n     \n\n整体页面代码如下：\n\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <title>Title</title>\n</head>\n<body>\n\n<script src="js/axios-0.18.0.js"><\/script>\n<script>\n    //1. get\n   /* axios({\n        method:"get",\n        url:"http://localhost:8080/ajax-demo/axiosServlet?username=zhangsan"\n    }).then(function (resp) {\n        alert(resp.data);\n    })*/\n\n    //2. post  在js中{} 表示一个js对象，而这个js对象中有三个属性\n    axios({\n        method:"post",\n        url:"http://localhost:8080/ajax-demo/axiosServlet",\n        data:"username=zhangsan"\n    }).then(function (resp) {\n        alert(resp.data);\n    })\n<\/script>\n</body>\n</html>\n\n\n\n# 请求方法别名\n\n为了方便起见， Axios 已经为所有支持的请求方法提供了别名。如下：\n\n * get 请求 ： axios.get(url[,config])\n\n * delete 请求 ： axios.delete(url[,config])\n\n * head 请求 ： axios.head(url[,config])\n\n * options 请求 ： axios.option(url[,config])\n\n * post 请求：axios.post(url[,data[,config])\n\n * put 请求：axios.put(url[,data[,config])\n\n * patch 请求：axios.patch(url[,data[,config])\n\n而我们只关注 get 请求和 post 请求。\n\n入门案例中的 get 请求代码可以改为如下：\n\naxios.get("http://localhost:8080/ajax-demo/axiosServlet?username=zhangsan").then(function (resp) {\n    alert(resp.data);\n});\n\n\n入门案例中的 post 请求代码可以改为如下：\n\naxios.post("http://localhost:8080/ajax-demo/axiosServlet","username=zhangsan").then(function (resp) {\n    alert(resp.data);\n})\n',normalizedContent:'# axios\n\naxios 对原生的ajax进行封装，简化书写。\n\naxios官网是：https://www.axios-http.cn\n\n\n# 基本使用\n\naxios 使用是比较简单的，分为以下两步：\n\n * 引入 axios 的 js 文件\n   \n   <script src="js/axios-0.18.0.js"><\/script>\n   \n\n * 使用axios 发送请求，并获取响应结果\n   \n   * 发送 get 请求\n     \n     axios({\n         method:"get",\n         url:"http://localhost:8080/ajax-demo1/ajaxdemo1?username=zhangsan"\n     }).then(function (resp){\n         alert(resp.data);\n     })\n     \n   \n   * 发送 post 请求\n     \n     axios({\n         method:"post",\n         url:"http://localhost:8080/ajax-demo1/ajaxdemo1",\n         data:"username=zhangsan"\n     }).then(function (resp){\n         alert(resp.data);\n     });\n     \n\naxios() 是用来发送异步请求的，小括号中使用 js 对象传递请求相关的参数：\n\n * method 属性：用来设置请求方式的。取值为 get 或者 post。\n * url 属性：用来书写请求的资源路径。如果是 get 请求，需要将请求参数拼接到路径的后面，格式为： url?参数名=参数值&参数名2=参数值2。\n * data 属性：作为请求体被发送的数据。也就是说如果是 post 请求的话，数据需要作为 data 属性的值。\n\nthen() 需要传递一个匿名函数。我们将 then() 中传递的匿名函数称为 回调函数，意思是该匿名函数在发送请求时不会被调用，而是在成功响应后调用的函数。而该回调函数中的 resp 参数是对响应的数据进行封装的对象，通过 resp.data 可以获取到响应的数据。\n\n\n# 快速入门\n\n\n# 后端实现\n\n定义一个用于接收请求的servlet，代码如下：\n\n@webservlet("/axiosservlet")\npublic class axiosservlet extends httpservlet {\n    @override\n    protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        system.out.println("get...");\n        //1. 接收请求参数\n        string username = request.getparameter("username");\n        system.out.println(username);\n        //2. 响应数据\n        response.getwriter().write("hello axios~");\n    }\n\n    @override\n    protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        system.out.println("post...");\n        this.doget(request, response);\n    }\n}\n\n\n\n# 前端实现\n\n * 引入 js 文件\n   \n   <script src="js/axios-0.18.0.js"><\/script>\n   \n\n * 发送 ajax 请求\n   \n   * get 请求\n     \n     axios({\n         method:"get",\n         url:"http://localhost:8080/ajax-demo/axiosservlet?username=zhangsan"\n     }).then(function (resp) {\n         alert(resp.data);\n     })\n     \n   \n   * post 请求\n     \n     axios({\n         method:"post",\n         url:"http://localhost:8080/ajax-demo/axiosservlet",\n         data:"username=zhangsan"\n     }).then(function (resp) {\n         alert(resp.data);\n     })\n     \n\n整体页面代码如下：\n\n<!doctype html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>title</title>\n</head>\n<body>\n\n<script src="js/axios-0.18.0.js"><\/script>\n<script>\n    //1. get\n   /* axios({\n        method:"get",\n        url:"http://localhost:8080/ajax-demo/axiosservlet?username=zhangsan"\n    }).then(function (resp) {\n        alert(resp.data);\n    })*/\n\n    //2. post  在js中{} 表示一个js对象，而这个js对象中有三个属性\n    axios({\n        method:"post",\n        url:"http://localhost:8080/ajax-demo/axiosservlet",\n        data:"username=zhangsan"\n    }).then(function (resp) {\n        alert(resp.data);\n    })\n<\/script>\n</body>\n</html>\n\n\n\n# 请求方法别名\n\n为了方便起见， axios 已经为所有支持的请求方法提供了别名。如下：\n\n * get 请求 ： axios.get(url[,config])\n\n * delete 请求 ： axios.delete(url[,config])\n\n * head 请求 ： axios.head(url[,config])\n\n * options 请求 ： axios.option(url[,config])\n\n * post 请求：axios.post(url[,data[,config])\n\n * put 请求：axios.put(url[,data[,config])\n\n * patch 请求：axios.patch(url[,data[,config])\n\n而我们只关注 get 请求和 post 请求。\n\n入门案例中的 get 请求代码可以改为如下：\n\naxios.get("http://localhost:8080/ajax-demo/axiosservlet?username=zhangsan").then(function (resp) {\n    alert(resp.data);\n});\n\n\n入门案例中的 post 请求代码可以改为如下：\n\naxios.post("http://localhost:8080/ajax-demo/axiosservlet","username=zhangsan").then(function (resp) {\n    alert(resp.data);\n})\n',charsets:{cjk:!0}},{title:"Json",frontmatter:{autoSort:95,title:"Json",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/63836e/",categories:["后端","JavaWeb"],tags:["知识","JavaWeb"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/20.JavaWeb/30.JSON.html",relativePath:"01.后端/20.JavaWeb/30.JSON.md",key:"v-29dae873",path:"/pages/63836e/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:11},{level:2,title:"JSON 基础语法",slug:"json-基础语法",normalizedTitle:"json 基础语法",charIndex:713},{level:3,title:"定义格式",slug:"定义格式",normalizedTitle:"定义格式",charIndex:82},{level:3,title:"代码演示",slug:"代码演示",normalizedTitle:"代码演示",charIndex:1038},{level:2,title:"JSON串和Java对象的相互转换",slug:"json串和java对象的相互转换",normalizedTitle:"json串和java对象的相互转换",charIndex:3112},{level:3,title:"Fastjson 概述",slug:"fastjson-概述",normalizedTitle:"fastjson 概述",charIndex:3377},{level:3,title:"Fastjson 使用",slug:"fastjson-使用",normalizedTitle:"fastjson 使用",charIndex:3488},{level:3,title:"代码演示",slug:"代码演示-2",normalizedTitle:"代码演示",charIndex:1038}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"概述 JSON 基础语法 定义格式 代码演示 JSON串和Java对象的相互转换 Fastjson 概述 Fastjson 使用 代码演示",content:'# JSON\n\n\n# 概述\n\n概念：JavaScript Object Notation。JavaScript 对象表示法.\n\n如下是 JavaScript 对象的定义格式：\n\n{\n\tname:"zhangsan",\n\tage:23,\n\tcity:"北京"\n}\n\n\n接下来我们再看看 JSON 的格式：\n\n{\n\t"name":"zhangsan",\n\t"age":23,\n\t"city":"北京"\n}\n\n\n通过上面 js 对象格式和 json 格式进行对比，发现两个格式特别像。只不过 js 对象中的属性名可以使用引号（可以是单引号，也可以是双引号）；而 json 格式中的键要求必须使用双引号括起来，这是 json 格式的规定。json 格式的数据有什么作用呢？\n\n作用：由于其语法格式简单，层次结构鲜明，现多用于作为数据载体，在网络中进行数据传输。如下图所示就是服务端给浏览器响应的数据，这个数据比较简单，如果现需要将 JAVA 对象中封装的数据响应回给浏览器的话，应该以何种数据传输呢？\n\n大家还记得 ajax 的概念吗？ 是 异步的 JavaScript 和 xml。这里的 xml就是以前进行数据传递的方式，如下：\n\n<student>\n    <name>张三</name>\n    <age>23</age>\n    <city>北京</city>\n</student>\n\n\n再看 json 描述以上数据的写法：\n\n{\t\n\t"name":"张三",\n    "age":23,\n    "city":"北京"\n}\n\n\n上面两种格式进行对比后就会发现 json 格式数据的简单，以及所占的字节数少等优点。\n\n\n# JSON 基础语法\n\n\n# 定义格式\n\nJSON 本质就是一个字符串，但是该字符串内容是有一定的格式要求的。 定义格式如下：\n\nvar 变量名 = \'{"key":value,"key":value,...}\';\n\n\nJSON 串的键要求必须使用双引号括起来，而值根据要表示的类型确定。value 的数据类型分为如下\n\n * 数字（整数或浮点数）\n * 字符串（使用双引号括起来）\n * 逻辑值（true或者false）\n * 数组（在方括号中）\n * 对象（在花括号中）\n * null\n\n示例：\n\nvar jsonStr = \'{"name":"zhangsan","age":23,"addr":["北京","上海","西安"]}\'\n\n\n\n# 代码演示\n\n创建一个页面，在该页面的 <script> 标签中定义json字符串\n\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <title>Title</title>\n</head>\n<body>\n<script>\n    //1. 定义JSON字符串\n    var jsonStr = \'{"name":"zhangsan","age":23,"addr":["北京","上海","西安"]}\'\n    alert(jsonStr);\n\n<\/script>\n</body>\n</html>\n\n\n通过浏览器打开，页面效果如下图所示\n\n现在我们需要获取到该 JSON 串中的 name 属性值，应该怎么处理呢？\n\n如果它是一个 js 对象，我们就可以通过 js对象.属性名 的方式来获取数据。JS 提供了一个对象 JSON ，该对象有如下两个方法：\n\n * parse(str) ：将 JSON串转换为 js 对象。使用方式是： var jsObject = JSON.parse(jsonStr);\n * stringify(obj) ：将 js 对象转换为 JSON 串。使用方式是：var jsonStr = JSON.stringify(jsObject)\n\n代码演示：\n\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <title>Title</title>\n</head>\n<body>\n<script>\n    //1. 定义JSON字符串\n    var jsonStr = \'{"name":"zhangsan","age":23,"addr":["北京","上海","西安"]}\'\n    alert(jsonStr);\n\n    //2. 将 JSON 字符串转为 JS 对象\n    let jsObject = JSON.parse(jsonStr);\n    alert(jsObject)\n    alert(jsObject.name)\n    //3. 将 JS 对象转换为 JSON 字符串\n    let jsonStr2 = JSON.stringify(jsObject);\n    alert(jsonStr2)\n<\/script>\n</body>\n</html>\n\n\n# 发送异步请求携带参数\n\n后面我们使用 axios 发送请求时，如果要携带复杂的数据时都会以 JSON 格式进行传递，如下\n\naxios({\n    method:"post",\n    url:"http://localhost:8080/ajax-demo/axiosServlet",\n    data:"username=zhangsan"\n}).then(function (resp) {\n    alert(resp.data);\n})\n\n\n请求参数不可能由我们自己拼接字符串吧？肯定不用，可以提前定义一个 js 对象，用来封装需要提交的参数，然后使用 JSON.stringify(js对象) 转换为 JSON 串，再将该 JSON 串作为 axios 的 data 属性值进行请求参数的提交。如下：\n\nvar jsObject = {name:"张三"};\n\naxios({\n    method:"post",\n    url:"http://localhost:8080/ajax-demo/axiosServlet",\n    data: JSON.stringify(jsObject)\n}).then(function (resp) {\n    alert(resp.data);\n})\n\n\n而 axios 是一个很强大的工具。我们只需要将需要提交的参数封装成 js 对象，并将该 js 对象作为 axios 的 data 属性值进行，它会自动将 js 对象转换为 JSON 串进行提交。如下：\n\nvar jsObject = {name:"张三"};\n\naxios({\n    method:"post",\n    url:"http://localhost:8080/ajax-demo/axiosServlet",\n    data:jsObject  //这里 axios 会将该js对象转换为 json 串的\n}).then(function (resp) {\n    alert(resp.data);\n})\n\n\n> 注意：\n> \n>  * js 提供的 JSON 对象我们只需要了解一下即可。因为 axios 会自动对 js 对象和 JSON 串进行想换转换。\n>  * 发送异步请求时，如果请求参数是 JSON 格式，那请求方式必须是 POST。因为 JSON 串需要放在请求体中。\n\n\n# JSON串和Java对象的相互转换\n\n学习完 json 后，接下来聊聊 json 的作用。以后我们会以 json 格式的数据进行前后端交互。前端发送请求时，如果是复杂的数据就会以 json 提交给后端；而后端如果需要响应一些复杂的数据时，也需要以 json 格式将数据响应回给浏览器。\n\n在后端我们就需要重点学习以下两部分操作：\n\n * 请求数据：JSON字符串转为Java对象\n * 响应数据：Java对象转为JSON字符串\n\n接下来给大家介绍一套 API，可以实现上面两部分操作。这套 API 就是 Fastjson\n\n\n# Fastjson 概述\n\nFastjson 是阿里巴巴提供的一个Java语言编写的高性能功能完善的 JSON 库，是目前Java语言中最快的 JSON 库，可以实现 Java 对象和 JSON 字符串的相互转换。\n\n\n# Fastjson 使用\n\nFastjson 使用也是比较简单的，分为以下三步完成\n\n 1. 导入坐标\n    \n    <dependency>\n        <groupId>com.alibaba</groupId>\n        <artifactId>fastjson</artifactId>\n        <version>1.2.62</version>\n    </dependency>\n    \n\n 2. Java对象转JSON\n    \n    String jsonStr = JSON.toJSONString(obj);\n    \n    \n    将 Java 对象转换为 JSON 串，只需要使用 Fastjson 提供的 JSON 类中的 toJSONString() 静态方法即可。\n\n 3. JSON字符串转Java对象\n    \n    User user = JSON.parseObject(jsonStr, User.class);\n    \n    \n    将 json 转换为 Java 对象，只需要使用 Fastjson 提供的 JSON 类中的 parseObject() 静态方法即可。\n\n\n# 代码演示\n\n * 引入坐标\n\n * 创建一个类，专门用来测试 Java 对象和 JSON 串的相互转换，代码如下：\n   \n   public class FastJsonDemo {\n   \n       public static void main(String[] args) {\n           //1. 将Java对象转为JSON字符串\n           User user = new User();\n           user.setId(1);\n           user.setUsername("zhangsan");\n           user.setPassword("123");\n   \n           String jsonString = JSON.toJSONString(user);\n           System.out.println(jsonString);//{"id":1,"password":"123","username":"zhangsan"}\n   \n   \n           //2. 将JSON字符串转为Java对象\n           User u = JSON.parseObject("{\\"id\\":1,\\"password\\":\\"123\\",\\"username\\":\\"zhangsan\\"}", User.class);\n           System.out.println(u);\n       }\n   }\n   ',normalizedContent:'# json\n\n\n# 概述\n\n概念：javascript object notation。javascript 对象表示法.\n\n如下是 javascript 对象的定义格式：\n\n{\n\tname:"zhangsan",\n\tage:23,\n\tcity:"北京"\n}\n\n\n接下来我们再看看 json 的格式：\n\n{\n\t"name":"zhangsan",\n\t"age":23,\n\t"city":"北京"\n}\n\n\n通过上面 js 对象格式和 json 格式进行对比，发现两个格式特别像。只不过 js 对象中的属性名可以使用引号（可以是单引号，也可以是双引号）；而 json 格式中的键要求必须使用双引号括起来，这是 json 格式的规定。json 格式的数据有什么作用呢？\n\n作用：由于其语法格式简单，层次结构鲜明，现多用于作为数据载体，在网络中进行数据传输。如下图所示就是服务端给浏览器响应的数据，这个数据比较简单，如果现需要将 java 对象中封装的数据响应回给浏览器的话，应该以何种数据传输呢？\n\n大家还记得 ajax 的概念吗？ 是 异步的 javascript 和 xml。这里的 xml就是以前进行数据传递的方式，如下：\n\n<student>\n    <name>张三</name>\n    <age>23</age>\n    <city>北京</city>\n</student>\n\n\n再看 json 描述以上数据的写法：\n\n{\t\n\t"name":"张三",\n    "age":23,\n    "city":"北京"\n}\n\n\n上面两种格式进行对比后就会发现 json 格式数据的简单，以及所占的字节数少等优点。\n\n\n# json 基础语法\n\n\n# 定义格式\n\njson 本质就是一个字符串，但是该字符串内容是有一定的格式要求的。 定义格式如下：\n\nvar 变量名 = \'{"key":value,"key":value,...}\';\n\n\njson 串的键要求必须使用双引号括起来，而值根据要表示的类型确定。value 的数据类型分为如下\n\n * 数字（整数或浮点数）\n * 字符串（使用双引号括起来）\n * 逻辑值（true或者false）\n * 数组（在方括号中）\n * 对象（在花括号中）\n * null\n\n示例：\n\nvar jsonstr = \'{"name":"zhangsan","age":23,"addr":["北京","上海","西安"]}\'\n\n\n\n# 代码演示\n\n创建一个页面，在该页面的 <script> 标签中定义json字符串\n\n<!doctype html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>title</title>\n</head>\n<body>\n<script>\n    //1. 定义json字符串\n    var jsonstr = \'{"name":"zhangsan","age":23,"addr":["北京","上海","西安"]}\'\n    alert(jsonstr);\n\n<\/script>\n</body>\n</html>\n\n\n通过浏览器打开，页面效果如下图所示\n\n现在我们需要获取到该 json 串中的 name 属性值，应该怎么处理呢？\n\n如果它是一个 js 对象，我们就可以通过 js对象.属性名 的方式来获取数据。js 提供了一个对象 json ，该对象有如下两个方法：\n\n * parse(str) ：将 json串转换为 js 对象。使用方式是： var jsobject = json.parse(jsonstr);\n * stringify(obj) ：将 js 对象转换为 json 串。使用方式是：var jsonstr = json.stringify(jsobject)\n\n代码演示：\n\n<!doctype html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>title</title>\n</head>\n<body>\n<script>\n    //1. 定义json字符串\n    var jsonstr = \'{"name":"zhangsan","age":23,"addr":["北京","上海","西安"]}\'\n    alert(jsonstr);\n\n    //2. 将 json 字符串转为 js 对象\n    let jsobject = json.parse(jsonstr);\n    alert(jsobject)\n    alert(jsobject.name)\n    //3. 将 js 对象转换为 json 字符串\n    let jsonstr2 = json.stringify(jsobject);\n    alert(jsonstr2)\n<\/script>\n</body>\n</html>\n\n\n# 发送异步请求携带参数\n\n后面我们使用 axios 发送请求时，如果要携带复杂的数据时都会以 json 格式进行传递，如下\n\naxios({\n    method:"post",\n    url:"http://localhost:8080/ajax-demo/axiosservlet",\n    data:"username=zhangsan"\n}).then(function (resp) {\n    alert(resp.data);\n})\n\n\n请求参数不可能由我们自己拼接字符串吧？肯定不用，可以提前定义一个 js 对象，用来封装需要提交的参数，然后使用 json.stringify(js对象) 转换为 json 串，再将该 json 串作为 axios 的 data 属性值进行请求参数的提交。如下：\n\nvar jsobject = {name:"张三"};\n\naxios({\n    method:"post",\n    url:"http://localhost:8080/ajax-demo/axiosservlet",\n    data: json.stringify(jsobject)\n}).then(function (resp) {\n    alert(resp.data);\n})\n\n\n而 axios 是一个很强大的工具。我们只需要将需要提交的参数封装成 js 对象，并将该 js 对象作为 axios 的 data 属性值进行，它会自动将 js 对象转换为 json 串进行提交。如下：\n\nvar jsobject = {name:"张三"};\n\naxios({\n    method:"post",\n    url:"http://localhost:8080/ajax-demo/axiosservlet",\n    data:jsobject  //这里 axios 会将该js对象转换为 json 串的\n}).then(function (resp) {\n    alert(resp.data);\n})\n\n\n> 注意：\n> \n>  * js 提供的 json 对象我们只需要了解一下即可。因为 axios 会自动对 js 对象和 json 串进行想换转换。\n>  * 发送异步请求时，如果请求参数是 json 格式，那请求方式必须是 post。因为 json 串需要放在请求体中。\n\n\n# json串和java对象的相互转换\n\n学习完 json 后，接下来聊聊 json 的作用。以后我们会以 json 格式的数据进行前后端交互。前端发送请求时，如果是复杂的数据就会以 json 提交给后端；而后端如果需要响应一些复杂的数据时，也需要以 json 格式将数据响应回给浏览器。\n\n在后端我们就需要重点学习以下两部分操作：\n\n * 请求数据：json字符串转为java对象\n * 响应数据：java对象转为json字符串\n\n接下来给大家介绍一套 api，可以实现上面两部分操作。这套 api 就是 fastjson\n\n\n# fastjson 概述\n\nfastjson 是阿里巴巴提供的一个java语言编写的高性能功能完善的 json 库，是目前java语言中最快的 json 库，可以实现 java 对象和 json 字符串的相互转换。\n\n\n# fastjson 使用\n\nfastjson 使用也是比较简单的，分为以下三步完成\n\n 1. 导入坐标\n    \n    <dependency>\n        <groupid>com.alibaba</groupid>\n        <artifactid>fastjson</artifactid>\n        <version>1.2.62</version>\n    </dependency>\n    \n\n 2. java对象转json\n    \n    string jsonstr = json.tojsonstring(obj);\n    \n    \n    将 java 对象转换为 json 串，只需要使用 fastjson 提供的 json 类中的 tojsonstring() 静态方法即可。\n\n 3. json字符串转java对象\n    \n    user user = json.parseobject(jsonstr, user.class);\n    \n    \n    将 json 转换为 java 对象，只需要使用 fastjson 提供的 json 类中的 parseobject() 静态方法即可。\n\n\n# 代码演示\n\n * 引入坐标\n\n * 创建一个类，专门用来测试 java 对象和 json 串的相互转换，代码如下：\n   \n   public class fastjsondemo {\n   \n       public static void main(string[] args) {\n           //1. 将java对象转为json字符串\n           user user = new user();\n           user.setid(1);\n           user.setusername("zhangsan");\n           user.setpassword("123");\n   \n           string jsonstring = json.tojsonstring(user);\n           system.out.println(jsonstring);//{"id":1,"password":"123","username":"zhangsan"}\n   \n   \n           //2. 将json字符串转为java对象\n           user u = json.parseobject("{\\"id\\":1,\\"password\\":\\"123\\",\\"username\\":\\"zhangsan\\"}", user.class);\n           system.out.println(u);\n       }\n   }\n   ',charsets:{cjk:!0}},{title:"VUE",frontmatter:{autoSort:94,title:"VUE",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/7250e2/",categories:["后端","JavaWeb"],tags:["知识","JavaWeb"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/20.JavaWeb/40.VUE.html",relativePath:"01.后端/20.JavaWeb/40.VUE.md",key:"v-f09d403a",path:"/pages/7250e2/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:10},{level:2,title:"快速入门",slug:"快速入门",normalizedTitle:"快速入门",charIndex:781},{level:2,title:"Vue 指令",slug:"vue-指令",normalizedTitle:"vue 指令",charIndex:1984},{level:3,title:"v-bind & v-model 指令",slug:"v-bind-v-model-指令",normalizedTitle:"v-bind &amp; v-model 指令",charIndex:null},{level:3,title:"v-on 指令",slug:"v-on-指令",normalizedTitle:"v-on 指令",charIndex:3230},{level:3,title:"条件判断指令",slug:"条件判断指令",normalizedTitle:"条件判断指令",charIndex:4309},{level:3,title:"v-for 指令",slug:"v-for-指令",normalizedTitle:"v-for 指令",charIndex:5643},{level:2,title:"生命周期",slug:"生命周期",normalizedTitle:"生命周期",charIndex:6439},{level:2,title:"案例",slug:"案例",normalizedTitle:"案例",charIndex:176},{level:3,title:"需求",slug:"需求",normalizedTitle:"需求",charIndex:6664},{level:3,title:"查询所有功能",slug:"查询所有功能",normalizedTitle:"查询所有功能",charIndex:6743},{level:3,title:"添加功能",slug:"添加功能",normalizedTitle:"添加功能",charIndex:6701}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"概述 快速入门 Vue 指令 v-bind & v-model 指令 v-on 指令 条件判断指令 v-for 指令 生命周期 案例 需求 查询所有功能 添加功能",content:'# VUE\n\n\n# 概述\n\n接下来我们学习一款前端的框架，就是 VUE。\n\nVue 是一套前端框架，免除原生JavaScript中的DOM操作，简化书写。\n\n我们之前也学习过后端的框架 Mybatis ，Mybatis 是用来简化 jdbc 代码编写的；而 VUE 是前端的框架，是用来简化 JavaScript 代码编写的。前一天我们做了一个综合性的案例，里面进行了大量的DOM操作，如下\n\n学习了 VUE 后，这部分代码我们就不需要再写了。那么 VUE 是如何简化 DOM 书写呢？\n\n**基于MVVM(Model-View-ViewModel)思想，实现数据的双向绑定，将编程的关注点放在数据上。**之前我们是将关注点放在了 DOM 操作上；而要了解 MVVM 思想，必须先聊聊 MVC 思想，如下图就是 MVC 思想图解\n\nC 就是咱们 js 代码，M 就是数据，而 V 是页面上展示的内容，如下图是我们之前写的代码\n\nMVC 思想是没法进行双向绑定的。双向绑定是指当数据模型数据发生变化时，页面展示的会随之发生变化，而如果表单数据发生变化，绑定的模型数据也随之发生变化。接下来我们聊聊 MVVM 思想，如下图是三个组件图解\n\n图中的 Model 就是我们的数据，View 是视图，也就是页面标签，用户可以通过浏览器看到的内容；Model 和 View 是通过 ViewModel 对象进行双向绑定的，而 ViewModel 对象是 Vue 提供的。接下来让大家看一下双向绑定的效果，下图是提前准备的代码，输入框绑定了 username 模型数据，而在页面上也使用 {{}} 绑定了 username 模型数据\n\n通过浏览器打开该页面可以看到如下页面\n\n当我们在输入框中输入内容，而输入框后面随之实时的展示我们输入的内容，这就是双向绑定的效果。\n\n\n# 快速入门\n\nVue 使用起来是比较简单的，总共分为如下三步：\n\n 1. 新建 HTML 页面，引入 Vue.js文件\n    \n    <script src="js/vue.js"><\/script>\n    \n\n 2. 在JS代码区域，创建Vue核心对象，进行数据绑定\n    \n    new Vue({\n        el: "#app",\n        data() {\n            return {\n                username: ""\n            }\n        }\n    });\n    \n    \n    创建 Vue 对象时，需要传递一个 js 对象，而该对象中需要如下属性：\n    \n    * el ： 用来指定哪儿些标签受 Vue 管理。 该属性取值 #app 中的 app 需要是受管理的标签的id属性值\n    * data ：用来定义数据模型\n    * methods ：用来定义函数。这个我们在后面就会用到\n\n 3. 编写视图\n    \n    <div id="app">\n        <input name="username" v-model="username" >\n        {{username}}\n    </div>\n    \n    \n    {{}} 是 Vue 中定义的 插值表达式 ，在里面写数据模型，到时候会将该模型的数据值展示在这个位置。\n\n整体代码如下：\n\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <title>Title</title>\n</head>\n<body>\n<div id="app">\n    <input v-model="username">\n    \x3c!--插值表达式--\x3e\n    {{username}}\n</div>\n<script src="js/vue.js"><\/script>\n<script>\n    //1. 创建Vue核心对象\n    new Vue({\n        el:"#app",\n        data(){  // data() 是 ECMAScript 6 版本的新的写法\n            return {\n                username:""\n            }\n        }\n\n        /*data: function () {\n            return {\n                username:""\n            }\n        }*/\n    });\n\n<\/script>\n</body>\n</html>\n\n\n\n# Vue 指令\n\n**指令：**HTML 标签上带有 v- 前缀的特殊属性，不同指令具有不同含义。例如：v-if，v-for…\n\n常用的指令有：\n\n指令          作用\nv-bind      为HTML标签绑定属性值，如设置 href , css样式等\nv-model     在表单元素上创建双向数据绑定\nv-on        为HTML标签绑定事件\nv-if        条件性的渲染某元素，判定为true时渲染,否则不渲染\nv-else      \nv-else-if   \nv-show      根据条件展示某元素，区别在于切换的是display属性的值\nv-for       列表渲染，遍历容器的元素或者对象的属性\n\n接下来我们挨个学习这些指令\n\n\n# v-bind & v-model 指令\n\n * v-bind\n   \n   该指令可以给标签原有属性绑定模型数据。这样模型数据发生变化，标签属性值也随之发生变化\n   \n   例如：\n   \n   <a v-bind:href="url">百度一下</a>\n   \n   \n   上面的 v-bind:" 可以简化写成 : ，如下：\n   \n   \x3c!--\n   \tv-bind 可以省略\n   --\x3e\n   <a :href="url">百度一下</a>\n   \n\n * v-model\n   \n   该指令可以给表单项标签绑定模型数据。这样就能实现双向绑定效果。例如：\n   \n   <input name="username" v-model="username">\n   \n\n代码演示：\n\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <title>Title</title>\n</head>\n<body>\n<div id="app">\n    <a v-bind:href="url">点击一下</a>\n    <a :href="url">点击一下</a>\n    <input v-model="url">\n</div>\n\n<script src="js/vue.js"><\/script>\n<script>\n    //1. 创建Vue核心对象\n    new Vue({\n        el:"#app",\n        data(){\n            return {\n                username:"",\n                url:"https://www.baidu.com"\n            }\n        }\n    });\n<\/script>\n</body>\n</html>\n\n\n通过浏览器打开上面页面，并且使用检查查看超链接的路径，该路径会根据输入框输入的路径变化而变化，这是因为超链接和输入框绑定的是同一个模型数据\n\n\n# v-on 指令\n\n我们在页面定义一个按钮，并给该按钮使用 v-on 指令绑定单击事件，html代码如下\n\n<input type="button" value="一个按钮" v-on:click="show()">\n\n\n而使用 v-on 时还可以使用简化的写法，将 v-on: 替换成 @，html代码如下\n\n<input type="button" value="一个按钮" @click="show()">\n\n\n上面代码绑定的 show() 需要在 Vue 对象中的 methods 属性中定义出来\n\nnew Vue({\n    el: "#app",\n    methods: {\n        show(){\n            alert("我被点了");\n        }\n    }\n});\n\n\n> 注意：v-on: 后面的事件名称是之前原生事件属性名去掉on。\n> \n> 例如：\n> \n>  * 单击事件 ： 事件属性名是 onclick，而在vue中使用是 v-on:click\n>  * 失去焦点事件：事件属性名是 onblur，而在vue中使用时 v-on:blur\n\n整体页面代码如下：\n\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <title>Title</title>\n</head>\n<body>\n<div id="app">\n    <input type="button" value="一个按钮" v-on:click="show()"><br>\n    <input type="button" value="一个按钮" @click="show()">\n</div>\n<script src="js/vue.js"><\/script>\n<script>\n    //1. 创建Vue核心对象\n    new Vue({\n        el:"#app",\n        data(){\n            return {\n                username:"",\n            }\n        },\n        methods:{\n            show(){\n                alert("我被点了...");\n            }\n        }\n    });\n<\/script>\n</body>\n</html>\n\n\n\n# 条件判断指令\n\n接下来通过代码演示一下。在 Vue中定义一个 count 的数据模型，如下\n\n//1. 创建Vue核心对象\nnew Vue({\n    el:"#app",\n    data(){\n        return {\n            count:3\n        }\n    }\n});\n\n\n现在要实现，当 count 模型的数据是3时，在页面上展示 div1 内容；当 count 模型的数据是4时，在页面上展示 div2 内容；count 模型数据是其他值时，在页面上展示 div3。这里为了动态改变模型数据 count 的值，再定义一个输入框绑定 count 模型数据。html 代码如下：\n\n<div id="app">\n    <div v-if="count ** 3">div1</div>\n    <div v-else-if="count ** 4">div2</div>\n    <div v-else>div3</div>\n    <hr>\n    <input v-model="count">\n</div>\n\n\n整体页面代码如下：\n\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <title>Title</title>\n</head>\n<body>\n<div id="app">\n    <div v-if="count ** 3">div1</div>\n    <div v-else-if="count ** 4">div2</div>\n    <div v-else>div3</div>\n    <hr>\n    <input v-model="count">\n</div>\n\n<script src="js/vue.js"><\/script>\n<script>\n    //1. 创建Vue核心对象\n    new Vue({\n        el:"#app",\n        data(){\n            return {\n                count:3\n            }\n        }\n    });\n<\/script>\n</body>\n</html>\n\n\n通过浏览器打开页面并在输入框输入不同的值，效果如下\n\n然后我们在看看 v-show 指令的效果，如果模型数据 count 的值是3时，展示 div v-show 内容，否则不展示，html页面代码如下\n\n<div v-show="count ** 3">div v-show</div>\n<br>\n<input v-model="count">\n\n\n浏览器打开效果如下：\n\n通过上面的演示，发现 v-show 和 v-if 效果一样，那它们到底有什么区别呢？我们根据浏览器的检查功能查看源代码\n\n通过上图可以看出 v-show 不展示的原理是给对应的标签添加 display css属性，并将该属性值设置为 none ，这样就达到了隐藏的效果。而 v-if 指令是条件不满足时根本就不会渲染。\n\n\n# v-for 指令\n\n这个指令看到名字就知道是用来遍历的，该指令使用的格式如下：\n\n<标签 v-for="变量名 in 集合模型数据">\n    {{变量名}}\n</标签>\n\n\n> 注意：需要循环那个标签，v-for 指令就写在那个标签上。\n\n如果在页面需要使用到集合模型数据的索引，就需要使用如下格式：\n\n<标签 v-for="(变量名,索引变量) in 集合模型数据">\n    \x3c!--索引变量是从0开始，所以要表示序号的话，需要手动的加1--\x3e\n   {{索引变量 + 1}} {{变量名}}\n</标签>\n\n\n代码演示：\n\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <title>Title</title>\n</head>\n<body>\n<div id="app">\n    <div v-for="addr in addrs">\n        {{addr}} <br>\n    </div>\n\n    <hr>\n    <div v-for="(addr,i) in addrs">\n        {{i+1}}--{{addr}} <br>\n    </div>\n</div>\n\n<script src="js/vue.js"><\/script>\n<script>\n\n    //1. 创建Vue核心对象\n    new Vue({\n        el:"#app",\n        data(){\n            return {\n                addrs:["北京","上海","西安"]\n            }\n        }\n    });\n<\/script>\n</body>\n</html>\n\n\n通过浏览器打开效果如下\n\n\n# 生命周期\n\n生命周期的八个阶段：每触发一个生命周期事件，会自动执行一个生命周期方法，这些生命周期方法也被称为钩子方法。\n\n下图是 Vue 官网提供的从创建 Vue 到效果 Vue 对象的整个过程及各个阶段对应的钩子函数\n\n看到上面的图，大家无需过多的关注这张图。这些钩子方法我们只关注 mounted 就行了。\n\nmounted：挂载完成，Vue初始化成功，HTML页面渲染成功。而以后我们会在该方法中发送异步请求，加载数据。\n\n\n# 案例\n\n\n# 需求\n\n使用 Vue 简化我们在前一天ajax学完后做的品牌列表数据查询和添加功能\n\n此案例只是使用 Vue 对前端代码进行优化，后端代码无需修改。\n\n\n# 查询所有功能\n\n 1. 在 brand.html 页面引入 vue 的js文件\n    \n    <script src="js/vue.js"><\/script>\n    \n\n 2. 创建 Vue 对象\n    \n    * 在 Vue 对象中定义模型数据\n    * 在钩子函数中发送异步请求，并将响应的数据赋值给数据模型\n    \n    new Vue({\n        el: "#app",\n        data(){\n            return{\n                brands:[]\n            }\n        },\n        mounted(){\n            // 页面加载完成后，发送异步请求，查询数据\n            var _this = this;\n            axios({\n                method:"get",\n                url:"http://localhost:8080/brand-demo/selectAllServlet"\n            }).then(function (resp) {\n                _this.brands = resp.data;\n            })\n        }\n    })\n    \n\n 3. 修改视图\n    \n    * 定义 <div id="app"></div> ，指定该 div 标签受 Vue 管理\n    \n    * 将 body 标签中所有的内容拷贝作为上面 div 标签中\n    \n    * 删除表格的多余数据行，只留下一个\n    \n    * 在表格中的数据行上使用 v-for 指令遍历\n      \n      <tr v-for="(brand,i) in brands" align="center">\n          <td>{{i + 1}}</td>\n          <td>{{brand.brandName}}</td>\n          <td>{{brand.companyName}}</td>\n          <td>{{brand.ordered}}</td>\n          <td>{{brand.description}}</td>\n          <td>{{brand.statusStr}}</td>\n          <td><a href="#">修改</a> <a href="#">删除</a></td>\n      </tr>\n      \n\n整体页面代码如下：\n\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <title>Title</title>\n</head>\n<body>\n<div id="app">\n    <a href="addBrand.html"><input type="button" value="新增"></a><br>\n    <hr>\n    <table id="brandTable" border="1" cellspacing="0" width="100%">\n        <tr>\n            <th>序号</th>\n            <th>品牌名称</th>\n            <th>企业名称</th>\n            <th>排序</th>\n            <th>品牌介绍</th>\n            <th>状态</th>\n            <th>操作</th>\n        </tr>\n        \x3c!--\n            使用v-for遍历tr\n        --\x3e\n        <tr v-for="(brand,i) in brands" align="center">\n            <td>{{i + 1}}</td>\n            <td>{{brand.brandName}}</td>\n            <td>{{brand.companyName}}</td>\n            <td>{{brand.ordered}}</td>\n            <td>{{brand.description}}</td>\n            <td>{{brand.statusStr}}</td>\n            <td><a href="#">修改</a> <a href="#">删除</a></td>\n        </tr>\n    </table>\n</div>\n<script src="js/axios-0.18.0.js"><\/script>\n<script src="js/vue.js"><\/script>\n\n<script>\n    new Vue({\n        el: "#app",\n        data(){\n            return{\n                brands:[]\n            }\n        },\n        mounted(){\n            // 页面加载完成后，发送异步请求，查询数据\n            var _this = this;\n            axios({\n                method:"get",\n                url:"http://localhost:8080/brand-demo/selectAllServlet"\n            }).then(function (resp) {\n                _this.brands = resp.data;\n            })\n        }\n    })\n<\/script>\n</body>\n</html>\n\n\n\n# 添加功能\n\n页面操作效果如下：\n\n整体流程如下\n\n> 注意：前端代码的关键点在于使用 v-model 指令给标签项绑定模型数据，利用双向绑定特性，在发送异步请求时提交数据。\n\n 1. 在 addBrand.html 页面引入 vue 的js文件\n    \n    <script src="js/vue.js"><\/script>\n    \n\n 2. 创建 Vue 对象\n    \n    * 在 Vue 对象中定义模型数据 brand\n    * 定义一个 submitForm() 函数，用于给 提交 按钮提供绑定的函数\n    * 在 submitForm() 函数中发送 ajax 请求，并将模型数据 brand 作为参数进行传递\n    \n    new Vue({\n        el: "#app",\n        data(){\n            return {\n                brand:{}\n            }\n        },\n        methods:{\n            submitForm(){\n                // 发送ajax请求，添加\n                var _this = this;\n                axios({\n                    method:"post",\n                    url:"http://localhost:8080/brand-demo/addServlet",\n                    data:_this.brand\n                }).then(function (resp) {\n                    // 判断响应数据是否为 success\n                    if(resp.data ** "success"){\n                        location.href = "http://localhost:8080/brand-demo/brand.html";\n                    }\n                })\n    \n            }\n        }\n    })\n    \n\n 3. 修改视图\n    \n    * 定义 <div id="app"></div> ，指定该 div 标签受 Vue 管理\n    \n    * 将 body 标签中所有的内容拷贝作为上面 div 标签中\n    \n    * 给每一个表单项标签绑定模型数据。最后这些数据要被封装到 brand 对象中\n      \n      <div id="app">\n          <h3>添加品牌</h3>\n          <form action="" method="post">\n              品牌名称：<input id="brandName" v-model="brand.brandName" name="brandName"><br>\n              企业名称：<input id="companyName" v-model="brand.companyName" name="companyName"><br>\n              排序：<input id="ordered" v-model="brand.ordered" name="ordered"><br>\n              描述信息：<textarea rows="5" cols="20" id="description" v-model="brand.description" name="description"></textarea><br>\n              状态：\n              <input type="radio" name="status" v-model="brand.status" value="0">禁用\n              <input type="radio" name="status" v-model="brand.status" value="1">启用<br>\n      \n              <input type="button" id="btn" @click="submitForm" value="提交">\n          </form>\n      </div>\n      \n\n整体页面代码如下：\n\n<!DOCTYPE html>\n<html lang="en">\n\n<head>\n    <meta charset="UTF-8">\n    <title>添加品牌</title>\n</head>\n<body>\n<div id="app">\n    <h3>添加品牌</h3>\n    <form action="" method="post">\n        品牌名称：<input id="brandName" v-model="brand.brandName" name="brandName"><br>\n        企业名称：<input id="companyName" v-model="brand.companyName" name="companyName"><br>\n        排序：<input id="ordered" v-model="brand.ordered" name="ordered"><br>\n        描述信息：<textarea rows="5" cols="20" id="description" v-model="brand.description" name="description"></textarea><br>\n        状态：\n        <input type="radio" name="status" v-model="brand.status" value="0">禁用\n        <input type="radio" name="status" v-model="brand.status" value="1">启用<br>\n\n        <input type="button" id="btn" @click="submitForm" value="提交">\n    </form>\n</div>\n<script src="js/axios-0.18.0.js"><\/script>\n<script src="js/vue.js"><\/script>\n<script>\n    new Vue({\n        el: "#app",\n        data(){\n            return {\n                brand:{}\n            }\n        },\n        methods:{\n            submitForm(){\n                // 发送ajax请求，添加\n                var _this = this;\n                axios({\n                    method:"post",\n                    url:"http://localhost:8080/brand-demo/addServlet",\n                    data:_this.brand\n                }).then(function (resp) {\n                    // 判断响应数据是否为 success\n                    if(resp.data ** "success"){\n                        location.href = "http://localhost:8080/brand-demo/brand.html";\n                    }\n                })\n            }\n        }\n    })\n<\/script>\n</body>\n</html>\n\n\n通过上面的优化，前端代码确实简化了不少。但是页面依旧是不怎么好看，那么接下来我们学习 Element，它可以美化页面。',normalizedContent:'# vue\n\n\n# 概述\n\n接下来我们学习一款前端的框架，就是 vue。\n\nvue 是一套前端框架，免除原生javascript中的dom操作，简化书写。\n\n我们之前也学习过后端的框架 mybatis ，mybatis 是用来简化 jdbc 代码编写的；而 vue 是前端的框架，是用来简化 javascript 代码编写的。前一天我们做了一个综合性的案例，里面进行了大量的dom操作，如下\n\n学习了 vue 后，这部分代码我们就不需要再写了。那么 vue 是如何简化 dom 书写呢？\n\n**基于mvvm(model-view-viewmodel)思想，实现数据的双向绑定，将编程的关注点放在数据上。**之前我们是将关注点放在了 dom 操作上；而要了解 mvvm 思想，必须先聊聊 mvc 思想，如下图就是 mvc 思想图解\n\nc 就是咱们 js 代码，m 就是数据，而 v 是页面上展示的内容，如下图是我们之前写的代码\n\nmvc 思想是没法进行双向绑定的。双向绑定是指当数据模型数据发生变化时，页面展示的会随之发生变化，而如果表单数据发生变化，绑定的模型数据也随之发生变化。接下来我们聊聊 mvvm 思想，如下图是三个组件图解\n\n图中的 model 就是我们的数据，view 是视图，也就是页面标签，用户可以通过浏览器看到的内容；model 和 view 是通过 viewmodel 对象进行双向绑定的，而 viewmodel 对象是 vue 提供的。接下来让大家看一下双向绑定的效果，下图是提前准备的代码，输入框绑定了 username 模型数据，而在页面上也使用 {{}} 绑定了 username 模型数据\n\n通过浏览器打开该页面可以看到如下页面\n\n当我们在输入框中输入内容，而输入框后面随之实时的展示我们输入的内容，这就是双向绑定的效果。\n\n\n# 快速入门\n\nvue 使用起来是比较简单的，总共分为如下三步：\n\n 1. 新建 html 页面，引入 vue.js文件\n    \n    <script src="js/vue.js"><\/script>\n    \n\n 2. 在js代码区域，创建vue核心对象，进行数据绑定\n    \n    new vue({\n        el: "#app",\n        data() {\n            return {\n                username: ""\n            }\n        }\n    });\n    \n    \n    创建 vue 对象时，需要传递一个 js 对象，而该对象中需要如下属性：\n    \n    * el ： 用来指定哪儿些标签受 vue 管理。 该属性取值 #app 中的 app 需要是受管理的标签的id属性值\n    * data ：用来定义数据模型\n    * methods ：用来定义函数。这个我们在后面就会用到\n\n 3. 编写视图\n    \n    <div id="app">\n        <input name="username" v-model="username" >\n        {{username}}\n    </div>\n    \n    \n    {{}} 是 vue 中定义的 插值表达式 ，在里面写数据模型，到时候会将该模型的数据值展示在这个位置。\n\n整体代码如下：\n\n<!doctype html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>title</title>\n</head>\n<body>\n<div id="app">\n    <input v-model="username">\n    \x3c!--插值表达式--\x3e\n    {{username}}\n</div>\n<script src="js/vue.js"><\/script>\n<script>\n    //1. 创建vue核心对象\n    new vue({\n        el:"#app",\n        data(){  // data() 是 ecmascript 6 版本的新的写法\n            return {\n                username:""\n            }\n        }\n\n        /*data: function () {\n            return {\n                username:""\n            }\n        }*/\n    });\n\n<\/script>\n</body>\n</html>\n\n\n\n# vue 指令\n\n**指令：**html 标签上带有 v- 前缀的特殊属性，不同指令具有不同含义。例如：v-if，v-for…\n\n常用的指令有：\n\n指令          作用\nv-bind      为html标签绑定属性值，如设置 href , css样式等\nv-model     在表单元素上创建双向数据绑定\nv-on        为html标签绑定事件\nv-if        条件性的渲染某元素，判定为true时渲染,否则不渲染\nv-else      \nv-else-if   \nv-show      根据条件展示某元素，区别在于切换的是display属性的值\nv-for       列表渲染，遍历容器的元素或者对象的属性\n\n接下来我们挨个学习这些指令\n\n\n# v-bind & v-model 指令\n\n * v-bind\n   \n   该指令可以给标签原有属性绑定模型数据。这样模型数据发生变化，标签属性值也随之发生变化\n   \n   例如：\n   \n   <a v-bind:href="url">百度一下</a>\n   \n   \n   上面的 v-bind:" 可以简化写成 : ，如下：\n   \n   \x3c!--\n   \tv-bind 可以省略\n   --\x3e\n   <a :href="url">百度一下</a>\n   \n\n * v-model\n   \n   该指令可以给表单项标签绑定模型数据。这样就能实现双向绑定效果。例如：\n   \n   <input name="username" v-model="username">\n   \n\n代码演示：\n\n<!doctype html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>title</title>\n</head>\n<body>\n<div id="app">\n    <a v-bind:href="url">点击一下</a>\n    <a :href="url">点击一下</a>\n    <input v-model="url">\n</div>\n\n<script src="js/vue.js"><\/script>\n<script>\n    //1. 创建vue核心对象\n    new vue({\n        el:"#app",\n        data(){\n            return {\n                username:"",\n                url:"https://www.baidu.com"\n            }\n        }\n    });\n<\/script>\n</body>\n</html>\n\n\n通过浏览器打开上面页面，并且使用检查查看超链接的路径，该路径会根据输入框输入的路径变化而变化，这是因为超链接和输入框绑定的是同一个模型数据\n\n\n# v-on 指令\n\n我们在页面定义一个按钮，并给该按钮使用 v-on 指令绑定单击事件，html代码如下\n\n<input type="button" value="一个按钮" v-on:click="show()">\n\n\n而使用 v-on 时还可以使用简化的写法，将 v-on: 替换成 @，html代码如下\n\n<input type="button" value="一个按钮" @click="show()">\n\n\n上面代码绑定的 show() 需要在 vue 对象中的 methods 属性中定义出来\n\nnew vue({\n    el: "#app",\n    methods: {\n        show(){\n            alert("我被点了");\n        }\n    }\n});\n\n\n> 注意：v-on: 后面的事件名称是之前原生事件属性名去掉on。\n> \n> 例如：\n> \n>  * 单击事件 ： 事件属性名是 onclick，而在vue中使用是 v-on:click\n>  * 失去焦点事件：事件属性名是 onblur，而在vue中使用时 v-on:blur\n\n整体页面代码如下：\n\n<!doctype html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>title</title>\n</head>\n<body>\n<div id="app">\n    <input type="button" value="一个按钮" v-on:click="show()"><br>\n    <input type="button" value="一个按钮" @click="show()">\n</div>\n<script src="js/vue.js"><\/script>\n<script>\n    //1. 创建vue核心对象\n    new vue({\n        el:"#app",\n        data(){\n            return {\n                username:"",\n            }\n        },\n        methods:{\n            show(){\n                alert("我被点了...");\n            }\n        }\n    });\n<\/script>\n</body>\n</html>\n\n\n\n# 条件判断指令\n\n接下来通过代码演示一下。在 vue中定义一个 count 的数据模型，如下\n\n//1. 创建vue核心对象\nnew vue({\n    el:"#app",\n    data(){\n        return {\n            count:3\n        }\n    }\n});\n\n\n现在要实现，当 count 模型的数据是3时，在页面上展示 div1 内容；当 count 模型的数据是4时，在页面上展示 div2 内容；count 模型数据是其他值时，在页面上展示 div3。这里为了动态改变模型数据 count 的值，再定义一个输入框绑定 count 模型数据。html 代码如下：\n\n<div id="app">\n    <div v-if="count ** 3">div1</div>\n    <div v-else-if="count ** 4">div2</div>\n    <div v-else>div3</div>\n    <hr>\n    <input v-model="count">\n</div>\n\n\n整体页面代码如下：\n\n<!doctype html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>title</title>\n</head>\n<body>\n<div id="app">\n    <div v-if="count ** 3">div1</div>\n    <div v-else-if="count ** 4">div2</div>\n    <div v-else>div3</div>\n    <hr>\n    <input v-model="count">\n</div>\n\n<script src="js/vue.js"><\/script>\n<script>\n    //1. 创建vue核心对象\n    new vue({\n        el:"#app",\n        data(){\n            return {\n                count:3\n            }\n        }\n    });\n<\/script>\n</body>\n</html>\n\n\n通过浏览器打开页面并在输入框输入不同的值，效果如下\n\n然后我们在看看 v-show 指令的效果，如果模型数据 count 的值是3时，展示 div v-show 内容，否则不展示，html页面代码如下\n\n<div v-show="count ** 3">div v-show</div>\n<br>\n<input v-model="count">\n\n\n浏览器打开效果如下：\n\n通过上面的演示，发现 v-show 和 v-if 效果一样，那它们到底有什么区别呢？我们根据浏览器的检查功能查看源代码\n\n通过上图可以看出 v-show 不展示的原理是给对应的标签添加 display css属性，并将该属性值设置为 none ，这样就达到了隐藏的效果。而 v-if 指令是条件不满足时根本就不会渲染。\n\n\n# v-for 指令\n\n这个指令看到名字就知道是用来遍历的，该指令使用的格式如下：\n\n<标签 v-for="变量名 in 集合模型数据">\n    {{变量名}}\n</标签>\n\n\n> 注意：需要循环那个标签，v-for 指令就写在那个标签上。\n\n如果在页面需要使用到集合模型数据的索引，就需要使用如下格式：\n\n<标签 v-for="(变量名,索引变量) in 集合模型数据">\n    \x3c!--索引变量是从0开始，所以要表示序号的话，需要手动的加1--\x3e\n   {{索引变量 + 1}} {{变量名}}\n</标签>\n\n\n代码演示：\n\n<!doctype html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>title</title>\n</head>\n<body>\n<div id="app">\n    <div v-for="addr in addrs">\n        {{addr}} <br>\n    </div>\n\n    <hr>\n    <div v-for="(addr,i) in addrs">\n        {{i+1}}--{{addr}} <br>\n    </div>\n</div>\n\n<script src="js/vue.js"><\/script>\n<script>\n\n    //1. 创建vue核心对象\n    new vue({\n        el:"#app",\n        data(){\n            return {\n                addrs:["北京","上海","西安"]\n            }\n        }\n    });\n<\/script>\n</body>\n</html>\n\n\n通过浏览器打开效果如下\n\n\n# 生命周期\n\n生命周期的八个阶段：每触发一个生命周期事件，会自动执行一个生命周期方法，这些生命周期方法也被称为钩子方法。\n\n下图是 vue 官网提供的从创建 vue 到效果 vue 对象的整个过程及各个阶段对应的钩子函数\n\n看到上面的图，大家无需过多的关注这张图。这些钩子方法我们只关注 mounted 就行了。\n\nmounted：挂载完成，vue初始化成功，html页面渲染成功。而以后我们会在该方法中发送异步请求，加载数据。\n\n\n# 案例\n\n\n# 需求\n\n使用 vue 简化我们在前一天ajax学完后做的品牌列表数据查询和添加功能\n\n此案例只是使用 vue 对前端代码进行优化，后端代码无需修改。\n\n\n# 查询所有功能\n\n 1. 在 brand.html 页面引入 vue 的js文件\n    \n    <script src="js/vue.js"><\/script>\n    \n\n 2. 创建 vue 对象\n    \n    * 在 vue 对象中定义模型数据\n    * 在钩子函数中发送异步请求，并将响应的数据赋值给数据模型\n    \n    new vue({\n        el: "#app",\n        data(){\n            return{\n                brands:[]\n            }\n        },\n        mounted(){\n            // 页面加载完成后，发送异步请求，查询数据\n            var _this = this;\n            axios({\n                method:"get",\n                url:"http://localhost:8080/brand-demo/selectallservlet"\n            }).then(function (resp) {\n                _this.brands = resp.data;\n            })\n        }\n    })\n    \n\n 3. 修改视图\n    \n    * 定义 <div id="app"></div> ，指定该 div 标签受 vue 管理\n    \n    * 将 body 标签中所有的内容拷贝作为上面 div 标签中\n    \n    * 删除表格的多余数据行，只留下一个\n    \n    * 在表格中的数据行上使用 v-for 指令遍历\n      \n      <tr v-for="(brand,i) in brands" align="center">\n          <td>{{i + 1}}</td>\n          <td>{{brand.brandname}}</td>\n          <td>{{brand.companyname}}</td>\n          <td>{{brand.ordered}}</td>\n          <td>{{brand.description}}</td>\n          <td>{{brand.statusstr}}</td>\n          <td><a href="#">修改</a> <a href="#">删除</a></td>\n      </tr>\n      \n\n整体页面代码如下：\n\n<!doctype html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>title</title>\n</head>\n<body>\n<div id="app">\n    <a href="addbrand.html"><input type="button" value="新增"></a><br>\n    <hr>\n    <table id="brandtable" border="1" cellspacing="0" width="100%">\n        <tr>\n            <th>序号</th>\n            <th>品牌名称</th>\n            <th>企业名称</th>\n            <th>排序</th>\n            <th>品牌介绍</th>\n            <th>状态</th>\n            <th>操作</th>\n        </tr>\n        \x3c!--\n            使用v-for遍历tr\n        --\x3e\n        <tr v-for="(brand,i) in brands" align="center">\n            <td>{{i + 1}}</td>\n            <td>{{brand.brandname}}</td>\n            <td>{{brand.companyname}}</td>\n            <td>{{brand.ordered}}</td>\n            <td>{{brand.description}}</td>\n            <td>{{brand.statusstr}}</td>\n            <td><a href="#">修改</a> <a href="#">删除</a></td>\n        </tr>\n    </table>\n</div>\n<script src="js/axios-0.18.0.js"><\/script>\n<script src="js/vue.js"><\/script>\n\n<script>\n    new vue({\n        el: "#app",\n        data(){\n            return{\n                brands:[]\n            }\n        },\n        mounted(){\n            // 页面加载完成后，发送异步请求，查询数据\n            var _this = this;\n            axios({\n                method:"get",\n                url:"http://localhost:8080/brand-demo/selectallservlet"\n            }).then(function (resp) {\n                _this.brands = resp.data;\n            })\n        }\n    })\n<\/script>\n</body>\n</html>\n\n\n\n# 添加功能\n\n页面操作效果如下：\n\n整体流程如下\n\n> 注意：前端代码的关键点在于使用 v-model 指令给标签项绑定模型数据，利用双向绑定特性，在发送异步请求时提交数据。\n\n 1. 在 addbrand.html 页面引入 vue 的js文件\n    \n    <script src="js/vue.js"><\/script>\n    \n\n 2. 创建 vue 对象\n    \n    * 在 vue 对象中定义模型数据 brand\n    * 定义一个 submitform() 函数，用于给 提交 按钮提供绑定的函数\n    * 在 submitform() 函数中发送 ajax 请求，并将模型数据 brand 作为参数进行传递\n    \n    new vue({\n        el: "#app",\n        data(){\n            return {\n                brand:{}\n            }\n        },\n        methods:{\n            submitform(){\n                // 发送ajax请求，添加\n                var _this = this;\n                axios({\n                    method:"post",\n                    url:"http://localhost:8080/brand-demo/addservlet",\n                    data:_this.brand\n                }).then(function (resp) {\n                    // 判断响应数据是否为 success\n                    if(resp.data ** "success"){\n                        location.href = "http://localhost:8080/brand-demo/brand.html";\n                    }\n                })\n    \n            }\n        }\n    })\n    \n\n 3. 修改视图\n    \n    * 定义 <div id="app"></div> ，指定该 div 标签受 vue 管理\n    \n    * 将 body 标签中所有的内容拷贝作为上面 div 标签中\n    \n    * 给每一个表单项标签绑定模型数据。最后这些数据要被封装到 brand 对象中\n      \n      <div id="app">\n          <h3>添加品牌</h3>\n          <form action="" method="post">\n              品牌名称：<input id="brandname" v-model="brand.brandname" name="brandname"><br>\n              企业名称：<input id="companyname" v-model="brand.companyname" name="companyname"><br>\n              排序：<input id="ordered" v-model="brand.ordered" name="ordered"><br>\n              描述信息：<textarea rows="5" cols="20" id="description" v-model="brand.description" name="description"></textarea><br>\n              状态：\n              <input type="radio" name="status" v-model="brand.status" value="0">禁用\n              <input type="radio" name="status" v-model="brand.status" value="1">启用<br>\n      \n              <input type="button" id="btn" @click="submitform" value="提交">\n          </form>\n      </div>\n      \n\n整体页面代码如下：\n\n<!doctype html>\n<html lang="en">\n\n<head>\n    <meta charset="utf-8">\n    <title>添加品牌</title>\n</head>\n<body>\n<div id="app">\n    <h3>添加品牌</h3>\n    <form action="" method="post">\n        品牌名称：<input id="brandname" v-model="brand.brandname" name="brandname"><br>\n        企业名称：<input id="companyname" v-model="brand.companyname" name="companyname"><br>\n        排序：<input id="ordered" v-model="brand.ordered" name="ordered"><br>\n        描述信息：<textarea rows="5" cols="20" id="description" v-model="brand.description" name="description"></textarea><br>\n        状态：\n        <input type="radio" name="status" v-model="brand.status" value="0">禁用\n        <input type="radio" name="status" v-model="brand.status" value="1">启用<br>\n\n        <input type="button" id="btn" @click="submitform" value="提交">\n    </form>\n</div>\n<script src="js/axios-0.18.0.js"><\/script>\n<script src="js/vue.js"><\/script>\n<script>\n    new vue({\n        el: "#app",\n        data(){\n            return {\n                brand:{}\n            }\n        },\n        methods:{\n            submitform(){\n                // 发送ajax请求，添加\n                var _this = this;\n                axios({\n                    method:"post",\n                    url:"http://localhost:8080/brand-demo/addservlet",\n                    data:_this.brand\n                }).then(function (resp) {\n                    // 判断响应数据是否为 success\n                    if(resp.data ** "success"){\n                        location.href = "http://localhost:8080/brand-demo/brand.html";\n                    }\n                })\n            }\n        }\n    })\n<\/script>\n</body>\n</html>\n\n\n通过上面的优化，前端代码确实简化了不少。但是页面依旧是不怎么好看，那么接下来我们学习 element，它可以美化页面。',charsets:{cjk:!0}},{title:"JSP",frontmatter:{autoSort:94,title:"JSP",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/4f9642/",categories:["后端","JavaWeb"],tags:["知识","JavaWeb"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/20.JavaWeb/35.JSP.html",relativePath:"01.后端/20.JavaWeb/35.JSP.md",key:"v-17c18643",path:"/pages/4f9642/",headers:[{level:2,title:"JSP 概述",slug:"jsp-概述",normalizedTitle:"jsp 概述",charIndex:17},{level:2,title:"JSP 快速入门",slug:"jsp-快速入门",normalizedTitle:"jsp 快速入门",charIndex:1104},{level:3,title:"搭建环境",slug:"搭建环境",normalizedTitle:"搭建环境",charIndex:1137},{level:3,title:"导入 JSP 依赖",slug:"导入-jsp-依赖",normalizedTitle:"导入 jsp 依赖",charIndex:2315},{level:3,title:"创建 jsp 页面",slug:"创建-jsp-页面",normalizedTitle:"创建 jsp 页面",charIndex:2602},{level:3,title:"编写代码",slug:"编写代码",normalizedTitle:"编写代码",charIndex:2667},{level:3,title:"测试",slug:"测试",normalizedTitle:"测试",charIndex:932},{level:2,title:"JSP 原理",slug:"jsp-原理",normalizedTitle:"jsp 原理",charIndex:3053},{level:2,title:"JSP 脚本",slug:"jsp-脚本",normalizedTitle:"jsp 脚本",charIndex:3916},{level:3,title:"JSP 脚本分类",slug:"jsp-脚本分类",normalizedTitle:"jsp 脚本分类",charIndex:3995},{level:3,title:"案例",slug:"案例",normalizedTitle:"案例",charIndex:3955},{level:3,title:"JSP 缺点",slug:"jsp-缺点",normalizedTitle:"jsp 缺点",charIndex:10436},{level:2,title:"EL 表达式",slug:"el-表达式",normalizedTitle:"el 表达式",charIndex:11425},{level:3,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:21},{level:3,title:"代码演示",slug:"代码演示",normalizedTitle:"代码演示",charIndex:4143},{level:3,title:"域对象",slug:"域对象",normalizedTitle:"域对象",charIndex:11519},{level:2,title:"JSTL标签",slug:"jstl标签",normalizedTitle:"jstl标签",charIndex:13218},{level:3,title:"概述",slug:"概述-2",normalizedTitle:"概述",charIndex:21},{level:3,title:"if 标签",slug:"if-标签",normalizedTitle:"if 标签",charIndex:13942},{level:3,title:"forEach 标签",slug:"foreach-标签",normalizedTitle:"foreach 标签",charIndex:15365},{level:2,title:"MVC模式和三层架构",slug:"mvc模式和三层架构",normalizedTitle:"mvc模式和三层架构",charIndex:17434},{level:3,title:"MVC模式",slug:"mvc模式",normalizedTitle:"mvc模式",charIndex:17434},{level:3,title:"三层架构",slug:"三层架构",normalizedTitle:"三层架构",charIndex:17440},{level:3,title:"MVC 和 三层架构",slug:"mvc-和-三层架构",normalizedTitle:"mvc 和 三层架构",charIndex:18293},{level:2,title:"案例",slug:"案例-2",normalizedTitle:"案例",charIndex:3955},{level:3,title:"环境准备",slug:"环境准备",normalizedTitle:"环境准备",charIndex:18674},{level:3,title:"查询所有",slug:"查询所有",normalizedTitle:"查询所有",charIndex:25336},{level:3,title:"添加",slug:"添加",normalizedTitle:"添加",charIndex:14123},{level:3,title:"修改",slug:"修改",normalizedTitle:"修改",charIndex:893}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"JSP 概述 JSP 快速入门 搭建环境 导入 JSP 依赖 创建 jsp 页面 编写代码 测试 JSP 原理 JSP 脚本 JSP 脚本分类 案例 JSP 缺点 EL 表达式 概述 代码演示 域对象 JSTL标签 概述 if 标签 forEach 标签 MVC模式和三层架构 MVC模式 三层架构 MVC 和 三层架构 案例 环境准备 查询所有 添加 修改",content:'# JSP\n\n浅谈JSP\n\n\n# JSP 概述\n\n**JSP（全称：Java Server Pages）：Java 服务端页面。**是一种动态的网页技术，其中既可以定义 HTML、JS、CSS等静态内容，还可以定义 Java代码的动态内容，也就是 JSP = HTML + Java。如下就是jsp代码\n\n<html>\n    <head>\n        <title>Title</title>\n    </head>\n    <body>\n        <h1>JSP,Hello World</h1>\n        <%\n        \tSystem.out.println("hello,jsp~");\n        %>\n    </body>\n</html>\n\n\n上面代码 h1 标签内容是展示在页面上，而 Java 的输出语句是输出在 idea 的控制台。\n\n那么，JSP 能做什么呢？现在我们只用 servlet 实现功能，看存在什么问题。如下图所示，当我们登陆成功后，需要在页面上展示用户名\n\n上图的用户名是动态展示，也就是谁登陆就展示谁的用户名。只用 servlet 如何实现呢？在今天的资料里已经提供好了一个 LoginServlet ，该 servlet 就是实现这个功能的，现将资料中的 LoginServlet.java 拷贝到 request-demo 项目中来演示。接下来启动服务器并访问登陆页面\n\n输入了 zhangsan 用户的登陆信息后点击 登陆 按钮，就能看到如下图效果\n\n\n\n当然如果是 lisi 登陆的，在该页面展示的就是 lisi,欢迎您，动态的展示效果就实现了。那么 LoginServlet 到底是如何实现的，我们看看它里面的内容\n\n上面的代码有大量使用到 writer 对象向页面写标签内容，这样我们的代码就显得很麻烦；将来如果展示的效果出现了问题，排错也显得有点力不从心。而 JSP 是如何解决这个问题的呢？在资料中也提供了一个 login.jsp 页面，该页面也能实现该功能，现将该页面拷贝到项目的 webapp下，需要修改 login.html 中表单数据提交的路径为下图\n\n重新启动服务器并进行测试，发现也可以实现同样的功能。那么 login.jsp 又是如何实现的呢？那我们来看看 login.jsp 的代码\n\n上面代码可以看到里面基本都是 HTML 标签，而动态数据使用 Java 代码进行展示；这样操作看起来要比用 servlet 实现要舒服很多。\n\nJSP 作用：简化开发，避免了在Servlet中直接输出HTML标签。\n\n\n# JSP 快速入门\n\n接下来我们做一个简单的快速入门代码。\n\n\n# 搭建环境\n\n创建一个maven的 web 项目，项目结构如下：\n\npom.xml 文件内容如下：\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>org.example</groupId>\n    <artifactId>jsp-demo</artifactId>\n    <version>1.0-SNAPSHOT</version>\n    <packaging>war</packaging>\n\n    <properties>\n        <maven.compiler.source>8</maven.compiler.source>\n        <maven.compiler.target>8</maven.compiler.target>\n    </properties>\n\n    <dependencies>\n      <dependency>\n            <groupId>javax.servlet</groupId>\n            <artifactId>javax.servlet-api</artifactId>\n            <version>3.1.0</version>\n            <scope>provided</scope>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.tomcat.maven</groupId>\n                <artifactId>tomcat7-maven-plugin</artifactId>\n                <version>2.2</version>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n\n\n\n# 导入 JSP 依赖\n\n在 dependencies 标签中导入 JSP 的依赖，如下\n\n<dependency>\n    <groupId>javax.servlet.jsp</groupId>\n    <artifactId>jsp-api</artifactId>\n    <version>2.2</version>\n    <scope>provided</scope>\n</dependency>\n\n\n该依赖的 scope 必须设置为 provided，因为 tomcat 中有这个jar包了，所以在打包时我们是不希望将该依赖打进到我们工程的war包中。\n\n\n# 创建 jsp 页面\n\n在项目的 webapp 下创建jsp页面\n\n通过上面方式创建一个名为 hello.jsp 的页面。\n\n\n# 编写代码\n\n在 hello.jsp 页面中书写 HTML 标签和 Java 代码，如下\n\n<%@ page contentType="text/html;charset=UTF-8" language="java" %>\n<html>\n<head>\n    <title>Title</title>\n</head>\n<body>\n    <h1>hello jsp</h1>\n\n    <%\n        System.out.println("hello,jsp~");\n    %>\n</body>\n</html>\n\n\n\n# 测试\n\n启动服务器并在浏览器地址栏输入 http://localhost:8080/jsp-demo/hello.jsp，我们可以在页面上看到如下内容\n\n同时也可以看到在 idea 的控制台看到输出的 hello,jsp~ 内容。\n\n\n# JSP 原理\n\n我们之前说 JSP 就是一个页面，那么在 JSP 中写 html 标签，我们能理解，但是为什么还可以写 Java 代码呢？\n\n因为 **JSP 本质上就是一个 Servlet。**接下来我们聊聊访问jsp时的流程\n\n 1. 浏览器第一次访问 hello.jsp 页面\n 2. tomcat 会将 hello.jsp 转换为名为 hello_jsp.java 的一个 Servlet\n 3. tomcat 再将转换的 servlet 编译成字节码文件 hello_jsp.class\n 4. tomcat 会执行该字节码文件，向外提供服务\n\n我们可以到项目所在磁盘目录下找 target\\tomcat\\work\\Tomcat\\localhost\\jsp-demo\\org\\apache\\jsp 目录，而这个目录下就能看到转换后的 servlet\n\n打开 hello_jsp.java 文件，来查看里面的代码\n\n由上面的类的继承关系可以看到继承了名为 HttpJspBase 这个类，那我们在看该类的继承关系。到资料中的找如下目录： 资料\\tomcat源码\\apache-tomcat-8.5.68-src\\java\\org\\apache\\jasper\\runtime ，该目录下就有 HttpJspBase 类，查看该类的继承关系\n\n可以看到该类继承了 HttpServlet ；那么 hello_jsp 这个类就间接的继承了 HttpServlet ，也就说明 hello_jsp 是一个 servlet。\n\n继续阅读 hello_jsp 类的代码，可以看到有一个名为 _jspService() 的方法，该方法就是每次访问 jsp 时自动执行的方法，和 servlet 中的 service 方法一样 。\n\n而在 _jspService() 方法中可以看到往浏览器写标签的代码：\n\n以前我们自己写 servlet 时，这部分代码是由我们自己来写，现在有了 jsp 后，由tomcat完成这部分功能。\n\n\n# JSP 脚本\n\nJSP脚本用于在 JSP页面内定义 Java代码。在之前的入门案例中我们就在 JSP 页面定义的 Java 代码就是 JSP 脚本。\n\n\n# JSP 脚本分类\n\nJSP 脚本有如下三个分类：\n\n * <%...%>：内容会直接放到_jspService()方法之中\n * <%=…%>：内容会放到out.print()中，作为out.print()的参数\n * <%!…%>：内容会放到_jspService()方法之外，被类直接包含\n\n代码演示：\n\n在 hello.jsp 中书写\n\n<%\n    System.out.println("hello,jsp~");\n    int i = 3;\n%>\n\n\n通过浏览器访问 hello.jsp 后，查看转换的 hello_jsp.java 文件，i 变量定义在了 _jspService() 方法中\n\n在 hello.jsp 中书写\n\n<%="hello"%>\n<%=i%>\n\n\n通过浏览器访问 hello.jsp 后，查看转换的 hello_jsp.java 文件，该脚本的内容被放在了 out.print() 中，作为参数\n\n在 hello.jsp 中书写\n\n<%!\n    void  show(){}\n\tString name = "zhangsan";\n%>\n\n\n通过浏览器访问 hello.jsp 后，查看转换的 hello_jsp.java 文件，该脚本的内容被放在了成员位置\n\n\n# 案例\n\n# 需求\n\n使用JSP脚本展示品牌数据\n\n说明：\n\n * 在资料 资料\\1. JSP案例素材 中提供了 brand.html 静态页面\n * 在该案例中数据不从数据库中查询，而是在 JSP 页面上写死\n\n# 实现\n\n * 将资料 资料\\1. JSP案例素材 中的 Brand.java 文件放置到项目的 com.itheima.pojo 包下\n\n * 在项目的 webapp 中创建 brand.jsp ，并将 brand.html页面中的内容拷贝过来。brand.jsp 内容如下\n   \n   <%@ page contentType="text/html;charset=UTF-8" language="java" %>\n   <!DOCTYPE html>\n   <html lang="en">\n   <head>\n       <meta charset="UTF-8">\n       <title>Title</title>\n   </head>\n   <body>\n   <input type="button" value="新增"><br>\n   <hr>\n       <table border="1" cellspacing="0" width="800">\n           <tr>\n               <th>序号</th>\n               <th>品牌名称</th>\n               <th>企业名称</th>\n               <th>排序</th>\n               <th>品牌介绍</th>\n               <th>状态</th>\n               <th>操作</th>\n   \n           </tr>\n           <tr align="center">\n               <td>1</td>\n               <td>三只松鼠</td>\n               <td>三只松鼠</td>\n               <td>100</td>\n               <td>三只松鼠，好吃不上火</td>\n               <td>启用</td>\n               <td><a href="#">修改</a> <a href="#">删除</a></td>\n           </tr>\n   \n           <tr align="center">\n               <td>2</td>\n               <td>优衣库</td>\n               <td>优衣库</td>\n               <td>10</td>\n               <td>优衣库，服适人生</td>\n               <td>禁用</td>\n   \n               <td><a href="#">修改</a> <a href="#">删除</a></td>\n           </tr>\n   \n           <tr align="center">\n               <td>3</td>\n               <td>小米</td>\n               <td>小米科技有限公司</td>\n               <td>1000</td>\n               <td>为发烧而生</td>\n               <td>启用</td>\n   \n               <td><a href="#">修改</a> <a href="#">删除</a></td>\n           </tr>\n       </table>\n   </body>\n   </html>\n   \n   \n   现在页面中的数据都是假数据。\n\n * 在 brand.jsp 中准备一些数据\n   \n   <%\n       // 查询数据库\n       List<Brand> brands = new ArrayList<Brand>();\n       brands.add(new Brand(1,"三只松鼠","三只松鼠",100,"三只松鼠，好吃不上火",1));\n       brands.add(new Brand(2,"优衣库","优衣库",200,"优衣库，服适人生",0));\n       brands.add(new Brand(3,"小米","小米科技有限公司",1000,"为发烧而生",1));\n   %>\n   \n   \n   > **注意：**这里的类是需要导包的\n\n * 将 brand.jsp 页面中的 table 标签中的数据改为动态的\n   \n   <table border="1" cellspacing="0" width="800">\n       <tr>\n           <th>序号</th>\n           <th>品牌名称</th>\n           <th>企业名称</th>\n           <th>排序</th>\n           <th>品牌介绍</th>\n           <th>状态</th>\n           <th>操作</th>\n   \n       </tr>\n       \n       <%\n        for (int i = 0; i < brands.size(); i++) {\n            //获取集合中的 每一个 Brand 对象\n            Brand brand = brands.get(i);\n        }\n       %>\n       <tr align="center">\n           <td>1</td>\n           <td>三只松鼠</td>\n           <td>三只松鼠</td>\n           <td>100</td>\n           <td>三只松鼠，好吃不上火</td>\n           <td>启用</td>\n           <td><a href="#">修改</a> <a href="#">删除</a></td>\n       </tr>\n   </table>\n   \n   \n   上面的for循环需要将 tr 标签包裹起来，这样才能实现循环的效果，代码改进为\n   \n   <table border="1" cellspacing="0" width="800">\n       <tr>\n           <th>序号</th>\n           <th>品牌名称</th>\n           <th>企业名称</th>\n           <th>排序</th>\n           <th>品牌介绍</th>\n           <th>状态</th>\n           <th>操作</th>\n   \n       </tr>\n       \n       <%\n        for (int i = 0; i < brands.size(); i++) {\n            //获取集合中的 每一个 Brand 对象\n            Brand brand = brands.get(i);\n       %>\n       \t <tr align="center">\n               <td>1</td>\n               <td>三只松鼠</td>\n               <td>三只松鼠</td>\n               <td>100</td>\n               <td>三只松鼠，好吃不上火</td>\n               <td>启用</td>\n               <td><a href="#">修改</a> <a href="#">删除</a></td>\n           </tr>\n       <%\n        }\n       %>\n      \n   </table>\n   \n   \n   > 注意：<%%> 里面写的是 Java 代码，而外边写的是 HTML 标签\n   \n   上面代码中的 td 标签中的数据都需要是动态的，所以还需要改进\n   \n   <table border="1" cellspacing="0" width="800">\n       <tr>\n           <th>序号</th>\n           <th>品牌名称</th>\n           <th>企业名称</th>\n           <th>排序</th>\n           <th>品牌介绍</th>\n           <th>状态</th>\n           <th>操作</th>\n   \n       </tr>\n       \n       <%\n        for (int i = 0; i < brands.size(); i++) {\n            //获取集合中的 每一个 Brand 对象\n            Brand brand = brands.get(i);\n       %>\n       \t <tr align="center">\n               <td><%=brand.getId()%></td>\n               <td><%=brand.getBrandName()%></td>\n               <td><%=brand.getCompanyName()%></td>\n               <td><%=brand.getOrdered()%></td>\n               <td><%=brand.getDescription()%></td>\n               <td><%=brand.getStatus() ** 1 ? "启用":"禁用"%></td>\n               <td><a href="#">修改</a> <a href="#">删除</a></td>\n           </tr>\n       <%\n        }\n       %>\n      \n   </table>\n   \n\n# 成品代码\n\n<%@ page import="com.itheima.pojo.Brand" %>\n<%@ page import="java.util.List" %>\n<%@ page import="java.util.ArrayList" %>\n<%@ page contentType="text/html;charset=UTF-8" language="java" %>\n\n<%\n    // 查询数据库\n    List<Brand> brands = new ArrayList<Brand>();\n    brands.add(new Brand(1,"三只松鼠","三只松鼠",100,"三只松鼠，好吃不上火",1));\n    brands.add(new Brand(2,"优衣库","优衣库",200,"优衣库，服适人生",0));\n    brands.add(new Brand(3,"小米","小米科技有限公司",1000,"为发烧而生",1));\n\n%>\n\n\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <title>Title</title>\n</head>\n<body>\n<input type="button" value="新增"><br>\n<hr>\n<table border="1" cellspacing="0" width="800">\n    <tr>\n        <th>序号</th>\n        <th>品牌名称</th>\n        <th>企业名称</th>\n        <th>排序</th>\n        <th>品牌介绍</th>\n        <th>状态</th>\n        <th>操作</th>\n    </tr>\n    <%\n        for (int i = 0; i < brands.size(); i++) {\n            Brand brand = brands.get(i);\n    %>\n\n    <tr align="center">\n        <td><%=brand.getId()%></td>\n        <td><%=brand.getBrandName()%></td>\n        <td><%=brand.getCompanyName()%></td>\n        <td><%=brand.getOrdered()%></td>\n        <td><%=brand.getDescription()%></td>\n\t\t<td><%=brand.getStatus() ** 1 ? "启用":"禁用"%></td>\n        <td><a href="#">修改</a> <a href="#">删除</a></td>\n    </tr>\n\n    <%\n        }\n    %>\n</table>\n</body>\n</html>\n\n\n# 测试\n\n在浏览器地址栏输入 http://localhost:8080/jsp-demo/brand.jsp ，页面展示效果如下\n\n\n# JSP 缺点\n\n通过上面的案例，我们可以看到 JSP 的很多缺点。\n\n由于 JSP页面内，既可以定义 HTML 标签，又可以定义 Java代码，造成了以下问题：\n\n * 书写麻烦：特别是复杂的页面\n   \n   既要写 HTML 标签，还要写 Java 代码\n\n * 阅读麻烦\n   \n   上面案例的代码，相信你后期再看这段代码时还需要花费很长的时间去梳理\n\n * 复杂度高：运行需要依赖于各种环境，JRE，JSP容器，JavaEE…\n\n * 占内存和磁盘：JSP会自动生成.java和.class文件占磁盘，运行的是.class文件占内存\n\n * 调试困难：出错后，需要找到自动生成的.java文件进行调试\n\n * 不利于团队协作：前端人员不会 Java，后端人员不精 HTML\n   \n   如果页面布局发生变化，前端工程师对静态页面进行修改，然后再交给后端工程师，由后端工程师再将该页面改为 JSP 页面\n\n由于上述的问题， **JSP 已逐渐退出历史舞台，**以后开发更多的是使用 HTML + Ajax 来替代。Ajax 是我们后续会重点学习的技术。有个这个技术后，前端工程师负责前端页面开发，而后端工程师只负责前端代码开发。下来对技术的发展进行简单的说明\n\n 1. 第一阶段：使用 servlet 即实现逻辑代码编写，也对页面进行拼接。这种模式我们之前也接触过\n\n 2. 第二阶段：随着技术的发展，出现了 JSP ，人们发现 JSP 使用起来比 Servlet 方便很多，但是还是要在 JSP 中嵌套 Java 代码，也不利于后期的维护\n\n 3. 第三阶段：使用 Servlet 进行逻辑代码开发，而使用 JSP 进行数据展示\n\n 4. 第四阶段：使用 servlet 进行后端逻辑代码开发，而使用 HTML 进行数据展示。而这里面就存在问题，HTML 是静态页面，怎么进行动态数据展示呢？这就是 ajax 的作用了。\n\n那既然 JSP 已经逐渐的退出历史舞台，那我们为什么还要学习 JSP 呢？原因有两点：\n\n * 一些公司可能有些老项目还在用 JSP ，所以要求我们必须动 JSP\n * 我们如果不经历这些复杂的过程，就不能体现后面阶段开发的简单\n\n接下来我们来学习第三阶段，使用 EL表达式 和 JSTL 标签库替换 JSP 中的 Java 代码。\n\n\n# EL 表达式\n\n\n# 概述\n\nEL（全称Expression Language ）表达式语言，用于简化 JSP 页面内的 Java 代码。\n\nEL 表达式的主要作用是 获取数据。其实就是从域对象中获取数据，然后将数据展示在页面上。\n\n而 EL 表达式的语法也比较简单，${expression} 。例如：${brands} 就是获取域中存储的 key 为 brands 的数据。\n\n\n# 代码演示\n\n * 定义servlet，在 servlet 中封装一些数据并存储到 request 域对象中并转发到 el-demo.jsp 页面。\n   \n   @WebServlet("/demo1")\n   public class ServletDemo1 extends HttpServlet {\n       @Override\n       protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n           //1. 准备数据\n           List<Brand> brands = new ArrayList<Brand>();\n           brands.add(new Brand(1,"三只松鼠","三只松鼠",100,"三只松鼠，好吃不上火",1));\n           brands.add(new Brand(2,"优衣库","优衣库",200,"优衣库，服适人生",0));\n           brands.add(new Brand(3,"小米","小米科技有限公司",1000,"为发烧而生",1));\n   \n           //2. 存储到request域中\n           request.setAttribute("brands",brands);\n   \n           //3. 转发到 el-demo.jsp\n           request.getRequestDispatcher("/el-demo.jsp").forward(request,response);\n       }\n   \n       @Override\n       protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n           this.doGet(request, response);\n       }\n   }\n   \n   \n   > 注意： 此处需要用转发，因为转发才可以使用 request 对象作为域对象进行数据共享\n\n * 在 el-demo.jsp 中通过 EL表达式 获取数据\n   \n   <%@ page contentType="text/html;charset=UTF-8" language="java" %>\n   <html>\n   <head>\n       <title>Title</title>\n   </head>\n   <body>\n       ${brands}\n   </body>\n   </html>\n   \n\n * 在浏览器的地址栏输入 http://localhost:8080/jsp-demo/demo1 ，页面效果如下：\n\n\n# 域对象\n\nJavaWeb中有四大域对象，分别是：\n\n * page：当前页面有效\n * request：当前请求有效\n * session：当前会话有效\n * application：当前应用有效\n\nel 表达式获取数据，会依次从这4个域中寻找，直到找到为止。而这四个域对象的作用范围如下图所示\n\n例如： ${brands}，el 表达式获取数据，会先从page域对象中获取数据，如果没有再到 requet 域对象中获取数据，如果再没有再到 session 域对象中获取，如果还没有才会到 application 中获取数据。\n\n\n# JSTL标签\n\n\n# 概述\n\nJSP标准标签库(Jsp Standarded Tag Library) ，使用标签取代JSP页面上的Java代码。如下代码就是JSTL标签\n\n<c:if test="${flag ** 1}">\n    男\n</c:if>\n<c:if test="${flag ** 2}">\n    女\n</c:if>\n\n\n上面代码看起来是不是比 JSP 中嵌套 Java 代码看起来舒服好了。而且前端工程师对标签是特别敏感的，他们看到这段代码是能看懂的。\n\nJSTL 提供了很多标签，如下图\n\n我们只对两个最常用的标签进行讲解，<c:forEach> 标签和 <c:if> 标签。\n\nJSTL 使用也是比较简单的，分为如下步骤：\n\n * 导入坐标\n   \n   <dependency>\n       <groupId>jstl</groupId>\n       <artifactId>jstl</artifactId>\n       <version>1.2</version>\n   </dependency>\n   <dependency>\n       <groupId>taglibs</groupId>\n       <artifactId>standard</artifactId>\n       <version>1.1.2</version>\n   </dependency>\n   \n\n * 在JSP页面上引入JSTL标签库\n   \n   <%@ taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core" %> \n   \n\n * 使用标签\n\n\n# if 标签\n\n<c:if>：相当于 if 判断\n\n * 属性：test，用于定义条件表达式\n\n<c:if test="${flag ** 1}">\n    男\n</c:if>\n<c:if test="${flag ** 2}">\n    女\n</c:if>\n\n\n代码演示：\n\n * 定义一个 servlet ，在该 servlet 中向 request 域对象中添加 键是 status ，值为 1 的数据\n   \n   @WebServlet("/demo2")\n   public class ServletDemo2 extends HttpServlet {\n       @Override\n       protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n           //1. 存储数据到request域中\n           request.setAttribute("status",1);\n   \n           //2. 转发到 jstl-if.jsp\n           数据request.getRequestDispatcher("/jstl-if.jsp").forward(request,response);\n       }\n   \n       @Override\n       protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n           this.doGet(request, response);\n       }\n   }\n   \n\n * 定义 jstl-if.jsp 页面，在该页面使用 <c:if> 标签\n   \n   <%@ page contentType="text/html;charset=UTF-8" language="java" %>\n   <%@ taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core" %>\n   <html>\n   <head>\n       <title>Title</title>\n   </head>\n   <body>\n       <%--\n           c:if：来完成逻辑判断，替换java  if else\n       --%>\n       <c:if test="${status **1}">\n           启用\n       </c:if>\n   \n       <c:if test="${status **0}">\n           禁用\n       </c:if>\n   </body>\n   </html>\n   \n   \n   > 注意： 在该页面已经要引入 JSTL核心标签库\n   > \n   > <%@ taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core" %>\n\n\n# forEach 标签\n\n<c:forEach>：相当于 for 循环。java中有增强for循环和普通for循环，JSTL 中的 <c:forEach> 也有两种用法\n\n# 用法一\n\n类似于 Java 中的增强for循环。涉及到的 <c:forEach> 中的属性如下\n\n * items：被遍历的容器\n\n * var：遍历产生的临时变量\n\n * varStatus：遍历状态对象\n\n如下代码，是从域对象中获取名为 brands 数据，该数据是一个集合；遍历遍历，并给该集合中的每一个元素起名为 brand，是 Brand对象。在循环里面使用 EL表达式获取每一个Brand对象的属性值\n\n<c:forEach items="${brands}" var="brand">\n    <tr align="center">\n        <td>${brand.id}</td>\n        <td>${brand.brandName}</td>\n        <td>${brand.companyName}</td>\n        <td>${brand.description}</td>\n    </tr>\n</c:forEach>\n\n\n代码演示：\n\n * servlet 还是使用之前的名为 ServletDemo1 。\n\n * 定义名为 jstl-foreach.jsp 页面，内容如下：\n   \n   <%@ page contentType="text/html;charset=UTF-8" language="java" %>\n   <%@ taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core" %>\n   \n   <!DOCTYPE html>\n   <html lang="en">\n   <head>\n       <meta charset="UTF-8">\n       <title>Title</title>\n   </head>\n   <body>\n   <input type="button" value="新增"><br>\n   <hr>\n   <table border="1" cellspacing="0" width="800">\n       <tr>\n           <th>序号</th>\n           <th>品牌名称</th>\n           <th>企业名称</th>\n           <th>排序</th>\n           <th>品牌介绍</th>\n           <th>状态</th>\n           <th>操作</th>\n       </tr>\n   \n       <c:forEach items="${brands}" var="brand" varStatus="status">\n           <tr align="center">\n               <%--<td>${brand.id}</td>--%>\n               <td>${status.count}</td>\n               <td>${brand.brandName}</td>\n               <td>${brand.companyName}</td>\n               <td>${brand.ordered}</td>\n               <td>${brand.description}</td>\n               <c:if test="${brand.status ** 1}">\n                   <td>启用</td>\n               </c:if>\n               <c:if test="${brand.status != 1}">\n                   <td>禁用</td>\n               </c:if>\n               <td><a href="#">修改</a> <a href="#">删除</a></td>\n           </tr>\n       </c:forEach>\n   </table>\n   </body>\n   </html>\n   \n\n# 用法二\n\n类似于 Java 中的普通for循环。涉及到的 <c:forEach> 中的属性如下\n\n * begin：开始数\n\n * end：结束数\n\n * step：步长\n\n实例代码：\n\n从0循环到10，变量名是 i ，每次自增1\n\n<c:forEach begin="0" end="10" step="1" var="i">\n    ${i}\n</c:forEach>\n\n\n\n# MVC模式和三层架构\n\nMVC 模式和三层架构是一些理论的知识，将来我们使用了它们进行代码开发会让我们代码维护性和扩展性更好。\n\n\n# MVC模式\n\nMVC 是一种分层开发的模式，其中：\n\n * M：Model，业务模型，处理业务\n\n * V：View，视图，界面展示\n\n * C：Controller，控制器，处理请求，调用模型和视图\n\n控制器（serlvlet）用来接收浏览器发送过来的请求，控制器调用模型（JavaBean）来获取数据，比如从数据库查询数据；控制器获取到数据后再交由视图（JSP）进行数据展示。\n\nMVC 好处：\n\n * 职责单一，互不影响。每个角色做它自己的事，各司其职。\n\n * 有利于分工协作。\n\n * 有利于组件重用\n\n\n# 三层架构\n\n三层架构是将我们的项目分成了三个层面，分别是 表现层、业务逻辑层、数据访问层。\n\n * 数据访问层：对数据库的CRUD基本操作\n * 业务逻辑层：对业务逻辑进行封装，组合数据访问层层中基本功能，形成复杂的业务逻辑功能。例如 注册业务功能 ，我们会先调用 数据访问层 的 selectByName() 方法判断该用户名是否存在，如果不存在再调用 数据访问层 的 insert() 方法进行数据的添加操作\n * 表现层：接收请求，封装数据，调用业务逻辑层，响应数据\n\n而整个流程是，浏览器发送请求，表现层的Servlet接收请求并调用业务逻辑层的方法进行业务逻辑处理，而业务逻辑层方法调用数据访问层方法进行数据的操作，依次返回到serlvet，然后servlet将数据交由 JSP 进行展示。\n\n三层架构的每一层都有特有的包名称：\n\n * 表现层： com.itheima.controller 或者 com.itheima.web\n * 业务逻辑层：com.itheima.service\n * 数据访问层：com.itheima.dao 或者 com.itheima.mapper\n\n后期我们还会学习一些框架，不同的框架是对不同层进行封装的\n\n\n# MVC 和 三层架构\n\n通过 MVC 和 三层架构 的学习，有些人肯定混淆了。那他们有什么区别和联系？\n\n如上图上半部分是 MVC 模式，上图下半部分是三层架构。 MVC 模式 中的 C（控制器）和 V（视图）就是 三层架构 中的表现层，而 MVC 模式 中的 M（模型）就是 三层架构 中的 业务逻辑层 和 数据访问层。\n\n可以将 MVC 模式 理解成是一个大的概念，而 三层架构 是对 MVC 模式 实现架构的思想。 那么我们以后按照要求将不同层的代码写在不同的包下，每一层里功能职责做到单一，将来如果将表现层的技术换掉，而业务逻辑层和数据访问层的代码不需要发生变化。\n\n\n# 案例\n\n需求：完成品牌数据的增删改查操作\n\n这个功能我们之前一直在做，而这个案例是将今天学习的所有的内容（包含 MVC模式 和 三层架构）进行应用，并将整个流程贯穿起来。\n\n\n# 环境准备\n\n环境准备工作，我们分以下步骤实现：\n\n * 创建新的模块 brand_demo，引入坐标\n\n * 创建三层架构的包结构\n\n * 数据库表 tb_brand\n\n * 实体类 Brand\n\n * MyBatis 基础环境\n   \n   * Mybatis-config.xml\n   \n   * BrandMapper.xml\n   \n   * BrandMapper接口\n\n# 创建工程\n\n创建新的模块 brand_demo，引入坐标。我们只要分析出要用到哪儿些技术，那么需要哪儿些坐标也就明确了\n\n * 需要操作数据库。mysql的驱动包\n * 要使用mybatis框架。mybaits的依赖包\n * web项目需要用到servlet和jsp。servlet和jsp的依赖包\n * 需要使用 jstl 进行数据展示。jstl的依赖包\n\npom.xml 内容如下：\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n    <groupId>org.example</groupId>\n    <artifactId>brand-demo</artifactId>\n    <version>1.0-SNAPSHOT</version>\n    <packaging>war</packaging>\n\n    <properties>\n        <maven.compiler.source>8</maven.compiler.source>\n        <maven.compiler.target>8</maven.compiler.target>\n    </properties>\n\n    <dependencies>\n        \x3c!-- mybatis --\x3e\n        <dependency>\n            <groupId>org.mybatis</groupId>\n            <artifactId>mybatis</artifactId>\n            <version>3.5.5</version>\n        </dependency>\n\n        \x3c!--mysql--\x3e\n        <dependency>\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n            <version>5.1.34</version>\n        </dependency>\n\n        \x3c!--servlet--\x3e\n        <dependency>\n            <groupId>javax.servlet</groupId>\n            <artifactId>javax.servlet-api</artifactId>\n            <version>3.1.0</version>\n            <scope>provided</scope>\n        </dependency>\n\n        \x3c!--jsp--\x3e\n        <dependency>\n            <groupId>javax.servlet.jsp</groupId>\n            <artifactId>jsp-api</artifactId>\n            <version>2.2</version>\n            <scope>provided</scope>\n        </dependency>\n\n        \x3c!--jstl--\x3e\n        <dependency>\n            <groupId>jstl</groupId>\n            <artifactId>jstl</artifactId>\n            <version>1.2</version>\n        </dependency>\n        <dependency>\n            <groupId>taglibs</groupId>\n            <artifactId>standard</artifactId>\n            <version>1.1.2</version>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.tomcat.maven</groupId>\n                <artifactId>tomcat7-maven-plugin</artifactId>\n                <version>2.2</version>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n\n\n# 创建包\n\n创建不同的包结构，用来存储不同的类。包结构如下\n\n# 创建表\n\n-- 删除tb_brand表\ndrop table if exists tb_brand;\n-- 创建tb_brand表\ncreate table tb_brand\n(\n    -- id 主键\n    id           int primary key auto_increment,\n    -- 品牌名称\n    brand_name   varchar(20),\n    -- 企业名称\n    company_name varchar(20),\n    -- 排序字段\n    ordered      int,\n    -- 描述信息\n    description  varchar(100),\n    -- 状态：0：禁用  1：启用\n    status       int\n);\n-- 添加数据\ninsert into tb_brand (brand_name, company_name, ordered, description, status)\nvalues (\'三只松鼠\', \'三只松鼠股份有限公司\', 5, \'好吃不上火\', 0),\n       (\'华为\', \'华为技术有限公司\', 100, \'华为致力于把数字世界带入每个人、每个家庭、每个组织，构建万物互联的智能世界\', 1),\n       (\'小米\', \'小米科技有限公司\', 50, \'are you ok\', 1);\n\n\n# 创建实体类\n\n在 pojo 包下创建名为 Brand 的类。\n\npublic class Brand {\n    // id 主键\n    private Integer id;\n    // 品牌名称\n    private String brandName;\n    // 企业名称\n    private String companyName;\n    // 排序字段\n    private Integer ordered;\n    // 描述信息\n    private String description;\n    // 状态：0：禁用  1：启用\n    private Integer status;\n\n\n    public Brand() {\n    }\n\n    public Brand(Integer id, String brandName, String companyName, String description) {\n        this.id = id;\n        this.brandName = brandName;\n        this.companyName = companyName;\n        this.description = description;\n    }\n\n    public Brand(Integer id, String brandName, String companyName, Integer ordered, String description, Integer status) {\n        this.id = id;\n        this.brandName = brandName;\n        this.companyName = companyName;\n        this.ordered = ordered;\n        this.description = description;\n        this.status = status;\n    }\n\n    public Integer getId() {\n        return id;\n    }\n\n    public void setId(Integer id) {\n        this.id = id;\n    }\n\n    public String getBrandName() {\n        return brandName;\n    }\n\n    public void setBrandName(String brandName) {\n        this.brandName = brandName;\n    }\n\n    public String getCompanyName() {\n        return companyName;\n    }\n\n    public void setCompanyName(String companyName) {\n        this.companyName = companyName;\n    }\n\n    public Integer getOrdered() {\n        return ordered;\n    }\n\n    public void setOrdered(Integer ordered) {\n        this.ordered = ordered;\n    }\n\n    public String getDescription() {\n        return description;\n    }\n\n    public void setDescription(String description) {\n        this.description = description;\n    }\n\n    public Integer getStatus() {\n        return status;\n    }\n\n    public void setStatus(Integer status) {\n        this.status = status;\n    }\n\n    @Override\n    public String toString() {\n        return "Brand{" +\n                "id=" + id +\n                ", brandName=\'" + brandName + \'\\\'\' +\n                ", companyName=\'" + companyName + \'\\\'\' +\n                ", ordered=" + ordered +\n                ", description=\'" + description + \'\\\'\' +\n                ", status=" + status +\n                \'}\';\n    }\n}\n\n\n\n# 准备mybatis环境\n\n定义核心配置文件 Mybatis-config.xml ，并将该文件放置在 resources 下\n\n<?xml version="1.0" encoding="UTF-8" ?>\n<!DOCTYPE configuration\n        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"\n        "http://mybatis.org/dtd/mybatis-3-config.dtd">\n<configuration>\n    \x3c!--起别名--\x3e\n    <typeAliases>\n        <package name="com.itheima.pojo"/>\n    </typeAliases>\n\n    <environments default="development">\n        <environment id="development">\n            <transactionManager type="JDBC"/>\n            <dataSource type="POOLED">\n                <property name="driver" value="com.mysql.jdbc.Driver"/>\n                <property name="url" value="jdbc:mysql:///db1?useSSL=false&amp;useServerPrepStmts=true"/>\n                <property name="username" value="root"/>\n                <property name="password" value="1234"/>\n            </dataSource>\n        </environment>\n    </environments>\n    <mappers>\n        \x3c!--扫描mapper--\x3e\n        <package name="com.itheima.mapper"/>\n    </mappers>\n</configuration>\n\n\n在 resources 下创建放置映射配置文件的目录结构 com/itheima/mapper，并在该目录下创建映射配置文件 BrandMapper.xml\n\n<?xml version="1.0" encoding="UTF-8" ?>\n<!DOCTYPE mapper\n        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"\n        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">\n<mapper namespace="com.itheima.mapper.BrandMapper">\n    \n</mapper>\n\n\n\n# 查询所有\n\n\n\n当我们点击 index.html 页面中的 查询所有 这个超链接时，就能查询到上图右半部分的数据。\n\n对于上述的功能，点击 查询所有 超链接是需要先请后端的 servlet ，由 servlet 跳转到对应的页面进行数据的动态展示。而整个流程如下图：\n\n# 编写BrandMapper\n\n在 mapper 包下创建创建 BrandMapper 接口，在接口中定义 selectAll() 方法\n\n/**\n  * 查询所有\n  * @return\n  */\n@Select("select * from tb_brand")\nList<Brand> selectAll();\n\n\n# 编写工具类\n\n在 com.itheima 包下创建 utils 包，并在该包下创建名为 SqlSessionFactoryUtils 工具类\n\npublic class SqlSessionFactoryUtils {\n\n    private static SqlSessionFactory sqlSessionFactory;\n\n    static {\n        //静态代码块会随着类的加载而自动执行，且只执行一次\n        try {\n            String resource = "mybatis-config.xml";\n            InputStream inputStream = Resources.getResourceAsStream(resource);\n            sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream);\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n\n    public static SqlSessionFactory getSqlSessionFactory(){\n        return sqlSessionFactory;\n    }\n}\n\n\n# 编写BrandService\n\n在 service 包下创建 BrandService 类\n\npublic class BrandService {\n    SqlSessionFactory factory = SqlSessionFactoryUtils.getSqlSessionFactory();\n\n    /**\n     * 查询所有\n     * @return\n     */\n    public List<Brand> selectAll(){\n        //调用BrandMapper.selectAll()\n\n        //2. 获取SqlSession\n        SqlSession sqlSession = factory.openSession();\n        //3. 获取BrandMapper\n        BrandMapper mapper = sqlSession.getMapper(BrandMapper.class);\n\n        //4. 调用方法\n        List<Brand> brands = mapper.selectAll();\n\n        sqlSession.close();\n\n        return brands;\n    }\n}\n\n\n# 编写Servlet\n\n在 web 包下创建名为 SelectAllServlet 的 servlet，该 servlet 的逻辑如下：\n\n * 调用 BrandService 的 selectAll() 方法进行业务逻辑处理，并接收返回的结果\n * 将上一步返回的结果存储到 request 域对象中\n * 跳转到 brand.jsp 页面进行数据的展示\n\n具体的代码如下：\n\n@WebServlet("/selectAllServlet")\npublic class SelectAllServlet extends HttpServlet {\n    private  BrandService service = new BrandService();\n\n    @Override\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n\n        //1. 调用BrandService完成查询\n        List<Brand> brands = service.selectAll();\n        //2. 存入request域中\n        request.setAttribute("brands",brands);\n        //3. 转发到brand.jsp\n        request.getRequestDispatcher("/brand.jsp").forward(request,response);\n    }\n\n    @Override\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        this.doGet(request, response);\n    }\n}\n\n\n# 编写brand.jsp页面\n\n将资料 资料\\2. 品牌增删改查案例\\静态页面 下的 brand.html 页面拷贝到项目的 webapp 目录下，并将该页面改成 brand.jsp 页面，而 brand.jsp 页面在表格中使用 JSTL 和 EL表达式 从request域对象中获取名为 brands 的集合数据并展示出来。页面内容如下：\n\n<%@ page contentType="text/html;charset=UTF-8" language="java" %>\n<%@ taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core" %>\n\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <title>Title</title>\n</head>\n<body>\n<hr>\n<table border="1" cellspacing="0" width="80%">\n    <tr>\n        <th>序号</th>\n        <th>品牌名称</th>\n        <th>企业名称</th>\n        <th>排序</th>\n        <th>品牌介绍</th>\n        <th>状态</th>\n        <th>操作</th>\n    </tr>\n\n    <c:forEach items="${brands}" var="brand" varStatus="status">\n        <tr align="center">\n            <%--<td>${brand.id}</td>--%>\n            <td>${status.count}</td>\n            <td>${brand.brandName}</td>\n            <td>${brand.companyName}</td>\n            <td>${brand.ordered}</td>\n            <td>${brand.description}</td>\n            <c:if test="${brand.status ** 1}">\n                <td>启用</td>\n            </c:if>\n            <c:if test="${brand.status != 1}">\n                <td>禁用</td>\n            </c:if>\n            <td><a href="/brand-demo/selectByIdServlet?id=${brand.id}">修改</a> <a href="#">删除</a></td>\n        </tr>\n    </c:forEach>\n</table>\n</body>\n</html>\n\n\n# 测试\n\n启动服务器，并在浏览器输入 http://localhost:8080/brand-demo/index.html，看到如下 查询所有 的超链接，点击该链接就可以查询出所有的品牌数据\n\n\n\n为什么出现这个问题呢？是因为查询到的字段名和实体类对象的属性名没有一一对应。相比看到这大家一定会解决了，就是在映射配置文件中使用 resultMap 标签定义映射关系。映射配置文件内容如下：\n\n<?xml version="1.0" encoding="UTF-8" ?>\n<!DOCTYPE mapper\n        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"\n        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">\n<mapper namespace="com.itheima.mapper.BrandMapper">\n\n    <resultMap id="brandResultMap" type="brand">\n        <result column="brand_name" property="brandName"></result>\n        <result column="company_name" property="companyName"></result>\n    </resultMap>\n</mapper>\n\n\n并且在 BrandMapper 接口中的 selectAll() 上使用 @ResuleMap 注解指定使用该映射\n\n/**\n  * 查询所有\n  * @return\n  */\n@Select("select * from tb_brand")\n@ResultMap("brandResultMap")\nList<Brand> selectAll();\n\n\n重启服务器，再次访问就能看到我们想要的数据了\n\n\n\n\n# 添加\n\n\n\n上图是做 添加 功能流程。点击 新增 按钮后，会先跳转到 addBrand.jsp 新增页面，在该页面输入要添加的数据，输入完毕后点击 提交 按钮，需要将数据提交到后端，而后端进行数据添加操作，并重新将所有的数据查询出来。整个流程如下：\n\n\n\n接下来我们根据流程来实现功能：\n\n# 编写BrandMapper方法\n\n在 BrandMapper 接口，在接口中定义 add(Brand brand) 方法\n\n@Insert("insert into tb_brand values(null,#{brandName},#{companyName},#{ordered},#{description},#{status})")\nvoid add(Brand brand);\n\n\n# 编写BrandService方法\n\n在 BrandService 类中定义添加品牌数据方法 add(Brand brand)\n\n \t/**\n     * 添加\n     * @param brand\n     */\n    public void add(Brand brand){\n\n        //2. 获取SqlSession\n        SqlSession sqlSession = factory.openSession();\n        //3. 获取BrandMapper\n        BrandMapper mapper = sqlSession.getMapper(BrandMapper.class);\n\n        //4. 调用方法\n        mapper.add(brand);\n\n        //提交事务\n        sqlSession.commit();\n        //释放资源\n        sqlSession.close();\n    }\n\n\n# 改进brand.jsp页面\n\n我们需要在该页面表格的上面添加 新增 按钮\n\n<input type="button" value="新增" id="add"><br>\n\n\n并给该按钮绑定单击事件，当点击了该按钮需要跳转到 brand.jsp 添加品牌数据的页面\n\n<script>\n    document.getElementById("add").onclick = function (){\n        location.href = "/brand-demo/addBrand.jsp";\n    }\n<\/script>\n\n\n> **注意：**该 script 标签建议放在 body 结束标签前面。\n\n# 编写addBrand.jsp页面\n\n从资料 资料\\2. 品牌增删改查案例\\静态页面 中将 addBrand.html 页面拷贝到项目的 webapp 下，并改成 addBrand.jsp 动态页面\n\n<%@ page contentType="text/html;charset=UTF-8" language="java" %>\n<!DOCTYPE html>\n<html lang="en">\n\n<head>\n    <meta charset="UTF-8">\n    <title>添加品牌</title>\n</head>\n<body>\n<h3>添加品牌</h3>\n<form action="/brand-demo/addServlet" method="post">\n    品牌名称：<input name="brandName"><br>\n    企业名称：<input name="companyName"><br>\n    排序：<input name="ordered"><br>\n    描述信息：<textarea rows="5" cols="20" name="description"></textarea><br>\n    状态：\n    <input type="radio" name="status" value="0">禁用\n    <input type="radio" name="status" value="1">启用<br>\n\n    <input type="submit" value="提交">\n</form>\n</body>\n</html>\n\n\n# 编写servlet\n\n在 web 包下创建 AddServlet 的 servlet，该 servlet 的逻辑如下:\n\n * 设置处理post请求乱码的字符集\n * 接收客户端提交的数据\n * 将接收到的数据封装到 Brand 对象中\n * 调用 BrandService 的add() 方法进行添加的业务逻辑处理\n * 跳转到 selectAllServlet 资源重新查询数据\n\n具体的代码如下：\n\n@WebServlet("/addServlet")\npublic class AddServlet extends HttpServlet {\n    private BrandService service = new BrandService();\n\n\n    @Override\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n\n        //处理POST请求的乱码问题\n        request.setCharacterEncoding("utf-8");\n\n        //1. 接收表单提交的数据，封装为一个Brand对象\n        String brandName = request.getParameter("brandName");\n        String companyName = request.getParameter("companyName");\n        String ordered = request.getParameter("ordered");\n        String description = request.getParameter("description");\n        String status = request.getParameter("status");\n\n        //封装为一个Brand对象\n        Brand brand = new Brand();\n        brand.setBrandName(brandName);\n        brand.setCompanyName(companyName);\n        brand.setOrdered(Integer.parseInt(ordered));\n        brand.setDescription(description);\n        brand.setStatus(Integer.parseInt(status));\n\n        //2. 调用service 完成添加\n        service.add(brand);\n\n        //3. 转发到查询所有Servlet\n        request.getRequestDispatcher("/selectAllServlet").forward(request,response);\n    }\n\n    @Override\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        this.doGet(request, response);\n    }\n}\n\n\n# 测试\n\n点击 brand.jsp 页面的 新增 按钮，会跳转到 addBrand.jsp页面\n\n点击 提交 按钮，就能看到如下页面，里面就包含我们刚添加的数据\n\n\n\n\n# 修改\n\n\n\n点击每条数据后面的 编辑 按钮会跳转到修改页面，如下图：\n\n\n\n在该修改页面我们可以看到将 编辑 按钮所在行的数据 回显 到表单，然后需要修改那个数据在表单中进行修改，然后点击 提交 的按钮将数据提交到后端，后端再将数据存储到数据库中。\n\n从上面的例子我们知道 修改 功能需要从两方面进行实现，数据回显和修改操作。\n\n# 回显数据\n\n\n\n上图就是回显数据的效果。要实现这个效果，那当点击 修改 按钮时不能直接跳转到 update.jsp 页面，而是需要先带着当前行数据的 id 请求后端程序，后端程序根据 id 查询数据，将数据存储到域对象中跳转到 update.jsp 页面进行数据展示。整体流程如下\n\n# 编写BrandMapper方法\n\n在 BrandMapper 接口，在接口中定义 selectById(int id) 方法\n\n\t/**\n     * 根据id查询\n     * @param id\n     * @return\n     */\n    @Select("select * from tb_brand where id = #{id}")\n    @ResultMap("brandResultMap")\n    Brand selectById(int id);\n\n\n# 编写BrandService方法\n\n在 BrandService 类中定义根据id查询数据方法 selectById(int id)\n\n    /**\n     * 根据id查询\n     * @return\n     */\n    public Brand selectById(int id){\n        //调用BrandMapper.selectAll()\n        //2. 获取SqlSession\n        SqlSession sqlSession = factory.openSession();\n        //3. 获取BrandMapper\n        BrandMapper mapper = sqlSession.getMapper(BrandMapper.class);\n        //4. 调用方法\n        Brand brand = mapper.selectById(id);\n        sqlSession.close();\n        return brand;\n    }\n\n\n# 编写servlet\n\n在 web 包下创建 SelectByIdServlet 的 servlet，该 servlet 的逻辑如下:\n\n * 获取请求数据 id\n * 调用 BrandService 的 selectById() 方法进行数据查询的业务逻辑\n * 将查询到的数据存储到 request 域对象中\n * 跳转到 update.jsp 页面进行数据真实\n\n具体代码如下：\n\n@WebServlet("/selectByIdServlet")\npublic class SelectByIdServlet extends HttpServlet {\n    private  BrandService service = new BrandService();\n\n    @Override\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        //1. 接收id\n        String id = request.getParameter("id");\n        //2. 调用service查询\n        Brand brand = service.selectById(Integer.parseInt(id));\n        //3. 存储到request中\n        request.setAttribute("brand",brand);\n        //4. 转发到update.jsp\n        request.getRequestDispatcher("/update.jsp").forward(request,response);\n    }\n\n    @Override\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        this.doGet(request, response);\n    }\n}\n\n\n# 编写update.jsp页面\n\n拷贝 addBrand.jsp 页面，改名为 update.jsp 并做出以下修改：\n\n * title 标签内容改为 修改品牌\n\n * form 标签的 action 属性值改为 /brand-demo/updateServlet\n\n * input 标签要进行数据回显，需要设置 value 属性\n   \n   品牌名称：<input name="brandName" value="${brand.brandName}"><br>\n   企业名称：<input name="companyName" value="${brand.companyName}"><br>\n   排序：<input name="ordered" value="${brand.ordered}"><br>\n   \n\n * textarea 标签要进行数据回显，需要在标签体中使用 EL表达式\n   \n   描述信息：<textarea rows="5" cols="20" name="description">${brand.description} </textarea><br>\n   \n\n * 单选框使用 if 标签需要判断 brand.status 的值是 1 还是 0 在指定的单选框上使用 checked 属性，表示被选中状态\n   \n   状态：\n   <c:if test="${brand.status ** 0}">\n       <input type="radio" name="status" value="0" checked>禁用\n       <input type="radio" name="status" value="1">启用<br>\n   </c:if>\n   \n   <c:if test="${brand.status ** 1}">\n       <input type="radio" name="status" value="0" >禁用\n       <input type="radio" name="status" value="1" checked>启用<br>\n   </c:if>\n   \n\n综上，update.jsp 代码如下：\n\n<%@ page contentType="text/html;charset=UTF-8" language="java" %>\n<%@ taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core" %>\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <title>修改品牌</title>\n</head>\n<body>\n<h3>修改品牌</h3>\n<form action="/brand-demo/updateServlet" method="post">\n\n    品牌名称：<input name="brandName" value="${brand.brandName}"><br>\n    企业名称：<input name="companyName" value="${brand.companyName}"><br>\n    排序：<input name="ordered" value="${brand.ordered}"><br>\n    描述信息：<textarea rows="5" cols="20" name="description">${brand.description} </textarea><br>\n    状态：\n    <c:if test="${brand.status ** 0}">\n        <input type="radio" name="status" value="0" checked>禁用\n        <input type="radio" name="status" value="1">启用<br>\n    </c:if>\n\n    <c:if test="${brand.status ** 1}">\n        <input type="radio" name="status" value="0" >禁用\n        <input type="radio" name="status" value="1" checked>启用<br>\n    </c:if>\n\n    <input type="submit" value="提交">\n</form>\n</body>\n</html>\n\n\n# 修改数据\n\n做完回显数据后，接下来我们要做修改数据了，而下图是修改数据的效果：\n\n\n\n在修改页面进行数据修改，点击 提交 按钮，会将数据提交到后端程序，后端程序会对表中的数据进行修改操作，然后重新进行数据的查询操作。整体流程如下：\n\n# 编写BrandMapper方法\n\n在 BrandMapper 接口，在接口中定义 update(Brand brand) 方法\n\n/**\n  * 修改\n  * @param brand\n  */\n@Update("update tb_brand set brand_name = #{brandName},company_name = #{companyName},ordered = #{ordered},description = #{description},status = #{status} where id = #{id}")\nvoid update(Brand brand);\n\n\n# 编写BrandService方法\n\n在 BrandService 类中定义根据id查询数据方法 update(Brand brand)\n\n\t/**\n     * 修改\n     * @param brand\n     */\n    public void update(Brand brand){\n        //2. 获取SqlSession\n        SqlSession sqlSession = factory.openSession();\n        //3. 获取BrandMapper\n        BrandMapper mapper = sqlSession.getMapper(BrandMapper.class);\n        //4. 调用方法\n        mapper.update(brand);\n        //提交事务\n        sqlSession.commit();\n        //释放资源\n        sqlSession.close();\n    }\n\n\n# 编写servlet\n\n在 web 包下创建 AddServlet 的 servlet，该 servlet 的逻辑如下:\n\n * 设置处理post请求乱码的字符集\n * 接收客户端提交的数据\n * 将接收到的数据封装到 Brand 对象中\n * 调用 BrandService 的update() 方法进行添加的业务逻辑处理\n * 跳转到 selectAllServlet 资源重新查询数据\n\n具体的代码如下：\n\n@WebServlet("/updateServlet")\npublic class UpdateServlet extends HttpServlet {\n    private BrandService service = new BrandService();\n\n    @Override\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n\n        //处理POST请求的乱码问题\n        request.setCharacterEncoding("utf-8");\n        //1. 接收表单提交的数据，封装为一个Brand对象\n        String id = request.getParameter("id");\n        String brandName = request.getParameter("brandName");\n        String companyName = request.getParameter("companyName");\n        String ordered = request.getParameter("ordered");\n        String description = request.getParameter("description");\n        String status = request.getParameter("status");\n\n        //封装为一个Brand对象\n        Brand brand = new Brand();\n        brand.setId(Integer.parseInt(id));\n        brand.setBrandName(brandName);\n        brand.setCompanyName(companyName);\n        brand.setOrdered(Integer.parseInt(ordered));\n        brand.setDescription(description);\n        brand.setStatus(Integer.parseInt(status));\n\n        //2. 调用service 完成修改\n        service.update(brand);\n\n        //3. 转发到查询所有Servlet\n        request.getRequestDispatcher("/selectAllServlet").forward(request,response);\n    }\n\n    @Override\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        this.doGet(request, response);\n    }\n}\n\n\n存在问题：update.jsp 页面提交数据时是没有携带主键数据的，而后台修改数据需要根据主键进行修改。\n\n针对这个问题，我们不希望页面将主键id展示给用户看，但是又希望在提交数据时能将主键id提交到后端。此时我们就想到了在学习 HTML 时学习的隐藏域，在 update.jsp 页面的表单中添加如下代码：\n\n<%--隐藏域，提交id--%>\n<input type="hidden" name="id" value="${brand.id}">\n\n\nupdate.jsp 页面的最终代码如下：\n\n<%@ page contentType="text/html;charset=UTF-8" language="java" %>\n<%@ taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core" %>\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <title>修改品牌</title>\n</head>\n<body>\n<h3>修改品牌</h3>\n<form action="/brand-demo/updateServlet" method="post">\n\n    <%--隐藏域，提交id--%>\n    <input type="hidden" name="id" value="${brand.id}">\n\n    品牌名称：<input name="brandName" value="${brand.brandName}"><br>\n    企业名称：<input name="companyName" value="${brand.companyName}"><br>\n    排序：<input name="ordered" value="${brand.ordered}"><br>\n    描述信息：<textarea rows="5" cols="20" name="description">${brand.description} </textarea><br>\n    状态：\n    <c:if test="${brand.status ** 0}">\n        <input type="radio" name="status" value="0" checked>禁用\n        <input type="radio" name="status" value="1">启用<br>\n    </c:if>\n\n    <c:if test="${brand.status ** 1}">\n        <input type="radio" name="status" value="0" >禁用\n        <input type="radio" name="status" value="1" checked>启用<br>\n    </c:if>\n    <input type="submit" value="提交">\n</form>\n</body>\n</html>\n',normalizedContent:'# jsp\n\n浅谈jsp\n\n\n# jsp 概述\n\n**jsp（全称：java server pages）：java 服务端页面。**是一种动态的网页技术，其中既可以定义 html、js、css等静态内容，还可以定义 java代码的动态内容，也就是 jsp = html + java。如下就是jsp代码\n\n<html>\n    <head>\n        <title>title</title>\n    </head>\n    <body>\n        <h1>jsp,hello world</h1>\n        <%\n        \tsystem.out.println("hello,jsp~");\n        %>\n    </body>\n</html>\n\n\n上面代码 h1 标签内容是展示在页面上，而 java 的输出语句是输出在 idea 的控制台。\n\n那么，jsp 能做什么呢？现在我们只用 servlet 实现功能，看存在什么问题。如下图所示，当我们登陆成功后，需要在页面上展示用户名\n\n上图的用户名是动态展示，也就是谁登陆就展示谁的用户名。只用 servlet 如何实现呢？在今天的资料里已经提供好了一个 loginservlet ，该 servlet 就是实现这个功能的，现将资料中的 loginservlet.java 拷贝到 request-demo 项目中来演示。接下来启动服务器并访问登陆页面\n\n输入了 zhangsan 用户的登陆信息后点击 登陆 按钮，就能看到如下图效果\n\n\n\n当然如果是 lisi 登陆的，在该页面展示的就是 lisi,欢迎您，动态的展示效果就实现了。那么 loginservlet 到底是如何实现的，我们看看它里面的内容\n\n上面的代码有大量使用到 writer 对象向页面写标签内容，这样我们的代码就显得很麻烦；将来如果展示的效果出现了问题，排错也显得有点力不从心。而 jsp 是如何解决这个问题的呢？在资料中也提供了一个 login.jsp 页面，该页面也能实现该功能，现将该页面拷贝到项目的 webapp下，需要修改 login.html 中表单数据提交的路径为下图\n\n重新启动服务器并进行测试，发现也可以实现同样的功能。那么 login.jsp 又是如何实现的呢？那我们来看看 login.jsp 的代码\n\n上面代码可以看到里面基本都是 html 标签，而动态数据使用 java 代码进行展示；这样操作看起来要比用 servlet 实现要舒服很多。\n\njsp 作用：简化开发，避免了在servlet中直接输出html标签。\n\n\n# jsp 快速入门\n\n接下来我们做一个简单的快速入门代码。\n\n\n# 搭建环境\n\n创建一个maven的 web 项目，项目结构如下：\n\npom.xml 文件内容如下：\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n\n    <groupid>org.example</groupid>\n    <artifactid>jsp-demo</artifactid>\n    <version>1.0-snapshot</version>\n    <packaging>war</packaging>\n\n    <properties>\n        <maven.compiler.source>8</maven.compiler.source>\n        <maven.compiler.target>8</maven.compiler.target>\n    </properties>\n\n    <dependencies>\n      <dependency>\n            <groupid>javax.servlet</groupid>\n            <artifactid>javax.servlet-api</artifactid>\n            <version>3.1.0</version>\n            <scope>provided</scope>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupid>org.apache.tomcat.maven</groupid>\n                <artifactid>tomcat7-maven-plugin</artifactid>\n                <version>2.2</version>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n\n\n\n# 导入 jsp 依赖\n\n在 dependencies 标签中导入 jsp 的依赖，如下\n\n<dependency>\n    <groupid>javax.servlet.jsp</groupid>\n    <artifactid>jsp-api</artifactid>\n    <version>2.2</version>\n    <scope>provided</scope>\n</dependency>\n\n\n该依赖的 scope 必须设置为 provided，因为 tomcat 中有这个jar包了，所以在打包时我们是不希望将该依赖打进到我们工程的war包中。\n\n\n# 创建 jsp 页面\n\n在项目的 webapp 下创建jsp页面\n\n通过上面方式创建一个名为 hello.jsp 的页面。\n\n\n# 编写代码\n\n在 hello.jsp 页面中书写 html 标签和 java 代码，如下\n\n<%@ page contenttype="text/html;charset=utf-8" language="java" %>\n<html>\n<head>\n    <title>title</title>\n</head>\n<body>\n    <h1>hello jsp</h1>\n\n    <%\n        system.out.println("hello,jsp~");\n    %>\n</body>\n</html>\n\n\n\n# 测试\n\n启动服务器并在浏览器地址栏输入 http://localhost:8080/jsp-demo/hello.jsp，我们可以在页面上看到如下内容\n\n同时也可以看到在 idea 的控制台看到输出的 hello,jsp~ 内容。\n\n\n# jsp 原理\n\n我们之前说 jsp 就是一个页面，那么在 jsp 中写 html 标签，我们能理解，但是为什么还可以写 java 代码呢？\n\n因为 **jsp 本质上就是一个 servlet。**接下来我们聊聊访问jsp时的流程\n\n 1. 浏览器第一次访问 hello.jsp 页面\n 2. tomcat 会将 hello.jsp 转换为名为 hello_jsp.java 的一个 servlet\n 3. tomcat 再将转换的 servlet 编译成字节码文件 hello_jsp.class\n 4. tomcat 会执行该字节码文件，向外提供服务\n\n我们可以到项目所在磁盘目录下找 target\\tomcat\\work\\tomcat\\localhost\\jsp-demo\\org\\apache\\jsp 目录，而这个目录下就能看到转换后的 servlet\n\n打开 hello_jsp.java 文件，来查看里面的代码\n\n由上面的类的继承关系可以看到继承了名为 httpjspbase 这个类，那我们在看该类的继承关系。到资料中的找如下目录： 资料\\tomcat源码\\apache-tomcat-8.5.68-src\\java\\org\\apache\\jasper\\runtime ，该目录下就有 httpjspbase 类，查看该类的继承关系\n\n可以看到该类继承了 httpservlet ；那么 hello_jsp 这个类就间接的继承了 httpservlet ，也就说明 hello_jsp 是一个 servlet。\n\n继续阅读 hello_jsp 类的代码，可以看到有一个名为 _jspservice() 的方法，该方法就是每次访问 jsp 时自动执行的方法，和 servlet 中的 service 方法一样 。\n\n而在 _jspservice() 方法中可以看到往浏览器写标签的代码：\n\n以前我们自己写 servlet 时，这部分代码是由我们自己来写，现在有了 jsp 后，由tomcat完成这部分功能。\n\n\n# jsp 脚本\n\njsp脚本用于在 jsp页面内定义 java代码。在之前的入门案例中我们就在 jsp 页面定义的 java 代码就是 jsp 脚本。\n\n\n# jsp 脚本分类\n\njsp 脚本有如下三个分类：\n\n * <%...%>：内容会直接放到_jspservice()方法之中\n * <%=…%>：内容会放到out.print()中，作为out.print()的参数\n * <%!…%>：内容会放到_jspservice()方法之外，被类直接包含\n\n代码演示：\n\n在 hello.jsp 中书写\n\n<%\n    system.out.println("hello,jsp~");\n    int i = 3;\n%>\n\n\n通过浏览器访问 hello.jsp 后，查看转换的 hello_jsp.java 文件，i 变量定义在了 _jspservice() 方法中\n\n在 hello.jsp 中书写\n\n<%="hello"%>\n<%=i%>\n\n\n通过浏览器访问 hello.jsp 后，查看转换的 hello_jsp.java 文件，该脚本的内容被放在了 out.print() 中，作为参数\n\n在 hello.jsp 中书写\n\n<%!\n    void  show(){}\n\tstring name = "zhangsan";\n%>\n\n\n通过浏览器访问 hello.jsp 后，查看转换的 hello_jsp.java 文件，该脚本的内容被放在了成员位置\n\n\n# 案例\n\n# 需求\n\n使用jsp脚本展示品牌数据\n\n说明：\n\n * 在资料 资料\\1. jsp案例素材 中提供了 brand.html 静态页面\n * 在该案例中数据不从数据库中查询，而是在 jsp 页面上写死\n\n# 实现\n\n * 将资料 资料\\1. jsp案例素材 中的 brand.java 文件放置到项目的 com.itheima.pojo 包下\n\n * 在项目的 webapp 中创建 brand.jsp ，并将 brand.html页面中的内容拷贝过来。brand.jsp 内容如下\n   \n   <%@ page contenttype="text/html;charset=utf-8" language="java" %>\n   <!doctype html>\n   <html lang="en">\n   <head>\n       <meta charset="utf-8">\n       <title>title</title>\n   </head>\n   <body>\n   <input type="button" value="新增"><br>\n   <hr>\n       <table border="1" cellspacing="0" width="800">\n           <tr>\n               <th>序号</th>\n               <th>品牌名称</th>\n               <th>企业名称</th>\n               <th>排序</th>\n               <th>品牌介绍</th>\n               <th>状态</th>\n               <th>操作</th>\n   \n           </tr>\n           <tr align="center">\n               <td>1</td>\n               <td>三只松鼠</td>\n               <td>三只松鼠</td>\n               <td>100</td>\n               <td>三只松鼠，好吃不上火</td>\n               <td>启用</td>\n               <td><a href="#">修改</a> <a href="#">删除</a></td>\n           </tr>\n   \n           <tr align="center">\n               <td>2</td>\n               <td>优衣库</td>\n               <td>优衣库</td>\n               <td>10</td>\n               <td>优衣库，服适人生</td>\n               <td>禁用</td>\n   \n               <td><a href="#">修改</a> <a href="#">删除</a></td>\n           </tr>\n   \n           <tr align="center">\n               <td>3</td>\n               <td>小米</td>\n               <td>小米科技有限公司</td>\n               <td>1000</td>\n               <td>为发烧而生</td>\n               <td>启用</td>\n   \n               <td><a href="#">修改</a> <a href="#">删除</a></td>\n           </tr>\n       </table>\n   </body>\n   </html>\n   \n   \n   现在页面中的数据都是假数据。\n\n * 在 brand.jsp 中准备一些数据\n   \n   <%\n       // 查询数据库\n       list<brand> brands = new arraylist<brand>();\n       brands.add(new brand(1,"三只松鼠","三只松鼠",100,"三只松鼠，好吃不上火",1));\n       brands.add(new brand(2,"优衣库","优衣库",200,"优衣库，服适人生",0));\n       brands.add(new brand(3,"小米","小米科技有限公司",1000,"为发烧而生",1));\n   %>\n   \n   \n   > **注意：**这里的类是需要导包的\n\n * 将 brand.jsp 页面中的 table 标签中的数据改为动态的\n   \n   <table border="1" cellspacing="0" width="800">\n       <tr>\n           <th>序号</th>\n           <th>品牌名称</th>\n           <th>企业名称</th>\n           <th>排序</th>\n           <th>品牌介绍</th>\n           <th>状态</th>\n           <th>操作</th>\n   \n       </tr>\n       \n       <%\n        for (int i = 0; i < brands.size(); i++) {\n            //获取集合中的 每一个 brand 对象\n            brand brand = brands.get(i);\n        }\n       %>\n       <tr align="center">\n           <td>1</td>\n           <td>三只松鼠</td>\n           <td>三只松鼠</td>\n           <td>100</td>\n           <td>三只松鼠，好吃不上火</td>\n           <td>启用</td>\n           <td><a href="#">修改</a> <a href="#">删除</a></td>\n       </tr>\n   </table>\n   \n   \n   上面的for循环需要将 tr 标签包裹起来，这样才能实现循环的效果，代码改进为\n   \n   <table border="1" cellspacing="0" width="800">\n       <tr>\n           <th>序号</th>\n           <th>品牌名称</th>\n           <th>企业名称</th>\n           <th>排序</th>\n           <th>品牌介绍</th>\n           <th>状态</th>\n           <th>操作</th>\n   \n       </tr>\n       \n       <%\n        for (int i = 0; i < brands.size(); i++) {\n            //获取集合中的 每一个 brand 对象\n            brand brand = brands.get(i);\n       %>\n       \t <tr align="center">\n               <td>1</td>\n               <td>三只松鼠</td>\n               <td>三只松鼠</td>\n               <td>100</td>\n               <td>三只松鼠，好吃不上火</td>\n               <td>启用</td>\n               <td><a href="#">修改</a> <a href="#">删除</a></td>\n           </tr>\n       <%\n        }\n       %>\n      \n   </table>\n   \n   \n   > 注意：<%%> 里面写的是 java 代码，而外边写的是 html 标签\n   \n   上面代码中的 td 标签中的数据都需要是动态的，所以还需要改进\n   \n   <table border="1" cellspacing="0" width="800">\n       <tr>\n           <th>序号</th>\n           <th>品牌名称</th>\n           <th>企业名称</th>\n           <th>排序</th>\n           <th>品牌介绍</th>\n           <th>状态</th>\n           <th>操作</th>\n   \n       </tr>\n       \n       <%\n        for (int i = 0; i < brands.size(); i++) {\n            //获取集合中的 每一个 brand 对象\n            brand brand = brands.get(i);\n       %>\n       \t <tr align="center">\n               <td><%=brand.getid()%></td>\n               <td><%=brand.getbrandname()%></td>\n               <td><%=brand.getcompanyname()%></td>\n               <td><%=brand.getordered()%></td>\n               <td><%=brand.getdescription()%></td>\n               <td><%=brand.getstatus() ** 1 ? "启用":"禁用"%></td>\n               <td><a href="#">修改</a> <a href="#">删除</a></td>\n           </tr>\n       <%\n        }\n       %>\n      \n   </table>\n   \n\n# 成品代码\n\n<%@ page import="com.itheima.pojo.brand" %>\n<%@ page import="java.util.list" %>\n<%@ page import="java.util.arraylist" %>\n<%@ page contenttype="text/html;charset=utf-8" language="java" %>\n\n<%\n    // 查询数据库\n    list<brand> brands = new arraylist<brand>();\n    brands.add(new brand(1,"三只松鼠","三只松鼠",100,"三只松鼠，好吃不上火",1));\n    brands.add(new brand(2,"优衣库","优衣库",200,"优衣库，服适人生",0));\n    brands.add(new brand(3,"小米","小米科技有限公司",1000,"为发烧而生",1));\n\n%>\n\n\n<!doctype html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>title</title>\n</head>\n<body>\n<input type="button" value="新增"><br>\n<hr>\n<table border="1" cellspacing="0" width="800">\n    <tr>\n        <th>序号</th>\n        <th>品牌名称</th>\n        <th>企业名称</th>\n        <th>排序</th>\n        <th>品牌介绍</th>\n        <th>状态</th>\n        <th>操作</th>\n    </tr>\n    <%\n        for (int i = 0; i < brands.size(); i++) {\n            brand brand = brands.get(i);\n    %>\n\n    <tr align="center">\n        <td><%=brand.getid()%></td>\n        <td><%=brand.getbrandname()%></td>\n        <td><%=brand.getcompanyname()%></td>\n        <td><%=brand.getordered()%></td>\n        <td><%=brand.getdescription()%></td>\n\t\t<td><%=brand.getstatus() ** 1 ? "启用":"禁用"%></td>\n        <td><a href="#">修改</a> <a href="#">删除</a></td>\n    </tr>\n\n    <%\n        }\n    %>\n</table>\n</body>\n</html>\n\n\n# 测试\n\n在浏览器地址栏输入 http://localhost:8080/jsp-demo/brand.jsp ，页面展示效果如下\n\n\n# jsp 缺点\n\n通过上面的案例，我们可以看到 jsp 的很多缺点。\n\n由于 jsp页面内，既可以定义 html 标签，又可以定义 java代码，造成了以下问题：\n\n * 书写麻烦：特别是复杂的页面\n   \n   既要写 html 标签，还要写 java 代码\n\n * 阅读麻烦\n   \n   上面案例的代码，相信你后期再看这段代码时还需要花费很长的时间去梳理\n\n * 复杂度高：运行需要依赖于各种环境，jre，jsp容器，javaee…\n\n * 占内存和磁盘：jsp会自动生成.java和.class文件占磁盘，运行的是.class文件占内存\n\n * 调试困难：出错后，需要找到自动生成的.java文件进行调试\n\n * 不利于团队协作：前端人员不会 java，后端人员不精 html\n   \n   如果页面布局发生变化，前端工程师对静态页面进行修改，然后再交给后端工程师，由后端工程师再将该页面改为 jsp 页面\n\n由于上述的问题， **jsp 已逐渐退出历史舞台，**以后开发更多的是使用 html + ajax 来替代。ajax 是我们后续会重点学习的技术。有个这个技术后，前端工程师负责前端页面开发，而后端工程师只负责前端代码开发。下来对技术的发展进行简单的说明\n\n 1. 第一阶段：使用 servlet 即实现逻辑代码编写，也对页面进行拼接。这种模式我们之前也接触过\n\n 2. 第二阶段：随着技术的发展，出现了 jsp ，人们发现 jsp 使用起来比 servlet 方便很多，但是还是要在 jsp 中嵌套 java 代码，也不利于后期的维护\n\n 3. 第三阶段：使用 servlet 进行逻辑代码开发，而使用 jsp 进行数据展示\n\n 4. 第四阶段：使用 servlet 进行后端逻辑代码开发，而使用 html 进行数据展示。而这里面就存在问题，html 是静态页面，怎么进行动态数据展示呢？这就是 ajax 的作用了。\n\n那既然 jsp 已经逐渐的退出历史舞台，那我们为什么还要学习 jsp 呢？原因有两点：\n\n * 一些公司可能有些老项目还在用 jsp ，所以要求我们必须动 jsp\n * 我们如果不经历这些复杂的过程，就不能体现后面阶段开发的简单\n\n接下来我们来学习第三阶段，使用 el表达式 和 jstl 标签库替换 jsp 中的 java 代码。\n\n\n# el 表达式\n\n\n# 概述\n\nel（全称expression language ）表达式语言，用于简化 jsp 页面内的 java 代码。\n\nel 表达式的主要作用是 获取数据。其实就是从域对象中获取数据，然后将数据展示在页面上。\n\n而 el 表达式的语法也比较简单，${expression} 。例如：${brands} 就是获取域中存储的 key 为 brands 的数据。\n\n\n# 代码演示\n\n * 定义servlet，在 servlet 中封装一些数据并存储到 request 域对象中并转发到 el-demo.jsp 页面。\n   \n   @webservlet("/demo1")\n   public class servletdemo1 extends httpservlet {\n       @override\n       protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n           //1. 准备数据\n           list<brand> brands = new arraylist<brand>();\n           brands.add(new brand(1,"三只松鼠","三只松鼠",100,"三只松鼠，好吃不上火",1));\n           brands.add(new brand(2,"优衣库","优衣库",200,"优衣库，服适人生",0));\n           brands.add(new brand(3,"小米","小米科技有限公司",1000,"为发烧而生",1));\n   \n           //2. 存储到request域中\n           request.setattribute("brands",brands);\n   \n           //3. 转发到 el-demo.jsp\n           request.getrequestdispatcher("/el-demo.jsp").forward(request,response);\n       }\n   \n       @override\n       protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n           this.doget(request, response);\n       }\n   }\n   \n   \n   > 注意： 此处需要用转发，因为转发才可以使用 request 对象作为域对象进行数据共享\n\n * 在 el-demo.jsp 中通过 el表达式 获取数据\n   \n   <%@ page contenttype="text/html;charset=utf-8" language="java" %>\n   <html>\n   <head>\n       <title>title</title>\n   </head>\n   <body>\n       ${brands}\n   </body>\n   </html>\n   \n\n * 在浏览器的地址栏输入 http://localhost:8080/jsp-demo/demo1 ，页面效果如下：\n\n\n# 域对象\n\njavaweb中有四大域对象，分别是：\n\n * page：当前页面有效\n * request：当前请求有效\n * session：当前会话有效\n * application：当前应用有效\n\nel 表达式获取数据，会依次从这4个域中寻找，直到找到为止。而这四个域对象的作用范围如下图所示\n\n例如： ${brands}，el 表达式获取数据，会先从page域对象中获取数据，如果没有再到 requet 域对象中获取数据，如果再没有再到 session 域对象中获取，如果还没有才会到 application 中获取数据。\n\n\n# jstl标签\n\n\n# 概述\n\njsp标准标签库(jsp standarded tag library) ，使用标签取代jsp页面上的java代码。如下代码就是jstl标签\n\n<c:if test="${flag ** 1}">\n    男\n</c:if>\n<c:if test="${flag ** 2}">\n    女\n</c:if>\n\n\n上面代码看起来是不是比 jsp 中嵌套 java 代码看起来舒服好了。而且前端工程师对标签是特别敏感的，他们看到这段代码是能看懂的。\n\njstl 提供了很多标签，如下图\n\n我们只对两个最常用的标签进行讲解，<c:foreach> 标签和 <c:if> 标签。\n\njstl 使用也是比较简单的，分为如下步骤：\n\n * 导入坐标\n   \n   <dependency>\n       <groupid>jstl</groupid>\n       <artifactid>jstl</artifactid>\n       <version>1.2</version>\n   </dependency>\n   <dependency>\n       <groupid>taglibs</groupid>\n       <artifactid>standard</artifactid>\n       <version>1.1.2</version>\n   </dependency>\n   \n\n * 在jsp页面上引入jstl标签库\n   \n   <%@ taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core" %> \n   \n\n * 使用标签\n\n\n# if 标签\n\n<c:if>：相当于 if 判断\n\n * 属性：test，用于定义条件表达式\n\n<c:if test="${flag ** 1}">\n    男\n</c:if>\n<c:if test="${flag ** 2}">\n    女\n</c:if>\n\n\n代码演示：\n\n * 定义一个 servlet ，在该 servlet 中向 request 域对象中添加 键是 status ，值为 1 的数据\n   \n   @webservlet("/demo2")\n   public class servletdemo2 extends httpservlet {\n       @override\n       protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n           //1. 存储数据到request域中\n           request.setattribute("status",1);\n   \n           //2. 转发到 jstl-if.jsp\n           数据request.getrequestdispatcher("/jstl-if.jsp").forward(request,response);\n       }\n   \n       @override\n       protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n           this.doget(request, response);\n       }\n   }\n   \n\n * 定义 jstl-if.jsp 页面，在该页面使用 <c:if> 标签\n   \n   <%@ page contenttype="text/html;charset=utf-8" language="java" %>\n   <%@ taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core" %>\n   <html>\n   <head>\n       <title>title</title>\n   </head>\n   <body>\n       <%--\n           c:if：来完成逻辑判断，替换java  if else\n       --%>\n       <c:if test="${status **1}">\n           启用\n       </c:if>\n   \n       <c:if test="${status **0}">\n           禁用\n       </c:if>\n   </body>\n   </html>\n   \n   \n   > 注意： 在该页面已经要引入 jstl核心标签库\n   > \n   > <%@ taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core" %>\n\n\n# foreach 标签\n\n<c:foreach>：相当于 for 循环。java中有增强for循环和普通for循环，jstl 中的 <c:foreach> 也有两种用法\n\n# 用法一\n\n类似于 java 中的增强for循环。涉及到的 <c:foreach> 中的属性如下\n\n * items：被遍历的容器\n\n * var：遍历产生的临时变量\n\n * varstatus：遍历状态对象\n\n如下代码，是从域对象中获取名为 brands 数据，该数据是一个集合；遍历遍历，并给该集合中的每一个元素起名为 brand，是 brand对象。在循环里面使用 el表达式获取每一个brand对象的属性值\n\n<c:foreach items="${brands}" var="brand">\n    <tr align="center">\n        <td>${brand.id}</td>\n        <td>${brand.brandname}</td>\n        <td>${brand.companyname}</td>\n        <td>${brand.description}</td>\n    </tr>\n</c:foreach>\n\n\n代码演示：\n\n * servlet 还是使用之前的名为 servletdemo1 。\n\n * 定义名为 jstl-foreach.jsp 页面，内容如下：\n   \n   <%@ page contenttype="text/html;charset=utf-8" language="java" %>\n   <%@ taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core" %>\n   \n   <!doctype html>\n   <html lang="en">\n   <head>\n       <meta charset="utf-8">\n       <title>title</title>\n   </head>\n   <body>\n   <input type="button" value="新增"><br>\n   <hr>\n   <table border="1" cellspacing="0" width="800">\n       <tr>\n           <th>序号</th>\n           <th>品牌名称</th>\n           <th>企业名称</th>\n           <th>排序</th>\n           <th>品牌介绍</th>\n           <th>状态</th>\n           <th>操作</th>\n       </tr>\n   \n       <c:foreach items="${brands}" var="brand" varstatus="status">\n           <tr align="center">\n               <%--<td>${brand.id}</td>--%>\n               <td>${status.count}</td>\n               <td>${brand.brandname}</td>\n               <td>${brand.companyname}</td>\n               <td>${brand.ordered}</td>\n               <td>${brand.description}</td>\n               <c:if test="${brand.status ** 1}">\n                   <td>启用</td>\n               </c:if>\n               <c:if test="${brand.status != 1}">\n                   <td>禁用</td>\n               </c:if>\n               <td><a href="#">修改</a> <a href="#">删除</a></td>\n           </tr>\n       </c:foreach>\n   </table>\n   </body>\n   </html>\n   \n\n# 用法二\n\n类似于 java 中的普通for循环。涉及到的 <c:foreach> 中的属性如下\n\n * begin：开始数\n\n * end：结束数\n\n * step：步长\n\n实例代码：\n\n从0循环到10，变量名是 i ，每次自增1\n\n<c:foreach begin="0" end="10" step="1" var="i">\n    ${i}\n</c:foreach>\n\n\n\n# mvc模式和三层架构\n\nmvc 模式和三层架构是一些理论的知识，将来我们使用了它们进行代码开发会让我们代码维护性和扩展性更好。\n\n\n# mvc模式\n\nmvc 是一种分层开发的模式，其中：\n\n * m：model，业务模型，处理业务\n\n * v：view，视图，界面展示\n\n * c：controller，控制器，处理请求，调用模型和视图\n\n控制器（serlvlet）用来接收浏览器发送过来的请求，控制器调用模型（javabean）来获取数据，比如从数据库查询数据；控制器获取到数据后再交由视图（jsp）进行数据展示。\n\nmvc 好处：\n\n * 职责单一，互不影响。每个角色做它自己的事，各司其职。\n\n * 有利于分工协作。\n\n * 有利于组件重用\n\n\n# 三层架构\n\n三层架构是将我们的项目分成了三个层面，分别是 表现层、业务逻辑层、数据访问层。\n\n * 数据访问层：对数据库的crud基本操作\n * 业务逻辑层：对业务逻辑进行封装，组合数据访问层层中基本功能，形成复杂的业务逻辑功能。例如 注册业务功能 ，我们会先调用 数据访问层 的 selectbyname() 方法判断该用户名是否存在，如果不存在再调用 数据访问层 的 insert() 方法进行数据的添加操作\n * 表现层：接收请求，封装数据，调用业务逻辑层，响应数据\n\n而整个流程是，浏览器发送请求，表现层的servlet接收请求并调用业务逻辑层的方法进行业务逻辑处理，而业务逻辑层方法调用数据访问层方法进行数据的操作，依次返回到serlvet，然后servlet将数据交由 jsp 进行展示。\n\n三层架构的每一层都有特有的包名称：\n\n * 表现层： com.itheima.controller 或者 com.itheima.web\n * 业务逻辑层：com.itheima.service\n * 数据访问层：com.itheima.dao 或者 com.itheima.mapper\n\n后期我们还会学习一些框架，不同的框架是对不同层进行封装的\n\n\n# mvc 和 三层架构\n\n通过 mvc 和 三层架构 的学习，有些人肯定混淆了。那他们有什么区别和联系？\n\n如上图上半部分是 mvc 模式，上图下半部分是三层架构。 mvc 模式 中的 c（控制器）和 v（视图）就是 三层架构 中的表现层，而 mvc 模式 中的 m（模型）就是 三层架构 中的 业务逻辑层 和 数据访问层。\n\n可以将 mvc 模式 理解成是一个大的概念，而 三层架构 是对 mvc 模式 实现架构的思想。 那么我们以后按照要求将不同层的代码写在不同的包下，每一层里功能职责做到单一，将来如果将表现层的技术换掉，而业务逻辑层和数据访问层的代码不需要发生变化。\n\n\n# 案例\n\n需求：完成品牌数据的增删改查操作\n\n这个功能我们之前一直在做，而这个案例是将今天学习的所有的内容（包含 mvc模式 和 三层架构）进行应用，并将整个流程贯穿起来。\n\n\n# 环境准备\n\n环境准备工作，我们分以下步骤实现：\n\n * 创建新的模块 brand_demo，引入坐标\n\n * 创建三层架构的包结构\n\n * 数据库表 tb_brand\n\n * 实体类 brand\n\n * mybatis 基础环境\n   \n   * mybatis-config.xml\n   \n   * brandmapper.xml\n   \n   * brandmapper接口\n\n# 创建工程\n\n创建新的模块 brand_demo，引入坐标。我们只要分析出要用到哪儿些技术，那么需要哪儿些坐标也就明确了\n\n * 需要操作数据库。mysql的驱动包\n * 要使用mybatis框架。mybaits的依赖包\n * web项目需要用到servlet和jsp。servlet和jsp的依赖包\n * 需要使用 jstl 进行数据展示。jstl的依赖包\n\npom.xml 内容如下：\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n    <groupid>org.example</groupid>\n    <artifactid>brand-demo</artifactid>\n    <version>1.0-snapshot</version>\n    <packaging>war</packaging>\n\n    <properties>\n        <maven.compiler.source>8</maven.compiler.source>\n        <maven.compiler.target>8</maven.compiler.target>\n    </properties>\n\n    <dependencies>\n        \x3c!-- mybatis --\x3e\n        <dependency>\n            <groupid>org.mybatis</groupid>\n            <artifactid>mybatis</artifactid>\n            <version>3.5.5</version>\n        </dependency>\n\n        \x3c!--mysql--\x3e\n        <dependency>\n            <groupid>mysql</groupid>\n            <artifactid>mysql-connector-java</artifactid>\n            <version>5.1.34</version>\n        </dependency>\n\n        \x3c!--servlet--\x3e\n        <dependency>\n            <groupid>javax.servlet</groupid>\n            <artifactid>javax.servlet-api</artifactid>\n            <version>3.1.0</version>\n            <scope>provided</scope>\n        </dependency>\n\n        \x3c!--jsp--\x3e\n        <dependency>\n            <groupid>javax.servlet.jsp</groupid>\n            <artifactid>jsp-api</artifactid>\n            <version>2.2</version>\n            <scope>provided</scope>\n        </dependency>\n\n        \x3c!--jstl--\x3e\n        <dependency>\n            <groupid>jstl</groupid>\n            <artifactid>jstl</artifactid>\n            <version>1.2</version>\n        </dependency>\n        <dependency>\n            <groupid>taglibs</groupid>\n            <artifactid>standard</artifactid>\n            <version>1.1.2</version>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupid>org.apache.tomcat.maven</groupid>\n                <artifactid>tomcat7-maven-plugin</artifactid>\n                <version>2.2</version>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n\n\n# 创建包\n\n创建不同的包结构，用来存储不同的类。包结构如下\n\n# 创建表\n\n-- 删除tb_brand表\ndrop table if exists tb_brand;\n-- 创建tb_brand表\ncreate table tb_brand\n(\n    -- id 主键\n    id           int primary key auto_increment,\n    -- 品牌名称\n    brand_name   varchar(20),\n    -- 企业名称\n    company_name varchar(20),\n    -- 排序字段\n    ordered      int,\n    -- 描述信息\n    description  varchar(100),\n    -- 状态：0：禁用  1：启用\n    status       int\n);\n-- 添加数据\ninsert into tb_brand (brand_name, company_name, ordered, description, status)\nvalues (\'三只松鼠\', \'三只松鼠股份有限公司\', 5, \'好吃不上火\', 0),\n       (\'华为\', \'华为技术有限公司\', 100, \'华为致力于把数字世界带入每个人、每个家庭、每个组织，构建万物互联的智能世界\', 1),\n       (\'小米\', \'小米科技有限公司\', 50, \'are you ok\', 1);\n\n\n# 创建实体类\n\n在 pojo 包下创建名为 brand 的类。\n\npublic class brand {\n    // id 主键\n    private integer id;\n    // 品牌名称\n    private string brandname;\n    // 企业名称\n    private string companyname;\n    // 排序字段\n    private integer ordered;\n    // 描述信息\n    private string description;\n    // 状态：0：禁用  1：启用\n    private integer status;\n\n\n    public brand() {\n    }\n\n    public brand(integer id, string brandname, string companyname, string description) {\n        this.id = id;\n        this.brandname = brandname;\n        this.companyname = companyname;\n        this.description = description;\n    }\n\n    public brand(integer id, string brandname, string companyname, integer ordered, string description, integer status) {\n        this.id = id;\n        this.brandname = brandname;\n        this.companyname = companyname;\n        this.ordered = ordered;\n        this.description = description;\n        this.status = status;\n    }\n\n    public integer getid() {\n        return id;\n    }\n\n    public void setid(integer id) {\n        this.id = id;\n    }\n\n    public string getbrandname() {\n        return brandname;\n    }\n\n    public void setbrandname(string brandname) {\n        this.brandname = brandname;\n    }\n\n    public string getcompanyname() {\n        return companyname;\n    }\n\n    public void setcompanyname(string companyname) {\n        this.companyname = companyname;\n    }\n\n    public integer getordered() {\n        return ordered;\n    }\n\n    public void setordered(integer ordered) {\n        this.ordered = ordered;\n    }\n\n    public string getdescription() {\n        return description;\n    }\n\n    public void setdescription(string description) {\n        this.description = description;\n    }\n\n    public integer getstatus() {\n        return status;\n    }\n\n    public void setstatus(integer status) {\n        this.status = status;\n    }\n\n    @override\n    public string tostring() {\n        return "brand{" +\n                "id=" + id +\n                ", brandname=\'" + brandname + \'\\\'\' +\n                ", companyname=\'" + companyname + \'\\\'\' +\n                ", ordered=" + ordered +\n                ", description=\'" + description + \'\\\'\' +\n                ", status=" + status +\n                \'}\';\n    }\n}\n\n\n\n# 准备mybatis环境\n\n定义核心配置文件 mybatis-config.xml ，并将该文件放置在 resources 下\n\n<?xml version="1.0" encoding="utf-8" ?>\n<!doctype configuration\n        public "-//mybatis.org//dtd config 3.0//en"\n        "http://mybatis.org/dtd/mybatis-3-config.dtd">\n<configuration>\n    \x3c!--起别名--\x3e\n    <typealiases>\n        <package name="com.itheima.pojo"/>\n    </typealiases>\n\n    <environments default="development">\n        <environment id="development">\n            <transactionmanager type="jdbc"/>\n            <datasource type="pooled">\n                <property name="driver" value="com.mysql.jdbc.driver"/>\n                <property name="url" value="jdbc:mysql:///db1?usessl=false&amp;useserverprepstmts=true"/>\n                <property name="username" value="root"/>\n                <property name="password" value="1234"/>\n            </datasource>\n        </environment>\n    </environments>\n    <mappers>\n        \x3c!--扫描mapper--\x3e\n        <package name="com.itheima.mapper"/>\n    </mappers>\n</configuration>\n\n\n在 resources 下创建放置映射配置文件的目录结构 com/itheima/mapper，并在该目录下创建映射配置文件 brandmapper.xml\n\n<?xml version="1.0" encoding="utf-8" ?>\n<!doctype mapper\n        public "-//mybatis.org//dtd mapper 3.0//en"\n        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">\n<mapper namespace="com.itheima.mapper.brandmapper">\n    \n</mapper>\n\n\n\n# 查询所有\n\n\n\n当我们点击 index.html 页面中的 查询所有 这个超链接时，就能查询到上图右半部分的数据。\n\n对于上述的功能，点击 查询所有 超链接是需要先请后端的 servlet ，由 servlet 跳转到对应的页面进行数据的动态展示。而整个流程如下图：\n\n# 编写brandmapper\n\n在 mapper 包下创建创建 brandmapper 接口，在接口中定义 selectall() 方法\n\n/**\n  * 查询所有\n  * @return\n  */\n@select("select * from tb_brand")\nlist<brand> selectall();\n\n\n# 编写工具类\n\n在 com.itheima 包下创建 utils 包，并在该包下创建名为 sqlsessionfactoryutils 工具类\n\npublic class sqlsessionfactoryutils {\n\n    private static sqlsessionfactory sqlsessionfactory;\n\n    static {\n        //静态代码块会随着类的加载而自动执行，且只执行一次\n        try {\n            string resource = "mybatis-config.xml";\n            inputstream inputstream = resources.getresourceasstream(resource);\n            sqlsessionfactory = new sqlsessionfactorybuilder().build(inputstream);\n        } catch (ioexception e) {\n            e.printstacktrace();\n        }\n    }\n\n    public static sqlsessionfactory getsqlsessionfactory(){\n        return sqlsessionfactory;\n    }\n}\n\n\n# 编写brandservice\n\n在 service 包下创建 brandservice 类\n\npublic class brandservice {\n    sqlsessionfactory factory = sqlsessionfactoryutils.getsqlsessionfactory();\n\n    /**\n     * 查询所有\n     * @return\n     */\n    public list<brand> selectall(){\n        //调用brandmapper.selectall()\n\n        //2. 获取sqlsession\n        sqlsession sqlsession = factory.opensession();\n        //3. 获取brandmapper\n        brandmapper mapper = sqlsession.getmapper(brandmapper.class);\n\n        //4. 调用方法\n        list<brand> brands = mapper.selectall();\n\n        sqlsession.close();\n\n        return brands;\n    }\n}\n\n\n# 编写servlet\n\n在 web 包下创建名为 selectallservlet 的 servlet，该 servlet 的逻辑如下：\n\n * 调用 brandservice 的 selectall() 方法进行业务逻辑处理，并接收返回的结果\n * 将上一步返回的结果存储到 request 域对象中\n * 跳转到 brand.jsp 页面进行数据的展示\n\n具体的代码如下：\n\n@webservlet("/selectallservlet")\npublic class selectallservlet extends httpservlet {\n    private  brandservice service = new brandservice();\n\n    @override\n    protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n\n        //1. 调用brandservice完成查询\n        list<brand> brands = service.selectall();\n        //2. 存入request域中\n        request.setattribute("brands",brands);\n        //3. 转发到brand.jsp\n        request.getrequestdispatcher("/brand.jsp").forward(request,response);\n    }\n\n    @override\n    protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        this.doget(request, response);\n    }\n}\n\n\n# 编写brand.jsp页面\n\n将资料 资料\\2. 品牌增删改查案例\\静态页面 下的 brand.html 页面拷贝到项目的 webapp 目录下，并将该页面改成 brand.jsp 页面，而 brand.jsp 页面在表格中使用 jstl 和 el表达式 从request域对象中获取名为 brands 的集合数据并展示出来。页面内容如下：\n\n<%@ page contenttype="text/html;charset=utf-8" language="java" %>\n<%@ taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core" %>\n\n<!doctype html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>title</title>\n</head>\n<body>\n<hr>\n<table border="1" cellspacing="0" width="80%">\n    <tr>\n        <th>序号</th>\n        <th>品牌名称</th>\n        <th>企业名称</th>\n        <th>排序</th>\n        <th>品牌介绍</th>\n        <th>状态</th>\n        <th>操作</th>\n    </tr>\n\n    <c:foreach items="${brands}" var="brand" varstatus="status">\n        <tr align="center">\n            <%--<td>${brand.id}</td>--%>\n            <td>${status.count}</td>\n            <td>${brand.brandname}</td>\n            <td>${brand.companyname}</td>\n            <td>${brand.ordered}</td>\n            <td>${brand.description}</td>\n            <c:if test="${brand.status ** 1}">\n                <td>启用</td>\n            </c:if>\n            <c:if test="${brand.status != 1}">\n                <td>禁用</td>\n            </c:if>\n            <td><a href="/brand-demo/selectbyidservlet?id=${brand.id}">修改</a> <a href="#">删除</a></td>\n        </tr>\n    </c:foreach>\n</table>\n</body>\n</html>\n\n\n# 测试\n\n启动服务器，并在浏览器输入 http://localhost:8080/brand-demo/index.html，看到如下 查询所有 的超链接，点击该链接就可以查询出所有的品牌数据\n\n\n\n为什么出现这个问题呢？是因为查询到的字段名和实体类对象的属性名没有一一对应。相比看到这大家一定会解决了，就是在映射配置文件中使用 resultmap 标签定义映射关系。映射配置文件内容如下：\n\n<?xml version="1.0" encoding="utf-8" ?>\n<!doctype mapper\n        public "-//mybatis.org//dtd mapper 3.0//en"\n        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">\n<mapper namespace="com.itheima.mapper.brandmapper">\n\n    <resultmap id="brandresultmap" type="brand">\n        <result column="brand_name" property="brandname"></result>\n        <result column="company_name" property="companyname"></result>\n    </resultmap>\n</mapper>\n\n\n并且在 brandmapper 接口中的 selectall() 上使用 @resulemap 注解指定使用该映射\n\n/**\n  * 查询所有\n  * @return\n  */\n@select("select * from tb_brand")\n@resultmap("brandresultmap")\nlist<brand> selectall();\n\n\n重启服务器，再次访问就能看到我们想要的数据了\n\n\n\n\n# 添加\n\n\n\n上图是做 添加 功能流程。点击 新增 按钮后，会先跳转到 addbrand.jsp 新增页面，在该页面输入要添加的数据，输入完毕后点击 提交 按钮，需要将数据提交到后端，而后端进行数据添加操作，并重新将所有的数据查询出来。整个流程如下：\n\n\n\n接下来我们根据流程来实现功能：\n\n# 编写brandmapper方法\n\n在 brandmapper 接口，在接口中定义 add(brand brand) 方法\n\n@insert("insert into tb_brand values(null,#{brandname},#{companyname},#{ordered},#{description},#{status})")\nvoid add(brand brand);\n\n\n# 编写brandservice方法\n\n在 brandservice 类中定义添加品牌数据方法 add(brand brand)\n\n \t/**\n     * 添加\n     * @param brand\n     */\n    public void add(brand brand){\n\n        //2. 获取sqlsession\n        sqlsession sqlsession = factory.opensession();\n        //3. 获取brandmapper\n        brandmapper mapper = sqlsession.getmapper(brandmapper.class);\n\n        //4. 调用方法\n        mapper.add(brand);\n\n        //提交事务\n        sqlsession.commit();\n        //释放资源\n        sqlsession.close();\n    }\n\n\n# 改进brand.jsp页面\n\n我们需要在该页面表格的上面添加 新增 按钮\n\n<input type="button" value="新增" id="add"><br>\n\n\n并给该按钮绑定单击事件，当点击了该按钮需要跳转到 brand.jsp 添加品牌数据的页面\n\n<script>\n    document.getelementbyid("add").onclick = function (){\n        location.href = "/brand-demo/addbrand.jsp";\n    }\n<\/script>\n\n\n> **注意：**该 script 标签建议放在 body 结束标签前面。\n\n# 编写addbrand.jsp页面\n\n从资料 资料\\2. 品牌增删改查案例\\静态页面 中将 addbrand.html 页面拷贝到项目的 webapp 下，并改成 addbrand.jsp 动态页面\n\n<%@ page contenttype="text/html;charset=utf-8" language="java" %>\n<!doctype html>\n<html lang="en">\n\n<head>\n    <meta charset="utf-8">\n    <title>添加品牌</title>\n</head>\n<body>\n<h3>添加品牌</h3>\n<form action="/brand-demo/addservlet" method="post">\n    品牌名称：<input name="brandname"><br>\n    企业名称：<input name="companyname"><br>\n    排序：<input name="ordered"><br>\n    描述信息：<textarea rows="5" cols="20" name="description"></textarea><br>\n    状态：\n    <input type="radio" name="status" value="0">禁用\n    <input type="radio" name="status" value="1">启用<br>\n\n    <input type="submit" value="提交">\n</form>\n</body>\n</html>\n\n\n# 编写servlet\n\n在 web 包下创建 addservlet 的 servlet，该 servlet 的逻辑如下:\n\n * 设置处理post请求乱码的字符集\n * 接收客户端提交的数据\n * 将接收到的数据封装到 brand 对象中\n * 调用 brandservice 的add() 方法进行添加的业务逻辑处理\n * 跳转到 selectallservlet 资源重新查询数据\n\n具体的代码如下：\n\n@webservlet("/addservlet")\npublic class addservlet extends httpservlet {\n    private brandservice service = new brandservice();\n\n\n    @override\n    protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n\n        //处理post请求的乱码问题\n        request.setcharacterencoding("utf-8");\n\n        //1. 接收表单提交的数据，封装为一个brand对象\n        string brandname = request.getparameter("brandname");\n        string companyname = request.getparameter("companyname");\n        string ordered = request.getparameter("ordered");\n        string description = request.getparameter("description");\n        string status = request.getparameter("status");\n\n        //封装为一个brand对象\n        brand brand = new brand();\n        brand.setbrandname(brandname);\n        brand.setcompanyname(companyname);\n        brand.setordered(integer.parseint(ordered));\n        brand.setdescription(description);\n        brand.setstatus(integer.parseint(status));\n\n        //2. 调用service 完成添加\n        service.add(brand);\n\n        //3. 转发到查询所有servlet\n        request.getrequestdispatcher("/selectallservlet").forward(request,response);\n    }\n\n    @override\n    protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        this.doget(request, response);\n    }\n}\n\n\n# 测试\n\n点击 brand.jsp 页面的 新增 按钮，会跳转到 addbrand.jsp页面\n\n点击 提交 按钮，就能看到如下页面，里面就包含我们刚添加的数据\n\n\n\n\n# 修改\n\n\n\n点击每条数据后面的 编辑 按钮会跳转到修改页面，如下图：\n\n\n\n在该修改页面我们可以看到将 编辑 按钮所在行的数据 回显 到表单，然后需要修改那个数据在表单中进行修改，然后点击 提交 的按钮将数据提交到后端，后端再将数据存储到数据库中。\n\n从上面的例子我们知道 修改 功能需要从两方面进行实现，数据回显和修改操作。\n\n# 回显数据\n\n\n\n上图就是回显数据的效果。要实现这个效果，那当点击 修改 按钮时不能直接跳转到 update.jsp 页面，而是需要先带着当前行数据的 id 请求后端程序，后端程序根据 id 查询数据，将数据存储到域对象中跳转到 update.jsp 页面进行数据展示。整体流程如下\n\n# 编写brandmapper方法\n\n在 brandmapper 接口，在接口中定义 selectbyid(int id) 方法\n\n\t/**\n     * 根据id查询\n     * @param id\n     * @return\n     */\n    @select("select * from tb_brand where id = #{id}")\n    @resultmap("brandresultmap")\n    brand selectbyid(int id);\n\n\n# 编写brandservice方法\n\n在 brandservice 类中定义根据id查询数据方法 selectbyid(int id)\n\n    /**\n     * 根据id查询\n     * @return\n     */\n    public brand selectbyid(int id){\n        //调用brandmapper.selectall()\n        //2. 获取sqlsession\n        sqlsession sqlsession = factory.opensession();\n        //3. 获取brandmapper\n        brandmapper mapper = sqlsession.getmapper(brandmapper.class);\n        //4. 调用方法\n        brand brand = mapper.selectbyid(id);\n        sqlsession.close();\n        return brand;\n    }\n\n\n# 编写servlet\n\n在 web 包下创建 selectbyidservlet 的 servlet，该 servlet 的逻辑如下:\n\n * 获取请求数据 id\n * 调用 brandservice 的 selectbyid() 方法进行数据查询的业务逻辑\n * 将查询到的数据存储到 request 域对象中\n * 跳转到 update.jsp 页面进行数据真实\n\n具体代码如下：\n\n@webservlet("/selectbyidservlet")\npublic class selectbyidservlet extends httpservlet {\n    private  brandservice service = new brandservice();\n\n    @override\n    protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        //1. 接收id\n        string id = request.getparameter("id");\n        //2. 调用service查询\n        brand brand = service.selectbyid(integer.parseint(id));\n        //3. 存储到request中\n        request.setattribute("brand",brand);\n        //4. 转发到update.jsp\n        request.getrequestdispatcher("/update.jsp").forward(request,response);\n    }\n\n    @override\n    protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        this.doget(request, response);\n    }\n}\n\n\n# 编写update.jsp页面\n\n拷贝 addbrand.jsp 页面，改名为 update.jsp 并做出以下修改：\n\n * title 标签内容改为 修改品牌\n\n * form 标签的 action 属性值改为 /brand-demo/updateservlet\n\n * input 标签要进行数据回显，需要设置 value 属性\n   \n   品牌名称：<input name="brandname" value="${brand.brandname}"><br>\n   企业名称：<input name="companyname" value="${brand.companyname}"><br>\n   排序：<input name="ordered" value="${brand.ordered}"><br>\n   \n\n * textarea 标签要进行数据回显，需要在标签体中使用 el表达式\n   \n   描述信息：<textarea rows="5" cols="20" name="description">${brand.description} </textarea><br>\n   \n\n * 单选框使用 if 标签需要判断 brand.status 的值是 1 还是 0 在指定的单选框上使用 checked 属性，表示被选中状态\n   \n   状态：\n   <c:if test="${brand.status ** 0}">\n       <input type="radio" name="status" value="0" checked>禁用\n       <input type="radio" name="status" value="1">启用<br>\n   </c:if>\n   \n   <c:if test="${brand.status ** 1}">\n       <input type="radio" name="status" value="0" >禁用\n       <input type="radio" name="status" value="1" checked>启用<br>\n   </c:if>\n   \n\n综上，update.jsp 代码如下：\n\n<%@ page contenttype="text/html;charset=utf-8" language="java" %>\n<%@ taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core" %>\n<!doctype html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>修改品牌</title>\n</head>\n<body>\n<h3>修改品牌</h3>\n<form action="/brand-demo/updateservlet" method="post">\n\n    品牌名称：<input name="brandname" value="${brand.brandname}"><br>\n    企业名称：<input name="companyname" value="${brand.companyname}"><br>\n    排序：<input name="ordered" value="${brand.ordered}"><br>\n    描述信息：<textarea rows="5" cols="20" name="description">${brand.description} </textarea><br>\n    状态：\n    <c:if test="${brand.status ** 0}">\n        <input type="radio" name="status" value="0" checked>禁用\n        <input type="radio" name="status" value="1">启用<br>\n    </c:if>\n\n    <c:if test="${brand.status ** 1}">\n        <input type="radio" name="status" value="0" >禁用\n        <input type="radio" name="status" value="1" checked>启用<br>\n    </c:if>\n\n    <input type="submit" value="提交">\n</form>\n</body>\n</html>\n\n\n# 修改数据\n\n做完回显数据后，接下来我们要做修改数据了，而下图是修改数据的效果：\n\n\n\n在修改页面进行数据修改，点击 提交 按钮，会将数据提交到后端程序，后端程序会对表中的数据进行修改操作，然后重新进行数据的查询操作。整体流程如下：\n\n# 编写brandmapper方法\n\n在 brandmapper 接口，在接口中定义 update(brand brand) 方法\n\n/**\n  * 修改\n  * @param brand\n  */\n@update("update tb_brand set brand_name = #{brandname},company_name = #{companyname},ordered = #{ordered},description = #{description},status = #{status} where id = #{id}")\nvoid update(brand brand);\n\n\n# 编写brandservice方法\n\n在 brandservice 类中定义根据id查询数据方法 update(brand brand)\n\n\t/**\n     * 修改\n     * @param brand\n     */\n    public void update(brand brand){\n        //2. 获取sqlsession\n        sqlsession sqlsession = factory.opensession();\n        //3. 获取brandmapper\n        brandmapper mapper = sqlsession.getmapper(brandmapper.class);\n        //4. 调用方法\n        mapper.update(brand);\n        //提交事务\n        sqlsession.commit();\n        //释放资源\n        sqlsession.close();\n    }\n\n\n# 编写servlet\n\n在 web 包下创建 addservlet 的 servlet，该 servlet 的逻辑如下:\n\n * 设置处理post请求乱码的字符集\n * 接收客户端提交的数据\n * 将接收到的数据封装到 brand 对象中\n * 调用 brandservice 的update() 方法进行添加的业务逻辑处理\n * 跳转到 selectallservlet 资源重新查询数据\n\n具体的代码如下：\n\n@webservlet("/updateservlet")\npublic class updateservlet extends httpservlet {\n    private brandservice service = new brandservice();\n\n    @override\n    protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n\n        //处理post请求的乱码问题\n        request.setcharacterencoding("utf-8");\n        //1. 接收表单提交的数据，封装为一个brand对象\n        string id = request.getparameter("id");\n        string brandname = request.getparameter("brandname");\n        string companyname = request.getparameter("companyname");\n        string ordered = request.getparameter("ordered");\n        string description = request.getparameter("description");\n        string status = request.getparameter("status");\n\n        //封装为一个brand对象\n        brand brand = new brand();\n        brand.setid(integer.parseint(id));\n        brand.setbrandname(brandname);\n        brand.setcompanyname(companyname);\n        brand.setordered(integer.parseint(ordered));\n        brand.setdescription(description);\n        brand.setstatus(integer.parseint(status));\n\n        //2. 调用service 完成修改\n        service.update(brand);\n\n        //3. 转发到查询所有servlet\n        request.getrequestdispatcher("/selectallservlet").forward(request,response);\n    }\n\n    @override\n    protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        this.doget(request, response);\n    }\n}\n\n\n存在问题：update.jsp 页面提交数据时是没有携带主键数据的，而后台修改数据需要根据主键进行修改。\n\n针对这个问题，我们不希望页面将主键id展示给用户看，但是又希望在提交数据时能将主键id提交到后端。此时我们就想到了在学习 html 时学习的隐藏域，在 update.jsp 页面的表单中添加如下代码：\n\n<%--隐藏域，提交id--%>\n<input type="hidden" name="id" value="${brand.id}">\n\n\nupdate.jsp 页面的最终代码如下：\n\n<%@ page contenttype="text/html;charset=utf-8" language="java" %>\n<%@ taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core" %>\n<!doctype html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>修改品牌</title>\n</head>\n<body>\n<h3>修改品牌</h3>\n<form action="/brand-demo/updateservlet" method="post">\n\n    <%--隐藏域，提交id--%>\n    <input type="hidden" name="id" value="${brand.id}">\n\n    品牌名称：<input name="brandname" value="${brand.brandname}"><br>\n    企业名称：<input name="companyname" value="${brand.companyname}"><br>\n    排序：<input name="ordered" value="${brand.ordered}"><br>\n    描述信息：<textarea rows="5" cols="20" name="description">${brand.description} </textarea><br>\n    状态：\n    <c:if test="${brand.status ** 0}">\n        <input type="radio" name="status" value="0" checked>禁用\n        <input type="radio" name="status" value="1">启用<br>\n    </c:if>\n\n    <c:if test="${brand.status ** 1}">\n        <input type="radio" name="status" value="0" >禁用\n        <input type="radio" name="status" value="1" checked>启用<br>\n    </c:if>\n    <input type="submit" value="提交">\n</form>\n</body>\n</html>\n',charsets:{cjk:!0}},{title:"Element",frontmatter:{autoSort:93,title:"Element",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/f1ca40/",categories:["后端","JavaWeb"],tags:["知识","JavaWeb"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/20.JavaWeb/45.Element.html",relativePath:"01.后端/20.JavaWeb/45.Element.md",key:"v-80ad8e7a",path:"/pages/f1ca40/",headers:[{level:2,title:"快速入门",slug:"快速入门",normalizedTitle:"快速入门",charIndex:284},{level:2,title:"Element 布局",slug:"element-布局",normalizedTitle:"element 布局",charIndex:2681},{level:3,title:"Layout 局部",slug:"layout-局部",normalizedTitle:"layout 局部",charIndex:2752},{level:3,title:"Container 布局容器",slug:"container-布局容器",normalizedTitle:"container 布局容器",charIndex:2733},{level:2,title:"案例",slug:"案例",normalizedTitle:"案例",charIndex:10372},{level:3,title:"准备基本页面",slug:"准备基本页面",normalizedTitle:"准备基本页面",charIndex:10511},{level:3,title:"完成表格展示",slug:"完成表格展示",normalizedTitle:"完成表格展示",charIndex:10882},{level:3,title:"完成搜索表单展示",slug:"完成搜索表单展示",normalizedTitle:"完成搜索表单展示",charIndex:11698},{level:3,title:"完成批量删除和新增按钮展示",slug:"完成批量删除和新增按钮展示",normalizedTitle:"完成批量删除和新增按钮展示",charIndex:11809},{level:3,title:"完成对话框展示",slug:"完成对话框展示",normalizedTitle:"完成对话框展示",charIndex:11867},{level:3,title:"完成分页条展示",slug:"完成分页条展示",normalizedTitle:"完成分页条展示",charIndex:11946},{level:3,title:"完整页面代码",slug:"完整页面代码",normalizedTitle:"完整页面代码",charIndex:12400}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"快速入门 Element 布局 Layout 局部 Container 布局容器 案例 准备基本页面 完成表格展示 完成搜索表单展示 完成批量删除和新增按钮展示 完成对话框展示 完成分页条展示 完整页面代码",content:'# Element\n\nElement：是饿了么公司前端开发团队提供的一套基于 Vue 的网站组件库，用于快速构建网页。\n\nElement 提供了很多组件（组成网页的部件）供我们使用。例如 超链接、按钮、图片、表格等等~\n\n如下图左边的是我们编写页面看到的按钮，上图右边的是 Element 提供的页面效果，效果一目了然。\n\n我们学习 Element 其实就是学习怎么从官网拷贝组件到我们自己的页面并进行修改，官网网址是\n\nhttps://element.eleme.cn/#/zh-CN\n\n\n进入官网能看到如下页面\n\n\n\n接下来直接点击 组件 ，页面如下\n\n\n\n\n# 快速入门\n\n 1. 将资源 04-资料\\02-element 下的 element-ui 文件夹直接拷贝到项目的 webapp 下。目录结构如下\n\n 2. 创建页面，并在页面引入Element 的css、js文件 和 Vue.js\n    \n    <script src="vue.js"><\/script>\n    <script src="element-ui/lib/index.js"><\/script>\n    <link rel="stylesheet" href="element-ui/lib/theme-chalk/index.css">\n    \n\n 3. .创建Vue核心对象\n    \n    Element 是基于 Vue 的，所以使用Element时必须要创建 Vue 对象\n    \n    <script>\n        new Vue({\n            el:"#app"\n        })\n    <\/script>\n    \n\n 4. 官网复制Element组件代码\n    \n    在左菜单栏找到 Button 按钮 ，然后找到自己喜欢的按钮样式，点击 显示代码 ，在下面就会展示出对应的代码，将这些代码拷贝到我们自己的页面即可。\n\n整体页面代码如下：\n\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <title>Title</title>\n</head>\n<body>\n<div id="app">\n\n\n    <el-row>\n     \t<el-button>默认按钮</el-button>\n        <el-button type="primary">主要按钮</el-button>\n        <el-button type="success">成功按钮</el-button>\n        <el-button type="info">信息按钮</el-button>\n        <el-button type="warning">警告按钮</el-button>\n        <el-button type="danger">删除</el-button>\n    </el-row>\n    <el-row>\n        <el-button plain>朴素按钮</el-button>\n        <el-button type="primary" plain>主要按钮</el-button>\n        <el-button type="success" plain>成功按钮</el-button>\n        <el-button type="info" plain>信息按钮</el-button>\n        <el-button type="warning" plain>警告按钮</el-button>\n        <el-button type="danger" plain>危险按钮</el-button>\n    </el-row>\n\n    <el-row>\n        <el-button round>圆角按钮</el-button>\n        <el-button type="primary" round>主要按钮</el-button>\n        <el-button type="success" round>成功按钮</el-button>\n        <el-button type="info" round>信息按钮</el-button>\n        <el-button type="warning" round>警告按钮</el-button>\n        <el-button type="danger" round>危险按钮</el-button>\n    </el-row>\n\n    <el-row>\n        <el-button icon="el-icon-search" circle></el-button>\n        <el-button type="primary" icon="el-icon-edit" circle></el-button>\n        <el-button type="success" icon="el-icon-check" circle></el-button>\n        <el-button type="info" icon="el-icon-message" circle></el-button>\n        <el-button type="warning" icon="el-icon-star-off" circle></el-button>\n        <el-button type="danger" icon="el-icon-delete" circle></el-button>\n    </el-row>\n</div>\n\n<script src="js/vue.js"><\/script>\n<script src="element-ui/lib/index.js"><\/script>\n<link rel="stylesheet" href="element-ui/lib/theme-chalk/index.css">\n\n<script>\n    new Vue({\n        el:"#app"\n    })\n<\/script>\n\n</body>\n</html>\n\n\n\n# Element 布局\n\nElement 提供了两种布局方式，分别是：\n\n * Layout 布局\n * Container 布局容器\n\n\n# Layout 局部\n\n通过基础的 24 分栏，迅速简便地创建布局。也就是默认将一行分为 24 栏，根据页面要求给每一列设置所占的栏数。\n\n在左菜单栏找到 Layout 布局 ，然后找到自己喜欢的按钮样式，点击 显示代码 ，在下面就会展示出对应的代码，显示出的代码中有样式，有html标签。将样式拷贝我们自己页面的 head 标签内，将html标签拷贝到 <div id="app"></div> 标签内。\n\n整体页面代码如下：\n\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <title>Title</title>\n\n    <style>\n        .el-row {\n            margin-bottom: 20px;\n        }\n        .el-col {\n            border-radius: 4px;\n        }\n        .bg-purple-dark {\n            background: #99a9bf;\n        }\n        .bg-purple {\n            background: #d3dce6;\n        }\n        .bg-purple-light {\n            background: #e5e9f2;\n        }\n        .grid-content {\n            border-radius: 4px;\n            min-height: 36px;\n        }\n        .row-bg {\n            padding: 10px 0;\n            background-color: #f9fafc;\n        }\n    </style>\n</head>\n<body>\n<div id="app">\n    <el-row>\n        <el-col :span="24"><div class="grid-content bg-purple-dark"></div></el-col>\n    </el-row>\n    <el-row>\n        <el-col :span="12"><div class="grid-content bg-purple"></div></el-col>\n        <el-col :span="12"><div class="grid-content bg-purple-light"></div></el-col>\n    </el-row>\n    <el-row>\n        <el-col :span="8"><div class="grid-content bg-purple"></div></el-col>\n        <el-col :span="8"><div class="grid-content bg-purple-light"></div></el-col>\n        <el-col :span="8"><div class="grid-content bg-purple"></div></el-col>\n    </el-row>\n    <el-row>\n        <el-col :span="6"><div class="grid-content bg-purple"></div></el-col>\n        <el-col :span="6"><div class="grid-content bg-purple-light"></div></el-col>\n        <el-col :span="6"><div class="grid-content bg-purple"></div></el-col>\n        <el-col :span="6"><div class="grid-content bg-purple-light"></div></el-col>\n    </el-row>\n    <el-row>\n        <el-col :span="4"><div class="grid-content bg-purple"></div></el-col>\n        <el-col :span="4"><div class="grid-content bg-purple-light"></div></el-col>\n        <el-col :span="4"><div class="grid-content bg-purple"></div></el-col>\n        <el-col :span="4"><div class="grid-content bg-purple-light"></div></el-col>\n        <el-col :span="4"><div class="grid-content bg-purple"></div></el-col>\n        <el-col :span="4"><div class="grid-content bg-purple-light"></div></el-col>\n    </el-row>\n</div>\n<script src="js/vue.js"><\/script>\n<script src="element-ui/lib/index.js"><\/script>\n<link rel="stylesheet" href="element-ui/lib/theme-chalk/index.css">\n\n<script>\n    new Vue({\n        el:"#app"\n    })\n<\/script>\n</body>\n</html>\n\n\n现在需要添加一行，要求该行显示8个格子，通过计算每个格子占 3 栏，具体的html 代码如下\n\n\x3c!--\n添加一行，8个格子  24/8 = 3\n--\x3e\n<el-row>\n    <el-col :span="3"><div class="grid-content bg-purple"></div></el-col>\n    <el-col :span="3"><div class="grid-content bg-purple-light"></div></el-col>\n    <el-col :span="3"><div class="grid-content bg-purple"></div></el-col>\n    <el-col :span="3"><div class="grid-content bg-purple-light"></div></el-col>\n    <el-col :span="3"><div class="grid-content bg-purple"></div></el-col>\n    <el-col :span="3"><div class="grid-content bg-purple-light"></div></el-col>\n    <el-col :span="3"><div class="grid-content bg-purple"></div></el-col>\n    <el-col :span="3"><div class="grid-content bg-purple-light"></div></el-col>\n</el-row>\n\n\n\n# Container 布局容器\n\n用于布局的容器组件，方便快速搭建页面的基本结构。如下图就是布局容器效果。\n\n如下图是官网提供的 Container 布局容器实例：\n\n该效果代码中包含了样式、页面标签、模型数据。将里面的样式 <style> 拷贝到我们自己页面的 head 标签中；将html标签拷贝到 <div id="app"></div> 标签中，再将数据模型拷贝到 vue 对象的 data() 中。\n\n整体页面代码如下：\n\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <title>Title</title>\n\n    <style>\n        .el-header {\n            background-color: #B3C0D1;\n            color: #333;\n            line-height: 60px;\n        }\n\n        .el-aside {\n            color: #333;\n        }\n    </style>\n</head>\n<body>\n<div id="app">\n    <el-container style="height: 500px; border: 1px solid #eee">\n        <el-aside width="200px" style="background-color: rgb(238, 241, 246)">\n            <el-menu :default-openeds="[\'1\', \'3\']">\n                <el-submenu index="1">\n                    <template slot="title"><i class="el-icon-message"></i>导航一</template>\n                    <el-menu-item-group>\n                        <template slot="title">分组一</template>\n                        <el-menu-item index="1-1">选项1</el-menu-item>\n                        <el-menu-item index="1-2">选项2</el-menu-item>\n                    </el-menu-item-group>\n                    <el-menu-item-group title="分组2">\n                        <el-menu-item index="1-3">选项3</el-menu-item>\n                    </el-menu-item-group>\n                    <el-submenu index="1-4">\n                        <template slot="title">选项4</template>\n                        <el-menu-item index="1-4-1">选项4-1</el-menu-item>\n                    </el-submenu>\n                </el-submenu>\n                <el-submenu index="2">\n                    <template slot="title"><i class="el-icon-menu"></i>导航二</template>\n                    <el-submenu index="2-1">\n                        <template slot="title">选项1</template>\n                        <el-menu-item index="2-1-1">选项1-1</el-menu-item>\n                    </el-submenu>\n                </el-submenu>\n                <el-submenu index="3">\n                    <template slot="title"><i class="el-icon-setting"></i>导航三</template>\n                    <el-menu-item-group>\n                        <template slot="title">分组一</template>\n                        <el-menu-item index="3-1">选项1</el-menu-item>\n                        <el-menu-item index="3-2">选项2</el-menu-item>\n                    </el-menu-item-group>\n                    <el-menu-item-group title="分组2">\n                        <el-menu-item index="3-3">选项3</el-menu-item>\n                    </el-menu-item-group>\n                    <el-submenu index="3-4">\n                        <template slot="title">选项4</template>\n                        <el-menu-item index="3-4-1">选项4-1</el-menu-item>\n                    </el-submenu>\n                </el-submenu>\n            </el-menu>\n        </el-aside>\n\n        <el-container>\n            <el-header style="text-align: right; font-size: 12px">\n                <el-dropdown>\n                    <i class="el-icon-setting" style="margin-right: 15px"></i>\n                    <el-dropdown-menu slot="dropdown">\n                        <el-dropdown-item>查看</el-dropdown-item>\n                        <el-dropdown-item>新增</el-dropdown-item>\n                        <el-dropdown-item>删除</el-dropdown-item>\n                    </el-dropdown-menu>\n                </el-dropdown>\n                <span>王小虎</span>\n            </el-header>\n\n            <el-main>\n                <el-table :data="tableData">\n                    <el-table-column prop="date" label="日期" width="140">\n                    </el-table-column>\n                    <el-table-column prop="name" label="姓名" width="120">\n                    </el-table-column>\n                    <el-table-column prop="address" label="地址">\n                    </el-table-column>\n                </el-table>\n            </el-main>\n        </el-container>\n    </el-container>\n</div>\n<script src="js/vue.js"><\/script>\n<script src="element-ui/lib/index.js"><\/script>\n<link rel="stylesheet" href="element-ui/lib/theme-chalk/index.css">\n\n<script>\n    new Vue({\n        el:"#app",\n        data() {\n            const item = {\n                date: \'2016-05-02\',\n                name: \'王小虎\',\n                address: \'上海市普陀区金沙江路 1518 弄\'\n            };\n            return {\n                tableData: Array(20).fill(item)\n            }\n        }\n    })\n<\/script>\n</body>\n</html>\n\n\n\n# 案例\n\n其他的组件我们通过完成一个页面来学习。\n\n我们要完成如下页面效果\n\n要完成该页面，我们需要先对这个页面进行分析，看页面由哪儿几部分组成，然后到官网进行拷贝并修改。页面总共有如下组成部分\n\n\n\n还有一个是当我们点击 新增 按钮，会在页面正中间弹出一个对话框，如下\n\n\n# 准备基本页面\n\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <title>Title</title>\n</head>\n<body>\n<div id="app">\n\t\n</div>\n\n<script src="js/vue.js"><\/script>\n<script src="element-ui/lib/index.js"><\/script>\n<link rel="stylesheet" href="element-ui/lib/theme-chalk/index.css">\n\n<script>\n    new Vue({\n        el: "#app"\n    })\n<\/script>\n</body>\n</html>\n\n\n\n# 完成表格展示\n\n使用 Element 整体的思路就是 拷贝 + 修改。\n\n# 拷贝\n\n\n\n在左菜单栏找到 Table 表格并点击，右边主体就会定位到表格这一块，找到我们需要的表格效果（如上图），点击 显示代码 就可以看到这个表格的代码了。\n\n将html标签拷贝到 <div id="app"></div> 中，如下：\n\n将css样式拷贝到我们页面的 head 标签中，如下\n\n将方法和模型数据拷贝到 Vue 对象指定的位置\n\n拷贝完成后通过浏览器打开可以看到表格的效果\n\n\n\n表格效果出来了，但是显示的表头和数据并不是我们想要的，所以接下来就需要对页面代码进行修改了。\n\n# 修改\n\n 1. 修改表头和数据\n    \n    下面是对表格代码进行分析的图解。根据下图说明修改自己的列数和列名\n    \n    修改完页面后，还需要对绑定的模型数据进行修改，下图是对模型数据进行分析的图解\n\n 2. 给表格添加操作列\n    \n    从之前的表格拷贝一列出来并对其进行修改。按钮是从官网的 Button 按钮 组件中拷贝并修改的\n\n 3. 给表格添加复选框列和标号列\n    \n    给表格添加复选框和标号列，效果如下\n    \n    \n    \n    此效果也是从 Element 官网进行拷贝，先找到对应的表格效果，然后将其对应代码拷贝到我们的代码中，如下是复选框列官网效果图和代码\n    \n    \n    \n    这里需要注意在 <el-table> 标签上有一个事件 @selection-change="handleSelectionChange" ，这里绑定的函数也需要从官网拷贝到我们自己的页面代码中，函数代码如下：\n    \n    \n    \n    从该函数中又发现还需要一个模型数据 multipleSelection ，所以还需要定义出该模型数据\n\n标号列也用同样的方式进行拷贝并修改。\n\n\n# 完成搜索表单展示\n\n在 Element 官网找到横排的表单效果，然后拷贝代码并进行修改\n\n\n\n点击上面的 显示代码 后，就会展示出对应的代码，下面是对这部分代码进行分析的图解\n\n\n\n然后根据我们要的效果修改代码。\n\n\n# 完成批量删除和新增按钮展示\n\n从 Element 官网找具有着色效果的按钮，并将代码拷贝到我们自己的页面上\n\n\n# 完成对话框展示\n\n在 Element 官网找对话框，如下：\n\n下面对官网提供的代码进行分析\n\n上图分析出来的模型数据需要在 Vue 对象中进行定义。\n\n\n# 完成分页条展示\n\n在 Element 官网找到 Pagination 分页 ，在页面主体部分找到我们需要的效果，如下\n\n点击 显示代码 ，找到 完整功能 对应的代码，接下来对该代码进行分析\n\n上面代码属性说明：\n\n * page-size ：每页显示的条目数\n\n * page-sizes ： 每页显示个数选择器的选项设置。\n   \n   :page-sizes="[100,200,300,400]" 对应的页面效果如下：\n\n * currentPage ：当前页码。我们点击那个页码，此属性值就是几。\n\n * total ：总记录数。用来设置总的数据条目数，该属性设置后， Element 会自动计算出需分多少页并给我们展示对应的页码。\n\n事件说明：\n\n * size-change ：pageSize 改变时会触发。也就是当我们改变了每页显示的条目数后，该事件会触发。\n * current-change ：currentPage 改变时会触发。也就是当我们点击了其他的页码后，该事件会触发。\n\n\n# 完整页面代码\n\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <title>Title</title>\n    <style>\n        .el-table .warning-row {\n            background: oldlace;\n        }\n        .el-table .success-row {\n            background: #f0f9eb;\n        }\n    </style>\n</head>\n<body>\n<div id="app">\n    \x3c!--搜索表单--\x3e\n    <el-form :inline="true" :model="brand" class="demo-form-inline">\n        <el-form-item label="当前状态">\n            <el-select v-model="brand.status" placeholder="当前状态">\n                <el-option label="启用" value="1"></el-option>\n                <el-option label="禁用" value="0"></el-option>\n            </el-select>\n        </el-form-item>\n\n        <el-form-item label="企业名称">\n            <el-input v-model="brand.companyName" placeholder="企业名称"></el-input>\n        </el-form-item>\n\n        <el-form-item label="品牌名称">\n            <el-input v-model="brand.brandName" placeholder="品牌名称"></el-input>\n        </el-form-item>\n\n        <el-form-item>\n            <el-button type="primary" @click="onSubmit">查询</el-button>\n        </el-form-item>\n    </el-form>\n\n    \x3c!--按钮--\x3e\n    <el-row>\n        <el-button type="danger" plain>批量删除</el-button>\n        <el-button type="primary" plain @click="dialogVisible = true">新增</el-button>\n    </el-row>\n    \n    \x3c!--添加数据对话框表单--\x3e\n    <el-dialog\n            title="编辑品牌"\n            :visible.sync="dialogVisible"\n            width="30%">\n        <el-form ref="form" :model="brand" label-width="80px">\n            <el-form-item label="品牌名称">\n                <el-input v-model="brand.brandName"></el-input>\n            </el-form-item>\n\n            <el-form-item label="企业名称">\n                <el-input v-model="brand.companyName"></el-input>\n            </el-form-item>\n\n            <el-form-item label="排序">\n                <el-input v-model="brand.ordered"></el-input>\n            </el-form-item>\n\n            <el-form-item label="备注">\n                <el-input type="textarea" v-model="brand.description"></el-input>\n            </el-form-item>\n\n            <el-form-item label="状态">\n                <el-switch v-model="brand.status"\n                           active-value="1"\n                           inactive-value="0"\n                ></el-switch>\n            </el-form-item>\n            <el-form-item>\n                <el-button type="primary" @click="addBrand">提交</el-button>\n                <el-button @click="dialogVisible = false">取消</el-button>\n            </el-form-item>\n        </el-form>\n    </el-dialog>\n\n    \x3c!--表格--\x3e\n    <template>\n        <el-table\n                :data="tableData"\n                style="width: 100%"\n                :row-class-name="tableRowClassName"\n                @selection-change="handleSelectionChange">\n            <el-table-column\n                    type="selection"\n                    width="55">\n            </el-table-column>\n            <el-table-column\n                    type="index"\n                    width="50">\n            </el-table-column>\n            <el-table-column\n                    prop="brandName"\n                    label="品牌名称"\n                    align="center">\n            </el-table-column>\n            <el-table-column\n                    prop="companyName"\n                    label="企业名称"\n                    align="center">\n            </el-table-column>\n            <el-table-column\n                    prop="ordered"\n                    align="center"\n                    label="排序">\n            </el-table-column>\n            <el-table-column\n                    prop="status"\n                    align="center"\n                    label="当前状态">\n            </el-table-column>\n            <el-table-column\n                    align="center"\n                    label="操作">\n                <el-row>\n                    <el-button type="primary">修改</el-button>\n                    <el-button type="danger">删除</el-button>\n                </el-row>\n            </el-table-column>\n\n        </el-table>\n    </template>\n\n    \x3c!--分页工具条--\x3e\n    <el-pagination\n            @size-change="handleSizeChange"\n            @current-change="handleCurrentChange"\n            :current-page="currentPage"\n            :page-sizes="[5, 10, 15, 20]"\n            :page-size="5"\n            layout="total, sizes, prev, pager, next, jumper"\n            :total="400">\n    </el-pagination>\n\n</div>\n<script src="js/vue.js"><\/script>\n<script src="element-ui/lib/index.js"><\/script>\n<link rel="stylesheet" href="element-ui/lib/theme-chalk/index.css">\n<script>\n    new Vue({\n        el: "#app",\n        methods: {\n            tableRowClassName({row, rowIndex}) {\n                if (rowIndex **= 1) {\n                    return \'warning-row\';\n                } else if (rowIndex **= 3) {\n                    return \'success-row\';\n                }\n                return \'\';\n            },\n            // 复选框选中后执行的方法\n            handleSelectionChange(val) {\n                this.multipleSelection = val;\n\n                console.log(this.multipleSelection)\n            },\n            // 查询方法\n            onSubmit() {\n                console.log(this.brand);\n            },\n            // 添加数据\n            addBrand(){\n                console.log(this.brand);\n            },\n            //分页\n            handleSizeChange(val) {\n                console.log(`每页 ${val} 条`);\n            },\n            handleCurrentChange(val) {\n                console.log(`当前页: ${val}`);\n            }\n        },\n        data() {\n            return {\n                // 当前页码\n                currentPage: 4,\n                // 添加数据对话框是否展示的标记\n                dialogVisible: false,\n\n                // 品牌模型数据\n                brand: {\n                    status: \'\',\n                    brandName: \'\',\n                    companyName: \'\',\n                    id:"",\n                    ordered:"",\n                    description:""\n                },\n                // 复选框选中数据集合\n                multipleSelection: [],\n                // 表格数据\n                tableData: [{\n                    brandName: \'华为\',\n                    companyName: \'华为科技有限公司\',\n                    ordered: \'100\',\n                    status: "1"\n                }, {\n                    brandName: \'华为\',\n                    companyName: \'华为科技有限公司\',\n                    ordered: \'100\',\n                    status: "1"\n                }, {\n                    brandName: \'华为\',\n                    companyName: \'华为科技有限公司\',\n                    ordered: \'100\',\n                    status: "1"\n                }, {\n                    brandName: \'华为\',\n                    companyName: \'华为科技有限公司\',\n                    ordered: \'100\',\n                    status: "1"\n                }]\n            }\n        }\n    })\n<\/script>\n</body>\n</html>\n',normalizedContent:'# element\n\nelement：是饿了么公司前端开发团队提供的一套基于 vue 的网站组件库，用于快速构建网页。\n\nelement 提供了很多组件（组成网页的部件）供我们使用。例如 超链接、按钮、图片、表格等等~\n\n如下图左边的是我们编写页面看到的按钮，上图右边的是 element 提供的页面效果，效果一目了然。\n\n我们学习 element 其实就是学习怎么从官网拷贝组件到我们自己的页面并进行修改，官网网址是\n\nhttps://element.eleme.cn/#/zh-cn\n\n\n进入官网能看到如下页面\n\n\n\n接下来直接点击 组件 ，页面如下\n\n\n\n\n# 快速入门\n\n 1. 将资源 04-资料\\02-element 下的 element-ui 文件夹直接拷贝到项目的 webapp 下。目录结构如下\n\n 2. 创建页面，并在页面引入element 的css、js文件 和 vue.js\n    \n    <script src="vue.js"><\/script>\n    <script src="element-ui/lib/index.js"><\/script>\n    <link rel="stylesheet" href="element-ui/lib/theme-chalk/index.css">\n    \n\n 3. .创建vue核心对象\n    \n    element 是基于 vue 的，所以使用element时必须要创建 vue 对象\n    \n    <script>\n        new vue({\n            el:"#app"\n        })\n    <\/script>\n    \n\n 4. 官网复制element组件代码\n    \n    在左菜单栏找到 button 按钮 ，然后找到自己喜欢的按钮样式，点击 显示代码 ，在下面就会展示出对应的代码，将这些代码拷贝到我们自己的页面即可。\n\n整体页面代码如下：\n\n<!doctype html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>title</title>\n</head>\n<body>\n<div id="app">\n\n\n    <el-row>\n     \t<el-button>默认按钮</el-button>\n        <el-button type="primary">主要按钮</el-button>\n        <el-button type="success">成功按钮</el-button>\n        <el-button type="info">信息按钮</el-button>\n        <el-button type="warning">警告按钮</el-button>\n        <el-button type="danger">删除</el-button>\n    </el-row>\n    <el-row>\n        <el-button plain>朴素按钮</el-button>\n        <el-button type="primary" plain>主要按钮</el-button>\n        <el-button type="success" plain>成功按钮</el-button>\n        <el-button type="info" plain>信息按钮</el-button>\n        <el-button type="warning" plain>警告按钮</el-button>\n        <el-button type="danger" plain>危险按钮</el-button>\n    </el-row>\n\n    <el-row>\n        <el-button round>圆角按钮</el-button>\n        <el-button type="primary" round>主要按钮</el-button>\n        <el-button type="success" round>成功按钮</el-button>\n        <el-button type="info" round>信息按钮</el-button>\n        <el-button type="warning" round>警告按钮</el-button>\n        <el-button type="danger" round>危险按钮</el-button>\n    </el-row>\n\n    <el-row>\n        <el-button icon="el-icon-search" circle></el-button>\n        <el-button type="primary" icon="el-icon-edit" circle></el-button>\n        <el-button type="success" icon="el-icon-check" circle></el-button>\n        <el-button type="info" icon="el-icon-message" circle></el-button>\n        <el-button type="warning" icon="el-icon-star-off" circle></el-button>\n        <el-button type="danger" icon="el-icon-delete" circle></el-button>\n    </el-row>\n</div>\n\n<script src="js/vue.js"><\/script>\n<script src="element-ui/lib/index.js"><\/script>\n<link rel="stylesheet" href="element-ui/lib/theme-chalk/index.css">\n\n<script>\n    new vue({\n        el:"#app"\n    })\n<\/script>\n\n</body>\n</html>\n\n\n\n# element 布局\n\nelement 提供了两种布局方式，分别是：\n\n * layout 布局\n * container 布局容器\n\n\n# layout 局部\n\n通过基础的 24 分栏，迅速简便地创建布局。也就是默认将一行分为 24 栏，根据页面要求给每一列设置所占的栏数。\n\n在左菜单栏找到 layout 布局 ，然后找到自己喜欢的按钮样式，点击 显示代码 ，在下面就会展示出对应的代码，显示出的代码中有样式，有html标签。将样式拷贝我们自己页面的 head 标签内，将html标签拷贝到 <div id="app"></div> 标签内。\n\n整体页面代码如下：\n\n<!doctype html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>title</title>\n\n    <style>\n        .el-row {\n            margin-bottom: 20px;\n        }\n        .el-col {\n            border-radius: 4px;\n        }\n        .bg-purple-dark {\n            background: #99a9bf;\n        }\n        .bg-purple {\n            background: #d3dce6;\n        }\n        .bg-purple-light {\n            background: #e5e9f2;\n        }\n        .grid-content {\n            border-radius: 4px;\n            min-height: 36px;\n        }\n        .row-bg {\n            padding: 10px 0;\n            background-color: #f9fafc;\n        }\n    </style>\n</head>\n<body>\n<div id="app">\n    <el-row>\n        <el-col :span="24"><div class="grid-content bg-purple-dark"></div></el-col>\n    </el-row>\n    <el-row>\n        <el-col :span="12"><div class="grid-content bg-purple"></div></el-col>\n        <el-col :span="12"><div class="grid-content bg-purple-light"></div></el-col>\n    </el-row>\n    <el-row>\n        <el-col :span="8"><div class="grid-content bg-purple"></div></el-col>\n        <el-col :span="8"><div class="grid-content bg-purple-light"></div></el-col>\n        <el-col :span="8"><div class="grid-content bg-purple"></div></el-col>\n    </el-row>\n    <el-row>\n        <el-col :span="6"><div class="grid-content bg-purple"></div></el-col>\n        <el-col :span="6"><div class="grid-content bg-purple-light"></div></el-col>\n        <el-col :span="6"><div class="grid-content bg-purple"></div></el-col>\n        <el-col :span="6"><div class="grid-content bg-purple-light"></div></el-col>\n    </el-row>\n    <el-row>\n        <el-col :span="4"><div class="grid-content bg-purple"></div></el-col>\n        <el-col :span="4"><div class="grid-content bg-purple-light"></div></el-col>\n        <el-col :span="4"><div class="grid-content bg-purple"></div></el-col>\n        <el-col :span="4"><div class="grid-content bg-purple-light"></div></el-col>\n        <el-col :span="4"><div class="grid-content bg-purple"></div></el-col>\n        <el-col :span="4"><div class="grid-content bg-purple-light"></div></el-col>\n    </el-row>\n</div>\n<script src="js/vue.js"><\/script>\n<script src="element-ui/lib/index.js"><\/script>\n<link rel="stylesheet" href="element-ui/lib/theme-chalk/index.css">\n\n<script>\n    new vue({\n        el:"#app"\n    })\n<\/script>\n</body>\n</html>\n\n\n现在需要添加一行，要求该行显示8个格子，通过计算每个格子占 3 栏，具体的html 代码如下\n\n\x3c!--\n添加一行，8个格子  24/8 = 3\n--\x3e\n<el-row>\n    <el-col :span="3"><div class="grid-content bg-purple"></div></el-col>\n    <el-col :span="3"><div class="grid-content bg-purple-light"></div></el-col>\n    <el-col :span="3"><div class="grid-content bg-purple"></div></el-col>\n    <el-col :span="3"><div class="grid-content bg-purple-light"></div></el-col>\n    <el-col :span="3"><div class="grid-content bg-purple"></div></el-col>\n    <el-col :span="3"><div class="grid-content bg-purple-light"></div></el-col>\n    <el-col :span="3"><div class="grid-content bg-purple"></div></el-col>\n    <el-col :span="3"><div class="grid-content bg-purple-light"></div></el-col>\n</el-row>\n\n\n\n# container 布局容器\n\n用于布局的容器组件，方便快速搭建页面的基本结构。如下图就是布局容器效果。\n\n如下图是官网提供的 container 布局容器实例：\n\n该效果代码中包含了样式、页面标签、模型数据。将里面的样式 <style> 拷贝到我们自己页面的 head 标签中；将html标签拷贝到 <div id="app"></div> 标签中，再将数据模型拷贝到 vue 对象的 data() 中。\n\n整体页面代码如下：\n\n<!doctype html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>title</title>\n\n    <style>\n        .el-header {\n            background-color: #b3c0d1;\n            color: #333;\n            line-height: 60px;\n        }\n\n        .el-aside {\n            color: #333;\n        }\n    </style>\n</head>\n<body>\n<div id="app">\n    <el-container style="height: 500px; border: 1px solid #eee">\n        <el-aside width="200px" style="background-color: rgb(238, 241, 246)">\n            <el-menu :default-openeds="[\'1\', \'3\']">\n                <el-submenu index="1">\n                    <template slot="title"><i class="el-icon-message"></i>导航一</template>\n                    <el-menu-item-group>\n                        <template slot="title">分组一</template>\n                        <el-menu-item index="1-1">选项1</el-menu-item>\n                        <el-menu-item index="1-2">选项2</el-menu-item>\n                    </el-menu-item-group>\n                    <el-menu-item-group title="分组2">\n                        <el-menu-item index="1-3">选项3</el-menu-item>\n                    </el-menu-item-group>\n                    <el-submenu index="1-4">\n                        <template slot="title">选项4</template>\n                        <el-menu-item index="1-4-1">选项4-1</el-menu-item>\n                    </el-submenu>\n                </el-submenu>\n                <el-submenu index="2">\n                    <template slot="title"><i class="el-icon-menu"></i>导航二</template>\n                    <el-submenu index="2-1">\n                        <template slot="title">选项1</template>\n                        <el-menu-item index="2-1-1">选项1-1</el-menu-item>\n                    </el-submenu>\n                </el-submenu>\n                <el-submenu index="3">\n                    <template slot="title"><i class="el-icon-setting"></i>导航三</template>\n                    <el-menu-item-group>\n                        <template slot="title">分组一</template>\n                        <el-menu-item index="3-1">选项1</el-menu-item>\n                        <el-menu-item index="3-2">选项2</el-menu-item>\n                    </el-menu-item-group>\n                    <el-menu-item-group title="分组2">\n                        <el-menu-item index="3-3">选项3</el-menu-item>\n                    </el-menu-item-group>\n                    <el-submenu index="3-4">\n                        <template slot="title">选项4</template>\n                        <el-menu-item index="3-4-1">选项4-1</el-menu-item>\n                    </el-submenu>\n                </el-submenu>\n            </el-menu>\n        </el-aside>\n\n        <el-container>\n            <el-header style="text-align: right; font-size: 12px">\n                <el-dropdown>\n                    <i class="el-icon-setting" style="margin-right: 15px"></i>\n                    <el-dropdown-menu slot="dropdown">\n                        <el-dropdown-item>查看</el-dropdown-item>\n                        <el-dropdown-item>新增</el-dropdown-item>\n                        <el-dropdown-item>删除</el-dropdown-item>\n                    </el-dropdown-menu>\n                </el-dropdown>\n                <span>王小虎</span>\n            </el-header>\n\n            <el-main>\n                <el-table :data="tabledata">\n                    <el-table-column prop="date" label="日期" width="140">\n                    </el-table-column>\n                    <el-table-column prop="name" label="姓名" width="120">\n                    </el-table-column>\n                    <el-table-column prop="address" label="地址">\n                    </el-table-column>\n                </el-table>\n            </el-main>\n        </el-container>\n    </el-container>\n</div>\n<script src="js/vue.js"><\/script>\n<script src="element-ui/lib/index.js"><\/script>\n<link rel="stylesheet" href="element-ui/lib/theme-chalk/index.css">\n\n<script>\n    new vue({\n        el:"#app",\n        data() {\n            const item = {\n                date: \'2016-05-02\',\n                name: \'王小虎\',\n                address: \'上海市普陀区金沙江路 1518 弄\'\n            };\n            return {\n                tabledata: array(20).fill(item)\n            }\n        }\n    })\n<\/script>\n</body>\n</html>\n\n\n\n# 案例\n\n其他的组件我们通过完成一个页面来学习。\n\n我们要完成如下页面效果\n\n要完成该页面，我们需要先对这个页面进行分析，看页面由哪儿几部分组成，然后到官网进行拷贝并修改。页面总共有如下组成部分\n\n\n\n还有一个是当我们点击 新增 按钮，会在页面正中间弹出一个对话框，如下\n\n\n# 准备基本页面\n\n<!doctype html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>title</title>\n</head>\n<body>\n<div id="app">\n\t\n</div>\n\n<script src="js/vue.js"><\/script>\n<script src="element-ui/lib/index.js"><\/script>\n<link rel="stylesheet" href="element-ui/lib/theme-chalk/index.css">\n\n<script>\n    new vue({\n        el: "#app"\n    })\n<\/script>\n</body>\n</html>\n\n\n\n# 完成表格展示\n\n使用 element 整体的思路就是 拷贝 + 修改。\n\n# 拷贝\n\n\n\n在左菜单栏找到 table 表格并点击，右边主体就会定位到表格这一块，找到我们需要的表格效果（如上图），点击 显示代码 就可以看到这个表格的代码了。\n\n将html标签拷贝到 <div id="app"></div> 中，如下：\n\n将css样式拷贝到我们页面的 head 标签中，如下\n\n将方法和模型数据拷贝到 vue 对象指定的位置\n\n拷贝完成后通过浏览器打开可以看到表格的效果\n\n\n\n表格效果出来了，但是显示的表头和数据并不是我们想要的，所以接下来就需要对页面代码进行修改了。\n\n# 修改\n\n 1. 修改表头和数据\n    \n    下面是对表格代码进行分析的图解。根据下图说明修改自己的列数和列名\n    \n    修改完页面后，还需要对绑定的模型数据进行修改，下图是对模型数据进行分析的图解\n\n 2. 给表格添加操作列\n    \n    从之前的表格拷贝一列出来并对其进行修改。按钮是从官网的 button 按钮 组件中拷贝并修改的\n\n 3. 给表格添加复选框列和标号列\n    \n    给表格添加复选框和标号列，效果如下\n    \n    \n    \n    此效果也是从 element 官网进行拷贝，先找到对应的表格效果，然后将其对应代码拷贝到我们的代码中，如下是复选框列官网效果图和代码\n    \n    \n    \n    这里需要注意在 <el-table> 标签上有一个事件 @selection-change="handleselectionchange" ，这里绑定的函数也需要从官网拷贝到我们自己的页面代码中，函数代码如下：\n    \n    \n    \n    从该函数中又发现还需要一个模型数据 multipleselection ，所以还需要定义出该模型数据\n\n标号列也用同样的方式进行拷贝并修改。\n\n\n# 完成搜索表单展示\n\n在 element 官网找到横排的表单效果，然后拷贝代码并进行修改\n\n\n\n点击上面的 显示代码 后，就会展示出对应的代码，下面是对这部分代码进行分析的图解\n\n\n\n然后根据我们要的效果修改代码。\n\n\n# 完成批量删除和新增按钮展示\n\n从 element 官网找具有着色效果的按钮，并将代码拷贝到我们自己的页面上\n\n\n# 完成对话框展示\n\n在 element 官网找对话框，如下：\n\n下面对官网提供的代码进行分析\n\n上图分析出来的模型数据需要在 vue 对象中进行定义。\n\n\n# 完成分页条展示\n\n在 element 官网找到 pagination 分页 ，在页面主体部分找到我们需要的效果，如下\n\n点击 显示代码 ，找到 完整功能 对应的代码，接下来对该代码进行分析\n\n上面代码属性说明：\n\n * page-size ：每页显示的条目数\n\n * page-sizes ： 每页显示个数选择器的选项设置。\n   \n   :page-sizes="[100,200,300,400]" 对应的页面效果如下：\n\n * currentpage ：当前页码。我们点击那个页码，此属性值就是几。\n\n * total ：总记录数。用来设置总的数据条目数，该属性设置后， element 会自动计算出需分多少页并给我们展示对应的页码。\n\n事件说明：\n\n * size-change ：pagesize 改变时会触发。也就是当我们改变了每页显示的条目数后，该事件会触发。\n * current-change ：currentpage 改变时会触发。也就是当我们点击了其他的页码后，该事件会触发。\n\n\n# 完整页面代码\n\n<!doctype html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>title</title>\n    <style>\n        .el-table .warning-row {\n            background: oldlace;\n        }\n        .el-table .success-row {\n            background: #f0f9eb;\n        }\n    </style>\n</head>\n<body>\n<div id="app">\n    \x3c!--搜索表单--\x3e\n    <el-form :inline="true" :model="brand" class="demo-form-inline">\n        <el-form-item label="当前状态">\n            <el-select v-model="brand.status" placeholder="当前状态">\n                <el-option label="启用" value="1"></el-option>\n                <el-option label="禁用" value="0"></el-option>\n            </el-select>\n        </el-form-item>\n\n        <el-form-item label="企业名称">\n            <el-input v-model="brand.companyname" placeholder="企业名称"></el-input>\n        </el-form-item>\n\n        <el-form-item label="品牌名称">\n            <el-input v-model="brand.brandname" placeholder="品牌名称"></el-input>\n        </el-form-item>\n\n        <el-form-item>\n            <el-button type="primary" @click="onsubmit">查询</el-button>\n        </el-form-item>\n    </el-form>\n\n    \x3c!--按钮--\x3e\n    <el-row>\n        <el-button type="danger" plain>批量删除</el-button>\n        <el-button type="primary" plain @click="dialogvisible = true">新增</el-button>\n    </el-row>\n    \n    \x3c!--添加数据对话框表单--\x3e\n    <el-dialog\n            title="编辑品牌"\n            :visible.sync="dialogvisible"\n            width="30%">\n        <el-form ref="form" :model="brand" label-width="80px">\n            <el-form-item label="品牌名称">\n                <el-input v-model="brand.brandname"></el-input>\n            </el-form-item>\n\n            <el-form-item label="企业名称">\n                <el-input v-model="brand.companyname"></el-input>\n            </el-form-item>\n\n            <el-form-item label="排序">\n                <el-input v-model="brand.ordered"></el-input>\n            </el-form-item>\n\n            <el-form-item label="备注">\n                <el-input type="textarea" v-model="brand.description"></el-input>\n            </el-form-item>\n\n            <el-form-item label="状态">\n                <el-switch v-model="brand.status"\n                           active-value="1"\n                           inactive-value="0"\n                ></el-switch>\n            </el-form-item>\n            <el-form-item>\n                <el-button type="primary" @click="addbrand">提交</el-button>\n                <el-button @click="dialogvisible = false">取消</el-button>\n            </el-form-item>\n        </el-form>\n    </el-dialog>\n\n    \x3c!--表格--\x3e\n    <template>\n        <el-table\n                :data="tabledata"\n                style="width: 100%"\n                :row-class-name="tablerowclassname"\n                @selection-change="handleselectionchange">\n            <el-table-column\n                    type="selection"\n                    width="55">\n            </el-table-column>\n            <el-table-column\n                    type="index"\n                    width="50">\n            </el-table-column>\n            <el-table-column\n                    prop="brandname"\n                    label="品牌名称"\n                    align="center">\n            </el-table-column>\n            <el-table-column\n                    prop="companyname"\n                    label="企业名称"\n                    align="center">\n            </el-table-column>\n            <el-table-column\n                    prop="ordered"\n                    align="center"\n                    label="排序">\n            </el-table-column>\n            <el-table-column\n                    prop="status"\n                    align="center"\n                    label="当前状态">\n            </el-table-column>\n            <el-table-column\n                    align="center"\n                    label="操作">\n                <el-row>\n                    <el-button type="primary">修改</el-button>\n                    <el-button type="danger">删除</el-button>\n                </el-row>\n            </el-table-column>\n\n        </el-table>\n    </template>\n\n    \x3c!--分页工具条--\x3e\n    <el-pagination\n            @size-change="handlesizechange"\n            @current-change="handlecurrentchange"\n            :current-page="currentpage"\n            :page-sizes="[5, 10, 15, 20]"\n            :page-size="5"\n            layout="total, sizes, prev, pager, next, jumper"\n            :total="400">\n    </el-pagination>\n\n</div>\n<script src="js/vue.js"><\/script>\n<script src="element-ui/lib/index.js"><\/script>\n<link rel="stylesheet" href="element-ui/lib/theme-chalk/index.css">\n<script>\n    new vue({\n        el: "#app",\n        methods: {\n            tablerowclassname({row, rowindex}) {\n                if (rowindex **= 1) {\n                    return \'warning-row\';\n                } else if (rowindex **= 3) {\n                    return \'success-row\';\n                }\n                return \'\';\n            },\n            // 复选框选中后执行的方法\n            handleselectionchange(val) {\n                this.multipleselection = val;\n\n                console.log(this.multipleselection)\n            },\n            // 查询方法\n            onsubmit() {\n                console.log(this.brand);\n            },\n            // 添加数据\n            addbrand(){\n                console.log(this.brand);\n            },\n            //分页\n            handlesizechange(val) {\n                console.log(`每页 ${val} 条`);\n            },\n            handlecurrentchange(val) {\n                console.log(`当前页: ${val}`);\n            }\n        },\n        data() {\n            return {\n                // 当前页码\n                currentpage: 4,\n                // 添加数据对话框是否展示的标记\n                dialogvisible: false,\n\n                // 品牌模型数据\n                brand: {\n                    status: \'\',\n                    brandname: \'\',\n                    companyname: \'\',\n                    id:"",\n                    ordered:"",\n                    description:""\n                },\n                // 复选框选中数据集合\n                multipleselection: [],\n                // 表格数据\n                tabledata: [{\n                    brandname: \'华为\',\n                    companyname: \'华为科技有限公司\',\n                    ordered: \'100\',\n                    status: "1"\n                }, {\n                    brandname: \'华为\',\n                    companyname: \'华为科技有限公司\',\n                    ordered: \'100\',\n                    status: "1"\n                }, {\n                    brandname: \'华为\',\n                    companyname: \'华为科技有限公司\',\n                    ordered: \'100\',\n                    status: "1"\n                }, {\n                    brandname: \'华为\',\n                    companyname: \'华为科技有限公司\',\n                    ordered: \'100\',\n                    status: "1"\n                }]\n            }\n        }\n    })\n<\/script>\n</body>\n</html>\n',charsets:{cjk:!0}},{title:"登录注册案例",frontmatter:{autoSort:10,title:"登录注册案例",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/4f0756/",categories:["后端","JavaWeb"],tags:["知识","JavaWeb"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/20.JavaWeb/55.%E7%94%A8%E6%88%B7%E7%99%BB%E5%BD%95%E6%B3%A8%E5%86%8C%E6%A1%88%E4%BE%8B.html",relativePath:"01.后端/20.JavaWeb/55.用户登录注册案例.md",key:"v-4c45192e",path:"/pages/4f0756/",headers:[{level:2,title:"需求分析",slug:"需求分析",normalizedTitle:"需求分析",charIndex:null},{level:2,title:"用户登录功能",slug:"用户登录功能",normalizedTitle:"用户登录功能",charIndex:null},{level:2,title:"记住我-设置Cookie",slug:"记住我-设置cookie",normalizedTitle:"记住我-设置cookie",charIndex:null},{level:2,title:"记住我-获取Cookie",slug:"记住我-获取cookie",normalizedTitle:"记住我-获取cookie",charIndex:null},{level:2,title:"用户注册功能",slug:"用户注册功能",normalizedTitle:"用户注册功能",charIndex:null},{level:2,title:"验证码-展示",slug:"验证码-展示",normalizedTitle:"验证码-展示",charIndex:null},{level:2,title:"验证码-校验",slug:"验证码-校验",normalizedTitle:"验证码-校验",charIndex:null}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"需求分析 用户登录功能 记住我-设置Cookie 记住我-获取Cookie 用户注册功能 验证码-展示 验证码-校验",content:"标签下添加欢迎当前用户的提示信息:",normalizedContent:"标签下添加欢迎当前用户的提示信息:",charsets:{cjk:!0}},{title:"工具包",frontmatter:{autoSort:87,title:"工具包",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/732670/",categories:["后端","Java","流"],tags:["知识","Java"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/10.Java/60.%E6%B5%81/70.%E5%B7%A5%E5%85%B7%E5%8C%85.html",relativePath:"01.后端/10.Java/60.流/70.工具包.md",key:"v-3d891436",path:"/pages/732670/",headers:[{level:2,title:"Commons-io",slug:"commons-io",normalizedTitle:"commons-io",charIndex:10},{level:2,title:"hutool",slug:"hutool",normalizedTitle:"hutool",charIndex:1975}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"Commons-io hutool",content:'# 工具包\n\n\n# Commons-io\n\n介绍：\n\nCommons是apache开源基金组织提供的工具包，里面有很多帮助我们提高开发效率的API\n\n比如：\n\nStringUtils   字符串工具类\n\nNumberUtils   数字工具类\n\nArrayUtils   数组工具类\n\nRandomUtils   随机数工具类\n\nDateUtils   日期工具类\n\nStopWatch   秒表工具类\n\nClassUtils   反射工具类\n\nSystemUtils   系统工具类\n\nMapUtils   集合工具类\n\nBeanutils   bean工具类\n\nCommons-io io的工具类\n\n等等.....\n\n其中：Commons-io是apache开源基金组织提供的一组有关IO操作的开源工具包。\n\n作用：提高IO流的开发效率。\n\n使用方式：\n\n1，新建lib文件夹\n\n2，把第三方jar包粘贴到文件夹中\n\n3，右键点击add as a library\n\n代码示例：\n\npublic class CommonsIODemo1 {\n    public static void main(String[] args) throws IOException {\n        /*\n          FileUtils类\n                static void copyFile(File srcFile, File destFile)                   复制文件\n                static void copyDirectory(File srcDir, File destDir)                复制文件夹\n                static void copyDirectoryToDirectory(File srcDir, File destDir)     复制文件夹\n                static void deleteDirectory(File directory)                         删除文件夹\n                static void cleanDirectory(File directory)                          清空文件夹\n                static String readFileToString(File file, Charset encoding)         读取文件中的数据变成成字符串\n                static void write(File file, CharSequence data, String encoding)    写出数据\n\n            IOUtils类\n                public static int copy(InputStream input, OutputStream output)      复制文件\n                public static int copyLarge(Reader input, Writer output)            复制大文件\n                public static String readLines(Reader input)                        读取数据\n                public static void write(String data, OutputStream output)          写出数据\n         */\n\n\n        /* File src = new File("myio\\\\a.txt");\n        File dest = new File("myio\\\\copy.txt");\n        FileUtils.copyFile(src,dest);*/\n\n\n        /*File src = new File("D:\\\\aaa");\n        File dest = new File("D:\\\\bbb");\n        FileUtils.copyDirectoryToDirectory(src,dest);*/\n\n        /*File src = new File("D:\\\\bbb");\n        FileUtils.cleanDirectory(src);*/\n\n\n\n    }\n}\n\n\n\n\n# hutool\n\n介绍：\n\nhutool是国人开发的开源工具包，里面有很多帮助我们提高开发效率的API\n\n比如：\n\nDateUtil  日期时间工具类\n\nTimeInterval  计时器工具类\n\nStrUtil  字符串工具类\n\nHexUtil   16进制工具类\n\nHashUtil   Hash算法类\n\nObjectUtil  对象工具类\n\nReflectUtil   反射工具类\n\nTypeUtil  泛型类型工具类\n\nPageUtil  分页工具类\n\nNumberUtil  数字工具类\n\n使用方式：\n\n1，新建lib文件夹\n\n2，把第三方jar包粘贴到文件夹中\n\n3，右键点击add as a library\n\n代码示例：\n\npublic class Test1 {\n    public static void main(String[] args) {\n    /*\n        FileUtil类:\n                file：根据参数创建一个file对象\n                touch：根据参数创建文件\n\n                writeLines：把集合中的数据写出到文件中，覆盖模式。\n                appendLines：把集合中的数据写出到文件中，续写模式。\n                readLines：指定字符编码，把文件中的数据，读到集合中。\n                readUtf8Lines：按照UTF-8的形式，把文件中的数据，读到集合中\n\n                copy：拷贝文件或者文件夹\n    */\n\n\n       /* File file1 = FileUtil.file("D:\\\\", "aaa", "bbb", "a.txt");\n        System.out.println(file1);//D:\\aaa\\bbb\\a.txt\n\n        File touch = FileUtil.touch(file1);\n        System.out.println(touch);\n\n\n        ArrayList<String> list = new ArrayList<>();\n        list.add("aaa");\n        list.add("aaa");\n        list.add("aaa");\n\n        File file2 = FileUtil.writeLines(list, "D:\\\\a.txt", "UTF-8");\n        System.out.println(file2);*/\n\n      /*  ArrayList<String> list = new ArrayList<>();\n        list.add("aaa");\n        list.add("aaa");\n        list.add("aaa");\n        File file3 = FileUtil.appendLines(list, "D:\\\\a.txt", "UTF-8");\n        System.out.println(file3);*/\n        List<String> list = FileUtil.readLines("D:\\\\a.txt", "UTF-8");\n        System.out.println(list);\n    }\n}\n',normalizedContent:'# 工具包\n\n\n# commons-io\n\n介绍：\n\ncommons是apache开源基金组织提供的工具包，里面有很多帮助我们提高开发效率的api\n\n比如：\n\nstringutils   字符串工具类\n\nnumberutils   数字工具类\n\narrayutils   数组工具类\n\nrandomutils   随机数工具类\n\ndateutils   日期工具类\n\nstopwatch   秒表工具类\n\nclassutils   反射工具类\n\nsystemutils   系统工具类\n\nmaputils   集合工具类\n\nbeanutils   bean工具类\n\ncommons-io io的工具类\n\n等等.....\n\n其中：commons-io是apache开源基金组织提供的一组有关io操作的开源工具包。\n\n作用：提高io流的开发效率。\n\n使用方式：\n\n1，新建lib文件夹\n\n2，把第三方jar包粘贴到文件夹中\n\n3，右键点击add as a library\n\n代码示例：\n\npublic class commonsiodemo1 {\n    public static void main(string[] args) throws ioexception {\n        /*\n          fileutils类\n                static void copyfile(file srcfile, file destfile)                   复制文件\n                static void copydirectory(file srcdir, file destdir)                复制文件夹\n                static void copydirectorytodirectory(file srcdir, file destdir)     复制文件夹\n                static void deletedirectory(file directory)                         删除文件夹\n                static void cleandirectory(file directory)                          清空文件夹\n                static string readfiletostring(file file, charset encoding)         读取文件中的数据变成成字符串\n                static void write(file file, charsequence data, string encoding)    写出数据\n\n            ioutils类\n                public static int copy(inputstream input, outputstream output)      复制文件\n                public static int copylarge(reader input, writer output)            复制大文件\n                public static string readlines(reader input)                        读取数据\n                public static void write(string data, outputstream output)          写出数据\n         */\n\n\n        /* file src = new file("myio\\\\a.txt");\n        file dest = new file("myio\\\\copy.txt");\n        fileutils.copyfile(src,dest);*/\n\n\n        /*file src = new file("d:\\\\aaa");\n        file dest = new file("d:\\\\bbb");\n        fileutils.copydirectorytodirectory(src,dest);*/\n\n        /*file src = new file("d:\\\\bbb");\n        fileutils.cleandirectory(src);*/\n\n\n\n    }\n}\n\n\n\n\n# hutool\n\n介绍：\n\nhutool是国人开发的开源工具包，里面有很多帮助我们提高开发效率的api\n\n比如：\n\ndateutil  日期时间工具类\n\ntimeinterval  计时器工具类\n\nstrutil  字符串工具类\n\nhexutil   16进制工具类\n\nhashutil   hash算法类\n\nobjectutil  对象工具类\n\nreflectutil   反射工具类\n\ntypeutil  泛型类型工具类\n\npageutil  分页工具类\n\nnumberutil  数字工具类\n\n使用方式：\n\n1，新建lib文件夹\n\n2，把第三方jar包粘贴到文件夹中\n\n3，右键点击add as a library\n\n代码示例：\n\npublic class test1 {\n    public static void main(string[] args) {\n    /*\n        fileutil类:\n                file：根据参数创建一个file对象\n                touch：根据参数创建文件\n\n                writelines：把集合中的数据写出到文件中，覆盖模式。\n                appendlines：把集合中的数据写出到文件中，续写模式。\n                readlines：指定字符编码，把文件中的数据，读到集合中。\n                readutf8lines：按照utf-8的形式，把文件中的数据，读到集合中\n\n                copy：拷贝文件或者文件夹\n    */\n\n\n       /* file file1 = fileutil.file("d:\\\\", "aaa", "bbb", "a.txt");\n        system.out.println(file1);//d:\\aaa\\bbb\\a.txt\n\n        file touch = fileutil.touch(file1);\n        system.out.println(touch);\n\n\n        arraylist<string> list = new arraylist<>();\n        list.add("aaa");\n        list.add("aaa");\n        list.add("aaa");\n\n        file file2 = fileutil.writelines(list, "d:\\\\a.txt", "utf-8");\n        system.out.println(file2);*/\n\n      /*  arraylist<string> list = new arraylist<>();\n        list.add("aaa");\n        list.add("aaa");\n        list.add("aaa");\n        file file3 = fileutil.appendlines(list, "d:\\\\a.txt", "utf-8");\n        system.out.println(file3);*/\n        list<string> list = fileutil.readlines("d:\\\\a.txt", "utf-8");\n        system.out.println(list);\n    }\n}\n',charsets:{cjk:!0}},{title:"函数",frontmatter:{autoSort:98,title:"函数",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/e00b6b/",categories:["后端","数据库","MySQL"],tags:["知识","数据库","MySQL"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/01.MySQL/05.%E5%87%BD%E6%95%B0.html",relativePath:"01.后端/30.数据库/01.MySQL/05.函数.md",key:"v-8d62e9e4",path:"/pages/e00b6b/",headers:[{level:2,title:"字符串函数",slug:"字符串函数",normalizedTitle:"字符串函数",charIndex:2},{level:2,title:"数值函数",slug:"数值函数",normalizedTitle:"数值函数",charIndex:496},{level:2,title:"日期函数",slug:"日期函数",normalizedTitle:"日期函数",charIndex:791},{level:2,title:"流程函数",slug:"流程函数",normalizedTitle:"流程函数",charIndex:1299}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"字符串函数 数值函数 日期函数 流程函数",content:"# 字符串函数\n\n-- 字符串函数\n-- concat  字符串拼接\nselect concat('Hello' , ' MySQL');\n\n-- lower  变小写字符串\nselect lower('Hello');\n\n-- upper  变大写字符串\nselect upper('Hello');\n\n-- lpad  向左填充\nselect lpad('01', 5, '-');\n\n-- rpad  向右填充\nselect rpad('01', 5, '-');\n\n-- trim  去除开头和结尾的空格\nselect trim(' Hello  MySQL ');\n\n-- substring  截取字符串\n-- 下标从1开始, (索引从1开始), 截取长度为2\nselect substring('Hello MySQL',1,5);\n\n\n-- 案例:  由于业务需求变更，企业员工的工号，统一为5位数，目前不足5位数的全部在前面补0。比如： 1号员工的工号应该为00001。\nupdate emp set workno = lpad(workno, 5, '0');\n\n\n\n# 数值函数\n\n-- 数值函数\n-- ceil 向上取整\nselect ceil(1.1);\n\n-- floor  向下取整\nselect floor(1.9);\n\n-- mod  求余\nselect mod(7,4);\n\n-- rand 0-1的随机数\nselect rand();\n\n-- round(x, y) 求参数x4舍五入的值, 保持y位小数 \nselect round(2.344,2); -- 2.34\n\n\n-- 案例: 通过数据库的函数，生成一个六位数的随机验证码。\nselect lpad(round(rand()*1000000 , 0), 6, '0');\n\n\n\n\n# 日期函数\n\n-- 日期函数\n-- curdate()  当前日期\nselect curdate();\n\n-- curtime()  当前时间\nselect curtime();\n\n-- now()  当前日期+当前时间\nselect now();\n\n-- YEAR , MONTH , DAY \nselect YEAR(now());  获取年\n\nselect MONTH(now());  获取月\n\nselect DAY(now());   获取日\n\n-- date_add  往后推 70 年  ，INTERVAL是固定的\nselect date_add(now(), INTERVAL 70 YEAR );\n\n-- datediff  两个时间，相差天数\nselect datediff('2021-10-01', '2021-12-01');\n\n\n-- 案例: 查询所有员工的入职天数，并根据入职天数倒序排序。\nselect name, datediff(curdate(), entrydate) as 'entrydays' from emp order by entrydays desc;\n\n\n\n# 流程函数\n\n\n\n-- 流程控制函数\n-- if（value, t, f） 如果value为true, 则返回t, 否则返回f\nselect if(false, 'Ok', 'Error');\n\n-- ifnull(value1. value2) 如果value1 不为null, 返回value1, 否则返回 value2\nselect ifnull('Ok','Default');\n\nselect ifnull('','Default');\n\nselect ifnull(null,'Default');\n\n-- case when then else end\n-- 需求: 查询emp表的员工姓名和工作地址 (北京/上海 ----\x3e 一线城市 , 其他 ----\x3e 二线城市)\nselect\n    name,\n    ( case workaddress when '北京' then '一线城市' when '上海' then '一线城市' else '二线城市' end ) as '工作地址'\nfrom emp;\n\n\n\n\n-- 案例: 统计班级各个学员的成绩，展示的规则如下：\n-- >= 85，展示优秀\n-- >= 60，展示及格\n-- 否则，展示不及格\n\ncreate table score(\n    id int comment 'ID',\n    name varchar(20) comment '姓名',\n    math int comment '数学',\n    english int comment '英语',\n    chinese int comment '语文'\n) comment '学员成绩表';\ninsert into score(id, name, math, english, chinese) VALUES (1, 'Tom', 67, 88, 95 ), (2, 'Rose' , 23, 66, 90),(3, 'Jack', 56, 98, 76);\n\n\n--\nselect\n    id,\n    name,\n    (case when math >= 85 then '优秀' when math >=60 then '及格' else '不及格' end ) '数学',\n    (case when english >= 85 then '优秀' when english >=60 then '及格' else '不及格' end ) '英语',\n    (case when chinese >= 85 then '优秀' when chinese >=60 then '及格' else '不及格' end ) '语文'\nfrom score;\n",normalizedContent:"# 字符串函数\n\n-- 字符串函数\n-- concat  字符串拼接\nselect concat('hello' , ' mysql');\n\n-- lower  变小写字符串\nselect lower('hello');\n\n-- upper  变大写字符串\nselect upper('hello');\n\n-- lpad  向左填充\nselect lpad('01', 5, '-');\n\n-- rpad  向右填充\nselect rpad('01', 5, '-');\n\n-- trim  去除开头和结尾的空格\nselect trim(' hello  mysql ');\n\n-- substring  截取字符串\n-- 下标从1开始, (索引从1开始), 截取长度为2\nselect substring('hello mysql',1,5);\n\n\n-- 案例:  由于业务需求变更，企业员工的工号，统一为5位数，目前不足5位数的全部在前面补0。比如： 1号员工的工号应该为00001。\nupdate emp set workno = lpad(workno, 5, '0');\n\n\n\n# 数值函数\n\n-- 数值函数\n-- ceil 向上取整\nselect ceil(1.1);\n\n-- floor  向下取整\nselect floor(1.9);\n\n-- mod  求余\nselect mod(7,4);\n\n-- rand 0-1的随机数\nselect rand();\n\n-- round(x, y) 求参数x4舍五入的值, 保持y位小数 \nselect round(2.344,2); -- 2.34\n\n\n-- 案例: 通过数据库的函数，生成一个六位数的随机验证码。\nselect lpad(round(rand()*1000000 , 0), 6, '0');\n\n\n\n\n# 日期函数\n\n-- 日期函数\n-- curdate()  当前日期\nselect curdate();\n\n-- curtime()  当前时间\nselect curtime();\n\n-- now()  当前日期+当前时间\nselect now();\n\n-- year , month , day \nselect year(now());  获取年\n\nselect month(now());  获取月\n\nselect day(now());   获取日\n\n-- date_add  往后推 70 年  ，interval是固定的\nselect date_add(now(), interval 70 year );\n\n-- datediff  两个时间，相差天数\nselect datediff('2021-10-01', '2021-12-01');\n\n\n-- 案例: 查询所有员工的入职天数，并根据入职天数倒序排序。\nselect name, datediff(curdate(), entrydate) as 'entrydays' from emp order by entrydays desc;\n\n\n\n# 流程函数\n\n\n\n-- 流程控制函数\n-- if（value, t, f） 如果value为true, 则返回t, 否则返回f\nselect if(false, 'ok', 'error');\n\n-- ifnull(value1. value2) 如果value1 不为null, 返回value1, 否则返回 value2\nselect ifnull('ok','default');\n\nselect ifnull('','default');\n\nselect ifnull(null,'default');\n\n-- case when then else end\n-- 需求: 查询emp表的员工姓名和工作地址 (北京/上海 ----\x3e 一线城市 , 其他 ----\x3e 二线城市)\nselect\n    name,\n    ( case workaddress when '北京' then '一线城市' when '上海' then '一线城市' else '二线城市' end ) as '工作地址'\nfrom emp;\n\n\n\n\n-- 案例: 统计班级各个学员的成绩，展示的规则如下：\n-- >= 85，展示优秀\n-- >= 60，展示及格\n-- 否则，展示不及格\n\ncreate table score(\n    id int comment 'id',\n    name varchar(20) comment '姓名',\n    math int comment '数学',\n    english int comment '英语',\n    chinese int comment '语文'\n) comment '学员成绩表';\ninsert into score(id, name, math, english, chinese) values (1, 'tom', 67, 88, 95 ), (2, 'rose' , 23, 66, 90),(3, 'jack', 56, 98, 76);\n\n\n--\nselect\n    id,\n    name,\n    (case when math >= 85 then '优秀' when math >=60 then '及格' else '不及格' end ) '数学',\n    (case when english >= 85 then '优秀' when english >=60 then '及格' else '不及格' end ) '英语',\n    (case when chinese >= 85 then '优秀' when chinese >=60 then '及格' else '不及格' end ) '语文'\nfrom score;\n",charsets:{cjk:!0}},{title:"《JavaWeb》",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"01.后端/20.JavaWeb",imgUrl:"/assets/img/web.png",description:"JavaWeb学习笔记--整理自黑马程序员，在原教程基础上添加学习笔记"}},title:"《JavaWeb》",date:"2023-06-30T20:30:40.000Z",permalink:"/back/javaWeb/",article:!1,comment:!1,editLink:!1,author:{name:"凉冰",link:"https://github.com/diana-devil"}},regularPath:"/01.%E5%90%8E%E7%AB%AF/20.JavaWeb/",relativePath:"01.后端/20.JavaWeb/README.md",key:"v-0d97dac8",path:"/back/javaWeb/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"会话技术",frontmatter:{autoSort:92,title:"会话技术",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/a450d7/",categories:["后端","JavaWeb"],tags:["知识","JavaWeb"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/20.JavaWeb/50.%E4%BC%9A%E8%AF%9D%E6%8A%80%E6%9C%AF.html",relativePath:"01.后端/20.JavaWeb/50.会话技术.md",key:"v-3f96471e",path:"/pages/a450d7/",headers:[{level:2,title:"会话跟踪技术的概述",slug:"会话跟踪技术的概述",normalizedTitle:"会话跟踪技术的概述",charIndex:11},{level:2,title:"Cookie",slug:"cookie",normalizedTitle:"cookie",charIndex:1409},{level:3,title:"Cookie的基本使用",slug:"cookie的基本使用",normalizedTitle:"cookie的基本使用",charIndex:1831},{level:3,title:"Cookie的原理分析",slug:"cookie的原理分析",normalizedTitle:"cookie的原理分析",charIndex:7347},{level:3,title:"Cookie的使用细节",slug:"cookie的使用细节",normalizedTitle:"cookie的使用细节",charIndex:6941},{level:2,title:"Session",slug:"session",normalizedTitle:"session",charIndex:1430},{level:3,title:"Session的基本使用",slug:"session的基本使用",normalizedTitle:"session的基本使用",charIndex:1533},{level:3,title:"Session的原理分析",slug:"session的原理分析",normalizedTitle:"session的原理分析",charIndex:16735},{level:3,title:"Session的使用细节",slug:"session的使用细节",normalizedTitle:"session的使用细节",charIndex:19534}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"会话跟踪技术的概述 Cookie Cookie的基本使用 Cookie的原理分析 Cookie的使用细节 Session Session的基本使用 Session的原理分析 Session的使用细节",content:'# 会话技术\n\n\n# 会话跟踪技术的概述\n\n对于会话跟踪这四个词，我们需要拆开来进行解释，首先要理解什么是会话，然后再去理解什么是会话跟踪:\n\n * 会话:用户打开浏览器，访问web服务器的资源，会话建立，直到有一方断开连接，会话结束。在一次会话中可以包含多次请求和响应。\n   \n   * 从浏览器发出请求到服务端响应数据给前端之后，一次会话(在浏览器和服务器之间)就被建立了\n   * 会话被建立后，如果浏览器或服务端都没有被关闭，则会话就会持续建立着\n   * 浏览器和服务器就可以继续使用该会话进行请求发送和响应，上述的整个过程就被称之为会话。\n   \n   用实际场景来理解下会话，比如在我们访问京东的时候，当打开浏览器进入京东首页后，浏览器和京东的服务器之间就建立了一次会话，后面的搜索商品,查看商品的详情,加入购物车等都是在这一次会话中完成。\n   \n   思考:下图中总共建立了几个会话?\n   \n   \n   \n   每个浏览器都会与服务端建立了一个会话，加起来总共是3个会话。\n\n * 会话跟踪:一种维护浏览器状态的方法，服务器需要识别多次请求是否来自于同一浏览器，以便在同一次会话的多次请求间共享数据。\n   \n   * 服务器会收到多个请求，这多个请求可能来自多个浏览器，如上图中的6个请求来自3个浏览器\n   * 服务器需要用来识别请求是否来自同一个浏览器\n   * 服务器用来识别浏览器的过程，这个过程就是会话跟踪\n   * 服务器识别浏览器后就可以在同一个会话中多次请求之间来共享数据\n   \n   那么我们又有一个问题需要思考，一个会话中的多次请求为什么要共享数据呢?有了这个数据共享功能后能实现哪些功能呢?\n   \n   * 购物车: 加入购物车和去购物车结算是两次请求，但是后面这次请求要想展示前一次请求所添加的商品，就需要用到数据共享。\n     \n     \n   \n   * 页面展示用户登录信息:很多网站，登录后访问多个功能发送多次请求后，浏览器上都会有当前登录用户的信息[用户名]，比如百度、京东、码云等。\n     \n     \n   \n   * 网站登录页面的记住我功能:当用户登录成功后，勾选记住我按钮后下次再登录的时候，网站就会自动填充用户名和密码，简化用户的登录操作，多次登录就会有多次请求，他们之间也涉及到共享数据\n     \n     \n   \n   * 登录页面的验证码功能:生成验证码和输入验证码点击注册这也是两次请求，这两次请求的数据之间要进行对比，相同则允许注册，不同则拒绝注册，该功能的实现也需要在同一次会话中共享数据。\n     \n     \n\n通过这几个例子的讲解，相信大家对会话追踪技术已经有了一定的理解，该技术在实际开发中也非常重要。那么接下来我们就需要去学习下会话跟踪技术，在学习这些技术之前，我们需要思考:为什么现在浏览器和服务器不支持数据共享呢?\n\n * 浏览器和服务器之间使用的是HTTP请求来进行数据传输\n * HTTP协议是无状态的，每次浏览器向服务器请求时，服务器都会将该请求视为新的请求\n * HTTP协议设计成无状态的目的是让每次请求之间相互独立，互不影响\n * 请求与请求之间独立后，就无法实现多次请求之间的数据共享\n\n分析完具体的原因后，那么该如何实现会话跟踪技术呢? 具体的实现方式有:\n\n(1)客户端会话跟踪技术：Cookie\n\n(2)服务端会话跟踪技术：Session\n\n这两个技术都可以实现会话跟踪，它们之间最大的区别:Cookie是存储在浏览器端而Session是存储在服务器端\n\n具体的学习思路为:\n\n * CooKie的基本使用、原理、使用细节\n * Session的基本使用、原理、使用细节\n * Cookie和Session的综合案例\n\n小结\n\n在这节中，我们主要介绍了下什么是会话和会话跟踪技术，需要注意的是:\n\n * HTTP协议是无状态的，靠HTTP协议是无法实现会话跟踪\n * 想要实现会话跟踪，就需要用到Cookie和Session\n\n这个Cookie和Session具体该如何使用，接下来就先从Cookie来学起。\n\n\n# Cookie\n\n学习Cookie，我们主要解决下面几个问题:\n\n * 什么是Cookie?\n * Cookie如何来使用?\n * Cookie是如何实现的?\n * Cookie的使用注意事项有哪些?\n\n\n# Cookie的基本使用\n\n1.概念\n\nCookie：客户端会话技术，将数据保存到客户端，以后每次请求都携带Cookie数据进行访问。\n\n2.Cookie的工作流程\n\n\n\n * 服务端提供了两个Servlet，分别是ServletA和ServletB\n * 浏览器发送HTTP请求1给服务端，服务端ServletA接收请求并进行业务处理\n * 服务端ServletA在处理的过程中可以创建一个Cookie对象并将name=zs的数据存入Cookie\n * 服务端ServletA在响应数据的时候，会把Cookie对象响应给浏览器\n * 浏览器接收到响应数据，会把Cookie对象中的数据存储在浏览器内存中，此时浏览器和服务端就建立了一次会话\n * 在同一次会话中浏览器再次发送HTTP请求2给服务端ServletB，浏览器会携带Cookie对象中的所有数据\n * ServletB接收到请求和数据后，就可以获取到存储在Cookie对象中的数据，这样同一个会话中的多次请求之间就实现了数据共享\n\n3.Cookie的基本使用\n\n对于Cookie的使用，我们更关注的应该是后台代码如何操作Cookie，对于Cookie的操作主要分两大类，本别是发送Cookie和获取Cookie,对于上面这两块内容，分别该如何实现呢?\n\n3.1 发送Cookie\n\n * 创建Cookie对象，并设置数据\n\nCookie cookie = new Cookie("key","value");\n\n\n * 发送Cookie到客户端：使用response对象\n\nresponse.addCookie(cookie);\n\n\n介绍完发送Cookie对应的步骤后，接下面通过一个案例来完成Cookie的发送，具体实现步骤为:\n\n> 需求:在Servlet中生成Cookie对象并存入数据，然后将数据发送给浏览器\n> \n> 1.创建Maven项目,项目名称为cookie-demo，并在pom.xml添加依赖\n> \n> 2.编写Servlet类，名称为AServlet\n> \n> 3.在AServlet中创建Cookie对象，存入数据，发送给前端\n> \n> 4.启动测试，在浏览器查看Cookie对象中的值\n\n(1)创建Maven项目cookie-demo，并在pom.xml添加依赖\n\n<properties>\n    <maven.compiler.source>8</maven.compiler.source>\n    <maven.compiler.target>8</maven.compiler.target>\n</properties>\n\n<dependencies>\n    \x3c!--servlet--\x3e\n    <dependency>\n        <groupId>javax.servlet</groupId>\n        <artifactId>javax.servlet-api</artifactId>\n        <version>3.1.0</version>\n        <scope>provided</scope>\n    </dependency>\n    \x3c!--jsp--\x3e\n    <dependency>\n        <groupId>javax.servlet.jsp</groupId>\n        <artifactId>jsp-api</artifactId>\n        <version>2.2</version>\n        <scope>provided</scope>\n    </dependency>\n    \x3c!--jstl--\x3e\n    <dependency>\n        <groupId>jstl</groupId>\n        <artifactId>jstl</artifactId>\n        <version>1.2</version>\n    </dependency>\n    <dependency>\n        <groupId>taglibs</groupId>\n        <artifactId>standard</artifactId>\n        <version>1.1.2</version>\n    </dependency>\n</dependencies>\n<build>\n    <plugins>\n        <plugin>\n            <groupId>org.apache.tomcat.maven</groupId>\n            <artifactId>tomcat7-maven-plugin</artifactId>\n            <version>2.2</version>\n        </plugin>\n    </plugins>\n</build>\n\n\n(2)编写Servlet类，名称为AServlet\n\n@WebServlet("/aServlet")\npublic class AServlet extends HttpServlet {\n    @Override\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n\n    }\n\n    @Override\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        this.doGet(request, response);\n    }\n}\n\n\n(3)在Servlet中创建Cookie对象，存入数据，发送给前端\n\n@WebServlet("/aServlet")\npublic class AServlet extends HttpServlet {\n    @Override\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        //发送Cookie\n        //1. 创建Cookie对象\n        Cookie cookie = new Cookie("username","zs");\n        //2. 发送Cookie，response\n        response.addCookie(cookie);\n    }\n\n    @Override\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        this.doGet(request, response);\n    }\n}\n\n\n（4）启动测试，在浏览器查看Cookie对象中的值\n\n访问http://localhost:8080/cookie-demo/aServlet\n\nchrome浏览器查看Cookie的值，有两种方式,分布式:\n\n方式一:\n\n\n\n方式二:选中打开开发者工具或者 使用快捷键F12 或者 Ctrl+Shift+I\n\n\n\n3.2 获取Cookie\n\n * 获取客户端携带的所有Cookie，使用request对象\n\nCookie[] cookies = request.getCookies();\n\n\n * 遍历数组，获取每一个Cookie对象：for\n * 使用Cookie对象方法获取数据\n\ncookie.getName();\ncookie.getValue();\n\n\n介绍完获取Cookie对应的步骤后，接下面再通过一个案例来完成Cookie的获取，具体实现步骤为:\n\n> 需求:在Servlet中获取前一个案例存入在Cookie对象中的数据\n> \n> 1.编写一个新Servlet类，名称为BServlet\n> \n> 2.在BServlet中使用request对象获取Cookie数组，遍历数组，从数据中获取指定名称对应的值\n> \n> 3.启动测试，在控制台打印出获取的值\n\n(1)编写一个新Servlet类，名称为BServlet\n\n@WebServlet("/bServlet")\npublic class BServlet extends HttpServlet {\n    @Override\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n\n    }\n\n    @Override\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        this.doGet(request, response);\n    }\n}\n\n\n（2）在BServlet中使用request对象获取Cookie数组，遍历数组，从数据中获取指定名称对应的值\n\n@WebServlet("/bServlet")\npublic class BServlet extends HttpServlet {\n    @Override\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        //获取Cookie\n        //1. 获取Cookie数组\n        Cookie[] cookies = request.getCookies();\n        //2. 遍历数组\n        for (Cookie cookie : cookies) {\n            //3. 获取数据\n            String name = cookie.getName();\n            if("username".equals(name)){\n                String value = cookie.getValue();\n                System.out.println(name+":"+value);\n                break;\n            }\n        }\n\n    }\n\n    @Override\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        this.doGet(request, response);\n    }\n}\n\n\n（3）启动测试，在控制台打印出获取的值\n\n访问http://localhost:8080/cookie-demo/bServlet\n\n\n\n在IDEA控制台就能看到输出的结果:\n\n\n\n**思考:**测试的时候\n\n * 在访问AServlet和BServlet的中间把关闭浏览器,重启浏览器后访问BServlet能否获取到Cookie中的数据?\n\n这个问题，我们会在Cookie的使用细节中讲，大家可以动手先试下。\n\n小结\n\n在这节中，我们主要讲解了Cookie的基本使用,包含两部分内容\n\n * 发送Cookie:\n   * 创建Cookie对象，并设置值:Cookie cookie = new Cookie("key","value");\n   * 发送Cookie到客户端使用的是Reponse对象:response.addCookie(cookie);\n * 获取Cookie:\n   * 使用Request对象获取Cookie数组:Cookie[] cookies = request.getCookies();\n   * 遍历数组\n   * 获取数组中每个Cookie对象的值:cookie.getName()和cookie.getValue()\n\n介绍完Cookie的基本使用之后，那么Cookie的底层到底是如何实现一次会话两次请求之间的数据共享呢?\n\n\n# Cookie的原理分析\n\n对于Cookie的实现原理是基于HTTP协议的,其中设计到HTTP协议中的两个请求头信息:\n\n * 响应头:set-cookie\n * 请求头: cookie\n\n\n\n * 前面的案例中已经能够实现，AServlet给前端发送Cookie,BServlet从request中获取Cookie的功能\n * 对于AServlet响应数据的时候，Tomcat服务器都是基于HTTP协议来响应数据\n * 当Tomcat发现后端要返回的是一个Cookie对象之后，Tomcat就会在响应头中添加一行数据**Set-Cookie:username=zs**\n * 浏览器获取到响应结果后，从响应头中就可以获取到Set-Cookie对应值username=zs,并将数据存储在浏览器的内存中\n * 浏览器再次发送请求给BServlet的时候，浏览器会自动在请求头中添加**Cookie: username=zs**发送给服务端BServlet\n * Request对象会把请求头中cookie对应的值封装成一个个Cookie对象，最终形成一个数组\n * BServlet通过Request对象获取到Cookie[]后，就可以从中获取自己需要的数据\n\n接下来，使用刚才的案例，把上述结论验证下:\n\n(1)访问AServlet对应的地址http://localhost:8080/cookie-demo/aServlet\n\n使用Chrom浏览器打开开发者工具(F12或Crtl+Shift+I)进行查看响应头中的数据\n\n\n\n（2）访问BServlet对应的地址`http://localhost:8080/cookie-demo/bServlet\n\n使用Chrom浏览器打开开发者工具(F12或Crtl+Shift+I)进行查看请求头中的数据\n\n\n\n\n# Cookie的使用细节\n\n在这节我们主要讲解两个知识，第一个是Cookie的存活时间，第二个是Cookie如何存储中文，首先来学习下Cookie的存活时间。\n\n# Cookie的存活时间\n\n前面让大家思考过一个问题:\n\n\n\n(1)浏览器发送请求给AServlet,AServlet会响应一个存有usernanme=zs的Cookie对象给浏览器\n\n(2)浏览器接收到响应数据将cookie存入到浏览器内存中\n\n(3)当浏览器再次发送请求给BServlet,BServlet就可以使用Request对象获取到Cookie数据\n\n(4)在发送请求到BServlet之前，如果把浏览器关闭再打开进行访问，BServlet能否获取到Cookie数据?\n\n注意：浏览器关闭再打开不是指打开一个新的选显卡，而且必须是先关闭再打开，顺序不能变。\n\n针对上面这个问题，通过演示，会发现，BServlet中无法再获取到Cookie数据，这是为什么呢?\n\n * 默认情况下，Cookie存储在浏览器内存中，当浏览器关闭，内存释放，则Cookie被销毁\n\n这个结论就印证了上面的演示效果，但是如果使用这种默认情况下的Cookie,有些需求就无法实现，比如:\n\n\n\n上面这个网站的登录页面上有一个记住我的功能，这个功能大家都比较熟悉\n\n * 第一次输入用户名和密码并勾选记住我然后进行登录\n * 下次再登陆的时候，用户名和密码就会被自动填充，不需要再重新输入登录\n * 比如记住我这个功能需要记住用户名和密码一个星期，那么使用默认情况下的Cookie就会出现问题\n * 因为默认情况，浏览器一关，Cookie就会从浏览器内存中删除，对于记住我功能就无法实现\n\n所以我们现在就遇到一个难题是如何将Cookie持久化存储?\n\nCookie其实已经为我们提供好了对应的API来完成这件事，这个API就是setMaxAge,\n\n * 设置Cookie存活时间\n\nsetMaxAge(int seconds)\n\n\n参数值为:\n\n1.正数：将Cookie写入浏览器所在电脑的硬盘，持久化存储。到时间自动删除\n\n2.负数：默认值，Cookie在当前浏览器内存中，当浏览器关闭，则Cookie被销毁\n\n3.零：删除对应Cookie\n\n接下来，咱们就在AServlet中去设置Cookie的存活时间。\n\n@WebServlet("/aServlet")\npublic class AServlet extends HttpServlet {\n    @Override\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        //发送Cookie\n        //1. 创建Cookie对象\n        Cookie cookie = new Cookie("username","zs");\n        //设置存活时间   ，1周 7天\n        cookie.setMaxAge(60*60*24*7); //易阅读，需程序计算\n\t\t//cookie.setMaxAge(604800); //不易阅读(可以使用注解弥补)，程序少进行一次计算\n        //2. 发送Cookie，response\n        response.addCookie(cookie);\n    }\n\n    @Override\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        this.doGet(request, response);\n    }\n}\n\n\n修改完代码后，启动测试，访问http://localhost:8080/cookie-demo/aServlet\n\n * 访问一个AServlet后，把浏览器关闭重启后，再去访问http://localhost:8080/cookie-demo/bServet,能在控制台打印出username:zs,说明Cookie没有随着浏览器关闭而被销毁\n * 通过浏览器查看Cookie的内容，会发现Cookie的相关信息\n\n\n\n# Cookie存储中文\n\n首先，先来演示一个效果，将之前username=zs的值改成username=张三，把汉字张三存入到Cookie中，看是什么效果:\n\n@WebServlet("/aServlet")\npublic class AServlet extends HttpServlet {\n    @Override\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n \t\t//发送Cookie\n        String value = "张三";\n        Cookie cookie = new Cookie("username",value);\n        //设置存活时间   ，1周 7天\n        cookie.setMaxAge(60*60*24*7);\n        //2. 发送Cookie，response\n        response.addCookie(cookie);\n    }\n\n    @Override\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        this.doGet(request, response);\n    }\n}\n\n\n启动访问测试，访问http://localhost:8080/cookie-demo/aServlet会发现浏览器会提示错误信息\n\n\n\n通过上面的案例演示，我们得到一个结论:\n\n * Cookie不能直接存储中文\n\nCookie不能存储中文，但是如果有这方面的需求，这个时候该如何解决呢?\n\n这个时候，我们可以使用之前学过的一个知识点叫URL编码，所以如果需要存储中文，就需要进行转码，具体的实现思路为:\n\n> 1.在AServlet中对中文进行URL编码，采用URLEncoder.encode()，将编码后的值存入Cookie中\n> \n> 2.在BServlet中获取Cookie中的值,获取的值为URL编码后的值\n> \n> 3.将获取的值在进行URL解码,采用URLDecoder.decode()，就可以获取到对应的中文值\n\n(1)在AServlet中对中文进行URL编码\n\n@WebServlet("/aServlet")\npublic class AServlet extends HttpServlet {\n    @Override\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        //发送Cookie\n        String value = "张三";\n        //对中文进行URL编码\n        value = URLEncoder.encode(value, "UTF-8");\n        System.out.println("存储数据："+value);\n        //将编码后的值存入Cookie中\n        Cookie cookie = new Cookie("username",value);\n        //设置存活时间   ，1周 7天\n        cookie.setMaxAge(60*60*24*7);\n        //2. 发送Cookie，response\n        response.addCookie(cookie);\n    }\n\n    @Override\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        this.doGet(request, response);\n    }\n}\n\n\n(2)在BServlet中获取值，并对值进行解码\n\n@WebServlet("/bServlet")\npublic class BServlet extends HttpServlet {\n    @Override\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        //获取Cookie\n        //1. 获取Cookie数组\n        Cookie[] cookies = request.getCookies();\n        //2. 遍历数组\n        for (Cookie cookie : cookies) {\n            //3. 获取数据\n            String name = cookie.getName();\n            if("username".equals(name)){\n                String value = cookie.getValue();//获取的是URL编码后的值 %E5%BC%A0%E4%B8%89\n                //URL解码\n                value = URLDecoder.decode(value,"UTF-8");\n                System.out.println(name+":"+value);//value解码后为 张三\n                break;\n            }\n        }\n\n    }\n\n    @Override\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        this.doGet(request, response);\n    }\n}\n\n\n至此，我们就可以将中文存入Cookie中进行使用。\n\n小结\n\nCookie的使用细节中，我们讲了Cookie的存活时间和存储中文:\n\n * 存活时间，需要掌握setMaxAage()API的使用\n\n * 存储中文，需要掌握URL编码和解码的使用\n\n\n# Session\n\nCookie已经能完成一次会话多次请求之间的数据共享，之前我们还提到过Session也可以实现，那么:\n\n * 什么是Session?\n * Session如何来使用?\n * Session是如何实现的?\n * Session的使用注意事项有哪些?\n\n\n# Session的基本使用\n\n1.概念\n\nSession：服务端会话跟踪技术：将数据保存到服务端。\n\n * Session是存储在服务端而Cookie是存储在客户端\n * 存储在客户端的数据容易被窃取和截获，存在很多不安全的因素\n * 存储在服务端的数据相比于客户端来说就更安全\n\n2.Session的工作流程\n\n\n\n * 在服务端的AServlet获取一个Session对象，把数据存入其中\n * 在服务端的BServlet获取到相同的Session对象，从中取出数据\n * 就可以实现一次会话中多次请求之间的数据共享了\n * 现在最大的问题是如何保证AServlet和BServlet使用的是同一个Session对象(在原理分析会讲解)?\n\n3.Session的基本使用\n\n在JavaEE中提供了HttpSession接口，来实现一次会话的多次请求之间数据共享功能。\n\n具体的使用步骤为:\n\n * 获取Session对象,使用的是request对象\n\nHttpSession session = request.getSession();\n\n\n * Session对象提供的功能:\n   \n   * 存储数据到 session 域中\n     \n     void setAttribute(String name, Object o)\n     \n   \n   * 根据 key，获取值\n     \n     Object getAttribute(String name)\n     \n   \n   * 根据 key，删除该键值对\n     \n     void removeAttribute(String name)\n     \n\n介绍完Session相关的API后，接下来通过一个案例来完成对Session的使用，具体实现步骤为:\n\n> 需求:在一个Servlet中往Session中存入数据，在另一个Servlet中获取Session中存入的数据\n> \n> 1.创建名为SessionDemo1的Servlet类\n> \n> 2.创建名为SessionDemo2的Servlet类\n> \n> 3.在SessionDemo1的方法中:获取Session对象、存储数据\n> \n> 4.在SessionDemo2的方法中:获取Session对象、获取数据\n> \n> 5.启动测试\n\n(1)创建名为SessionDemo1的Servlet类\n\n@WebServlet("/demo1")\npublic class SessionDemo1 extends HttpServlet {\n    @Override\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n    \n    }\n\n    @Override\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        this.doGet(request, response);\n    }\n}\n\n\n(2)创建名为SessionDemo2的Servlet类\n\n@WebServlet("/demo2")\npublic class SessionDemo2 extends HttpServlet {\n    @Override\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n    \n    }\n\n    @Override\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        this.doGet(request, response);\n    }\n}\n\n\n(3)SessionDemo1:获取Session对象、存储数据\n\n@WebServlet("/demo1")\npublic class SessionDemo1 extends HttpServlet {\n    @Override\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n    \t//存储到Session中\n        //1. 获取Session对象\n        HttpSession session = request.getSession();\n        //2. 存储数据\n        session.setAttribute("username","zs");\n    }\n\n    @Override\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        this.doGet(request, response);\n    }\n}\n\n\n(4)SessionDemo2:获取Session对象、获取数据\n\n@WebServlet("/demo2")\npublic class SessionDemo2 extends HttpServlet {\n    @Override\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        //获取数据，从session中\n        //1. 获取Session对象\n        HttpSession session = request.getSession();\n        //2. 获取数据\n        Object username = session.getAttribute("username");\n        System.out.println(username);\n    }\n\n    @Override\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        this.doGet(request, response);\n    }\n}\n\n\n(5)启动测试，\n\n * 先访问http://localhost:8080/cookie-demo/demo1,将数据存入Session\n * 在访问http://localhost:8080/cookie-demo/demo2,从Session中获取数据\n * 查看控制台\n\n\n\n通过案例的效果，能看到Session是能够在一次会话中两次请求之间共享数据。\n\n小结\n\n至此Session的基本使用就已经完成了，重点要掌握的是:\n\n * Session的获取\n   \n   HttpSession session = request.getSession();\n   \n\n * Session常用方法的使用\n   \n   void setAttribute(String name, Object o)\n   Object getAttribute(String name)\n   \n   \n   **注意:**Session中可以存储的是一个Object类型的数据，也就是说Session中可以存储任意数据类型。\n\n介绍完Session的基本使用之后，那么Session的底层到底是如何实现一次会话两次请求之间的数据共享呢?\n\n\n# Session的原理分析\n\n * Session是基于Cookie实现的\n\n这句话其实不太能详细的说明Session的底层实现，接下来，咱们一步步来分析下Session的具体实现原理:\n\n(1)前提条件\n\n\n\nSession要想实现一次会话多次请求之间的数据共享，就必须要保证多次请求获取Session的对象是同一个。\n\n那么它们是一个对象么？要验证这个结论也很简单，只需要在上面案例中的两个Servlet中分别打印下Session对象\n\nSessionDemo1\n\n@WebServlet("/demo1")\npublic class SessionDemo1 extends HttpServlet {\n    @Override\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n    \t//存储到Session中\n        //1. 获取Session对象\n        HttpSession session = request.getSession();\n        System.out.println(session);\n        //2. 存储数据\n        session.setAttribute("username","zs");\n    }\n\n    @Override\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        this.doGet(request, response);\n    }\n}\n\n\nSessionDemo2\n\n@WebServlet("/demo2")\npublic class SessionDemo2 extends HttpServlet {\n    @Override\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        //获取数据，从session中\n        //1. 获取Session对象\n        HttpSession session = request.getSession();\n        System.out.println(session);\n        //2. 获取数据\n        Object username = session.getAttribute("username");\n        System.out.println(username);\n    }\n\n    @Override\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        this.doGet(request, response);\n    }\n}\n\n\n启动测试，分别访问\n\nhttp://localhost:8080/cookie-demo/demo1\n\nhttp://localhost:8080/cookie-demo/demo2\n\n\n\n通过打印可以得到如下结论:\n\n * 两个Servlet类中获取的Session对象是同一个\n * 把demo1和demo2请求刷新多次，控制台最终打印的结果都是同一个\n\n那么问题又来了，如果新开一个浏览器，访问demo1或者demo2,打印在控制台的Session还是同一个对象么?\n\n\n\n注意:在一台电脑上演示的时候，如果是相同的浏览器必须要把浏览器全部关掉重新打开，才算新开的一个浏览器。\n\n当然也可以使用不同的浏览器进行测试，就不需要把之前的浏览器全部关闭。\n\n测试的结果：如果是不同浏览器或者重新打开浏览器后，打印的Session就不一样了。\n\n所以Session实现的也是一次会话中的多次请求之间的数据共享。\n\n那么最主要的问题就来了，Session是如何保证在一次会话中获取的Session对象是同一个呢?\n\n\n\n(1)demo1在第一次获取session对象的时候，session对象会有一个唯一的标识，假如是id:10\n\n(2)demo1在session中存入其他数据并处理完成所有业务后，需要通过Tomcat服务器响应结果给浏览器\n\n(3)Tomcat服务器发现业务处理中使用了session对象，就会把session的唯一标识id:10当做一个cookie，添加Set-Cookie:JESSIONID=10到响应头中，并响应给浏览器\n\n(4)浏览器接收到响应结果后，会把响应头中的coookie数据存储到浏览器的内存中\n\n(5)浏览器在同一会话中访问demo2的时候，会把cookie中的数据按照cookie: JESSIONID=10的格式添加到请求头中并发送给服务器Tomcat\n\n(6)demo2获取到请求后，从请求头中就读取cookie中的JSESSIONID值为10，然后就会到服务器内存中寻找id:10的session对象，如果找到了，就直接返回该对象，如果没有则新创建一个session对象\n\n(7)关闭打开浏览器后，因为浏览器的cookie已被销毁，所以就没有JESSIONID的数据，服务端获取到的session就是一个全新的session对象\n\n至此，Session是基于Cookie来实现的这就话，我们就解释完了，接下来通过实例来演示下:\n\n(1)使用chrome浏览器访问http://localhost:8080/cookie-demo/demo1,打开开发者模式(F12或Ctrl+Shift+I),查看**响应头(Response Headers)**数据:\n\n\n\n(2)使用chrome浏览器再次访问http://localhost:8080/cookie-demo/demo2，查看**请求头(Request Headers)**数据:\n\n\n\n小结\n\n介绍完Session的原理，我们只需要记住\n\n * Session是基于Cookie来实现的\n\n\n# Session的使用细节\n\n这节我们会主要讲解两个知识，第一个是Session的钝化和活化，第二个是Session的销毁，首先来学习什么是Session的钝化和活化？\n\n# Session钝化与活化\n\n首先需要大家思考的问题是:\n\n * 服务器重启后，Session中的数据是否还在?\n\n要想回答这个问题，我们可以先看下下面这幅图，\n\n\n\n(1)服务器端AServlet和BServlet共用的session对象应该是存储在服务器的内存中\n\n(2)服务器重新启动后，内存中的数据应该是已经被释放，对象也应该都销毁了\n\n所以session数据应该也已经不存在了。但是如果session不存在会引发什么问题呢?\n\n举个例子说明下，\n\n(1)用户把需要购买的商品添加到购物车，因为要实现同一个会话多次请求数据共享，所以假设把数据存入Session对象中\n\n(2)用户正要付钱的时候接到一个电话，付钱的动作就搁浅了\n\n(3)正在用户打电话的时候，购物网站因为某些原因需要重启\n\n(4)重启后session数据被销毁，购物车中的商品信息也就会随之而消失\n\n(5)用户想再次发起支付，就会出为问题\n\n所以说对于session的数据，我们应该做到就算服务器重启了，也应该能把数据保存下来才对。\n\n分析了这么多，那么Tomcat服务器在重启的时候，session数据到底会不会保存以及是如何保存的，我们可以通过实际案例来演示下:\n\n注意:这里所说的关闭和启动应该要确保是正常的关闭和启动。\n\n那如何才是正常关闭Tomcat服务器呢?\n\n需要使用命令行的方式来启动和停止Tomcat服务器:\n\n启动:进入到项目pom.xml所在目录，执行tomcat7:run\n\n\n\n停止:在启动的命令行界面，输入ctrl+c\n\n\n\n有了上述两个正常启动和关闭的方式后，接下来的测试流程是:\n\n(1)先启动Tomcat服务器\n\n(2)访问http://localhost:8080/cookie-demo/demo1将数据存入session中\n\n(3)正确停止Tomcat服务器\n\n(4)再次重新启动Tomcat服务器\n\n(5)访问http://localhost:8080/cookie-demo/demo2 查看是否能获取到session中的数据\n\n\n\n经过测试，会发现只要服务器是正常关闭和启动，session中的数据是可以被保存下来的。\n\n那么Tomcat服务器到底是如何做到的呢?\n\n具体的原因就是:Session的钝化和活化:\n\n * 钝化：在服务器正常关闭后，Tomcat会自动将Session数据写入硬盘的文件中\n   \n   * 钝化的数据路径为:项目目录\\target\\tomcat\\work\\Tomcat\\localhost\\项目名称\\SESSIONS.ser\n     \n     \n\n * 活化：再次启动服务器后，从文件中加载数据到Session中\n   \n   * 数据加载到Session中后，路径中的SESSIONS.ser文件会被删除掉\n\n对于上述的整个过程，大家只需要了解下即可。因为所有的过程都是Tomcat自己完成的，不需要我们参与。\n\n小结\n\nSession的钝化和活化介绍完后，需要我们注意的是:\n\n * session数据存储在服务端，服务器重启后，session数据会被保存\n\n * 浏览器被关闭启动后，重新建立的连接就已经是一个全新的会话，获取的session数据也是一个新的对象\n\n * session的数据要想共享，浏览器不能关闭，所以session数据不能长期保存数据\n\n * cookie是存储在客户端，是可以长期保存\n\n# Session销毁\n\nsession的销毁会有两种方式:\n\n * 默认情况下，无操作，30分钟自动销毁\n   \n   * 对于这个失效时间，是可以通过配置进行修改的\n     \n     * 在项目的web.xml中配置\n       \n       <?xml version="1.0" encoding="UTF-8"?>\n       <web-app xmlns="http://xmlns.jcp.org/xml/ns/javaee"\n                xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n                xsi:schemaLocation="http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd"\n                version="3.1">\n       \n           <session-config>\n               <session-timeout>100</session-timeout>\n           </session-config>\n       </web-app>\n       \n     \n     * 如果没有配置，默认是30分钟，默认值是在Tomcat的web.xml配置文件中写死的\n       \n       \n\n * 调用Session对象的invalidate()进行销毁\n   \n   * 在SessionDemo2类中添加session销毁的方法\n     \n     @WebServlet("/demo2")\n     public class SessionDemo2 extends HttpServlet {\n         @Override\n         protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n             //获取数据，从session中\n     \n             //1. 获取Session对象\n             HttpSession session = request.getSession();\n             System.out.println(session);\n     \n             // 销毁\n             session.invalidate();\n             //2. 获取数据\n             Object username = session.getAttribute("username");\n             System.out.println(username);\n         }\n     \n         @Override\n         protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n             this.doGet(request, response);\n         }\n     }\n     \n   \n   * 启动访问测试，先访问demo1将数据存入到session，再次访问demo2从session中获取数据\n     \n     \n   \n   * 该销毁方法一般会在用户退出的时候，需要将session销毁掉。\n\nCookie和Session小结\n\n * Cookie 和 Session 都是来完成一次会话内多次请求间数据共享的。\n\n所需两个对象放在一块，就需要思考:\n\nCookie和Session的区别是什么?\n\nCookie和Session的应用场景分别是什么?\n\n * 区别:\n   * 存储位置：Cookie 是将数据存储在客户端，Session 将数据存储在服务端\n   * 安全性：Cookie不安全，Session安全\n   * 数据大小：Cookie最大3KB，Session无大小限制\n   * 存储时间：Cookie可以通过setMaxAge()长期存储，Session默认30分钟\n   * 服务器性能：Cookie不占服务器资源，Session占用服务器资源\n * 应用场景:\n   * 购物车:使用Cookie来存储\n   * 以登录用户的名称展示:使用Session来存储\n   * 记住我功能:使用Cookie来存储\n   * 验证码:使用session来存储\n * 结论\n   * Cookie是用来保证用户在未登录情况下的身份识别\n   * Session是用来保存用户登录后的数据\n\n介绍完Cookie和Session以后，具体用哪个还是需要根据具体的业务进行具体分析。',normalizedContent:'# 会话技术\n\n\n# 会话跟踪技术的概述\n\n对于会话跟踪这四个词，我们需要拆开来进行解释，首先要理解什么是会话，然后再去理解什么是会话跟踪:\n\n * 会话:用户打开浏览器，访问web服务器的资源，会话建立，直到有一方断开连接，会话结束。在一次会话中可以包含多次请求和响应。\n   \n   * 从浏览器发出请求到服务端响应数据给前端之后，一次会话(在浏览器和服务器之间)就被建立了\n   * 会话被建立后，如果浏览器或服务端都没有被关闭，则会话就会持续建立着\n   * 浏览器和服务器就可以继续使用该会话进行请求发送和响应，上述的整个过程就被称之为会话。\n   \n   用实际场景来理解下会话，比如在我们访问京东的时候，当打开浏览器进入京东首页后，浏览器和京东的服务器之间就建立了一次会话，后面的搜索商品,查看商品的详情,加入购物车等都是在这一次会话中完成。\n   \n   思考:下图中总共建立了几个会话?\n   \n   \n   \n   每个浏览器都会与服务端建立了一个会话，加起来总共是3个会话。\n\n * 会话跟踪:一种维护浏览器状态的方法，服务器需要识别多次请求是否来自于同一浏览器，以便在同一次会话的多次请求间共享数据。\n   \n   * 服务器会收到多个请求，这多个请求可能来自多个浏览器，如上图中的6个请求来自3个浏览器\n   * 服务器需要用来识别请求是否来自同一个浏览器\n   * 服务器用来识别浏览器的过程，这个过程就是会话跟踪\n   * 服务器识别浏览器后就可以在同一个会话中多次请求之间来共享数据\n   \n   那么我们又有一个问题需要思考，一个会话中的多次请求为什么要共享数据呢?有了这个数据共享功能后能实现哪些功能呢?\n   \n   * 购物车: 加入购物车和去购物车结算是两次请求，但是后面这次请求要想展示前一次请求所添加的商品，就需要用到数据共享。\n     \n     \n   \n   * 页面展示用户登录信息:很多网站，登录后访问多个功能发送多次请求后，浏览器上都会有当前登录用户的信息[用户名]，比如百度、京东、码云等。\n     \n     \n   \n   * 网站登录页面的记住我功能:当用户登录成功后，勾选记住我按钮后下次再登录的时候，网站就会自动填充用户名和密码，简化用户的登录操作，多次登录就会有多次请求，他们之间也涉及到共享数据\n     \n     \n   \n   * 登录页面的验证码功能:生成验证码和输入验证码点击注册这也是两次请求，这两次请求的数据之间要进行对比，相同则允许注册，不同则拒绝注册，该功能的实现也需要在同一次会话中共享数据。\n     \n     \n\n通过这几个例子的讲解，相信大家对会话追踪技术已经有了一定的理解，该技术在实际开发中也非常重要。那么接下来我们就需要去学习下会话跟踪技术，在学习这些技术之前，我们需要思考:为什么现在浏览器和服务器不支持数据共享呢?\n\n * 浏览器和服务器之间使用的是http请求来进行数据传输\n * http协议是无状态的，每次浏览器向服务器请求时，服务器都会将该请求视为新的请求\n * http协议设计成无状态的目的是让每次请求之间相互独立，互不影响\n * 请求与请求之间独立后，就无法实现多次请求之间的数据共享\n\n分析完具体的原因后，那么该如何实现会话跟踪技术呢? 具体的实现方式有:\n\n(1)客户端会话跟踪技术：cookie\n\n(2)服务端会话跟踪技术：session\n\n这两个技术都可以实现会话跟踪，它们之间最大的区别:cookie是存储在浏览器端而session是存储在服务器端\n\n具体的学习思路为:\n\n * cookie的基本使用、原理、使用细节\n * session的基本使用、原理、使用细节\n * cookie和session的综合案例\n\n小结\n\n在这节中，我们主要介绍了下什么是会话和会话跟踪技术，需要注意的是:\n\n * http协议是无状态的，靠http协议是无法实现会话跟踪\n * 想要实现会话跟踪，就需要用到cookie和session\n\n这个cookie和session具体该如何使用，接下来就先从cookie来学起。\n\n\n# cookie\n\n学习cookie，我们主要解决下面几个问题:\n\n * 什么是cookie?\n * cookie如何来使用?\n * cookie是如何实现的?\n * cookie的使用注意事项有哪些?\n\n\n# cookie的基本使用\n\n1.概念\n\ncookie：客户端会话技术，将数据保存到客户端，以后每次请求都携带cookie数据进行访问。\n\n2.cookie的工作流程\n\n\n\n * 服务端提供了两个servlet，分别是servleta和servletb\n * 浏览器发送http请求1给服务端，服务端servleta接收请求并进行业务处理\n * 服务端servleta在处理的过程中可以创建一个cookie对象并将name=zs的数据存入cookie\n * 服务端servleta在响应数据的时候，会把cookie对象响应给浏览器\n * 浏览器接收到响应数据，会把cookie对象中的数据存储在浏览器内存中，此时浏览器和服务端就建立了一次会话\n * 在同一次会话中浏览器再次发送http请求2给服务端servletb，浏览器会携带cookie对象中的所有数据\n * servletb接收到请求和数据后，就可以获取到存储在cookie对象中的数据，这样同一个会话中的多次请求之间就实现了数据共享\n\n3.cookie的基本使用\n\n对于cookie的使用，我们更关注的应该是后台代码如何操作cookie，对于cookie的操作主要分两大类，本别是发送cookie和获取cookie,对于上面这两块内容，分别该如何实现呢?\n\n3.1 发送cookie\n\n * 创建cookie对象，并设置数据\n\ncookie cookie = new cookie("key","value");\n\n\n * 发送cookie到客户端：使用response对象\n\nresponse.addcookie(cookie);\n\n\n介绍完发送cookie对应的步骤后，接下面通过一个案例来完成cookie的发送，具体实现步骤为:\n\n> 需求:在servlet中生成cookie对象并存入数据，然后将数据发送给浏览器\n> \n> 1.创建maven项目,项目名称为cookie-demo，并在pom.xml添加依赖\n> \n> 2.编写servlet类，名称为aservlet\n> \n> 3.在aservlet中创建cookie对象，存入数据，发送给前端\n> \n> 4.启动测试，在浏览器查看cookie对象中的值\n\n(1)创建maven项目cookie-demo，并在pom.xml添加依赖\n\n<properties>\n    <maven.compiler.source>8</maven.compiler.source>\n    <maven.compiler.target>8</maven.compiler.target>\n</properties>\n\n<dependencies>\n    \x3c!--servlet--\x3e\n    <dependency>\n        <groupid>javax.servlet</groupid>\n        <artifactid>javax.servlet-api</artifactid>\n        <version>3.1.0</version>\n        <scope>provided</scope>\n    </dependency>\n    \x3c!--jsp--\x3e\n    <dependency>\n        <groupid>javax.servlet.jsp</groupid>\n        <artifactid>jsp-api</artifactid>\n        <version>2.2</version>\n        <scope>provided</scope>\n    </dependency>\n    \x3c!--jstl--\x3e\n    <dependency>\n        <groupid>jstl</groupid>\n        <artifactid>jstl</artifactid>\n        <version>1.2</version>\n    </dependency>\n    <dependency>\n        <groupid>taglibs</groupid>\n        <artifactid>standard</artifactid>\n        <version>1.1.2</version>\n    </dependency>\n</dependencies>\n<build>\n    <plugins>\n        <plugin>\n            <groupid>org.apache.tomcat.maven</groupid>\n            <artifactid>tomcat7-maven-plugin</artifactid>\n            <version>2.2</version>\n        </plugin>\n    </plugins>\n</build>\n\n\n(2)编写servlet类，名称为aservlet\n\n@webservlet("/aservlet")\npublic class aservlet extends httpservlet {\n    @override\n    protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n\n    }\n\n    @override\n    protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        this.doget(request, response);\n    }\n}\n\n\n(3)在servlet中创建cookie对象，存入数据，发送给前端\n\n@webservlet("/aservlet")\npublic class aservlet extends httpservlet {\n    @override\n    protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        //发送cookie\n        //1. 创建cookie对象\n        cookie cookie = new cookie("username","zs");\n        //2. 发送cookie，response\n        response.addcookie(cookie);\n    }\n\n    @override\n    protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        this.doget(request, response);\n    }\n}\n\n\n（4）启动测试，在浏览器查看cookie对象中的值\n\n访问http://localhost:8080/cookie-demo/aservlet\n\nchrome浏览器查看cookie的值，有两种方式,分布式:\n\n方式一:\n\n\n\n方式二:选中打开开发者工具或者 使用快捷键f12 或者 ctrl+shift+i\n\n\n\n3.2 获取cookie\n\n * 获取客户端携带的所有cookie，使用request对象\n\ncookie[] cookies = request.getcookies();\n\n\n * 遍历数组，获取每一个cookie对象：for\n * 使用cookie对象方法获取数据\n\ncookie.getname();\ncookie.getvalue();\n\n\n介绍完获取cookie对应的步骤后，接下面再通过一个案例来完成cookie的获取，具体实现步骤为:\n\n> 需求:在servlet中获取前一个案例存入在cookie对象中的数据\n> \n> 1.编写一个新servlet类，名称为bservlet\n> \n> 2.在bservlet中使用request对象获取cookie数组，遍历数组，从数据中获取指定名称对应的值\n> \n> 3.启动测试，在控制台打印出获取的值\n\n(1)编写一个新servlet类，名称为bservlet\n\n@webservlet("/bservlet")\npublic class bservlet extends httpservlet {\n    @override\n    protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n\n    }\n\n    @override\n    protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        this.doget(request, response);\n    }\n}\n\n\n（2）在bservlet中使用request对象获取cookie数组，遍历数组，从数据中获取指定名称对应的值\n\n@webservlet("/bservlet")\npublic class bservlet extends httpservlet {\n    @override\n    protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        //获取cookie\n        //1. 获取cookie数组\n        cookie[] cookies = request.getcookies();\n        //2. 遍历数组\n        for (cookie cookie : cookies) {\n            //3. 获取数据\n            string name = cookie.getname();\n            if("username".equals(name)){\n                string value = cookie.getvalue();\n                system.out.println(name+":"+value);\n                break;\n            }\n        }\n\n    }\n\n    @override\n    protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        this.doget(request, response);\n    }\n}\n\n\n（3）启动测试，在控制台打印出获取的值\n\n访问http://localhost:8080/cookie-demo/bservlet\n\n\n\n在idea控制台就能看到输出的结果:\n\n\n\n**思考:**测试的时候\n\n * 在访问aservlet和bservlet的中间把关闭浏览器,重启浏览器后访问bservlet能否获取到cookie中的数据?\n\n这个问题，我们会在cookie的使用细节中讲，大家可以动手先试下。\n\n小结\n\n在这节中，我们主要讲解了cookie的基本使用,包含两部分内容\n\n * 发送cookie:\n   * 创建cookie对象，并设置值:cookie cookie = new cookie("key","value");\n   * 发送cookie到客户端使用的是reponse对象:response.addcookie(cookie);\n * 获取cookie:\n   * 使用request对象获取cookie数组:cookie[] cookies = request.getcookies();\n   * 遍历数组\n   * 获取数组中每个cookie对象的值:cookie.getname()和cookie.getvalue()\n\n介绍完cookie的基本使用之后，那么cookie的底层到底是如何实现一次会话两次请求之间的数据共享呢?\n\n\n# cookie的原理分析\n\n对于cookie的实现原理是基于http协议的,其中设计到http协议中的两个请求头信息:\n\n * 响应头:set-cookie\n * 请求头: cookie\n\n\n\n * 前面的案例中已经能够实现，aservlet给前端发送cookie,bservlet从request中获取cookie的功能\n * 对于aservlet响应数据的时候，tomcat服务器都是基于http协议来响应数据\n * 当tomcat发现后端要返回的是一个cookie对象之后，tomcat就会在响应头中添加一行数据**set-cookie:username=zs**\n * 浏览器获取到响应结果后，从响应头中就可以获取到set-cookie对应值username=zs,并将数据存储在浏览器的内存中\n * 浏览器再次发送请求给bservlet的时候，浏览器会自动在请求头中添加**cookie: username=zs**发送给服务端bservlet\n * request对象会把请求头中cookie对应的值封装成一个个cookie对象，最终形成一个数组\n * bservlet通过request对象获取到cookie[]后，就可以从中获取自己需要的数据\n\n接下来，使用刚才的案例，把上述结论验证下:\n\n(1)访问aservlet对应的地址http://localhost:8080/cookie-demo/aservlet\n\n使用chrom浏览器打开开发者工具(f12或crtl+shift+i)进行查看响应头中的数据\n\n\n\n（2）访问bservlet对应的地址`http://localhost:8080/cookie-demo/bservlet\n\n使用chrom浏览器打开开发者工具(f12或crtl+shift+i)进行查看请求头中的数据\n\n\n\n\n# cookie的使用细节\n\n在这节我们主要讲解两个知识，第一个是cookie的存活时间，第二个是cookie如何存储中文，首先来学习下cookie的存活时间。\n\n# cookie的存活时间\n\n前面让大家思考过一个问题:\n\n\n\n(1)浏览器发送请求给aservlet,aservlet会响应一个存有usernanme=zs的cookie对象给浏览器\n\n(2)浏览器接收到响应数据将cookie存入到浏览器内存中\n\n(3)当浏览器再次发送请求给bservlet,bservlet就可以使用request对象获取到cookie数据\n\n(4)在发送请求到bservlet之前，如果把浏览器关闭再打开进行访问，bservlet能否获取到cookie数据?\n\n注意：浏览器关闭再打开不是指打开一个新的选显卡，而且必须是先关闭再打开，顺序不能变。\n\n针对上面这个问题，通过演示，会发现，bservlet中无法再获取到cookie数据，这是为什么呢?\n\n * 默认情况下，cookie存储在浏览器内存中，当浏览器关闭，内存释放，则cookie被销毁\n\n这个结论就印证了上面的演示效果，但是如果使用这种默认情况下的cookie,有些需求就无法实现，比如:\n\n\n\n上面这个网站的登录页面上有一个记住我的功能，这个功能大家都比较熟悉\n\n * 第一次输入用户名和密码并勾选记住我然后进行登录\n * 下次再登陆的时候，用户名和密码就会被自动填充，不需要再重新输入登录\n * 比如记住我这个功能需要记住用户名和密码一个星期，那么使用默认情况下的cookie就会出现问题\n * 因为默认情况，浏览器一关，cookie就会从浏览器内存中删除，对于记住我功能就无法实现\n\n所以我们现在就遇到一个难题是如何将cookie持久化存储?\n\ncookie其实已经为我们提供好了对应的api来完成这件事，这个api就是setmaxage,\n\n * 设置cookie存活时间\n\nsetmaxage(int seconds)\n\n\n参数值为:\n\n1.正数：将cookie写入浏览器所在电脑的硬盘，持久化存储。到时间自动删除\n\n2.负数：默认值，cookie在当前浏览器内存中，当浏览器关闭，则cookie被销毁\n\n3.零：删除对应cookie\n\n接下来，咱们就在aservlet中去设置cookie的存活时间。\n\n@webservlet("/aservlet")\npublic class aservlet extends httpservlet {\n    @override\n    protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        //发送cookie\n        //1. 创建cookie对象\n        cookie cookie = new cookie("username","zs");\n        //设置存活时间   ，1周 7天\n        cookie.setmaxage(60*60*24*7); //易阅读，需程序计算\n\t\t//cookie.setmaxage(604800); //不易阅读(可以使用注解弥补)，程序少进行一次计算\n        //2. 发送cookie，response\n        response.addcookie(cookie);\n    }\n\n    @override\n    protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        this.doget(request, response);\n    }\n}\n\n\n修改完代码后，启动测试，访问http://localhost:8080/cookie-demo/aservlet\n\n * 访问一个aservlet后，把浏览器关闭重启后，再去访问http://localhost:8080/cookie-demo/bservet,能在控制台打印出username:zs,说明cookie没有随着浏览器关闭而被销毁\n * 通过浏览器查看cookie的内容，会发现cookie的相关信息\n\n\n\n# cookie存储中文\n\n首先，先来演示一个效果，将之前username=zs的值改成username=张三，把汉字张三存入到cookie中，看是什么效果:\n\n@webservlet("/aservlet")\npublic class aservlet extends httpservlet {\n    @override\n    protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n \t\t//发送cookie\n        string value = "张三";\n        cookie cookie = new cookie("username",value);\n        //设置存活时间   ，1周 7天\n        cookie.setmaxage(60*60*24*7);\n        //2. 发送cookie，response\n        response.addcookie(cookie);\n    }\n\n    @override\n    protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        this.doget(request, response);\n    }\n}\n\n\n启动访问测试，访问http://localhost:8080/cookie-demo/aservlet会发现浏览器会提示错误信息\n\n\n\n通过上面的案例演示，我们得到一个结论:\n\n * cookie不能直接存储中文\n\ncookie不能存储中文，但是如果有这方面的需求，这个时候该如何解决呢?\n\n这个时候，我们可以使用之前学过的一个知识点叫url编码，所以如果需要存储中文，就需要进行转码，具体的实现思路为:\n\n> 1.在aservlet中对中文进行url编码，采用urlencoder.encode()，将编码后的值存入cookie中\n> \n> 2.在bservlet中获取cookie中的值,获取的值为url编码后的值\n> \n> 3.将获取的值在进行url解码,采用urldecoder.decode()，就可以获取到对应的中文值\n\n(1)在aservlet中对中文进行url编码\n\n@webservlet("/aservlet")\npublic class aservlet extends httpservlet {\n    @override\n    protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        //发送cookie\n        string value = "张三";\n        //对中文进行url编码\n        value = urlencoder.encode(value, "utf-8");\n        system.out.println("存储数据："+value);\n        //将编码后的值存入cookie中\n        cookie cookie = new cookie("username",value);\n        //设置存活时间   ，1周 7天\n        cookie.setmaxage(60*60*24*7);\n        //2. 发送cookie，response\n        response.addcookie(cookie);\n    }\n\n    @override\n    protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        this.doget(request, response);\n    }\n}\n\n\n(2)在bservlet中获取值，并对值进行解码\n\n@webservlet("/bservlet")\npublic class bservlet extends httpservlet {\n    @override\n    protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        //获取cookie\n        //1. 获取cookie数组\n        cookie[] cookies = request.getcookies();\n        //2. 遍历数组\n        for (cookie cookie : cookies) {\n            //3. 获取数据\n            string name = cookie.getname();\n            if("username".equals(name)){\n                string value = cookie.getvalue();//获取的是url编码后的值 %e5%bc%a0%e4%b8%89\n                //url解码\n                value = urldecoder.decode(value,"utf-8");\n                system.out.println(name+":"+value);//value解码后为 张三\n                break;\n            }\n        }\n\n    }\n\n    @override\n    protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        this.doget(request, response);\n    }\n}\n\n\n至此，我们就可以将中文存入cookie中进行使用。\n\n小结\n\ncookie的使用细节中，我们讲了cookie的存活时间和存储中文:\n\n * 存活时间，需要掌握setmaxaage()api的使用\n\n * 存储中文，需要掌握url编码和解码的使用\n\n\n# session\n\ncookie已经能完成一次会话多次请求之间的数据共享，之前我们还提到过session也可以实现，那么:\n\n * 什么是session?\n * session如何来使用?\n * session是如何实现的?\n * session的使用注意事项有哪些?\n\n\n# session的基本使用\n\n1.概念\n\nsession：服务端会话跟踪技术：将数据保存到服务端。\n\n * session是存储在服务端而cookie是存储在客户端\n * 存储在客户端的数据容易被窃取和截获，存在很多不安全的因素\n * 存储在服务端的数据相比于客户端来说就更安全\n\n2.session的工作流程\n\n\n\n * 在服务端的aservlet获取一个session对象，把数据存入其中\n * 在服务端的bservlet获取到相同的session对象，从中取出数据\n * 就可以实现一次会话中多次请求之间的数据共享了\n * 现在最大的问题是如何保证aservlet和bservlet使用的是同一个session对象(在原理分析会讲解)?\n\n3.session的基本使用\n\n在javaee中提供了httpsession接口，来实现一次会话的多次请求之间数据共享功能。\n\n具体的使用步骤为:\n\n * 获取session对象,使用的是request对象\n\nhttpsession session = request.getsession();\n\n\n * session对象提供的功能:\n   \n   * 存储数据到 session 域中\n     \n     void setattribute(string name, object o)\n     \n   \n   * 根据 key，获取值\n     \n     object getattribute(string name)\n     \n   \n   * 根据 key，删除该键值对\n     \n     void removeattribute(string name)\n     \n\n介绍完session相关的api后，接下来通过一个案例来完成对session的使用，具体实现步骤为:\n\n> 需求:在一个servlet中往session中存入数据，在另一个servlet中获取session中存入的数据\n> \n> 1.创建名为sessiondemo1的servlet类\n> \n> 2.创建名为sessiondemo2的servlet类\n> \n> 3.在sessiondemo1的方法中:获取session对象、存储数据\n> \n> 4.在sessiondemo2的方法中:获取session对象、获取数据\n> \n> 5.启动测试\n\n(1)创建名为sessiondemo1的servlet类\n\n@webservlet("/demo1")\npublic class sessiondemo1 extends httpservlet {\n    @override\n    protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n    \n    }\n\n    @override\n    protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        this.doget(request, response);\n    }\n}\n\n\n(2)创建名为sessiondemo2的servlet类\n\n@webservlet("/demo2")\npublic class sessiondemo2 extends httpservlet {\n    @override\n    protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n    \n    }\n\n    @override\n    protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        this.doget(request, response);\n    }\n}\n\n\n(3)sessiondemo1:获取session对象、存储数据\n\n@webservlet("/demo1")\npublic class sessiondemo1 extends httpservlet {\n    @override\n    protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n    \t//存储到session中\n        //1. 获取session对象\n        httpsession session = request.getsession();\n        //2. 存储数据\n        session.setattribute("username","zs");\n    }\n\n    @override\n    protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        this.doget(request, response);\n    }\n}\n\n\n(4)sessiondemo2:获取session对象、获取数据\n\n@webservlet("/demo2")\npublic class sessiondemo2 extends httpservlet {\n    @override\n    protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        //获取数据，从session中\n        //1. 获取session对象\n        httpsession session = request.getsession();\n        //2. 获取数据\n        object username = session.getattribute("username");\n        system.out.println(username);\n    }\n\n    @override\n    protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        this.doget(request, response);\n    }\n}\n\n\n(5)启动测试，\n\n * 先访问http://localhost:8080/cookie-demo/demo1,将数据存入session\n * 在访问http://localhost:8080/cookie-demo/demo2,从session中获取数据\n * 查看控制台\n\n\n\n通过案例的效果，能看到session是能够在一次会话中两次请求之间共享数据。\n\n小结\n\n至此session的基本使用就已经完成了，重点要掌握的是:\n\n * session的获取\n   \n   httpsession session = request.getsession();\n   \n\n * session常用方法的使用\n   \n   void setattribute(string name, object o)\n   object getattribute(string name)\n   \n   \n   **注意:**session中可以存储的是一个object类型的数据，也就是说session中可以存储任意数据类型。\n\n介绍完session的基本使用之后，那么session的底层到底是如何实现一次会话两次请求之间的数据共享呢?\n\n\n# session的原理分析\n\n * session是基于cookie实现的\n\n这句话其实不太能详细的说明session的底层实现，接下来，咱们一步步来分析下session的具体实现原理:\n\n(1)前提条件\n\n\n\nsession要想实现一次会话多次请求之间的数据共享，就必须要保证多次请求获取session的对象是同一个。\n\n那么它们是一个对象么？要验证这个结论也很简单，只需要在上面案例中的两个servlet中分别打印下session对象\n\nsessiondemo1\n\n@webservlet("/demo1")\npublic class sessiondemo1 extends httpservlet {\n    @override\n    protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n    \t//存储到session中\n        //1. 获取session对象\n        httpsession session = request.getsession();\n        system.out.println(session);\n        //2. 存储数据\n        session.setattribute("username","zs");\n    }\n\n    @override\n    protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        this.doget(request, response);\n    }\n}\n\n\nsessiondemo2\n\n@webservlet("/demo2")\npublic class sessiondemo2 extends httpservlet {\n    @override\n    protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        //获取数据，从session中\n        //1. 获取session对象\n        httpsession session = request.getsession();\n        system.out.println(session);\n        //2. 获取数据\n        object username = session.getattribute("username");\n        system.out.println(username);\n    }\n\n    @override\n    protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n        this.doget(request, response);\n    }\n}\n\n\n启动测试，分别访问\n\nhttp://localhost:8080/cookie-demo/demo1\n\nhttp://localhost:8080/cookie-demo/demo2\n\n\n\n通过打印可以得到如下结论:\n\n * 两个servlet类中获取的session对象是同一个\n * 把demo1和demo2请求刷新多次，控制台最终打印的结果都是同一个\n\n那么问题又来了，如果新开一个浏览器，访问demo1或者demo2,打印在控制台的session还是同一个对象么?\n\n\n\n注意:在一台电脑上演示的时候，如果是相同的浏览器必须要把浏览器全部关掉重新打开，才算新开的一个浏览器。\n\n当然也可以使用不同的浏览器进行测试，就不需要把之前的浏览器全部关闭。\n\n测试的结果：如果是不同浏览器或者重新打开浏览器后，打印的session就不一样了。\n\n所以session实现的也是一次会话中的多次请求之间的数据共享。\n\n那么最主要的问题就来了，session是如何保证在一次会话中获取的session对象是同一个呢?\n\n\n\n(1)demo1在第一次获取session对象的时候，session对象会有一个唯一的标识，假如是id:10\n\n(2)demo1在session中存入其他数据并处理完成所有业务后，需要通过tomcat服务器响应结果给浏览器\n\n(3)tomcat服务器发现业务处理中使用了session对象，就会把session的唯一标识id:10当做一个cookie，添加set-cookie:jessionid=10到响应头中，并响应给浏览器\n\n(4)浏览器接收到响应结果后，会把响应头中的coookie数据存储到浏览器的内存中\n\n(5)浏览器在同一会话中访问demo2的时候，会把cookie中的数据按照cookie: jessionid=10的格式添加到请求头中并发送给服务器tomcat\n\n(6)demo2获取到请求后，从请求头中就读取cookie中的jsessionid值为10，然后就会到服务器内存中寻找id:10的session对象，如果找到了，就直接返回该对象，如果没有则新创建一个session对象\n\n(7)关闭打开浏览器后，因为浏览器的cookie已被销毁，所以就没有jessionid的数据，服务端获取到的session就是一个全新的session对象\n\n至此，session是基于cookie来实现的这就话，我们就解释完了，接下来通过实例来演示下:\n\n(1)使用chrome浏览器访问http://localhost:8080/cookie-demo/demo1,打开开发者模式(f12或ctrl+shift+i),查看**响应头(response headers)**数据:\n\n\n\n(2)使用chrome浏览器再次访问http://localhost:8080/cookie-demo/demo2，查看**请求头(request headers)**数据:\n\n\n\n小结\n\n介绍完session的原理，我们只需要记住\n\n * session是基于cookie来实现的\n\n\n# session的使用细节\n\n这节我们会主要讲解两个知识，第一个是session的钝化和活化，第二个是session的销毁，首先来学习什么是session的钝化和活化？\n\n# session钝化与活化\n\n首先需要大家思考的问题是:\n\n * 服务器重启后，session中的数据是否还在?\n\n要想回答这个问题，我们可以先看下下面这幅图，\n\n\n\n(1)服务器端aservlet和bservlet共用的session对象应该是存储在服务器的内存中\n\n(2)服务器重新启动后，内存中的数据应该是已经被释放，对象也应该都销毁了\n\n所以session数据应该也已经不存在了。但是如果session不存在会引发什么问题呢?\n\n举个例子说明下，\n\n(1)用户把需要购买的商品添加到购物车，因为要实现同一个会话多次请求数据共享，所以假设把数据存入session对象中\n\n(2)用户正要付钱的时候接到一个电话，付钱的动作就搁浅了\n\n(3)正在用户打电话的时候，购物网站因为某些原因需要重启\n\n(4)重启后session数据被销毁，购物车中的商品信息也就会随之而消失\n\n(5)用户想再次发起支付，就会出为问题\n\n所以说对于session的数据，我们应该做到就算服务器重启了，也应该能把数据保存下来才对。\n\n分析了这么多，那么tomcat服务器在重启的时候，session数据到底会不会保存以及是如何保存的，我们可以通过实际案例来演示下:\n\n注意:这里所说的关闭和启动应该要确保是正常的关闭和启动。\n\n那如何才是正常关闭tomcat服务器呢?\n\n需要使用命令行的方式来启动和停止tomcat服务器:\n\n启动:进入到项目pom.xml所在目录，执行tomcat7:run\n\n\n\n停止:在启动的命令行界面，输入ctrl+c\n\n\n\n有了上述两个正常启动和关闭的方式后，接下来的测试流程是:\n\n(1)先启动tomcat服务器\n\n(2)访问http://localhost:8080/cookie-demo/demo1将数据存入session中\n\n(3)正确停止tomcat服务器\n\n(4)再次重新启动tomcat服务器\n\n(5)访问http://localhost:8080/cookie-demo/demo2 查看是否能获取到session中的数据\n\n\n\n经过测试，会发现只要服务器是正常关闭和启动，session中的数据是可以被保存下来的。\n\n那么tomcat服务器到底是如何做到的呢?\n\n具体的原因就是:session的钝化和活化:\n\n * 钝化：在服务器正常关闭后，tomcat会自动将session数据写入硬盘的文件中\n   \n   * 钝化的数据路径为:项目目录\\target\\tomcat\\work\\tomcat\\localhost\\项目名称\\sessions.ser\n     \n     \n\n * 活化：再次启动服务器后，从文件中加载数据到session中\n   \n   * 数据加载到session中后，路径中的sessions.ser文件会被删除掉\n\n对于上述的整个过程，大家只需要了解下即可。因为所有的过程都是tomcat自己完成的，不需要我们参与。\n\n小结\n\nsession的钝化和活化介绍完后，需要我们注意的是:\n\n * session数据存储在服务端，服务器重启后，session数据会被保存\n\n * 浏览器被关闭启动后，重新建立的连接就已经是一个全新的会话，获取的session数据也是一个新的对象\n\n * session的数据要想共享，浏览器不能关闭，所以session数据不能长期保存数据\n\n * cookie是存储在客户端，是可以长期保存\n\n# session销毁\n\nsession的销毁会有两种方式:\n\n * 默认情况下，无操作，30分钟自动销毁\n   \n   * 对于这个失效时间，是可以通过配置进行修改的\n     \n     * 在项目的web.xml中配置\n       \n       <?xml version="1.0" encoding="utf-8"?>\n       <web-app xmlns="http://xmlns.jcp.org/xml/ns/javaee"\n                xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n                xsi:schemalocation="http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd"\n                version="3.1">\n       \n           <session-config>\n               <session-timeout>100</session-timeout>\n           </session-config>\n       </web-app>\n       \n     \n     * 如果没有配置，默认是30分钟，默认值是在tomcat的web.xml配置文件中写死的\n       \n       \n\n * 调用session对象的invalidate()进行销毁\n   \n   * 在sessiondemo2类中添加session销毁的方法\n     \n     @webservlet("/demo2")\n     public class sessiondemo2 extends httpservlet {\n         @override\n         protected void doget(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n             //获取数据，从session中\n     \n             //1. 获取session对象\n             httpsession session = request.getsession();\n             system.out.println(session);\n     \n             // 销毁\n             session.invalidate();\n             //2. 获取数据\n             object username = session.getattribute("username");\n             system.out.println(username);\n         }\n     \n         @override\n         protected void dopost(httpservletrequest request, httpservletresponse response) throws servletexception, ioexception {\n             this.doget(request, response);\n         }\n     }\n     \n   \n   * 启动访问测试，先访问demo1将数据存入到session，再次访问demo2从session中获取数据\n     \n     \n   \n   * 该销毁方法一般会在用户退出的时候，需要将session销毁掉。\n\ncookie和session小结\n\n * cookie 和 session 都是来完成一次会话内多次请求间数据共享的。\n\n所需两个对象放在一块，就需要思考:\n\ncookie和session的区别是什么?\n\ncookie和session的应用场景分别是什么?\n\n * 区别:\n   * 存储位置：cookie 是将数据存储在客户端，session 将数据存储在服务端\n   * 安全性：cookie不安全，session安全\n   * 数据大小：cookie最大3kb，session无大小限制\n   * 存储时间：cookie可以通过setmaxage()长期存储，session默认30分钟\n   * 服务器性能：cookie不占服务器资源，session占用服务器资源\n * 应用场景:\n   * 购物车:使用cookie来存储\n   * 以登录用户的名称展示:使用session来存储\n   * 记住我功能:使用cookie来存储\n   * 验证码:使用session来存储\n * 结论\n   * cookie是用来保证用户在未登录情况下的身份识别\n   * session是用来保存用户登录后的数据\n\n介绍完cookie和session以后，具体用哪个还是需要根据具体的业务进行具体分析。',charsets:{cjk:!0}},{title:"事务",frontmatter:{autoSort:95,title:"事务",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/05f9b5/",categories:["后端","数据库","MySQL"],tags:["知识","数据库","MySQL"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/01.MySQL/20.%E4%BA%8B%E5%8A%A1.html",relativePath:"01.后端/30.数据库/01.MySQL/20.事务.md",key:"v-edd93478",path:"/pages/05f9b5/",headers:[{level:2,title:"事务简介",slug:"事务简介",normalizedTitle:"事务简介",charIndex:2},{level:2,title:"事务操作",slug:"事务操作",normalizedTitle:"事务操作",charIndex:156},{level:2,title:"事务四大特性（ACID）",slug:"事务四大特性-acid",normalizedTitle:"事务四大特性（acid）",charIndex:1074},{level:2,title:"并发事务问题",slug:"并发事务问题",normalizedTitle:"并发事务问题",charIndex:1379},{level:2,title:"事务隔离级别",slug:"事务隔离级别",normalizedTitle:"事务隔离级别",charIndex:2049}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"事务简介 事务操作 事务四大特性（ACID） 并发事务问题 事务隔离级别",content:"# 事务简介\n\n> 事务 是一组操作的集合，它是一个不可分割的工作单位，事务会把所有的操作作为一个整体一起向系统提交或撤销操作请求，即这些操作要么同时成功，要么同时失败。\n\n * 注意\n   \n   默认MySQL的事务是自动提交的，也就是说，当执行完一条DML语句时，MySQL会立即隐式的提交事务。\n\n\n# 事务操作\n\n 1. 未控制事务\n    \n    * 正常情况\n      \n      \n    \n    * 异常情况\n      \n      \n\n 2. 控制事务1\n    \n    手动修改数据库默认的自动提交行为，将自动提交修改为手动提交--不推荐\n    \n    此时我们执行的DML语句都不会提交, 需要手动的执行commit进行提交\n    \n    * 查看/设置事务的提交方式\n      \n      SELECT @@autocommit ; -- 1  1代表自动提交\n      SET @@autocommit = 0 ;-- 设置为0 表示更改为手动提交\n      \n    \n    * sql执行成功，提交事务\n      \n      commit;\n      \n    \n    * sql执行异常，回滚事务\n      \n      rollback;\n      \n\n 3. 控制事务2\n    \n    手动开启事务-- 推荐\n    \n    -- 方式二 转账操作 (张三给李四转账1000)\n    \n    -- 开启事务\n    start transaction ;\n    \n    -- 1. 查询张三账户余额\n    select * from account where name = '张三';\n    \n    -- 2. 将张三账户余额-1000\n    update account set money = money - 1000 where name = '张三';\n    \n    程序执行报错 ...\n    \n    -- 3. 将李四账户余额+1000\n    update account set money = money + 1000 where name = '李四';\n    \n    \n    -- 提交事务  如果上面sql正常执行，则会自动执行 commit\n    commit;\n    \n    -- 回滚事务  如果上面sql执行异常，则会自动执行 rollback\n    rollback;\n    \n\n\n# 事务四大特性（ACID）\n\n * 原子性--undo log\n   * 原子性（Atomicity）：事务是不可分割的最小操作单元，要么全部成功，要么全部失败。\n * 一致性-- undo log + redo log\n   * 一致性（Consistency）：事务完成时，必须使所有的数据都保持一致状态。\n * 隔离性-- MVCC + 锁\n   * 隔离性（Isolation）：数据库系统提供的隔离机制，保证事务在不受外部并发操作影响的独立。 环境下运行。\n * 持久性-- redo log\n   * 持久性（Durability）：事务一旦提交或回滚，它对数据库中的数据的改变就是永久的。\n\n\n# 并发事务问题\n\n * 脏读\n   \n   * 一个事务读到另外一个事务还没有提交的数据\n     \n     > 事务A执行了第2句，更新了id为1的数据，但是还没有提交事务；\n     > \n     > 此时事务B读取了更新后id为1的数据，即出现了脏读。\n     \n     \n\n * 不可重复读\n   \n   * 一个事务先后读取同一条记录，但两次读取的数据不同，称之为不可重复读。\n     \n     > 事务A在第1句查看了id为1的数据，设内容为a。\n     > \n     > 然后事务B提交了对id为1数据的修改；\n     > \n     > 事务A在第三句又一次查看了id为去的数据，结果内容为b;\n     > \n     > 两次查看结果不一致，即 不可重复读\n     \n     \n\n * 幻读\n   \n   * 一个事务按照条件查询数据时，没有对应的数据行，但是在插入数据时，又发现这行数据 已经存在，好像出现了 \"幻影\"。\n     \n     > 事务A执行第一句，select id为1的数据，但是显示没有数据；\n     > \n     > 此时事务B插入了id为1的数据；\n     > \n     > 事务A的第二句，想插入id为1的数据，但是提示，id重复（事务B插入了数据），不可插入；\n     > \n     > 事务A的第三句，再次查询id为1的数据，仍然显示没有数据；\n     > \n     > 查询没有，插入却提示存在，这种情况即为 幻读\n     \n     \n\n\n# 事务隔离级别\n\n注意：事务隔离级别越高，数据越安全，但是性能越低。\n\n\n\n * 查看事务隔离级别\n   \n   SELECT @@TRANSACTION_ISOLATION;\n   \n   -- mysql 默认是  REPEATABLE READ\n   \n\n * 设置事务隔离级别\n   \n   SET [ SESSION | GLOBAL ] TRANSACTION ISOLATION LEVEL \n   { READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ | SERIALIZABLE }\n   ",normalizedContent:"# 事务简介\n\n> 事务 是一组操作的集合，它是一个不可分割的工作单位，事务会把所有的操作作为一个整体一起向系统提交或撤销操作请求，即这些操作要么同时成功，要么同时失败。\n\n * 注意\n   \n   默认mysql的事务是自动提交的，也就是说，当执行完一条dml语句时，mysql会立即隐式的提交事务。\n\n\n# 事务操作\n\n 1. 未控制事务\n    \n    * 正常情况\n      \n      \n    \n    * 异常情况\n      \n      \n\n 2. 控制事务1\n    \n    手动修改数据库默认的自动提交行为，将自动提交修改为手动提交--不推荐\n    \n    此时我们执行的dml语句都不会提交, 需要手动的执行commit进行提交\n    \n    * 查看/设置事务的提交方式\n      \n      select @@autocommit ; -- 1  1代表自动提交\n      set @@autocommit = 0 ;-- 设置为0 表示更改为手动提交\n      \n    \n    * sql执行成功，提交事务\n      \n      commit;\n      \n    \n    * sql执行异常，回滚事务\n      \n      rollback;\n      \n\n 3. 控制事务2\n    \n    手动开启事务-- 推荐\n    \n    -- 方式二 转账操作 (张三给李四转账1000)\n    \n    -- 开启事务\n    start transaction ;\n    \n    -- 1. 查询张三账户余额\n    select * from account where name = '张三';\n    \n    -- 2. 将张三账户余额-1000\n    update account set money = money - 1000 where name = '张三';\n    \n    程序执行报错 ...\n    \n    -- 3. 将李四账户余额+1000\n    update account set money = money + 1000 where name = '李四';\n    \n    \n    -- 提交事务  如果上面sql正常执行，则会自动执行 commit\n    commit;\n    \n    -- 回滚事务  如果上面sql执行异常，则会自动执行 rollback\n    rollback;\n    \n\n\n# 事务四大特性（acid）\n\n * 原子性--undo log\n   * 原子性（atomicity）：事务是不可分割的最小操作单元，要么全部成功，要么全部失败。\n * 一致性-- undo log + redo log\n   * 一致性（consistency）：事务完成时，必须使所有的数据都保持一致状态。\n * 隔离性-- mvcc + 锁\n   * 隔离性（isolation）：数据库系统提供的隔离机制，保证事务在不受外部并发操作影响的独立。 环境下运行。\n * 持久性-- redo log\n   * 持久性（durability）：事务一旦提交或回滚，它对数据库中的数据的改变就是永久的。\n\n\n# 并发事务问题\n\n * 脏读\n   \n   * 一个事务读到另外一个事务还没有提交的数据\n     \n     > 事务a执行了第2句，更新了id为1的数据，但是还没有提交事务；\n     > \n     > 此时事务b读取了更新后id为1的数据，即出现了脏读。\n     \n     \n\n * 不可重复读\n   \n   * 一个事务先后读取同一条记录，但两次读取的数据不同，称之为不可重复读。\n     \n     > 事务a在第1句查看了id为1的数据，设内容为a。\n     > \n     > 然后事务b提交了对id为1数据的修改；\n     > \n     > 事务a在第三句又一次查看了id为去的数据，结果内容为b;\n     > \n     > 两次查看结果不一致，即 不可重复读\n     \n     \n\n * 幻读\n   \n   * 一个事务按照条件查询数据时，没有对应的数据行，但是在插入数据时，又发现这行数据 已经存在，好像出现了 \"幻影\"。\n     \n     > 事务a执行第一句，select id为1的数据，但是显示没有数据；\n     > \n     > 此时事务b插入了id为1的数据；\n     > \n     > 事务a的第二句，想插入id为1的数据，但是提示，id重复（事务b插入了数据），不可插入；\n     > \n     > 事务a的第三句，再次查询id为1的数据，仍然显示没有数据；\n     > \n     > 查询没有，插入却提示存在，这种情况即为 幻读\n     \n     \n\n\n# 事务隔离级别\n\n注意：事务隔离级别越高，数据越安全，但是性能越低。\n\n\n\n * 查看事务隔离级别\n   \n   select @@transaction_isolation;\n   \n   -- mysql 默认是  repeatable read\n   \n\n * 设置事务隔离级别\n   \n   set [ session | global ] transaction isolation level \n   { read uncommitted | read committed | repeatable read | serializable }\n   ",charsets:{cjk:!0}},{title:"单机安装Redis",frontmatter:{autoSort:100,title:"单机安装Redis",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/2d9387/",categories:["后端","数据库","Redis"],tags:["知识","数据库","Redis"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/05.Redis/101.%E5%8D%95%E6%9C%BA%E5%AE%89%E8%A3%85Redis.html",relativePath:"01.后端/30.数据库/05.Redis/101.单机安装Redis.md",key:"v-3bed72fe",path:"/pages/2d9387/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"首先需要安装Redis所需要的依赖：\n\nyum install -y gcc tcl\n\n\n然后将课前资料提供的Redis安装包上传到虚拟机的任意目录：\n\n\n\n例如，我放到了/tmp目录：\n\n\n\n解压缩：\n\ntar -xzf redis-6.2.4.tar.gz\n\n\n解压后：\n\n\n\n进入redis目录：\n\ncd redis-6.2.4\n\n\n运行编译命令：\n\nmake && make install\n\n\n如果没有出错，应该就安装成功了。\n\n然后修改redis.conf文件中的一些配置：\n\n# 绑定地址，默认是127.0.0.1，会导致只能在本地访问。修改为0.0.0.0则可以在任意IP访问\nbind 0.0.0.0\n# 保护模式，关闭保护模式\nprotected-mode no\n# 数据库数量，设置为1\ndatabases 1\n\n\n启动Redis：\n\nredis-server redis.conf\n\n\n停止redis服务：\n\nredis-cli shutdown\n",normalizedContent:"首先需要安装redis所需要的依赖：\n\nyum install -y gcc tcl\n\n\n然后将课前资料提供的redis安装包上传到虚拟机的任意目录：\n\n\n\n例如，我放到了/tmp目录：\n\n\n\n解压缩：\n\ntar -xzf redis-6.2.4.tar.gz\n\n\n解压后：\n\n\n\n进入redis目录：\n\ncd redis-6.2.4\n\n\n运行编译命令：\n\nmake && make install\n\n\n如果没有出错，应该就安装成功了。\n\n然后修改redis.conf文件中的一些配置：\n\n# 绑定地址，默认是127.0.0.1，会导致只能在本地访问。修改为0.0.0.0则可以在任意ip访问\nbind 0.0.0.0\n# 保护模式，关闭保护模式\nprotected-mode no\n# 数据库数量，设置为1\ndatabases 1\n\n\n启动redis：\n\nredis-server redis.conf\n\n\n停止redis服务：\n\nredis-cli shutdown\n",charsets:{cjk:!0}},{title:"Redis基础",frontmatter:{autoSort:100,title:"Redis基础",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/31e22b/",categories:["后端","数据库","Redis"],tags:["知识","数据库","Redis"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/05.Redis/05.Redis%E5%9F%BA%E7%A1%80.html",relativePath:"01.后端/30.数据库/05.Redis/05.Redis基础.md",key:"v-420ed683",path:"/pages/31e22b/",headers:[{level:2,title:"认识NoSQL",slug:"认识nosql",normalizedTitle:"认识nosql",charIndex:162},{level:3,title:"结构化与非结构化",slug:"结构化与非结构化",normalizedTitle:"结构化与非结构化",charIndex:272},{level:3,title:"关联和非关联",slug:"关联和非关联",normalizedTitle:"关联和非关联",charIndex:420},{level:3,title:"查询方式",slug:"查询方式",normalizedTitle:"查询方式",charIndex:799},{level:3,title:"事务",slug:"事务",normalizedTitle:"事务",charIndex:870},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:947},{level:2,title:"认识Redis",slug:"认识redis",normalizedTitle:"认识redis",charIndex:1259},{level:2,title:"安装Redis",slug:"安装redis",normalizedTitle:"安装redis",charIndex:1517},{level:3,title:"依赖库",slug:"依赖库",normalizedTitle:"依赖库",charIndex:1632},{level:3,title:"上传安装包并解压",slug:"上传安装包并解压",normalizedTitle:"上传安装包并解压",charIndex:1705},{level:3,title:"启动",slug:"启动",normalizedTitle:"启动",charIndex:2023},{level:3,title:"默认启动",slug:"默认启动",normalizedTitle:"默认启动",charIndex:2091},{level:3,title:"指定配置启动",slug:"指定配置启动",normalizedTitle:"指定配置启动",charIndex:2099},{level:3,title:"开机自启",slug:"开机自启",normalizedTitle:"开机自启",charIndex:2109},{level:2,title:"Redis桌面客户端",slug:"redis桌面客户端",normalizedTitle:"redis桌面客户端",charIndex:3547},{level:3,title:"Redis命令行客户端",slug:"redis命令行客户端",normalizedTitle:"redis命令行客户端",charIndex:3647},{level:3,title:"图形化桌面客户端",slug:"图形化桌面客户端",normalizedTitle:"图形化桌面客户端",charIndex:3625},{level:3,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:1517},{level:3,title:"建立连接",slug:"建立连接",normalizedTitle:"建立连接",charIndex:4278}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"认识NoSQL 结构化与非结构化 关联和非关联 查询方式 事务 总结 认识Redis 安装Redis 依赖库 上传安装包并解压 启动 默认启动 指定配置启动 开机自启 Redis桌面客户端 Redis命令行客户端 图形化桌面客户端 安装 建立连接",content:'Redis是一种键值型的NoSql数据库，这里有两个关键字：\n\n * 键值型\n\n * NoSql\n\n其中键值型，是指Redis中存储的数据都是以key、value对的形式存储，而value的形式多种多样，可以是字符串、数值、甚至json：\n\n\n\n而NoSql则是相对于传统关系型数据库而言，有很大差异的一种数据库。\n\n\n# 认识NoSQL\n\nNoSql可以翻译做Not Only Sql（不仅仅是SQL），或者是No Sql（非Sql的）数据库。是相对于传统关系型数据库而言，有很大差异的一种特殊的数据库，因此也称之为非关系型数据库。\n\n\n# 结构化与非结构化\n\n传统关系型数据库是结构化数据，每一张表都有严格的约束信息：字段名、字段数据类型、字段约束等等信息，插入的数据必须遵守这些约束：\n\n\n\n而NoSql则对数据库格式没有严格约束，往往形式松散，自由。\n\n可以是键值型：\n\n\n\n也可以是文档型：\n\n\n\n甚至可以是图格式：\n\n\n\n\n# 关联和非关联\n\n传统数据库的表与表之间往往存在关联，例如外键：\n\n\n\n而非关系型数据库不存在关联关系，要维护关系要么靠代码中的业务逻辑，要么靠数据之间的耦合：\n\n{\n  id: 1,\n  name: "张三",\n  orders: [\n    {\n       id: 1,\n       item: {\n\t id: 10, title: "荣耀6", price: 4999\n       }\n    },\n    {\n       id: 2,\n       item: {\n\t id: 20, title: "小米11", price: 3999\n       }\n    }\n  ]\n}\n\n\n此处要维护“张三”的订单与商品“荣耀”和“小米11”的关系，不得不冗余的将这两个商品保存在张三的订单文档中，不够优雅。还是建议用业务来维护关联关系。\n\n\n# 查询方式\n\n传统关系型数据库会基于Sql语句做查询，语法有统一标准；\n\n而不同的非关系数据库查询语法差异极大，五花八门各种各样。\n\n\n\n\n# 事务\n\n传统关系型数据库能满足事务ACID的原则。\n\n\n\n而非关系型数据库往往不支持事务，或者不能严格保证ACID的特性，只能实现基本的一致性。\n\n\n# 总结\n\n除了上述四点以外，在存储方式、扩展性、查询性能上关系型与非关系型也都有着显著差异，总结如下：\n\n\n\n * 存储方式\n   * 关系型数据库基于磁盘进行存储，会有大量的磁盘IO，对性能有一定影响\n   * 非关系型数据库，他们的操作更多的是依赖于内存来操作，内存的读写速度会非常快，性能自然会好一些\n\n * 扩展性\n   * 关系型数据库集群模式一般是主从，主从数据一致，起到数据备份的作用，称为垂直扩展。\n   * 非关系型数据库可以将数据拆分，存储在不同机器上，可以保存海量数据，解决内存大小有限的问题。称为水平扩展。\n   * 关系型数据库因为表之间存在关联关系，如果做水平扩展会给数据查询带来很多麻烦\n\n\n# 认识Redis\n\nRedis诞生于2009年全称是Remote Dictionary Server 远程词典服务器，是一个基于内存的键值型NoSQL数据库。\n\n特征：\n\n * 键值（key-value）型，value支持多种不同数据结构，功能丰富\n * 单线程，每个命令具备原子性\n * 低延迟，速度快（基于内存、IO多路复用、良好的编码）。\n * 支持数据持久化\n * 支持主从集群、分片集群\n * 支持多语言客户端\n\n作者：Antirez\n\nRedis的官方网站地址：https://redis.io/\n\n\n# 安装Redis\n\n大多数企业都是基于Linux服务器来部署项目，而且Redis官方也没有提供Windows版本的安装包。因此课程中我们会基于Linux系统来安装Redis.\n\n此处选择的Linux版本为CentOS 7.\n\n\n# 依赖库\n\nRedis是基于C语言编写的，因此首先需要安装Redis所需要的gcc依赖：\n\nyum install -y gcc tcl\n\n\n\n# 上传安装包并解压\n\n然后将课前资料提供的Redis安装包上传到虚拟机的任意目录：\n\n\n\n例如，我放到了/usr/local/src 目录：\n\n\n\n解压缩：\n\ntar -xzf redis-6.2.6.tar.gz\n\n\n解压后：\n\n\n\n进入redis目录：\n\ncd redis-6.2.6\n\n\n运行编译命令：\n\nmake && make install\n\n\n如果没有出错，应该就安装成功了。\n\n默认的安装路径是在 /usr/local/bin目录下：\n\n该目录已经默认配置到环境变量，因此可以在任意目录下运行这些命令。其中：\n\n * redis-cli：是redis提供的命令行客户端\n * redis-server：是redis的服务端启动脚本\n * redis-sentinel：是redis的哨兵启动脚本\n\n\n# 启动\n\nredis的启动方式有很多种，例如：\n\n * 默认启动\n * 指定配置启动\n * 开机自启\n\n\n# 默认启动\n\n安装完成后，在任意目录输入redis-server命令即可启动Redis：\n\nredis-server\n\n\n如图：\n\n\n\n这种启动属于前台启动，会阻塞整个会话窗口，窗口关闭或者按下CTRL + C则Redis停止。不推荐使用。\n\n\n# 指定配置启动\n\n如果要让Redis以后台方式启动，则必须修改Redis配置文件，就在我们之前解压的redis安装包下（/usr/local/src/redis-6.2.6），名字叫redis.conf：\n\n\n\n我们先将这个配置文件备份一份：\n\ncp redis.conf redis.conf.bck\n\n\n然后修改redis.conf文件中的一些配置：\n\n# 允许访问的地址，默认是127.0.0.1，会导致只能在本地访问。修改为0.0.0.0则可以在任意IP访问，生产环境不要设置为0.0.0.0\nbind 0.0.0.0\n# 守护进程，修改为yes后即可后台运行\ndaemonize yes \n# 密码，设置后访问Redis必须输入密码-没有设置\nrequirepass 123321\n\n\nRedis的其它常见配置：\n\n# 监听的端口\nport 6379\n# 工作目录，默认是当前目录，也就是运行redis-server时的命令，日志、持久化等文件会保存在这个目录\ndir .\n# 数据库数量，设置为1，代表只使用1个库，默认有16个库，编号0~15\ndatabases 1\n# 设置redis能够使用的最大内存-没有设置\nmaxmemory 512mb  \n# 日志文件，默认为空，不记录日志，可以指定日志文件名\nlogfile "redis.log"\n\n\n启动Redis：\n\n# 进入redis安装目录 \ncd /usr/local/src/redis-6.2.6\n# 启动\nredis-server redis.conf\n\n\n停止服务：\n\n# 利用redis-cli来执行 shutdown 命令，即可停止 Redis 服务，\n# 因为之前配置了密码，因此需要通过 -u 来指定密码\nredis-cli -u 123321 shutdown\n\n\n\n# 开机自启\n\n我们也可以通过配置来实现开机自启。\n\n首先，新建一个系统服务文件：\n\nvi /etc/systemd/system/redis.service\n\n\n内容如下：\n\n[Unit]\nDescription=redis-server\nAfter=network.target\n\n[Service]\nType=forking\nExecStart=/usr/local/bin/redis-server /usr/local/src/redis-6.2.6/redis.conf\nPrivateTmp=true\n\n[Install]\nWantedBy=multi-user.target\n\n\n然后重载系统服务：\n\nsystemctl daemon-reload\n\n\n现在，我们可以用下面这组命令来操作redis了：\n\n# 启动\nsystemctl start redis\n# 停止\nsystemctl stop redis\n# 重启\nsystemctl restart redis\n# 查看状态\nsystemctl status redis\n\n\n执行下面的命令，可以让redis开机自启：\n\nsystemctl enable redis\n\n\n\n# Redis桌面客户端\n\n安装完成Redis，我们就可以操作Redis，实现数据的CRUD了。这需要用到Redis客户端，包括：\n\n * 命令行客户端\n * 图形化桌面客户端\n * 编程客户端\n\n\n# Redis命令行客户端\n\nRedis安装完成后就自带了命令行客户端：redis-cli，使用方式如下：\n\nredis-cli [options] [commonds]\n\n\n其中常见的options有：\n\n * -h 127.0.0.1：指定要连接的redis节点的IP地址，默认是127.0.0.1\n * -p 6379：指定要连接的redis节点的端口，默认是6379\n * -a 123321：指定redis的访问密码\n\n其中的commonds就是Redis的操作命令，例如：\n\n * ping：与redis服务端做心跳测试，服务端正常会返回pong\n\n不指定commond时，会进入redis-cli的交互控制台：\n\n\n\n\n# 图形化桌面客户端\n\nGitHub上的大神编写了Redis的图形化桌面客户端，地址：https://github.com/uglide/RedisDesktopManager\n\n不过该仓库提供的是RedisDesktopManager的源码，并未提供windows安装包。\n\n在下面这个仓库可以找到安装包：https://github.com/lework/RedisDesktopManager-Windows/releases\n\n\n# 安装\n\n在课前资料中可以找到Redis的图形化桌面客户端：\n\n\n\n解压缩后，运行安装程序即可安装：\n\n\n\n安装完成后，在安装目录下找到rdm.exe文件：\n\n\n\n双击即可运行：\n\n\n\n\n# 建立连接\n\n点击左上角的连接到Redis服务器按钮：\n\n\n\n在弹出的窗口中填写Redis服务信息：\n\n\n\n点击确定后，在左侧菜单会出现这个链接：\n\n\n\n点击即可建立连接了。\n\n\n\nRedis默认有16个仓库，编号从0至15. 通过配置文件可以设置仓库数量，但是不超过16，并且不能自定义仓库名称。\n\n如果是基于redis-cli连接Redis服务，可以通过select命令来选择数据库：\n\n# 选择 0号库\nselect 0\n',normalizedContent:'redis是一种键值型的nosql数据库，这里有两个关键字：\n\n * 键值型\n\n * nosql\n\n其中键值型，是指redis中存储的数据都是以key、value对的形式存储，而value的形式多种多样，可以是字符串、数值、甚至json：\n\n\n\n而nosql则是相对于传统关系型数据库而言，有很大差异的一种数据库。\n\n\n# 认识nosql\n\nnosql可以翻译做not only sql（不仅仅是sql），或者是no sql（非sql的）数据库。是相对于传统关系型数据库而言，有很大差异的一种特殊的数据库，因此也称之为非关系型数据库。\n\n\n# 结构化与非结构化\n\n传统关系型数据库是结构化数据，每一张表都有严格的约束信息：字段名、字段数据类型、字段约束等等信息，插入的数据必须遵守这些约束：\n\n\n\n而nosql则对数据库格式没有严格约束，往往形式松散，自由。\n\n可以是键值型：\n\n\n\n也可以是文档型：\n\n\n\n甚至可以是图格式：\n\n\n\n\n# 关联和非关联\n\n传统数据库的表与表之间往往存在关联，例如外键：\n\n\n\n而非关系型数据库不存在关联关系，要维护关系要么靠代码中的业务逻辑，要么靠数据之间的耦合：\n\n{\n  id: 1,\n  name: "张三",\n  orders: [\n    {\n       id: 1,\n       item: {\n\t id: 10, title: "荣耀6", price: 4999\n       }\n    },\n    {\n       id: 2,\n       item: {\n\t id: 20, title: "小米11", price: 3999\n       }\n    }\n  ]\n}\n\n\n此处要维护“张三”的订单与商品“荣耀”和“小米11”的关系，不得不冗余的将这两个商品保存在张三的订单文档中，不够优雅。还是建议用业务来维护关联关系。\n\n\n# 查询方式\n\n传统关系型数据库会基于sql语句做查询，语法有统一标准；\n\n而不同的非关系数据库查询语法差异极大，五花八门各种各样。\n\n\n\n\n# 事务\n\n传统关系型数据库能满足事务acid的原则。\n\n\n\n而非关系型数据库往往不支持事务，或者不能严格保证acid的特性，只能实现基本的一致性。\n\n\n# 总结\n\n除了上述四点以外，在存储方式、扩展性、查询性能上关系型与非关系型也都有着显著差异，总结如下：\n\n\n\n * 存储方式\n   * 关系型数据库基于磁盘进行存储，会有大量的磁盘io，对性能有一定影响\n   * 非关系型数据库，他们的操作更多的是依赖于内存来操作，内存的读写速度会非常快，性能自然会好一些\n\n * 扩展性\n   * 关系型数据库集群模式一般是主从，主从数据一致，起到数据备份的作用，称为垂直扩展。\n   * 非关系型数据库可以将数据拆分，存储在不同机器上，可以保存海量数据，解决内存大小有限的问题。称为水平扩展。\n   * 关系型数据库因为表之间存在关联关系，如果做水平扩展会给数据查询带来很多麻烦\n\n\n# 认识redis\n\nredis诞生于2009年全称是remote dictionary server 远程词典服务器，是一个基于内存的键值型nosql数据库。\n\n特征：\n\n * 键值（key-value）型，value支持多种不同数据结构，功能丰富\n * 单线程，每个命令具备原子性\n * 低延迟，速度快（基于内存、io多路复用、良好的编码）。\n * 支持数据持久化\n * 支持主从集群、分片集群\n * 支持多语言客户端\n\n作者：antirez\n\nredis的官方网站地址：https://redis.io/\n\n\n# 安装redis\n\n大多数企业都是基于linux服务器来部署项目，而且redis官方也没有提供windows版本的安装包。因此课程中我们会基于linux系统来安装redis.\n\n此处选择的linux版本为centos 7.\n\n\n# 依赖库\n\nredis是基于c语言编写的，因此首先需要安装redis所需要的gcc依赖：\n\nyum install -y gcc tcl\n\n\n\n# 上传安装包并解压\n\n然后将课前资料提供的redis安装包上传到虚拟机的任意目录：\n\n\n\n例如，我放到了/usr/local/src 目录：\n\n\n\n解压缩：\n\ntar -xzf redis-6.2.6.tar.gz\n\n\n解压后：\n\n\n\n进入redis目录：\n\ncd redis-6.2.6\n\n\n运行编译命令：\n\nmake && make install\n\n\n如果没有出错，应该就安装成功了。\n\n默认的安装路径是在 /usr/local/bin目录下：\n\n该目录已经默认配置到环境变量，因此可以在任意目录下运行这些命令。其中：\n\n * redis-cli：是redis提供的命令行客户端\n * redis-server：是redis的服务端启动脚本\n * redis-sentinel：是redis的哨兵启动脚本\n\n\n# 启动\n\nredis的启动方式有很多种，例如：\n\n * 默认启动\n * 指定配置启动\n * 开机自启\n\n\n# 默认启动\n\n安装完成后，在任意目录输入redis-server命令即可启动redis：\n\nredis-server\n\n\n如图：\n\n\n\n这种启动属于前台启动，会阻塞整个会话窗口，窗口关闭或者按下ctrl + c则redis停止。不推荐使用。\n\n\n# 指定配置启动\n\n如果要让redis以后台方式启动，则必须修改redis配置文件，就在我们之前解压的redis安装包下（/usr/local/src/redis-6.2.6），名字叫redis.conf：\n\n\n\n我们先将这个配置文件备份一份：\n\ncp redis.conf redis.conf.bck\n\n\n然后修改redis.conf文件中的一些配置：\n\n# 允许访问的地址，默认是127.0.0.1，会导致只能在本地访问。修改为0.0.0.0则可以在任意ip访问，生产环境不要设置为0.0.0.0\nbind 0.0.0.0\n# 守护进程，修改为yes后即可后台运行\ndaemonize yes \n# 密码，设置后访问redis必须输入密码-没有设置\nrequirepass 123321\n\n\nredis的其它常见配置：\n\n# 监听的端口\nport 6379\n# 工作目录，默认是当前目录，也就是运行redis-server时的命令，日志、持久化等文件会保存在这个目录\ndir .\n# 数据库数量，设置为1，代表只使用1个库，默认有16个库，编号0~15\ndatabases 1\n# 设置redis能够使用的最大内存-没有设置\nmaxmemory 512mb  \n# 日志文件，默认为空，不记录日志，可以指定日志文件名\nlogfile "redis.log"\n\n\n启动redis：\n\n# 进入redis安装目录 \ncd /usr/local/src/redis-6.2.6\n# 启动\nredis-server redis.conf\n\n\n停止服务：\n\n# 利用redis-cli来执行 shutdown 命令，即可停止 redis 服务，\n# 因为之前配置了密码，因此需要通过 -u 来指定密码\nredis-cli -u 123321 shutdown\n\n\n\n# 开机自启\n\n我们也可以通过配置来实现开机自启。\n\n首先，新建一个系统服务文件：\n\nvi /etc/systemd/system/redis.service\n\n\n内容如下：\n\n[unit]\ndescription=redis-server\nafter=network.target\n\n[service]\ntype=forking\nexecstart=/usr/local/bin/redis-server /usr/local/src/redis-6.2.6/redis.conf\nprivatetmp=true\n\n[install]\nwantedby=multi-user.target\n\n\n然后重载系统服务：\n\nsystemctl daemon-reload\n\n\n现在，我们可以用下面这组命令来操作redis了：\n\n# 启动\nsystemctl start redis\n# 停止\nsystemctl stop redis\n# 重启\nsystemctl restart redis\n# 查看状态\nsystemctl status redis\n\n\n执行下面的命令，可以让redis开机自启：\n\nsystemctl enable redis\n\n\n\n# redis桌面客户端\n\n安装完成redis，我们就可以操作redis，实现数据的crud了。这需要用到redis客户端，包括：\n\n * 命令行客户端\n * 图形化桌面客户端\n * 编程客户端\n\n\n# redis命令行客户端\n\nredis安装完成后就自带了命令行客户端：redis-cli，使用方式如下：\n\nredis-cli [options] [commonds]\n\n\n其中常见的options有：\n\n * -h 127.0.0.1：指定要连接的redis节点的ip地址，默认是127.0.0.1\n * -p 6379：指定要连接的redis节点的端口，默认是6379\n * -a 123321：指定redis的访问密码\n\n其中的commonds就是redis的操作命令，例如：\n\n * ping：与redis服务端做心跳测试，服务端正常会返回pong\n\n不指定commond时，会进入redis-cli的交互控制台：\n\n\n\n\n# 图形化桌面客户端\n\ngithub上的大神编写了redis的图形化桌面客户端，地址：https://github.com/uglide/redisdesktopmanager\n\n不过该仓库提供的是redisdesktopmanager的源码，并未提供windows安装包。\n\n在下面这个仓库可以找到安装包：https://github.com/lework/redisdesktopmanager-windows/releases\n\n\n# 安装\n\n在课前资料中可以找到redis的图形化桌面客户端：\n\n\n\n解压缩后，运行安装程序即可安装：\n\n\n\n安装完成后，在安装目录下找到rdm.exe文件：\n\n\n\n双击即可运行：\n\n\n\n\n# 建立连接\n\n点击左上角的连接到redis服务器按钮：\n\n\n\n在弹出的窗口中填写redis服务信息：\n\n\n\n点击确定后，在左侧菜单会出现这个链接：\n\n\n\n点击即可建立连接了。\n\n\n\nredis默认有16个仓库，编号从0至15. 通过配置文件可以设置仓库数量，但是不超过16，并且不能自定义仓库名称。\n\n如果是基于redis-cli连接redis服务，可以通过select命令来选择数据库：\n\n# 选择 0号库\nselect 0\n',charsets:{cjk:!0}},{title:"SQL语句",frontmatter:{autoSort:99,title:"SQL语句",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/e5c957/",categories:["后端","数据库","MySQL"],tags:["知识","数据库","MySQL"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/01.MySQL/01.SQL%E8%AF%AD%E5%8F%A5.html",relativePath:"01.后端/30.数据库/01.MySQL/01.SQL语句.md",key:"v-4382d8a8",path:"/pages/e5c957/",headers:[{level:2,title:"DDL",slug:"ddl",normalizedTitle:"ddl",charIndex:127},{level:3,title:"数据库操作",slug:"数据库操作",normalizedTitle:"数据库操作",charIndex:190},{level:3,title:"表操作",slug:"表操作",normalizedTitle:"表操作",charIndex:497},{level:2,title:"DML",slug:"dml",normalizedTitle:"dml",charIndex:1831},{level:2,title:"DQL",slug:"dql",normalizedTitle:"dql",charIndex:3039},{level:3,title:"基本语法",slug:"基本语法",normalizedTitle:"基本语法",charIndex:3105},{level:3,title:"基础查询",slug:"基础查询",normalizedTitle:"基础查询",charIndex:3114},{level:3,title:"条件查询(where)",slug:"条件查询-where",normalizedTitle:"条件查询(where)",charIndex:3412},{level:3,title:"聚合函数",slug:"聚合函数",normalizedTitle:"聚合函数",charIndex:4779},{level:3,title:"分组查询(group by)(having)",slug:"分组查询-group-by-having",normalizedTitle:"分组查询(group by)(having)",charIndex:5275},{level:3,title:"排序查询",slug:"排序查询",normalizedTitle:"排序查询",charIndex:5849},{level:3,title:"分页查询",slug:"分页查询",normalizedTitle:"分页查询",charIndex:6103},{level:3,title:"案例",slug:"案例",normalizedTitle:"案例",charIndex:6423},{level:2,title:"DCL",slug:"dcl",normalizedTitle:"dcl",charIndex:12152},{level:3,title:"用户管理",slug:"用户管理",normalizedTitle:"用户管理",charIndex:12160},{level:3,title:"权限控制",slug:"权限控制",normalizedTitle:"权限控制",charIndex:12701}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"DDL 数据库操作 表操作 DML DQL 基本语法 基础查询 条件查询(where) 聚合函数 分组查询(group by)(having) 排序查询 分页查询 案例 DCL 用户管理 权限控制",content:"1、SQL语句可以单行书写，也可以多行书写，以分号结尾。 2、MySQL数据库的SQL语句不区分大小写，关键字建议大写。 3、注释： 单行注释：--注释内容 或 #注释内容（MySQL特有） 多行注释：/* 注释*/\n\n4、 SQL语句分类\n\n\n\n\n# DDL\n\nData Definition Language，数据定义语言，用来定义数据库对象(数据库，表，字段) 。\n\n\n# 数据库操作\n\n 1. 查询数据库\n    * show databases; —— 查询所有数据库的名称\n    * select database() ; ——查询当前使用的数据库\n 2. 创建数据库\n    * create database db1; ——db1指的是数据库名称\n    * create database if not exists db1;\n 3. 删除数据库\n    * drop database db2; ——db2指的是数据库名称\n    * drop database if exists db2;\n 4. 切换数据库\n    * use db1;——使用数据库db1\n\n\n# 表操作\n\n 1. 查询\n    \n    * show tables;—— 查询当前数据库下所有表的名称\n    * desc t1; ——t1是表名称 查询表结构\n\n 2. 创建\n    \n    建表语句\n    \n    create table emp(\n        id int comment '编号',\n        workno varchar(10) comment '工号',\n        name varchar(10) comment '姓名',\n        gender char(1) comment '性别',\n        age tinyint unsigned comment '年龄',\n        idcard char(18) comment '身份证号',\n        workaddress varchar(50) comment '工作地址',\n        entrydate date comment '入职时间'\n    )comment '员工表';\n    \n    \n    常用类型\n    \n    int     4bytes        整型            --id int\n    double  8bytes        双精度浮点数值    --score double(5,2)  --总长度5位，小数点后面长度2位\n    \n    char    0-255bytes    定长字符串       --name char(10)      \n    --总长度10位，空白地方补0;存储性能高，浪费空间\n    varchar 0-65535bytes  变长字符串       --name varchar(10)   \n    --总长度为name的实际长度;存储性能低，节约空间\n    \n    date  3bytes   日期值\n    time  3bytes   时间值\n    datetime  8bytes  混合日期和时间值\n    \n\n 3. 删除（删除表时，数据会一块删除）\n    \n    * drop table t2;——t2指的表名\n    * drop table if exists t2;\n\n 4. 修改\n    \n    * alter table t1 rename to t2; --修改表名 #t1为原表名，t2为要修改的表名。\n    * alter table t1 add row1 int; --添加一列 #row1为添加的列名，int为数据类型。\n    * alter table t1 modify row1 char;--修改数据类型 #char为要修改的数据类型。\n    * alter table t1 change row1 row2 varchar;\n    * --修改列名和和数据类型 #row1为原列名，row2为新列名，varchar为要修改的数据类型\n    * alter table t1 drop row2; --删除列 #row2为要删除的列\n\n\n# DML\n\nDML英文全称是Data Manipulation Language(数据操作语言)，用来对数据库中表的数据记录进 行增、删、改操作。\n\n * 添加数据（INSERT）\n * 修改数据（UPDATE）\n * 删除数据（DELETE）\n\n 1. 添加数据（INSERT）\n    \n    * 指定列添加\n      \n      * INSERT into stu(id,NAME) VALUES(1,\"diana\");\n    \n    * 给全部列添加\n      \n      INSERT into stu(id,NAME,sex,birthday,score,address,phone,state) VALUES(1,\"张三\",'男','1999-07-14',88.88,'123@qq.com','17854201283',0);\n      \n      INSERT into stu VALUES(1,\"张三\",'男','1999-07-14',88.88,'123@qq.com','17854201283',0); \n      \n    \n    * 批量添加\n      \n      * 批量添加某几列\n        \n        * INSERT into stu(id,NAME) VALUES(2,\"张三\"),(3,\"张三\"),(4,\"张三\");\n      \n      * 批量添加全部\n        \n        * INSERT into stu \n          VALUES(2,\"张三\",'男','1999-07-14',88.88,'123@qq.com','17854201283',0),\n          (3,\"张三\",'男','1999-07-14',88.88,'123@qq.com','17854201283',0),\n          (4,\"张三\",'男','1999-07-14',88.88,'123@qq.com','17854201283',0);\n          \n\n 2. 修改数据（UPDATE）\n    \n    按照条件(where )修改数据 ---\x3e若不加限制where,则会修改全部相关数据\n    \n    UPDATE stu set sex='女' where name='张三';\n    UPDATE stu set birthday='2000-01-01',score=99.99 where name='张三';\n    \n\n 3. 删除数据（DELETE）\n    \n    按照条件(where )删除数据 ---\x3e若不加限制where,则会删除表中的全部数据\n    \n    DELETE FROM stu where name='da';\n    \n\n\n# DQL\n\nDQL英文全称是Data Query Language(数据查询语言)，数据查询语言，用来查询数据库中表的记录。\n\n\n# 基本语法\n\n\n# 基础查询\n\nselect name,age from stu; -- 查询两列\n\nSELECT address from stu;  -- 有重复\nSELECT DISTINCT address from stu ; -- DISTINCT，去除重复记录\n\nSELECT name,math as 数学成绩,english as 英语成绩 from stu; -- as，起别名 as可省略\n\nSELECT * from stu;  --查询全部，建议全写上\nselect id,name,age,sex,birthday,math,english,address from stu;\n\n\n\n# 条件查询(where)\n\n * 精准查询\n   \n   SELECT *from stu where age>20; -- 查询年龄大于20岁的学员信息\n   \n   select *from stu where age>=21; -- 查询年龄大于等于21岁的学员信息\n   \n   SELECT *from stu where age>=20 AND age<=30; -- 查询年龄大于等于20，且小于等于30的信息\n   SELECT *from stu where age BETWEEN 20 AND 30; -- 查询年龄大于等于20，且小于等于30的信息\n   \n   SELECT *from stu where  birthday BETWEEN '1999-01-01' AND '1999-04-01'; -- 查询日期（date类型）在01-01 到04-01之间  --date类型的数据也可以比较\n   \n   SELECT *from stu where age =16; -- 查询等于16   -- 一个等号就是等于，不同于一般的编程语言\n   \n   SELECT *from stu where age !=16; -- 查询不等于16\n   SELECT *from stu where age <>16; -- 查询不等于16\n   \n   SELECT *from stu WHERE age=18 or age=16 or age=30;-- 查询等于16 或者等于18 或者等于30\n   SELECT *from stu WHERE age in (16,18,30);-- 查询等于16 或者等于18 或者等于30\n   \n   SELECT *from stu WHERE english is null; -- 查询是否为null\n   SELECT *from stu WHERE english is not null; -- 查询是否为null\n   \n\n * 模糊查询(like)\n   \n   通配符:\n   \t_ :代表单个任意字符\n   \t% :代表任意个数字符\n   \n   SELECT *from stu WHERE name like '凯%'; -- 查询姓 凯 的学员信息\n   SELECT *from stu WHERE name like '_波%'; -- 查询第二个是 波 的学员信息\n   SELECT *from stu WHERE name like '%爱%'; -- 查询名字中含 爱 的学员信息\n   SELECT *from stu WHERE name like '[张李王]三'; -- 查询张三，李三，王三\n   SELECT *from stu WHERE name like '[^张李王]三'; -- 查询不是张三，李三，王三的 赵三等\n   select * from emp where name like '__';   #J. 查询姓名为两个字的员工信息\n   select * from emp where idcard like '%X'; #K. 查询身份证号最后一位是X的员工信息\n   \n\n\n# 聚合函数\n\n聚合函数: 将一列数据当作一个整体，进行纵向计算。 null值不参与所有聚合函数的运算\n\ncount(age)     统计数量(一般选用不为null的列)\n                -- 主键\n                -- *\nmax(age)       最大值\nmin(age)       最小值\nsum(age)       求和\navg(age)       求平均\n\nSELECT COUNT(id) from stu ; -- 统计共有多少个  --要统计的列中最好不要有null\nSELECT COUNT(*) from stu ;  -- 统计共有多少个  --只要某一行不全为null，就能完成统计\nSELECT max(math) from stu;  -- 查询数学成绩的最高分\nSELECT min(math) from stu;  -- 查询数学成绩的最低分\nSELECT sum(math) from stu;  -- 查询数学成绩的总分\nSELECT avg(math) from stu;  -- 查询数学成绩的平均分\n\n\n\n# 分组查询(group by)(having)\n\n查询字段为聚合函数和分组字段，其他字段没有意义 执行顺序 where>聚合函数>having\n\n * 所有where不能对聚合函数进行判断，having可以\n * where是分组之前进行限定，having是分组之后进行限定\n\n -- 查询男同学和女同学的各自数学平均分\nSELECT sex,avg(math) from stu GROUP BY sex;\n\n-- 查询男同学和女同学的各自英语平均分，以及各自人数\nSELECT sex,avg(english),count(*) from stu GROUP BY sex;\n\n-- 查询男同学和女同学的各自数学平均分，以及各自人数，要求分数低于70的不参与分组\nSELECT sex,avg(math),count(*) from stu where math>=70 GROUP BY sex;\n\n -- 查询男同学和女同学的各自数学平均分，以及各自人数，(分组前--where)要求分数低于70的不参与分组，且分组之后(--having)人数大于2\nSELECT sex,avg(english),count(*) from stu where math>=70  GROUP BY sex having count(*)>2;\n\n\n\n# 排序查询\n\n排序方式:\n\tASC, 升序排列 --默认值\n\tDESC, 降序排列\n\nSELECT * from stu order by age; -- 按年龄升序排列\nSELECT * from stu order by age desc; -- 按年龄降序排列\nSELECT * from stu order by math desc,english asc;-- 先按数学成绩降序，若数学成绩一样，在按照英语成绩升序\n\n注:如果有多个排序条件,当前边的条件值一样时，才会根据第二个条件排序。\n\n\n\n# 分页查询\n\nlimit 起始索引，查询条目数;  --起始索引默认从0开始\n\nselect * from stu LIMIT 0,3; -- 从0开始查询3条数据\n\n-- 起始索引=（当前页码-1）*每页显示的条数\nselect * from stu LIMIT 0,3; -- 每页显示3条数据，查询第一页\nselect * from stu LIMIT 3,3; -- 每页显示3条数据，查询第二页\nselect * from stu LIMIT 6,3;-- 每页显示3条数据，查询第三页\n\ntips:\n\tlimit是mysql数据库的方言\n\trownumber 是oracle的方言\n\ttop 是SQLServer的方言\n\n\n\n# 案例\n\n-- --建表\ncreate table emp(\n    id int comment '编号',\n    workno varchar(10) comment '工号',\n    name varchar(10) comment '姓名',\n    gender char(1) comment '性别',\n    age tinyint unsigned comment '年龄',\n    idcard char(18) comment '身份证号',\n    workaddress varchar(50) comment '工作地址',\n    entrydate date comment '入职时间'\n)comment '员工表';\n\n--  --数据插入\nINSERT INTO emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nVALUES (1, '00001', '柳岩666', '女', 20, '123456789012345678', '北京', '2000-01-01');\nINSERT INTO emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nVALUES (2, '00002', '张无忌', '男', 18, '123456789012345670', '北京', '2005-09-01');\nINSERT INTO emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nVALUES (3, '00003', '韦一笑', '男', 38, '123456789712345670', '上海', '2005-08-01');\nINSERT INTO emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nVALUES (4, '00004', '赵敏', '女', 18, '123456757123845670', '北京', '2009-12-01');\nINSERT INTO emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nVALUES (5, '00005', '小昭', '女', 16, '123456769012345678', '上海', '2007-07-01');\nINSERT INTO emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nVALUES (6, '00006', '杨逍', '男', 28, '12345678931234567X', '北京', '2006-01-01');\nINSERT INTO emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nVALUES (7, '00007', '范瑶', '男', 40, '123456789212345670', '北京', '2005-05-01');\nINSERT INTO emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nVALUES (8, '00008', '黛绮丝', '女', 38, '123456157123645670', '天津', '2015-05-01');\nINSERT INTO emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nVALUES (9, '00009', '范凉凉', '女', 45, '123156789012345678', '北京', '2010-04-01');\nINSERT INTO emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nVALUES (10, '00010', '陈友谅', '男', 53, '123456789012345670', '上海', '2011-01-01');\nINSERT INTO emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nVALUES (11, '00011', '张士诚', '男', 55, '123567897123465670', '江苏', '2015-05-01');\nINSERT INTO emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nVALUES (12, '00012', '常遇春', '男', 32, '123446757152345670', '北京', '2004-02-01');\nINSERT INTO emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nVALUES (13, '00013', '张三丰', '男', 88, '123656789012345678', '江苏', '2020-11-01');\nINSERT INTO emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nVALUES (14, '00014', '灭绝', '女', 65, '123456719012345670', '西安', '2019-05-01');\nINSERT INTO emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nVALUES (15, '00015', '胡青牛', '男', 70, '12345674971234567X', '西安', '2018-04-01');\nINSERT INTO emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nVALUES (16, '00016', '周芷若', '女', 18, null, '北京', '2012-06-01');\n\n-- --条件查询\n-- A.查询年龄等于 88 的员工\nselect * from emp where age = 88;\n-- B. 查询年龄小于 20 的员工信息\nselect * from emp where age < 20;\n-- C. 查询年龄小于等于 20 的员工信息\nselect * from emp where age <= 20;\n-- D. 查询没有身份证号的员工信息\nselect * from emp where idcard is null;\n-- E. 查询有身份证号的员工信息\nselect * from emp where idcard is not null;\n-- F. 查询年龄不等于 88 的员工信息\nselect * from emp where age != 88;\n-- G. 查询年龄在15岁(包含) 到 20岁(包含)之间的员工信息\nselect * from emp where age between 15 and 20;\nselect * from emp where age >= 15 && age <= 20;\nselect * from emp where age >= 15 and age <= 20;\n-- H. 查询性别为 女 且年龄小于 25岁的员工信息\nselect * from emp where gender = '女' and age < 25;\n-- I. 查询年龄等于18 或 20 或 40 的员工信息\nselect * from emp where age = 18 or age = 20 or age = 40;\nselect * from emp where age in (18, 20, 40);\n-- J. 查询姓名为两个字的员工信息 _ %\nselect * from emp where name like '__';\n-- K. 查询身份证号最后一位是X的员工信息\nselect * from emp where idcard like '%X';\n\n\n-- --聚合函数\n-- A. 统计该企业员工数量\nselect count(*) from emp; -- 16\nselect count(id) from emp; -- 16\nselect count(idcard) from emp; -- 15  不记录null值\n-- B. 统计该企业员工的平均年龄\nselect avg(age) from emp;\n-- C. 统计该企业员工的最大年龄\nselect max(age) from emp;\n-- D. 统计该企业员工的最小年龄\nselect min(age) from emp;\n-- E. 统计西安地区员工的年龄之和\nselect sum(age) from emp where workaddress = '西安';\n\n\n-- -- 分组查询\n-- A. 根据性别分组 , 统计男性员工 和 女性员工的数量\nselect gender, count(*) from emp group by gender;\n-- B. 根据性别分组 , 统计男性员工 和 女性员工的平均年龄\nselect gender, avg(age) from emp group by gender;\n-- C. 查询年龄小于45的员工 , 并根据工作地址分组 , 获取员工数量大于等于3的工作地址\n-- having 是分组以后在进行过滤\n-- address_count 起别名\nselect workaddress, count(*) address_count from emp where age < 45 group by workaddress having address_count >= 3;\n-- D. 统计各个工作地址上班的男性及女性员工的数量\nselect workaddress, gender, count(*) from emp  group by gender, workaddress;\n\n\n-- -- 排序查询\n-- A. 根据年龄对公司的员工进行升序排序\nselect * from emp order by age;\nselect * from emp order by age asc;  -- asc 升序\n-- B. 根据入职时间, 对员工进行降序排序\nselect * from emp order by entrydate desc; -- desc 降序\n-- C. 根据年龄对公司的员工进行升序排序 , 年龄相同 , 再按照入职时间进行降序排序\nselect * from emp order by age, entrydate desc;\n\n\n-- -- 分页查询\n-- A. 查询第1页员工数据, 每页展示10条记录\n-- 0 是起始索引\n-- 10 是每页限制条数\nselect * from emp limit 0,10;\n-- B. 查询第2页员工数据, 每页展示10条记录 --------\x3e (页码-1)*页展示记录数  （2-1）* 10 = 10\nselect * from emp limit 10,10;\n\n\n-- -- 综合练习\n-- 1). 查询年龄为20,21,22,23岁的女性员工信息。\nselect * from emp where age in (20, 21, 22, 23) and gender = '女';\n-- 2). 查询性别为 男 ，并且年龄在 20-40 岁(含)以内的姓名为三个字的员工。\nselect * from emp where (age between 20 and 40) and gender = '男' and name like '___';\n-- 3). 统计员工表中, 年龄小于60岁的 , 男性员工和女性员工的人数。\nselect gender, count(*) from emp where age < 60 group by gender;\n-- 4). 查询所有年龄小于等于35岁员工的姓名和年龄，并对查询结果按年龄升序排序，如果年龄相同按入职时间降序排序。\nselect name, age from emp where age <= 35 order by age, entrydate desc;\n-- 5). 查询性别为男，且年龄在20-40 岁(含)以内的前3个员工信息，对查询的结果按年龄升序排序，年龄相同按入职时间升序排序。\nselect * from emp where gender = '男' and (age between 20 and 40) order by age, entrydate  limit 0, 3;\n\n\n\n# DCL\n\n\n# 用户管理\n\n 1. 查询用户\n    \n    select * from mysql.user;\n    \n\n 2. 创建用户\n    \n    CREATE USER '用户名'@'主机名' IDENTIFIED BY '密码';\n    CREATE USER 'diana'@'localhost' IDENTIFIED BY '123456'; -- localhost 只能在本机访问\n    CREATE USER 'diana'@'%' IDENTIFIED BY '123456';  -- % 代表任意主机都可以访问\n    \n\n 3. 修改用户密码\n    \n    ALTER USER '用户名'@'主机名' IDENTIFIED WITH mysql_native_password BY '新密码';\n    ALTER USER 'diana'@'localhost' IDENTIFIED WITH mysql_native_password BY '1234';\n    \n\n 4. 删除用户\n    \n    DROP USER '用户名'@'主机名';\n    DROP USER 'diana'@'localhost';\n    \n\n\n# 权限控制\n\n 1. 查询权限\n    \n    SHOW GRANTS FOR '用户名'@'主机名';\n    show grants for 'diana'@'%';\n    \n\n 2. 授予权限\n    \n    GRANT 权限列表 ON 数据库名.表名 TO '用户名'@'主机名';\n    grant all on db1.emp to 'diana'@'%';\n    grant all on db1.* to 'diana'@'%';  -- * 表示通配符；数据库名和表名 都可以使用通配符\n    \n\n 3. 撤销权限\n    \n    REVOKE 权限列表 ON 数据库名.表名 FROM '用户名'@'主机名';\n    revoke all on db1.emp from 'diana'@'%';\n    ",normalizedContent:"1、sql语句可以单行书写，也可以多行书写，以分号结尾。 2、mysql数据库的sql语句不区分大小写，关键字建议大写。 3、注释： 单行注释：--注释内容 或 #注释内容（mysql特有） 多行注释：/* 注释*/\n\n4、 sql语句分类\n\n\n\n\n# ddl\n\ndata definition language，数据定义语言，用来定义数据库对象(数据库，表，字段) 。\n\n\n# 数据库操作\n\n 1. 查询数据库\n    * show databases; —— 查询所有数据库的名称\n    * select database() ; ——查询当前使用的数据库\n 2. 创建数据库\n    * create database db1; ——db1指的是数据库名称\n    * create database if not exists db1;\n 3. 删除数据库\n    * drop database db2; ——db2指的是数据库名称\n    * drop database if exists db2;\n 4. 切换数据库\n    * use db1;——使用数据库db1\n\n\n# 表操作\n\n 1. 查询\n    \n    * show tables;—— 查询当前数据库下所有表的名称\n    * desc t1; ——t1是表名称 查询表结构\n\n 2. 创建\n    \n    建表语句\n    \n    create table emp(\n        id int comment '编号',\n        workno varchar(10) comment '工号',\n        name varchar(10) comment '姓名',\n        gender char(1) comment '性别',\n        age tinyint unsigned comment '年龄',\n        idcard char(18) comment '身份证号',\n        workaddress varchar(50) comment '工作地址',\n        entrydate date comment '入职时间'\n    )comment '员工表';\n    \n    \n    常用类型\n    \n    int     4bytes        整型            --id int\n    double  8bytes        双精度浮点数值    --score double(5,2)  --总长度5位，小数点后面长度2位\n    \n    char    0-255bytes    定长字符串       --name char(10)      \n    --总长度10位，空白地方补0;存储性能高，浪费空间\n    varchar 0-65535bytes  变长字符串       --name varchar(10)   \n    --总长度为name的实际长度;存储性能低，节约空间\n    \n    date  3bytes   日期值\n    time  3bytes   时间值\n    datetime  8bytes  混合日期和时间值\n    \n\n 3. 删除（删除表时，数据会一块删除）\n    \n    * drop table t2;——t2指的表名\n    * drop table if exists t2;\n\n 4. 修改\n    \n    * alter table t1 rename to t2; --修改表名 #t1为原表名，t2为要修改的表名。\n    * alter table t1 add row1 int; --添加一列 #row1为添加的列名，int为数据类型。\n    * alter table t1 modify row1 char;--修改数据类型 #char为要修改的数据类型。\n    * alter table t1 change row1 row2 varchar;\n    * --修改列名和和数据类型 #row1为原列名，row2为新列名，varchar为要修改的数据类型\n    * alter table t1 drop row2; --删除列 #row2为要删除的列\n\n\n# dml\n\ndml英文全称是data manipulation language(数据操作语言)，用来对数据库中表的数据记录进 行增、删、改操作。\n\n * 添加数据（insert）\n * 修改数据（update）\n * 删除数据（delete）\n\n 1. 添加数据（insert）\n    \n    * 指定列添加\n      \n      * insert into stu(id,name) values(1,\"diana\");\n    \n    * 给全部列添加\n      \n      insert into stu(id,name,sex,birthday,score,address,phone,state) values(1,\"张三\",'男','1999-07-14',88.88,'123@qq.com','17854201283',0);\n      \n      insert into stu values(1,\"张三\",'男','1999-07-14',88.88,'123@qq.com','17854201283',0); \n      \n    \n    * 批量添加\n      \n      * 批量添加某几列\n        \n        * insert into stu(id,name) values(2,\"张三\"),(3,\"张三\"),(4,\"张三\");\n      \n      * 批量添加全部\n        \n        * insert into stu \n          values(2,\"张三\",'男','1999-07-14',88.88,'123@qq.com','17854201283',0),\n          (3,\"张三\",'男','1999-07-14',88.88,'123@qq.com','17854201283',0),\n          (4,\"张三\",'男','1999-07-14',88.88,'123@qq.com','17854201283',0);\n          \n\n 2. 修改数据（update）\n    \n    按照条件(where )修改数据 ---\x3e若不加限制where,则会修改全部相关数据\n    \n    update stu set sex='女' where name='张三';\n    update stu set birthday='2000-01-01',score=99.99 where name='张三';\n    \n\n 3. 删除数据（delete）\n    \n    按照条件(where )删除数据 ---\x3e若不加限制where,则会删除表中的全部数据\n    \n    delete from stu where name='da';\n    \n\n\n# dql\n\ndql英文全称是data query language(数据查询语言)，数据查询语言，用来查询数据库中表的记录。\n\n\n# 基本语法\n\n\n# 基础查询\n\nselect name,age from stu; -- 查询两列\n\nselect address from stu;  -- 有重复\nselect distinct address from stu ; -- distinct，去除重复记录\n\nselect name,math as 数学成绩,english as 英语成绩 from stu; -- as，起别名 as可省略\n\nselect * from stu;  --查询全部，建议全写上\nselect id,name,age,sex,birthday,math,english,address from stu;\n\n\n\n# 条件查询(where)\n\n * 精准查询\n   \n   select *from stu where age>20; -- 查询年龄大于20岁的学员信息\n   \n   select *from stu where age>=21; -- 查询年龄大于等于21岁的学员信息\n   \n   select *from stu where age>=20 and age<=30; -- 查询年龄大于等于20，且小于等于30的信息\n   select *from stu where age between 20 and 30; -- 查询年龄大于等于20，且小于等于30的信息\n   \n   select *from stu where  birthday between '1999-01-01' and '1999-04-01'; -- 查询日期（date类型）在01-01 到04-01之间  --date类型的数据也可以比较\n   \n   select *from stu where age =16; -- 查询等于16   -- 一个等号就是等于，不同于一般的编程语言\n   \n   select *from stu where age !=16; -- 查询不等于16\n   select *from stu where age <>16; -- 查询不等于16\n   \n   select *from stu where age=18 or age=16 or age=30;-- 查询等于16 或者等于18 或者等于30\n   select *from stu where age in (16,18,30);-- 查询等于16 或者等于18 或者等于30\n   \n   select *from stu where english is null; -- 查询是否为null\n   select *from stu where english is not null; -- 查询是否为null\n   \n\n * 模糊查询(like)\n   \n   通配符:\n   \t_ :代表单个任意字符\n   \t% :代表任意个数字符\n   \n   select *from stu where name like '凯%'; -- 查询姓 凯 的学员信息\n   select *from stu where name like '_波%'; -- 查询第二个是 波 的学员信息\n   select *from stu where name like '%爱%'; -- 查询名字中含 爱 的学员信息\n   select *from stu where name like '[张李王]三'; -- 查询张三，李三，王三\n   select *from stu where name like '[^张李王]三'; -- 查询不是张三，李三，王三的 赵三等\n   select * from emp where name like '__';   #j. 查询姓名为两个字的员工信息\n   select * from emp where idcard like '%x'; #k. 查询身份证号最后一位是x的员工信息\n   \n\n\n# 聚合函数\n\n聚合函数: 将一列数据当作一个整体，进行纵向计算。 null值不参与所有聚合函数的运算\n\ncount(age)     统计数量(一般选用不为null的列)\n                -- 主键\n                -- *\nmax(age)       最大值\nmin(age)       最小值\nsum(age)       求和\navg(age)       求平均\n\nselect count(id) from stu ; -- 统计共有多少个  --要统计的列中最好不要有null\nselect count(*) from stu ;  -- 统计共有多少个  --只要某一行不全为null，就能完成统计\nselect max(math) from stu;  -- 查询数学成绩的最高分\nselect min(math) from stu;  -- 查询数学成绩的最低分\nselect sum(math) from stu;  -- 查询数学成绩的总分\nselect avg(math) from stu;  -- 查询数学成绩的平均分\n\n\n\n# 分组查询(group by)(having)\n\n查询字段为聚合函数和分组字段，其他字段没有意义 执行顺序 where>聚合函数>having\n\n * 所有where不能对聚合函数进行判断，having可以\n * where是分组之前进行限定，having是分组之后进行限定\n\n -- 查询男同学和女同学的各自数学平均分\nselect sex,avg(math) from stu group by sex;\n\n-- 查询男同学和女同学的各自英语平均分，以及各自人数\nselect sex,avg(english),count(*) from stu group by sex;\n\n-- 查询男同学和女同学的各自数学平均分，以及各自人数，要求分数低于70的不参与分组\nselect sex,avg(math),count(*) from stu where math>=70 group by sex;\n\n -- 查询男同学和女同学的各自数学平均分，以及各自人数，(分组前--where)要求分数低于70的不参与分组，且分组之后(--having)人数大于2\nselect sex,avg(english),count(*) from stu where math>=70  group by sex having count(*)>2;\n\n\n\n# 排序查询\n\n排序方式:\n\tasc, 升序排列 --默认值\n\tdesc, 降序排列\n\nselect * from stu order by age; -- 按年龄升序排列\nselect * from stu order by age desc; -- 按年龄降序排列\nselect * from stu order by math desc,english asc;-- 先按数学成绩降序，若数学成绩一样，在按照英语成绩升序\n\n注:如果有多个排序条件,当前边的条件值一样时，才会根据第二个条件排序。\n\n\n\n# 分页查询\n\nlimit 起始索引，查询条目数;  --起始索引默认从0开始\n\nselect * from stu limit 0,3; -- 从0开始查询3条数据\n\n-- 起始索引=（当前页码-1）*每页显示的条数\nselect * from stu limit 0,3; -- 每页显示3条数据，查询第一页\nselect * from stu limit 3,3; -- 每页显示3条数据，查询第二页\nselect * from stu limit 6,3;-- 每页显示3条数据，查询第三页\n\ntips:\n\tlimit是mysql数据库的方言\n\trownumber 是oracle的方言\n\ttop 是sqlserver的方言\n\n\n\n# 案例\n\n-- --建表\ncreate table emp(\n    id int comment '编号',\n    workno varchar(10) comment '工号',\n    name varchar(10) comment '姓名',\n    gender char(1) comment '性别',\n    age tinyint unsigned comment '年龄',\n    idcard char(18) comment '身份证号',\n    workaddress varchar(50) comment '工作地址',\n    entrydate date comment '入职时间'\n)comment '员工表';\n\n--  --数据插入\ninsert into emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nvalues (1, '00001', '柳岩666', '女', 20, '123456789012345678', '北京', '2000-01-01');\ninsert into emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nvalues (2, '00002', '张无忌', '男', 18, '123456789012345670', '北京', '2005-09-01');\ninsert into emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nvalues (3, '00003', '韦一笑', '男', 38, '123456789712345670', '上海', '2005-08-01');\ninsert into emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nvalues (4, '00004', '赵敏', '女', 18, '123456757123845670', '北京', '2009-12-01');\ninsert into emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nvalues (5, '00005', '小昭', '女', 16, '123456769012345678', '上海', '2007-07-01');\ninsert into emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nvalues (6, '00006', '杨逍', '男', 28, '12345678931234567x', '北京', '2006-01-01');\ninsert into emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nvalues (7, '00007', '范瑶', '男', 40, '123456789212345670', '北京', '2005-05-01');\ninsert into emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nvalues (8, '00008', '黛绮丝', '女', 38, '123456157123645670', '天津', '2015-05-01');\ninsert into emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nvalues (9, '00009', '范凉凉', '女', 45, '123156789012345678', '北京', '2010-04-01');\ninsert into emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nvalues (10, '00010', '陈友谅', '男', 53, '123456789012345670', '上海', '2011-01-01');\ninsert into emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nvalues (11, '00011', '张士诚', '男', 55, '123567897123465670', '江苏', '2015-05-01');\ninsert into emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nvalues (12, '00012', '常遇春', '男', 32, '123446757152345670', '北京', '2004-02-01');\ninsert into emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nvalues (13, '00013', '张三丰', '男', 88, '123656789012345678', '江苏', '2020-11-01');\ninsert into emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nvalues (14, '00014', '灭绝', '女', 65, '123456719012345670', '西安', '2019-05-01');\ninsert into emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nvalues (15, '00015', '胡青牛', '男', 70, '12345674971234567x', '西安', '2018-04-01');\ninsert into emp (id, workno, name, gender, age, idcard, workaddress, entrydate)\nvalues (16, '00016', '周芷若', '女', 18, null, '北京', '2012-06-01');\n\n-- --条件查询\n-- a.查询年龄等于 88 的员工\nselect * from emp where age = 88;\n-- b. 查询年龄小于 20 的员工信息\nselect * from emp where age < 20;\n-- c. 查询年龄小于等于 20 的员工信息\nselect * from emp where age <= 20;\n-- d. 查询没有身份证号的员工信息\nselect * from emp where idcard is null;\n-- e. 查询有身份证号的员工信息\nselect * from emp where idcard is not null;\n-- f. 查询年龄不等于 88 的员工信息\nselect * from emp where age != 88;\n-- g. 查询年龄在15岁(包含) 到 20岁(包含)之间的员工信息\nselect * from emp where age between 15 and 20;\nselect * from emp where age >= 15 && age <= 20;\nselect * from emp where age >= 15 and age <= 20;\n-- h. 查询性别为 女 且年龄小于 25岁的员工信息\nselect * from emp where gender = '女' and age < 25;\n-- i. 查询年龄等于18 或 20 或 40 的员工信息\nselect * from emp where age = 18 or age = 20 or age = 40;\nselect * from emp where age in (18, 20, 40);\n-- j. 查询姓名为两个字的员工信息 _ %\nselect * from emp where name like '__';\n-- k. 查询身份证号最后一位是x的员工信息\nselect * from emp where idcard like '%x';\n\n\n-- --聚合函数\n-- a. 统计该企业员工数量\nselect count(*) from emp; -- 16\nselect count(id) from emp; -- 16\nselect count(idcard) from emp; -- 15  不记录null值\n-- b. 统计该企业员工的平均年龄\nselect avg(age) from emp;\n-- c. 统计该企业员工的最大年龄\nselect max(age) from emp;\n-- d. 统计该企业员工的最小年龄\nselect min(age) from emp;\n-- e. 统计西安地区员工的年龄之和\nselect sum(age) from emp where workaddress = '西安';\n\n\n-- -- 分组查询\n-- a. 根据性别分组 , 统计男性员工 和 女性员工的数量\nselect gender, count(*) from emp group by gender;\n-- b. 根据性别分组 , 统计男性员工 和 女性员工的平均年龄\nselect gender, avg(age) from emp group by gender;\n-- c. 查询年龄小于45的员工 , 并根据工作地址分组 , 获取员工数量大于等于3的工作地址\n-- having 是分组以后在进行过滤\n-- address_count 起别名\nselect workaddress, count(*) address_count from emp where age < 45 group by workaddress having address_count >= 3;\n-- d. 统计各个工作地址上班的男性及女性员工的数量\nselect workaddress, gender, count(*) from emp  group by gender, workaddress;\n\n\n-- -- 排序查询\n-- a. 根据年龄对公司的员工进行升序排序\nselect * from emp order by age;\nselect * from emp order by age asc;  -- asc 升序\n-- b. 根据入职时间, 对员工进行降序排序\nselect * from emp order by entrydate desc; -- desc 降序\n-- c. 根据年龄对公司的员工进行升序排序 , 年龄相同 , 再按照入职时间进行降序排序\nselect * from emp order by age, entrydate desc;\n\n\n-- -- 分页查询\n-- a. 查询第1页员工数据, 每页展示10条记录\n-- 0 是起始索引\n-- 10 是每页限制条数\nselect * from emp limit 0,10;\n-- b. 查询第2页员工数据, 每页展示10条记录 --------\x3e (页码-1)*页展示记录数  （2-1）* 10 = 10\nselect * from emp limit 10,10;\n\n\n-- -- 综合练习\n-- 1). 查询年龄为20,21,22,23岁的女性员工信息。\nselect * from emp where age in (20, 21, 22, 23) and gender = '女';\n-- 2). 查询性别为 男 ，并且年龄在 20-40 岁(含)以内的姓名为三个字的员工。\nselect * from emp where (age between 20 and 40) and gender = '男' and name like '___';\n-- 3). 统计员工表中, 年龄小于60岁的 , 男性员工和女性员工的人数。\nselect gender, count(*) from emp where age < 60 group by gender;\n-- 4). 查询所有年龄小于等于35岁员工的姓名和年龄，并对查询结果按年龄升序排序，如果年龄相同按入职时间降序排序。\nselect name, age from emp where age <= 35 order by age, entrydate desc;\n-- 5). 查询性别为男，且年龄在20-40 岁(含)以内的前3个员工信息，对查询的结果按年龄升序排序，年龄相同按入职时间升序排序。\nselect * from emp where gender = '男' and (age between 20 and 40) order by age, entrydate  limit 0, 3;\n\n\n\n# dcl\n\n\n# 用户管理\n\n 1. 查询用户\n    \n    select * from mysql.user;\n    \n\n 2. 创建用户\n    \n    create user '用户名'@'主机名' identified by '密码';\n    create user 'diana'@'localhost' identified by '123456'; -- localhost 只能在本机访问\n    create user 'diana'@'%' identified by '123456';  -- % 代表任意主机都可以访问\n    \n\n 3. 修改用户密码\n    \n    alter user '用户名'@'主机名' identified with mysql_native_password by '新密码';\n    alter user 'diana'@'localhost' identified with mysql_native_password by '1234';\n    \n\n 4. 删除用户\n    \n    drop user '用户名'@'主机名';\n    drop user 'diana'@'localhost';\n    \n\n\n# 权限控制\n\n 1. 查询权限\n    \n    show grants for '用户名'@'主机名';\n    show grants for 'diana'@'%';\n    \n\n 2. 授予权限\n    \n    grant 权限列表 on 数据库名.表名 to '用户名'@'主机名';\n    grant all on db1.emp to 'diana'@'%';\n    grant all on db1.* to 'diana'@'%';  -- * 表示通配符；数据库名和表名 都可以使用通配符\n    \n\n 3. 撤销权限\n    \n    revoke 权限列表 on 数据库名.表名 from '用户名'@'主机名';\n    revoke all on db1.emp from 'diana'@'%';\n    ",charsets:{cjk:!0}},{title:"Redis常见命令",frontmatter:{autoSort:99,title:"Redis常见命令",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/948263/",categories:["后端","数据库","Redis"],tags:["知识","数据库","Redis"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/05.Redis/10.Redis%E5%B8%B8%E8%A7%81%E5%91%BD%E4%BB%A4.html",relativePath:"01.后端/30.数据库/05.Redis/10.Redis常见命令.md",key:"v-11f9a58f",path:"/pages/948263/",headers:[{level:2,title:"Redis数据结构介绍",slug:"redis数据结构介绍",normalizedTitle:"redis数据结构介绍",charIndex:2},{level:2,title:"Redis 通用命令",slug:"redis-通用命令",normalizedTitle:"redis 通用命令",charIndex:204},{level:2,title:"String命令",slug:"string命令",normalizedTitle:"string命令",charIndex:1918},{level:2,title:"Key的层级结构",slug:"key的层级结构",normalizedTitle:"key的层级结构",charIndex:3908},{level:2,title:"Hash命令",slug:"hash命令",normalizedTitle:"hash命令",charIndex:4532},{level:2,title:"List命令",slug:"list命令",normalizedTitle:"list命令",charIndex:6340},{level:2,title:"Set命令",slug:"set命令",normalizedTitle:"set命令",charIndex:7084},{level:2,title:"SortedSet类型",slug:"sortedset类型",normalizedTitle:"sortedset类型",charIndex:8629},{level:2,title:"GEO类型",slug:"geo类型",normalizedTitle:"geo类型",charIndex:10205},{level:2,title:"BitMap的命令",slug:"bitmap的命令",normalizedTitle:"bitmap的命令",charIndex:11403}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"Redis数据结构介绍 Redis 通用命令 String命令 Key的层级结构 Hash命令 List命令 Set命令 SortedSet类型 GEO类型 BitMap的命令",content:'# Redis数据结构介绍\n\nRedis是一个key-value的数据库，key一般是String类型，不过value的类型多种多样：\n\n\n\n贴心小建议：命令不要死记，学会查询就好啦\n\nRedis为了方便我们学习，将操作不同数据类型的命令也做了分组，在官网（ https://redis.io/commands ）可以查看到不同的命令：\n\n\n\n当然我们也可以通过Help命令来帮助我们去查看命令\n\n\n\n\n# Redis 通用命令\n\n通用指令是部分数据类型的，都可以使用的指令，常见的有：\n\n * KEYS：查看符合模板的所有key\n   \n   * **生成环境下尽量不要使用keys **\n     \n     * 模糊查询效率比较低，在加上单线程，所以会阻塞进程\n   \n   * 结合正则表达式来使用命令\n\n * DEL：删除一个指定的key\n\n * EXISTS：判断key是否存在\n   \n   * 存在返回1，不存在返回0\n\n * EXPIRE：给一个key设置有效期，有效期到期时该key会被自动删除\n   \n   * 单位秒\n\n * TTL：查看一个KEY的剩余有效期\n   \n   * -1 表示永久有效\n   * -2 表示key过期\n\n通过help [command] 可以查看一个命令的具体用法，例如：\n\n\n\n课堂代码如下\n\n * KEYS\n\n127.0.0.1:6379> keys *\n1) "name"\n2) "age"\n127.0.0.1:6379>\n\n# 查询以a开头的key\n127.0.0.1:6379> keys a*\n1) "age"\n127.0.0.1:6379>\n\n\n贴心小提示：在生产环境下，不推荐使用keys 命令，因为这个命令在key过多的情况下，效率不高\n\n * DEL\n\n127.0.0.1:6379> help del\n\n  DEL key [key ...]\n  summary: Delete a key\n  since: 1.0.0\n  group: generic\n\n127.0.0.1:6379> del name #删除单个\n(integer) 1  #成功删除1个\n\n127.0.0.1:6379> keys *\n1) "age"\n\n127.0.0.1:6379> MSET k1 v1 k2 v2 k3 v3 #批量添加数据\nOK\n\n127.0.0.1:6379> keys *\n1) "k3"\n2) "k2"\n3) "k1"\n4) "age"\n\n127.0.0.1:6379> del k1 k2 k3 k4\n(integer) 3   #此处返回的是成功删除的key，由于redis中只有k1,k2,k3 所以只成功删除3个，最终返回\n127.0.0.1:6379>\n\n127.0.0.1:6379> keys * #再查询全部的key\n1) "age"\t#只剩下一个了\n127.0.0.1:6379>\n\n\n贴心小提示：同学们在拷贝代码的时候，只需要拷贝对应的命令哦~\n\n * EXISTS\n\n127.0.0.1:6379> help EXISTS\n\n  EXISTS key [key ...]\n  summary: Determine if a key exists\n  since: 1.0.0\n  group: generic\n\n127.0.0.1:6379> exists age\n(integer) 1\n\n127.0.0.1:6379> exists name\n(integer) 0\n\n\n * EXPIRE\n\n贴心小提示：内存非常宝贵，对于一些数据，我们应当给他一些过期时间，当过期时间到了之后，他就会自动被删除~\n\n127.0.0.1:6379> expire age 10\n(integer) 1\n\n127.0.0.1:6379> ttl age\n(integer) 8\n\n127.0.0.1:6379> ttl age\n(integer) 6\n\n127.0.0.1:6379> ttl age\n(integer) -2\n\n127.0.0.1:6379> ttl age\n(integer) -2  #当这个key过期了，那么此时查询出来就是-2 \n\n127.0.0.1:6379> keys *\n(empty list or set)\n\n127.0.0.1:6379> set age 10 #如果没有设置过期时间\nOK\n\n127.0.0.1:6379> ttl age\n(integer) -1  # ttl的返回值就是-1\n\n\n\n# String命令\n\nString类型，也就是字符串类型，是Redis中最简单的存储类型。\n\n其value是字符串，不过根据字符串的格式不同，又可以分为3类：\n\n * string：普通字符串\n * int：整数类型，可以做自增.自减操作\n * float：浮点类型，可以做自增.自减操作\n\n\n\nString的常见命令有：\n\n * SET：添加或者修改已经存在的一个String类型的键值对\n\n * GET：根据key获取String类型的value\n\n * MSET：批量添加多个String类型的键值对\n\n * MGET：根据多个key获取多个String类型的value\n\n * INCR：让一个整型的key自增1\n\n * INCRBY:让一个整型的key自增并指定步长，例如：incrby num 2 让num值自增2\n\n * INCRBYFLOAT：让一个浮点类型的数字自增并指定步长\n\n * SETNX：添加一个String类型的键值对，前提是这个key不存在，否则不执行\n   \n   * 这个是真正的新增\n\n * SETEX：添加一个String类型的键值对，并且指定有效期\n   \n   * setex name 10 jack\n   * set name jack ex 10\n\n贴心小提示：以上命令除了INCRBYFLOAT 都是常用命令\n\n * SET 和GET: 如果key不存在则是新增，如果存在则是修改\n\n127.0.0.1:6379> set name Rose  //原来不存在\nOK\n\n127.0.0.1:6379> get name \n"Rose"\n\n127.0.0.1:6379> set name Jack //原来存在，就是修改\nOK\n\n127.0.0.1:6379> get name\n"Jack"\n\n\n * MSET和MGET\n\n127.0.0.1:6379> MSET k1 v1 k2 v2 k3 v3\nOK\n\n127.0.0.1:6379> MGET name age k1 k2 k3\n1) "Jack" //之前存在的name\n2) "10"   //之前存在的age\n3) "v1"\n4) "v2"\n5) "v3"\n\n\n * INCR和INCRBY和DECY\n\n127.0.0.1:6379> get age \n"10"\n\n127.0.0.1:6379> incr age //增加1\n(integer) 11\n    \n127.0.0.1:6379> get age //获得age\n"11"\n\n127.0.0.1:6379> incrby age 2 //一次增加2\n(integer) 13 //返回目前的age的值\n    \n127.0.0.1:6379> incrby age 2\n(integer) 15\n    \n127.0.0.1:6379> incrby age -1 //也可以增加负数，相当于减\n(integer) 14\n    \n127.0.0.1:6379> incrby age -2 //一次减少2个\n(integer) 12\n    \n127.0.0.1:6379> DECR age //相当于 incr 负数，减少正常用法\n(integer) 11\n    \n127.0.0.1:6379> get age \n"11"\n\n\n\n * SETNX\n\n127.0.0.1:6379> help setnx\n\n  SETNX key value\n  summary: Set the value of a key, only if the key does not exist\n  since: 1.0.0\n  group: string\n\n127.0.0.1:6379> set name Jack  //设置名称\nOK\n127.0.0.1:6379> setnx name lisi //如果key不存在，则添加成功\n(integer) 0\n127.0.0.1:6379> get name //由于name已经存在，所以lisi的操作失败\n"Jack"\n127.0.0.1:6379> setnx name2 lisi //name2 不存在，所以操作成功\n(integer) 1\n127.0.0.1:6379> get name2 \n"lisi"\n\n\n * SETEX\n\n127.0.0.1:6379> setex name 10 jack\nOK\n\n127.0.0.1:6379> ttl name\n(integer) 8\n\n127.0.0.1:6379> ttl name\n(integer) 7\n\n127.0.0.1:6379> ttl name\n(integer) 5\n\n\n\n# Key的层级结构\n\nRedis没有类似MySQL中的Table的概念，我们该如何区分不同类型的key呢？\n\n例如，需要存储用户.商品信息到redis，有一个用户id是1，有一个商品id恰好也是1，此时如果使用id作为key，那就会冲突了，该怎么办？\n\n我们可以通过给key添加前缀加以区分，不过这个前缀不是随便加的，有一定的规范：\n\nRedis的key允许有多个单词形成层级结构，多个单词之间用\':\'隔开，格式如下：\n\n\n\n这个格式并非固定，也可以根据自己的需求来删除或添加词条。\n\n例如我们的项目名称叫 heima，有user和product两种不同类型的数据，我们可以这样定义key：\n\n * user相关的key：heima:user:1\n\n * product相关的key：heima:product:1\n\n如果Value是一个Java对象，例如一个User对象，则可以将对象序列化为JSON字符串后存储：\n\nKEY               VALUE\nheima:user:1      {"id":1, "name": "Jack", "age": 21}\nheima:product:1   {"id":1, "name": "小米11", "price": 4999}\n\n一旦我们向redis采用这样的方式存储，那么在可视化界面中，redis会以层级结构来进行存储，形成类似于这样的结构，更加方便Redis获取数据\n\n\n\n\n# Hash命令\n\nHash类型，也叫散列，其value是一个无序字典，类似于Java中的HashMap结构。\n\nString结构是将对象序列化为JSON字符串后存储，当需要修改对象某个字段时很不方便：\n\n\n\nHash结构可以将对象中的每个字段独立存储，可以针对单个字段做CRUD：\n\n\n\nHash类型的常见命令\n\n * HSET key field value：添加或者修改hash类型key的field的值\n\n * HGET key field：获取一个hash类型key的field的值\n\n * HMSET：批量添加多个hash类型key的field的值\n\n * HMGET：批量获取多个hash类型key的field的值\n\n * HGETALL：获取一个hash类型的key中的所有的field和value\n\n * HKEYS：获取一个hash类型的key中的所有的field\n\n * HINCRBY:让一个hash类型key的字段值自增并指定步长\n\n * HSETNX：添加一个hash类型的key的field值，前提是这个field不存在，否则不执行\n\n贴心小提示：哈希结构也是我们以后实际开发中常用的命令哟\n\n * HSET和HGET\n\n127.0.0.1:6379> HSET heima:user:3 name Lucy//大key是 heima:user:3 小key是name，小value是Lucy\n(integer) 1\n127.0.0.1:6379> HSET heima:user:3 age 21// 如果操作不存在的数据，则是新增\n(integer) 1\n127.0.0.1:6379> HSET heima:user:3 age 17 //如果操作存在的数据，则是修改\n(integer) 0\n127.0.0.1:6379> HGET heima:user:3 name \n"Lucy"\n127.0.0.1:6379> HGET heima:user:3 age\n"17"\n\n\n * HMSET和HMGET\n\n127.0.0.1:6379> HMSET heima:user:4 name HanMeiMei\nOK\n127.0.0.1:6379> HMSET heima:user:4 name LiLei age 20 sex man\nOK\n127.0.0.1:6379> HMGET heima:user:4 name age sex\n1) "LiLei"\n2) "20"\n3) "man"\n\n\n * HGETALL\n\n127.0.0.1:6379> HGETALL heima:user:4\n1) "name"\n2) "LiLei"\n3) "age"\n4) "20"\n5) "sex"\n6) "man"\n\n\n * HKEYS和HVALS\n\n127.0.0.1:6379> HKEYS heima:user:4\n1) "name"\n2) "age"\n3) "sex"\n127.0.0.1:6379> HVALS heima:user:4\n1) "LiLei"\n2) "20"\n3) "man"\n\n\n * HINCRBY\n\n127.0.0.1:6379> HINCRBY  heima:user:4 age 2\n(integer) 22\n127.0.0.1:6379> HVALS heima:user:4\n1) "LiLei"\n2) "22"\n3) "man"\n127.0.0.1:6379> HINCRBY  heima:user:4 age -2\n(integer) 20\n\n\n * HSETNX\n\n127.0.0.1:6379> HSETNX heima:user4 sex woman\n(integer) 1\n127.0.0.1:6379> HGETALL heima:user:3\n1) "name"\n2) "Lucy"\n3) "age"\n4) "17"\n127.0.0.1:6379> HSETNX heima:user:3 sex woman\n(integer) 1\n127.0.0.1:6379> HGETALL heima:user:3\n1) "name"\n2) "Lucy"\n3) "age"\n4) "17"\n5) "sex"\n6) "woman"\n\n\n\n# List命令\n\nRedis中的List类型与Java中的LinkedList类似，可以看做是一个双向链表结构。既可以支持正向检索和也可以支持反向检索。\n\n特征也与LinkedList类似：\n\n * 有序\n   \n   * 按照插入顺序来存储\n\n * 元素可以重复\n\n * 插入和删除快\n\n * 查询速度一般\n\n常用来存储一个有序数据，例如：朋友圈点赞列表，评论列表等。\n\nList的常见命令有：\n\n * LPUSH key element ... ：向列表左侧插入一个或多个元素\n * LPOP key：移除并返回列表左侧的第一个元素，没有则返回nil\n * RPUSH key element ... ：向列表右侧插入一个或多个元素\n * RPOP key：移除并返回列表右侧的第一个元素\n * LRANGE key star end：返回一段角标范围内的所有元素\n * BLPOP和BRPOP：与LPOP和RPOP类似，只不过在没有元素时等待指定时间，而不是直接返回nil\n   * 阻塞式获取，当没有娶不到对应键值时，会等待一段时间\n\n\n\n * LPUSH和RPUSH\n\n127.0.0.1:6379> LPUSH users 1 2 3\n(integer) 3\n    3-2-1\n127.0.0.1:6379> RPUSH users 4 5 6\n(integer) 6\n    4-5-6\n\n\n * LPOP和RPOP\n\n127.0.0.1:6379> LPOP users\n"3"\n127.0.0.1:6379> RPOP users\n"6"\n\n\n * LRANGE\n\n127.0.0.1:6379> LRANGE users 1 2\n1) "1"\n2) "4"\n\n\n\n# Set命令\n\nRedis的Set结构与Java中的HashSet类似，可以看做是一个value为null的HashMap。因为也是一个hash表，因此具备与HashSet类似的特征：\n\n * 无序\n * 元素不可重复\n * 查找快\n * 支持交集.并集.差集等功能\n\nSet类型的常见命令\n\n * SADD key member ... ：向set中添加一个或多个元素\n\n * SREM key member ... : 移除set中的指定元素\n\n * SCARD key： 返回set中元素的个数\n\n * SISMEMBER key member：判断一个元素是否存在于set中\n\n * SMEMBERS：获取set中的所有元素\n\n * SINTER key1 key2 ... ：求key1与key2的交集\n\n * SDIFF key1 key2 ... ：求key1与key2的差集\n\n * SUNION key1 key2 ..：求key1和key2的并集\n\n例如两个集合：s1和s2:\n\n\n\n求交集：SINTER s1 s2\n\n求s1与s2的不同：SDIFF s1 s2\n\n\n\n具体命令\n\n127.0.0.1:6379> sadd s1 a b c\n(integer) 3\n127.0.0.1:6379> smembers s1\n1) "c"\n2) "b"\n3) "a"\n127.0.0.1:6379> srem s1 a\n(integer) 1\n    \n127.0.0.1:6379> SISMEMBER s1 a\n(integer) 0\n    \n127.0.0.1:6379> SISMEMBER s1 b\n(integer) 1\n    \n127.0.0.1:6379> SCARD s1\n(integer) 2\n\n\n案例\n\n * 将下列数据用Redis的Set集合来存储：\n * 张三的好友有：李四.王五.赵六\n * 李四的好友有：王五.麻子.二狗\n * 利用Set的命令实现下列功能：\n * 计算张三的好友有几人\n * 计算张三和李四有哪些共同好友\n * 查询哪些人是张三的好友却不是李四的好友\n * 查询张三和李四的好友总共有哪些人\n * 判断李四是否是张三的好友\n * 判断张三是否是李四的好友\n * 将李四从张三的好友列表中移除\n\n127.0.0.1:6379> SADD zs lisi wangwu zhaoliu\n(integer) 3\n    \n127.0.0.1:6379> SADD ls wangwu mazi ergou\n(integer) 3\n    \n127.0.0.1:6379> SCARD zs\n(integer) 3\n    \n127.0.0.1:6379> SINTER zs ls\n1) "wangwu"\n    \n127.0.0.1:6379> SDIFF zs ls\n1) "zhaoliu"\n2) "lisi"\n    \n127.0.0.1:6379> SUNION zs ls\n1) "wangwu"\n2) "zhaoliu"\n3) "lisi"\n4) "mazi"\n5) "ergou"\n    \n127.0.0.1:6379> SISMEMBER zs lisi\n(integer) 1\n    \n127.0.0.1:6379> SISMEMBER ls zhangsan\n(integer) 0\n    \n127.0.0.1:6379> SREM zs lisi\n(integer) 1\n    \n127.0.0.1:6379> SMEMBERS zs\n1) "zhaoliu"\n2) "wangwu"\n\n\n\n# SortedSet类型\n\nRedis的SortedSet是一个可排序的set集合，与Java中的TreeSet有些类似，但底层数据结构却差别很大。SortedSet中的每一个元素都带有一个score属性，可以基于score属性对元素排序，底层的实现是一个跳表（SkipList）加 hash表。\n\nSortedSet具备下列特性：\n\n * 可排序\n   \n   * 按分数排序\n\n * 元素不重复\n\n * 查询速度快\n\n因为SortedSet的可排序特性，经常被用来实现排行榜这样的功能。\n\nSortedSet的常见命令有：\n\n * ZADD key score member：添加一个或多个元素到sorted set ，如果已经存在则更新其score值\n\n * ZREM key member：删除sorted set中的一个指定元素\n\n * ZCARD key：获取sorted set中的元素个数\n\n * ZSCORE key member : 获取sorted set中的指定元素的score值\n\n * ZRANK key member：获取sorted set 中的指定元素的排名\n   \n   * 下标从0开始\n\n * ZCOUNT key min max：统计score值在给定范围内的所有元素的个数\n\n * ZINCRBY key increment member：让sorted set中的指定元素自增，步长为指定的increment值\n\n * ZRANGE key min max：按照score排序后，获取指定排名范围内的元素\n\n * ZRANGEBYSCORE key min max：按照score排序后，获取指定score范围内的元素\n\n * ZDIFF.ZINTER.ZUNION：求差集.交集.并集\n\n注意：所有的排名默认都是升序，如果要降序则在命令的Z后面添加REV即可，例如：\n\n * 升序获取sorted set 中的指定元素的排名：ZRANK key member\n * 降序获取sorted set 中的指定元素的排名：ZREVRANK key memeber\n\n练习题：\n\n将班级的下列学生得分存入Redis的SortedSet中：\n\nJack 85, Lucy 89, Rose 82, Tom 95, Jerry 78, Amy 92, Miles 76\n\n并实现下列功能：\n\n * 删除Tom同学\n * 获取Amy同学的分数\n * 获取Rose同学的排名\n * 查询80分以下有几个学生\n * 给Amy同学加2分\n * 查出成绩前3名的同学\n * 查出成绩80分以下的所有同学\n\n- 插入所有同学\n\tzadd sortedset 85 jack 89 lucy 82 rose 95 tom 78 jerry 92 amy 76 miles\n    \n- 删除Tom同学\n     zrem sortedset tom\n    \n- 获取Amy同学的分数 \n     zcore sortedset amy\n\n- 获取Rose同学的排名\n     zrank sortedset rose --升序排名\n     zrevrank sortedset rose --降序排名\n    \n- 查询80分以下有几个学生\n    zcount sortedset 0 80\n    \n- 给Amy同学加2分\n    zincrby sortedset 2 amy\n    \n- 查出成绩前3名的同学\n    zrevrange sortedset 0 2 --降序排列，下标从0开始\n    \n- 查出成绩80分以下的所有同学\n     zrangebyscore sortedset 0 80\n\n\n\n\n# GEO类型\n\nGEO就是Geolocation的简写形式，代表地理坐标。Redis在3.2版本中加入了对GEO的支持，允许存储地理坐标信息，帮助我们根据经纬度来检索数据。常见的命令有：\n\n * GEOADD：添加一个地理空间信息，包含：经度（longitude）、纬度（latitude）、值（member）\n * GEODIST：计算指定的两个点之间的距离并返回\n * GEOHASH：将指定member的坐标转为hash字符串形式并返回\n * GEOPOS：返回指定member的坐标\n * GEORADIUS：指定圆心、半径，找到该圆内包含的所有member，并按照与圆心之间的距离排序后返回。6.以后已废弃\n * GEOSEARCH：在指定范围内搜索member，并按照与指定点之间的距离排序后返回。范围可以是圆形或矩形。6.2.新功能\n   * 默认是升序\n * GEOSEARCHSTORE：与GEOSEARCH功能一致，不过可以把结果存储到一个指定的key。 6.2.新功能\n\n案例\n\n1.添加下面几条数据：\n\n * 北京南站（ 116.378248 39.865275 ） bjn\n * 北京站（ 116.42803 39.903738 ）bjz\n * 北京西站（ 116.322287 39.893729 ）bjx\n\n2.计算北京西站到北京站的距离\n\n3.搜索天安门（ 116.397904 39.909005 ）附近10km内的所有火车站，并按照距离升序排序\n\n4.读取北京站的地理坐标\n\n5.读取北京站的地理坐标的哈希值\n\n-- 1.添加下面几条数据\ngeoadd bj 116.378248 39.865275 bjn  116.42803 39.903738 bjz 116.322287 39.893729 bjx\n\n-- 2.计算北京西站到北京站的距离\ngeodist bj bjx bjz\n\n-- 3.搜索天安门（ 116.397904 39.909005 ）附近10km内的所有火车站，并按照距离升序排序\ngeosearch bj fromlonlat 116.397904 39.909005 byradius 10 km asc withdist\n--  fromlonlat 116.397904 39.909005  指定天安门的地理坐标\n--  byradius 10 km  半径为10Km\n--  asc 升序\n--  withdist 显示距离值\n\n-- 4.读取北京站的地理坐标\ngeopos bj bjz\n1) 1) "116.42802804708480835"\n   2) "39.90373880538094653"\n \n-- 5.读取北京站的地理坐标的哈希值\ngeohash bj bjz\n1) "wx4g12k21s0"\n\n\n\n# BitMap的命令\n\n * SETBIT：向指定位置（offset）存入一个0或1\n * GETBIT ：获取指定位置（offset）的bit值\n * BITCOUNT ：统计BitMap中值为1的bit位的数量\n * BITFIELD ：操作（查询、修改、自增）BitMap中bit数组中的指定位置（offset）的值\n * BITFIELD_RO ：获取BitMap中bit数组，并以十进制形式返回\n * BITOP ：将多个BitMap的结果做位运算（与 、或、异或）\n * BITPOS ：查找bit数组中指定范围内第一个0或1出现的位置\n\n案例\n\n 1. 周三，周四请假，周一周二正常签到\n\n 2. 获取周二签到情况\n\n 3. 获取这四天的签到次数\n\n 4. 获取周一和周二的签到情况\n\n 5. 查找第一次未签到的情况\n\n1. 周三，周四请假，周一周二正常签到\nsetbit qd 0 1\nsetbit qd 1 1\nsetbit qd 2 0\nsetbit qd 3 0\n\n-- 1100\n2. 获取周二签到情况\ngetbit qd 1       -- 1\n\n3. 获取这四天的签到次数\nbitcount qd  -- 2\n\n4. 获取周一和周二的签到情况\nbitfield qd get u2 0  -- 3\n-- u 表示无符号\n-- 2 表示查两位\n-- 0 偏移位，表示从第几位开始\n\n5. 查找第一次未签到的情况\n bitpos qd 0 -- 2\n\n',normalizedContent:'# redis数据结构介绍\n\nredis是一个key-value的数据库，key一般是string类型，不过value的类型多种多样：\n\n\n\n贴心小建议：命令不要死记，学会查询就好啦\n\nredis为了方便我们学习，将操作不同数据类型的命令也做了分组，在官网（ https://redis.io/commands ）可以查看到不同的命令：\n\n\n\n当然我们也可以通过help命令来帮助我们去查看命令\n\n\n\n\n# redis 通用命令\n\n通用指令是部分数据类型的，都可以使用的指令，常见的有：\n\n * keys：查看符合模板的所有key\n   \n   * **生成环境下尽量不要使用keys **\n     \n     * 模糊查询效率比较低，在加上单线程，所以会阻塞进程\n   \n   * 结合正则表达式来使用命令\n\n * del：删除一个指定的key\n\n * exists：判断key是否存在\n   \n   * 存在返回1，不存在返回0\n\n * expire：给一个key设置有效期，有效期到期时该key会被自动删除\n   \n   * 单位秒\n\n * ttl：查看一个key的剩余有效期\n   \n   * -1 表示永久有效\n   * -2 表示key过期\n\n通过help [command] 可以查看一个命令的具体用法，例如：\n\n\n\n课堂代码如下\n\n * keys\n\n127.0.0.1:6379> keys *\n1) "name"\n2) "age"\n127.0.0.1:6379>\n\n# 查询以a开头的key\n127.0.0.1:6379> keys a*\n1) "age"\n127.0.0.1:6379>\n\n\n贴心小提示：在生产环境下，不推荐使用keys 命令，因为这个命令在key过多的情况下，效率不高\n\n * del\n\n127.0.0.1:6379> help del\n\n  del key [key ...]\n  summary: delete a key\n  since: 1.0.0\n  group: generic\n\n127.0.0.1:6379> del name #删除单个\n(integer) 1  #成功删除1个\n\n127.0.0.1:6379> keys *\n1) "age"\n\n127.0.0.1:6379> mset k1 v1 k2 v2 k3 v3 #批量添加数据\nok\n\n127.0.0.1:6379> keys *\n1) "k3"\n2) "k2"\n3) "k1"\n4) "age"\n\n127.0.0.1:6379> del k1 k2 k3 k4\n(integer) 3   #此处返回的是成功删除的key，由于redis中只有k1,k2,k3 所以只成功删除3个，最终返回\n127.0.0.1:6379>\n\n127.0.0.1:6379> keys * #再查询全部的key\n1) "age"\t#只剩下一个了\n127.0.0.1:6379>\n\n\n贴心小提示：同学们在拷贝代码的时候，只需要拷贝对应的命令哦~\n\n * exists\n\n127.0.0.1:6379> help exists\n\n  exists key [key ...]\n  summary: determine if a key exists\n  since: 1.0.0\n  group: generic\n\n127.0.0.1:6379> exists age\n(integer) 1\n\n127.0.0.1:6379> exists name\n(integer) 0\n\n\n * expire\n\n贴心小提示：内存非常宝贵，对于一些数据，我们应当给他一些过期时间，当过期时间到了之后，他就会自动被删除~\n\n127.0.0.1:6379> expire age 10\n(integer) 1\n\n127.0.0.1:6379> ttl age\n(integer) 8\n\n127.0.0.1:6379> ttl age\n(integer) 6\n\n127.0.0.1:6379> ttl age\n(integer) -2\n\n127.0.0.1:6379> ttl age\n(integer) -2  #当这个key过期了，那么此时查询出来就是-2 \n\n127.0.0.1:6379> keys *\n(empty list or set)\n\n127.0.0.1:6379> set age 10 #如果没有设置过期时间\nok\n\n127.0.0.1:6379> ttl age\n(integer) -1  # ttl的返回值就是-1\n\n\n\n# string命令\n\nstring类型，也就是字符串类型，是redis中最简单的存储类型。\n\n其value是字符串，不过根据字符串的格式不同，又可以分为3类：\n\n * string：普通字符串\n * int：整数类型，可以做自增.自减操作\n * float：浮点类型，可以做自增.自减操作\n\n\n\nstring的常见命令有：\n\n * set：添加或者修改已经存在的一个string类型的键值对\n\n * get：根据key获取string类型的value\n\n * mset：批量添加多个string类型的键值对\n\n * mget：根据多个key获取多个string类型的value\n\n * incr：让一个整型的key自增1\n\n * incrby:让一个整型的key自增并指定步长，例如：incrby num 2 让num值自增2\n\n * incrbyfloat：让一个浮点类型的数字自增并指定步长\n\n * setnx：添加一个string类型的键值对，前提是这个key不存在，否则不执行\n   \n   * 这个是真正的新增\n\n * setex：添加一个string类型的键值对，并且指定有效期\n   \n   * setex name 10 jack\n   * set name jack ex 10\n\n贴心小提示：以上命令除了incrbyfloat 都是常用命令\n\n * set 和get: 如果key不存在则是新增，如果存在则是修改\n\n127.0.0.1:6379> set name rose  //原来不存在\nok\n\n127.0.0.1:6379> get name \n"rose"\n\n127.0.0.1:6379> set name jack //原来存在，就是修改\nok\n\n127.0.0.1:6379> get name\n"jack"\n\n\n * mset和mget\n\n127.0.0.1:6379> mset k1 v1 k2 v2 k3 v3\nok\n\n127.0.0.1:6379> mget name age k1 k2 k3\n1) "jack" //之前存在的name\n2) "10"   //之前存在的age\n3) "v1"\n4) "v2"\n5) "v3"\n\n\n * incr和incrby和decy\n\n127.0.0.1:6379> get age \n"10"\n\n127.0.0.1:6379> incr age //增加1\n(integer) 11\n    \n127.0.0.1:6379> get age //获得age\n"11"\n\n127.0.0.1:6379> incrby age 2 //一次增加2\n(integer) 13 //返回目前的age的值\n    \n127.0.0.1:6379> incrby age 2\n(integer) 15\n    \n127.0.0.1:6379> incrby age -1 //也可以增加负数，相当于减\n(integer) 14\n    \n127.0.0.1:6379> incrby age -2 //一次减少2个\n(integer) 12\n    \n127.0.0.1:6379> decr age //相当于 incr 负数，减少正常用法\n(integer) 11\n    \n127.0.0.1:6379> get age \n"11"\n\n\n\n * setnx\n\n127.0.0.1:6379> help setnx\n\n  setnx key value\n  summary: set the value of a key, only if the key does not exist\n  since: 1.0.0\n  group: string\n\n127.0.0.1:6379> set name jack  //设置名称\nok\n127.0.0.1:6379> setnx name lisi //如果key不存在，则添加成功\n(integer) 0\n127.0.0.1:6379> get name //由于name已经存在，所以lisi的操作失败\n"jack"\n127.0.0.1:6379> setnx name2 lisi //name2 不存在，所以操作成功\n(integer) 1\n127.0.0.1:6379> get name2 \n"lisi"\n\n\n * setex\n\n127.0.0.1:6379> setex name 10 jack\nok\n\n127.0.0.1:6379> ttl name\n(integer) 8\n\n127.0.0.1:6379> ttl name\n(integer) 7\n\n127.0.0.1:6379> ttl name\n(integer) 5\n\n\n\n# key的层级结构\n\nredis没有类似mysql中的table的概念，我们该如何区分不同类型的key呢？\n\n例如，需要存储用户.商品信息到redis，有一个用户id是1，有一个商品id恰好也是1，此时如果使用id作为key，那就会冲突了，该怎么办？\n\n我们可以通过给key添加前缀加以区分，不过这个前缀不是随便加的，有一定的规范：\n\nredis的key允许有多个单词形成层级结构，多个单词之间用\':\'隔开，格式如下：\n\n\n\n这个格式并非固定，也可以根据自己的需求来删除或添加词条。\n\n例如我们的项目名称叫 heima，有user和product两种不同类型的数据，我们可以这样定义key：\n\n * user相关的key：heima:user:1\n\n * product相关的key：heima:product:1\n\n如果value是一个java对象，例如一个user对象，则可以将对象序列化为json字符串后存储：\n\nkey               value\nheima:user:1      {"id":1, "name": "jack", "age": 21}\nheima:product:1   {"id":1, "name": "小米11", "price": 4999}\n\n一旦我们向redis采用这样的方式存储，那么在可视化界面中，redis会以层级结构来进行存储，形成类似于这样的结构，更加方便redis获取数据\n\n\n\n\n# hash命令\n\nhash类型，也叫散列，其value是一个无序字典，类似于java中的hashmap结构。\n\nstring结构是将对象序列化为json字符串后存储，当需要修改对象某个字段时很不方便：\n\n\n\nhash结构可以将对象中的每个字段独立存储，可以针对单个字段做crud：\n\n\n\nhash类型的常见命令\n\n * hset key field value：添加或者修改hash类型key的field的值\n\n * hget key field：获取一个hash类型key的field的值\n\n * hmset：批量添加多个hash类型key的field的值\n\n * hmget：批量获取多个hash类型key的field的值\n\n * hgetall：获取一个hash类型的key中的所有的field和value\n\n * hkeys：获取一个hash类型的key中的所有的field\n\n * hincrby:让一个hash类型key的字段值自增并指定步长\n\n * hsetnx：添加一个hash类型的key的field值，前提是这个field不存在，否则不执行\n\n贴心小提示：哈希结构也是我们以后实际开发中常用的命令哟\n\n * hset和hget\n\n127.0.0.1:6379> hset heima:user:3 name lucy//大key是 heima:user:3 小key是name，小value是lucy\n(integer) 1\n127.0.0.1:6379> hset heima:user:3 age 21// 如果操作不存在的数据，则是新增\n(integer) 1\n127.0.0.1:6379> hset heima:user:3 age 17 //如果操作存在的数据，则是修改\n(integer) 0\n127.0.0.1:6379> hget heima:user:3 name \n"lucy"\n127.0.0.1:6379> hget heima:user:3 age\n"17"\n\n\n * hmset和hmget\n\n127.0.0.1:6379> hmset heima:user:4 name hanmeimei\nok\n127.0.0.1:6379> hmset heima:user:4 name lilei age 20 sex man\nok\n127.0.0.1:6379> hmget heima:user:4 name age sex\n1) "lilei"\n2) "20"\n3) "man"\n\n\n * hgetall\n\n127.0.0.1:6379> hgetall heima:user:4\n1) "name"\n2) "lilei"\n3) "age"\n4) "20"\n5) "sex"\n6) "man"\n\n\n * hkeys和hvals\n\n127.0.0.1:6379> hkeys heima:user:4\n1) "name"\n2) "age"\n3) "sex"\n127.0.0.1:6379> hvals heima:user:4\n1) "lilei"\n2) "20"\n3) "man"\n\n\n * hincrby\n\n127.0.0.1:6379> hincrby  heima:user:4 age 2\n(integer) 22\n127.0.0.1:6379> hvals heima:user:4\n1) "lilei"\n2) "22"\n3) "man"\n127.0.0.1:6379> hincrby  heima:user:4 age -2\n(integer) 20\n\n\n * hsetnx\n\n127.0.0.1:6379> hsetnx heima:user4 sex woman\n(integer) 1\n127.0.0.1:6379> hgetall heima:user:3\n1) "name"\n2) "lucy"\n3) "age"\n4) "17"\n127.0.0.1:6379> hsetnx heima:user:3 sex woman\n(integer) 1\n127.0.0.1:6379> hgetall heima:user:3\n1) "name"\n2) "lucy"\n3) "age"\n4) "17"\n5) "sex"\n6) "woman"\n\n\n\n# list命令\n\nredis中的list类型与java中的linkedlist类似，可以看做是一个双向链表结构。既可以支持正向检索和也可以支持反向检索。\n\n特征也与linkedlist类似：\n\n * 有序\n   \n   * 按照插入顺序来存储\n\n * 元素可以重复\n\n * 插入和删除快\n\n * 查询速度一般\n\n常用来存储一个有序数据，例如：朋友圈点赞列表，评论列表等。\n\nlist的常见命令有：\n\n * lpush key element ... ：向列表左侧插入一个或多个元素\n * lpop key：移除并返回列表左侧的第一个元素，没有则返回nil\n * rpush key element ... ：向列表右侧插入一个或多个元素\n * rpop key：移除并返回列表右侧的第一个元素\n * lrange key star end：返回一段角标范围内的所有元素\n * blpop和brpop：与lpop和rpop类似，只不过在没有元素时等待指定时间，而不是直接返回nil\n   * 阻塞式获取，当没有娶不到对应键值时，会等待一段时间\n\n\n\n * lpush和rpush\n\n127.0.0.1:6379> lpush users 1 2 3\n(integer) 3\n    3-2-1\n127.0.0.1:6379> rpush users 4 5 6\n(integer) 6\n    4-5-6\n\n\n * lpop和rpop\n\n127.0.0.1:6379> lpop users\n"3"\n127.0.0.1:6379> rpop users\n"6"\n\n\n * lrange\n\n127.0.0.1:6379> lrange users 1 2\n1) "1"\n2) "4"\n\n\n\n# set命令\n\nredis的set结构与java中的hashset类似，可以看做是一个value为null的hashmap。因为也是一个hash表，因此具备与hashset类似的特征：\n\n * 无序\n * 元素不可重复\n * 查找快\n * 支持交集.并集.差集等功能\n\nset类型的常见命令\n\n * sadd key member ... ：向set中添加一个或多个元素\n\n * srem key member ... : 移除set中的指定元素\n\n * scard key： 返回set中元素的个数\n\n * sismember key member：判断一个元素是否存在于set中\n\n * smembers：获取set中的所有元素\n\n * sinter key1 key2 ... ：求key1与key2的交集\n\n * sdiff key1 key2 ... ：求key1与key2的差集\n\n * sunion key1 key2 ..：求key1和key2的并集\n\n例如两个集合：s1和s2:\n\n\n\n求交集：sinter s1 s2\n\n求s1与s2的不同：sdiff s1 s2\n\n\n\n具体命令\n\n127.0.0.1:6379> sadd s1 a b c\n(integer) 3\n127.0.0.1:6379> smembers s1\n1) "c"\n2) "b"\n3) "a"\n127.0.0.1:6379> srem s1 a\n(integer) 1\n    \n127.0.0.1:6379> sismember s1 a\n(integer) 0\n    \n127.0.0.1:6379> sismember s1 b\n(integer) 1\n    \n127.0.0.1:6379> scard s1\n(integer) 2\n\n\n案例\n\n * 将下列数据用redis的set集合来存储：\n * 张三的好友有：李四.王五.赵六\n * 李四的好友有：王五.麻子.二狗\n * 利用set的命令实现下列功能：\n * 计算张三的好友有几人\n * 计算张三和李四有哪些共同好友\n * 查询哪些人是张三的好友却不是李四的好友\n * 查询张三和李四的好友总共有哪些人\n * 判断李四是否是张三的好友\n * 判断张三是否是李四的好友\n * 将李四从张三的好友列表中移除\n\n127.0.0.1:6379> sadd zs lisi wangwu zhaoliu\n(integer) 3\n    \n127.0.0.1:6379> sadd ls wangwu mazi ergou\n(integer) 3\n    \n127.0.0.1:6379> scard zs\n(integer) 3\n    \n127.0.0.1:6379> sinter zs ls\n1) "wangwu"\n    \n127.0.0.1:6379> sdiff zs ls\n1) "zhaoliu"\n2) "lisi"\n    \n127.0.0.1:6379> sunion zs ls\n1) "wangwu"\n2) "zhaoliu"\n3) "lisi"\n4) "mazi"\n5) "ergou"\n    \n127.0.0.1:6379> sismember zs lisi\n(integer) 1\n    \n127.0.0.1:6379> sismember ls zhangsan\n(integer) 0\n    \n127.0.0.1:6379> srem zs lisi\n(integer) 1\n    \n127.0.0.1:6379> smembers zs\n1) "zhaoliu"\n2) "wangwu"\n\n\n\n# sortedset类型\n\nredis的sortedset是一个可排序的set集合，与java中的treeset有些类似，但底层数据结构却差别很大。sortedset中的每一个元素都带有一个score属性，可以基于score属性对元素排序，底层的实现是一个跳表（skiplist）加 hash表。\n\nsortedset具备下列特性：\n\n * 可排序\n   \n   * 按分数排序\n\n * 元素不重复\n\n * 查询速度快\n\n因为sortedset的可排序特性，经常被用来实现排行榜这样的功能。\n\nsortedset的常见命令有：\n\n * zadd key score member：添加一个或多个元素到sorted set ，如果已经存在则更新其score值\n\n * zrem key member：删除sorted set中的一个指定元素\n\n * zcard key：获取sorted set中的元素个数\n\n * zscore key member : 获取sorted set中的指定元素的score值\n\n * zrank key member：获取sorted set 中的指定元素的排名\n   \n   * 下标从0开始\n\n * zcount key min max：统计score值在给定范围内的所有元素的个数\n\n * zincrby key increment member：让sorted set中的指定元素自增，步长为指定的increment值\n\n * zrange key min max：按照score排序后，获取指定排名范围内的元素\n\n * zrangebyscore key min max：按照score排序后，获取指定score范围内的元素\n\n * zdiff.zinter.zunion：求差集.交集.并集\n\n注意：所有的排名默认都是升序，如果要降序则在命令的z后面添加rev即可，例如：\n\n * 升序获取sorted set 中的指定元素的排名：zrank key member\n * 降序获取sorted set 中的指定元素的排名：zrevrank key memeber\n\n练习题：\n\n将班级的下列学生得分存入redis的sortedset中：\n\njack 85, lucy 89, rose 82, tom 95, jerry 78, amy 92, miles 76\n\n并实现下列功能：\n\n * 删除tom同学\n * 获取amy同学的分数\n * 获取rose同学的排名\n * 查询80分以下有几个学生\n * 给amy同学加2分\n * 查出成绩前3名的同学\n * 查出成绩80分以下的所有同学\n\n- 插入所有同学\n\tzadd sortedset 85 jack 89 lucy 82 rose 95 tom 78 jerry 92 amy 76 miles\n    \n- 删除tom同学\n     zrem sortedset tom\n    \n- 获取amy同学的分数 \n     zcore sortedset amy\n\n- 获取rose同学的排名\n     zrank sortedset rose --升序排名\n     zrevrank sortedset rose --降序排名\n    \n- 查询80分以下有几个学生\n    zcount sortedset 0 80\n    \n- 给amy同学加2分\n    zincrby sortedset 2 amy\n    \n- 查出成绩前3名的同学\n    zrevrange sortedset 0 2 --降序排列，下标从0开始\n    \n- 查出成绩80分以下的所有同学\n     zrangebyscore sortedset 0 80\n\n\n\n\n# geo类型\n\ngeo就是geolocation的简写形式，代表地理坐标。redis在3.2版本中加入了对geo的支持，允许存储地理坐标信息，帮助我们根据经纬度来检索数据。常见的命令有：\n\n * geoadd：添加一个地理空间信息，包含：经度（longitude）、纬度（latitude）、值（member）\n * geodist：计算指定的两个点之间的距离并返回\n * geohash：将指定member的坐标转为hash字符串形式并返回\n * geopos：返回指定member的坐标\n * georadius：指定圆心、半径，找到该圆内包含的所有member，并按照与圆心之间的距离排序后返回。6.以后已废弃\n * geosearch：在指定范围内搜索member，并按照与指定点之间的距离排序后返回。范围可以是圆形或矩形。6.2.新功能\n   * 默认是升序\n * geosearchstore：与geosearch功能一致，不过可以把结果存储到一个指定的key。 6.2.新功能\n\n案例\n\n1.添加下面几条数据：\n\n * 北京南站（ 116.378248 39.865275 ） bjn\n * 北京站（ 116.42803 39.903738 ）bjz\n * 北京西站（ 116.322287 39.893729 ）bjx\n\n2.计算北京西站到北京站的距离\n\n3.搜索天安门（ 116.397904 39.909005 ）附近10km内的所有火车站，并按照距离升序排序\n\n4.读取北京站的地理坐标\n\n5.读取北京站的地理坐标的哈希值\n\n-- 1.添加下面几条数据\ngeoadd bj 116.378248 39.865275 bjn  116.42803 39.903738 bjz 116.322287 39.893729 bjx\n\n-- 2.计算北京西站到北京站的距离\ngeodist bj bjx bjz\n\n-- 3.搜索天安门（ 116.397904 39.909005 ）附近10km内的所有火车站，并按照距离升序排序\ngeosearch bj fromlonlat 116.397904 39.909005 byradius 10 km asc withdist\n--  fromlonlat 116.397904 39.909005  指定天安门的地理坐标\n--  byradius 10 km  半径为10km\n--  asc 升序\n--  withdist 显示距离值\n\n-- 4.读取北京站的地理坐标\ngeopos bj bjz\n1) 1) "116.42802804708480835"\n   2) "39.90373880538094653"\n \n-- 5.读取北京站的地理坐标的哈希值\ngeohash bj bjz\n1) "wx4g12k21s0"\n\n\n\n# bitmap的命令\n\n * setbit：向指定位置（offset）存入一个0或1\n * getbit ：获取指定位置（offset）的bit值\n * bitcount ：统计bitmap中值为1的bit位的数量\n * bitfield ：操作（查询、修改、自增）bitmap中bit数组中的指定位置（offset）的值\n * bitfield_ro ：获取bitmap中bit数组，并以十进制形式返回\n * bitop ：将多个bitmap的结果做位运算（与 、或、异或）\n * bitpos ：查找bit数组中指定范围内第一个0或1出现的位置\n\n案例\n\n 1. 周三，周四请假，周一周二正常签到\n\n 2. 获取周二签到情况\n\n 3. 获取这四天的签到次数\n\n 4. 获取周一和周二的签到情况\n\n 5. 查找第一次未签到的情况\n\n1. 周三，周四请假，周一周二正常签到\nsetbit qd 0 1\nsetbit qd 1 1\nsetbit qd 2 0\nsetbit qd 3 0\n\n-- 1100\n2. 获取周二签到情况\ngetbit qd 1       -- 1\n\n3. 获取这四天的签到次数\nbitcount qd  -- 2\n\n4. 获取周一和周二的签到情况\nbitfield qd get u2 0  -- 3\n-- u 表示无符号\n-- 2 表示查两位\n-- 0 偏移位，表示从第几位开始\n\n5. 查找第一次未签到的情况\n bitpos qd 0 -- 2\n\n',charsets:{cjk:!0}},{title:"约束",frontmatter:{autoSort:97,title:"约束",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/ab672a/",categories:["后端","数据库","MySQL"],tags:["知识","数据库","MySQL"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/01.MySQL/10.%E7%BA%A6%E6%9D%9F.html",relativePath:"01.后端/30.数据库/01.MySQL/10.约束.md",key:"v-35eddcf9",path:"/pages/ab672a/",headers:[{level:2,title:"前几个约束",slug:"前几个约束",normalizedTitle:"前几个约束",charIndex:67},{level:2,title:"外键约束",slug:"外键约束",normalizedTitle:"外键约束",charIndex:1438}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"前几个约束 外键约束",content:"概念：约束是作用于表中字段上的规则，用于限制存储在表中的数据。 目的：保证数据库中数据的正确、有效性和完整性。\n\n分类：\n\n\n\n\n# 前几个约束\n\n 1. 建表\n    \n    -- 创建带约束的表格\n     CREATE TABLE emp(\n         id INT PRIMARY KEY auto_increment,   -- 员工id，主键且自增长\n         ename VARCHAR(50) NOT NULL UNIQUE, -- 员工姓名，非空且唯一\n         joindate DATE NOT NULL ,-- 入职日期，非空\n         salary DOUBLE(7,2) NOT NULL,-- 工资，非空\n         bonus DOUBLE(7,2) DEFAULT 0-- 奖金，如果没有奖金默认为0\n     );\n    \n\n 2. 插入数据\n    \n    -- 插入数据\n    INSERT INTO emp(id,ename,joindate,salary,bonus) VALUES(1,'diana','1999-01-01',8800,5000);\n    INSERT INTO emp(id,ename,joindate,salary,bonus) VALUES(2,'凉冰','1999-02-01',8000,4000);\n    \n    \n    -- 主键约束 非空且唯一\n    INSERT INTO emp(id,ename,joindate,salary,bonus) VALUES(1,'凉冰','1999-02-01',8000,4000); -- 不能重复为1\n    INSERT INTO emp(id,ename,joindate,salary,bonus) VALUES(3,'莫甘娜','1999-03-01',10000,4000);\n    \n    -- 非空约束\n    INSERT INTO emp(id,ename,joindate,salary,bonus) VALUES(4,'凉冰',null,8000,4000);-- 不能为null\n    \n    -- 唯一约束\n    INSERT INTO emp(id,ename,joindate,salary,bonus) VALUES(5,'凉冰','1999-05-01',8000,4000);-- 不能重复为凉冰\n    \n    -- 默认约束\n    INSERT INTO emp(id,ename,joindate,salary) VALUES(4,'彦','1999-06-01',6000);-- 奖金默认为0\n    \n    -- 演示自增长\n    INSERT INTO emp(id,ename,joindate,salary,bonus) VALUES(NULL,'凯尔','1999-07-01',9000,1000);-- id 自动设置为5\n    INSERT INTO emp(ename,joindate,salary,bonus) VALUES('阿托','1999-08-01',5000,1000);-- id 自动设置为6\n    \n    -- 查询数据\n    SELECT * from emp;\n    \n\n\n# 外键约束\n\n 0. 数据准备\n    \n    -- 准备数据\n    create table dept(\n        id   int auto_increment comment 'ID' primary key,\n        name varchar(50) not null comment '部门名称'\n    )comment '部门表';\n    INSERT INTO dept (id, name) VALUES (1, '研发部'), (2, '市场部'),(3, '财务部'), (4, '销售部'), (5, '总经办');\n    \n    \n    create table emp(\n        id  int auto_increment comment 'ID' primary key,\n        name varchar(50) not null comment '姓名',\n        age  int comment '年龄',\n        job varchar(20) comment '职位',\n        salary int comment '薪资',\n        entrydate date comment '入职时间',\n        managerid int comment '直属领导ID',\n        dept_id int comment '部门ID'\n    )comment '员工表';\n    \n    INSERT INTO emp (id, name, age, job,salary, entrydate, managerid, dept_id) \n    VALUES (1, '金庸', 66, '总裁',20000, '2000-01-01', null,5),\n    (2, '张无忌', 20, '项目经理',12500, '2005-12-05', 1,1),\n    (3, '杨逍', 33, '开发', 8400,'2000-11-03', 2,1),\n    (4, '韦一笑', 48, '开发',11000, '2002-02-05', 2,1),\n    (5, '常遇春', 43, '开发',10500, '2004-09-07', 3,1),\n    (6, '小昭', 19, '程序员鼓励师',6600, '2004-10-12', 2,1);\n    \n\n 1. 创建带外键约束的表格\n    \n    --主表  要先创建表格，先添加数据\n    CREATE TABLE dept( \n       id int PRIMARY KEY auto_increment,\n       dep_name varchar(20),\n       addr varchar(20)\n    );\n    \n    \n    CREATE TABLE emp2( --从表\n       id int PRIMARY KEY auto_increment,\n       name varchar(20),\n       age int,\n       dep_id int,\n    -- 添加外键    \n    -- constraint 外键名 foreign key(外键列名) references 主表(外键关联的主表的列名)\n       CONSTRAINT fk_emp2_dept FOREIGN KEY(dep_id) REFERENCES dept(id)\n    );\n    \n\n 2. 创建表格后，手动添加约束\n    \n    CREATE TABLE emp2( --从表\n       id int PRIMARY KEY auto_increment,\n       name varchar(20),\n       age int,\n       dep_id int,\n    );\n    \n    \n    * 添加外键\n      \n      ALTER TABLE emp2 add \n      CONSTRAINT fk_emp2_dept FOREIGN KEY(dep_id) REFERENCES dept(id);\n      \n    \n    * 删除外键\n      \n      ALTER TABLE emp2 drop \n      FOREIGN KEY fk_emp2_dept;\n      \n\n 3. 外键删除更新行为\n    \n    \n    \n    * CASCADE\n    \n    -- dept(id) 更新/删除时，emp(dept_id)跟随更新/删除\n    alter table emp add \n    constraint fk_emp_dept_id foreign key (dept_id) references dept(id) \n    on update cascade on delete cascade;\n    \n    \n    * set null\n    \n    -- dept(id) 更新/删除时，emp(dept_id)置null\n    alter table emp add \n    constraint fk_emp_dept_id foreign key (dept_id) references dept(id) \n    on update set null on delete set null;\n    ",normalizedContent:"概念：约束是作用于表中字段上的规则，用于限制存储在表中的数据。 目的：保证数据库中数据的正确、有效性和完整性。\n\n分类：\n\n\n\n\n# 前几个约束\n\n 1. 建表\n    \n    -- 创建带约束的表格\n     create table emp(\n         id int primary key auto_increment,   -- 员工id，主键且自增长\n         ename varchar(50) not null unique, -- 员工姓名，非空且唯一\n         joindate date not null ,-- 入职日期，非空\n         salary double(7,2) not null,-- 工资，非空\n         bonus double(7,2) default 0-- 奖金，如果没有奖金默认为0\n     );\n    \n\n 2. 插入数据\n    \n    -- 插入数据\n    insert into emp(id,ename,joindate,salary,bonus) values(1,'diana','1999-01-01',8800,5000);\n    insert into emp(id,ename,joindate,salary,bonus) values(2,'凉冰','1999-02-01',8000,4000);\n    \n    \n    -- 主键约束 非空且唯一\n    insert into emp(id,ename,joindate,salary,bonus) values(1,'凉冰','1999-02-01',8000,4000); -- 不能重复为1\n    insert into emp(id,ename,joindate,salary,bonus) values(3,'莫甘娜','1999-03-01',10000,4000);\n    \n    -- 非空约束\n    insert into emp(id,ename,joindate,salary,bonus) values(4,'凉冰',null,8000,4000);-- 不能为null\n    \n    -- 唯一约束\n    insert into emp(id,ename,joindate,salary,bonus) values(5,'凉冰','1999-05-01',8000,4000);-- 不能重复为凉冰\n    \n    -- 默认约束\n    insert into emp(id,ename,joindate,salary) values(4,'彦','1999-06-01',6000);-- 奖金默认为0\n    \n    -- 演示自增长\n    insert into emp(id,ename,joindate,salary,bonus) values(null,'凯尔','1999-07-01',9000,1000);-- id 自动设置为5\n    insert into emp(ename,joindate,salary,bonus) values('阿托','1999-08-01',5000,1000);-- id 自动设置为6\n    \n    -- 查询数据\n    select * from emp;\n    \n\n\n# 外键约束\n\n 0. 数据准备\n    \n    -- 准备数据\n    create table dept(\n        id   int auto_increment comment 'id' primary key,\n        name varchar(50) not null comment '部门名称'\n    )comment '部门表';\n    insert into dept (id, name) values (1, '研发部'), (2, '市场部'),(3, '财务部'), (4, '销售部'), (5, '总经办');\n    \n    \n    create table emp(\n        id  int auto_increment comment 'id' primary key,\n        name varchar(50) not null comment '姓名',\n        age  int comment '年龄',\n        job varchar(20) comment '职位',\n        salary int comment '薪资',\n        entrydate date comment '入职时间',\n        managerid int comment '直属领导id',\n        dept_id int comment '部门id'\n    )comment '员工表';\n    \n    insert into emp (id, name, age, job,salary, entrydate, managerid, dept_id) \n    values (1, '金庸', 66, '总裁',20000, '2000-01-01', null,5),\n    (2, '张无忌', 20, '项目经理',12500, '2005-12-05', 1,1),\n    (3, '杨逍', 33, '开发', 8400,'2000-11-03', 2,1),\n    (4, '韦一笑', 48, '开发',11000, '2002-02-05', 2,1),\n    (5, '常遇春', 43, '开发',10500, '2004-09-07', 3,1),\n    (6, '小昭', 19, '程序员鼓励师',6600, '2004-10-12', 2,1);\n    \n\n 1. 创建带外键约束的表格\n    \n    --主表  要先创建表格，先添加数据\n    create table dept( \n       id int primary key auto_increment,\n       dep_name varchar(20),\n       addr varchar(20)\n    );\n    \n    \n    create table emp2( --从表\n       id int primary key auto_increment,\n       name varchar(20),\n       age int,\n       dep_id int,\n    -- 添加外键    \n    -- constraint 外键名 foreign key(外键列名) references 主表(外键关联的主表的列名)\n       constraint fk_emp2_dept foreign key(dep_id) references dept(id)\n    );\n    \n\n 2. 创建表格后，手动添加约束\n    \n    create table emp2( --从表\n       id int primary key auto_increment,\n       name varchar(20),\n       age int,\n       dep_id int,\n    );\n    \n    \n    * 添加外键\n      \n      alter table emp2 add \n      constraint fk_emp2_dept foreign key(dep_id) references dept(id);\n      \n    \n    * 删除外键\n      \n      alter table emp2 drop \n      foreign key fk_emp2_dept;\n      \n\n 3. 外键删除更新行为\n    \n    \n    \n    * cascade\n    \n    -- dept(id) 更新/删除时，emp(dept_id)跟随更新/删除\n    alter table emp add \n    constraint fk_emp_dept_id foreign key (dept_id) references dept(id) \n    on update cascade on delete cascade;\n    \n    \n    * set null\n    \n    -- dept(id) 更新/删除时，emp(dept_id)置null\n    alter table emp add \n    constraint fk_emp_dept_id foreign key (dept_id) references dept(id) \n    on update set null on delete set null;\n    ",charsets:{cjk:!0}},{title:"主从集群搭建",frontmatter:{autoSort:99,title:"主从集群搭建",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/564491/",categories:["后端","数据库","Redis"],tags:["知识","数据库","Redis"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/05.Redis/102.%E4%B8%BB%E4%BB%8E%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA.html",relativePath:"01.后端/30.数据库/05.Redis/102.主从集群搭建.md",key:"v-02f207c3",path:"/pages/564491/",headers:[{level:2,title:"集群结构",slug:"集群结构",normalizedTitle:"集群结构",charIndex:2},{level:2,title:"准备实例和配置",slug:"准备实例和配置",normalizedTitle:"准备实例和配置",charIndex:213},{level:2,title:"启动",slug:"启动",normalizedTitle:"启动",charIndex:1586},{level:2,title:"开启主从关系",slug:"开启主从关系",normalizedTitle:"开启主从关系",charIndex:1841},{level:2,title:"测试",slug:"测试",normalizedTitle:"测试",charIndex:2427}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"集群结构 准备实例和配置 启动 开启主从关系 测试",content:"# 集群结构\n\n我们搭建的主从集群结构如图：\n\n\n\n共包含三个节点，一个主节点，两个从节点。\n\n这里我们会在同一台虚拟机中开启3个redis实例，模拟主从集群，信息如下：\n\nIP                PORT   角色\n192.168.150.101   7001   master\n192.168.150.101   7002   slave\n192.168.150.101   7003   slave\n\n\n# 准备实例和配置\n\n要在同一台虚拟机开启3个实例，必须准备三份不同的配置文件和目录，配置文件所在目录也就是工作目录。\n\n1）创建目录\n\n我们创建三个文件夹，名字分别叫7001、7002、7003：\n\n# 进入/tmp目录\ncd /tmp\n# 创建目录\nmkdir 7001 7002 7003\n\n\n如图：\n\n\n\n2）恢复原始配置\n\n修改redis-6.2.4/redis.conf文件，将其中的持久化模式改为默认的RDB模式，AOF保持关闭状态。\n\n# 开启RDB\n# save \"\"\nsave 3600 1\nsave 300 100\nsave 60 10000\n\n# 关闭AOF\nappendonly no\n\n\n3）拷贝配置文件到每个实例目录\n\n然后将redis-6.2.4/redis.conf文件拷贝到三个目录中（在/tmp目录执行下列命令）：\n\n# 方式一：逐个拷贝\ncp redis-6.2.4/redis.conf 7001\ncp redis-6.2.4/redis.conf 7002\ncp redis-6.2.4/redis.conf 7003\n\n# 方式二：管道组合命令，一键拷贝\necho 7001 7002 7003 | xargs -t -n 1 cp redis-6.2.4/redis.conf\n\n\n4）修改每个实例的端口、工作目录\n\n修改每个文件夹内的配置文件，将端口分别修改为7001、7002、7003，将rdb文件保存位置都修改为自己所在目录（在/tmp目录执行下列命令）：\n\nsed -i -e 's/6379/7001/g' -e 's/dir .\\//dir \\/tmp\\/7001\\//g' 7001/redis.conf\nsed -i -e 's/6379/7002/g' -e 's/dir .\\//dir \\/tmp\\/7002\\//g' 7002/redis.conf\nsed -i -e 's/6379/7003/g' -e 's/dir .\\//dir \\/tmp\\/7003\\//g' 7003/redis.conf\n\n\n5）修改每个实例的声明IP\n\n虚拟机本身有多个IP，为了避免将来混乱，我们需要在redis.conf文件中指定每一个实例的绑定ip信息，格式如下：\n\n# redis实例的声明 IP\nreplica-announce-ip 192.168.150.101\n\n\n每个目录都要改，我们一键完成修改（在/tmp目录执行下列命令）：\n\n# 逐一执行\nsed -i '1a replica-announce-ip 192.168.150.101' 7001/redis.conf\nsed -i '1a replica-announce-ip 192.168.150.101' 7002/redis.conf\nsed -i '1a replica-announce-ip 192.168.150.101' 7003/redis.conf\n\n# 或者一键修改\nprintf '%s\\n' 7001 7002 7003 | xargs -I{} -t sed -i '1a replica-announce-ip 192.168.150.101' {}/redis.conf\n\n\n\n# 启动\n\n为了方便查看日志，我们打开3个ssh窗口，分别启动3个redis实例，启动命令：\n\n# 第1个\nredis-server 7001/redis.conf\n# 第2个\nredis-server 7002/redis.conf\n# 第3个\nredis-server 7003/redis.conf\n\n\n启动后：\n\n\n\n如果要一键停止，可以运行下面命令：\n\nprintf '%s\\n' 7001 7002 7003 | xargs -I{} -t redis-cli -p {} shutdown\n\n\n\n# 开启主从关系\n\n现在三个实例还没有任何关系，要配置主从可以使用replicaof 或者slaveof（5.0以前）命令。\n\n有临时和永久两种模式：\n\n * 修改配置文件（永久生效）\n   \n   * 在redis.conf中添加一行配置：slaveof <masterip> <masterport>\n\n * 使用redis-cli客户端连接到redis服务，执行slaveof命令（重启后失效）：\n   \n   slaveof <masterip> <masterport>\n   \n\n注意：在5.0以后新增命令replicaof，与salveof效果一致。\n\n这里我们为了演示方便，使用方式二。\n\n通过redis-cli命令连接7002，执行下面命令：\n\n# 连接 7002\nredis-cli -p 7002\n# 执行slaveof\nslaveof 192.168.150.101 7001\n\n\n通过redis-cli命令连接7003，执行下面命令：\n\n# 连接 7003\nredis-cli -p 7003\n# 执行slaveof\nslaveof 192.168.150.101 7001\n\n\n然后连接 7001节点，查看集群状态：\n\n# 连接 7001\nredis-cli -p 7001\n# 查看状态\ninfo replication\n\n\n结果：\n\n\n\n\n# 测试\n\n执行下列操作以测试：\n\n * 利用redis-cli连接7001，执行set num 123\n\n * 利用redis-cli连接7002，执行get num，再执行set num 666\n\n * 利用redis-cli连接7003，执行get num，再执行set num 888\n\n可以发现，只有在7001这个master节点上可以执行写操作，7002和7003这两个slave节点只能执行读操作。",normalizedContent:"# 集群结构\n\n我们搭建的主从集群结构如图：\n\n\n\n共包含三个节点，一个主节点，两个从节点。\n\n这里我们会在同一台虚拟机中开启3个redis实例，模拟主从集群，信息如下：\n\nip                port   角色\n192.168.150.101   7001   master\n192.168.150.101   7002   slave\n192.168.150.101   7003   slave\n\n\n# 准备实例和配置\n\n要在同一台虚拟机开启3个实例，必须准备三份不同的配置文件和目录，配置文件所在目录也就是工作目录。\n\n1）创建目录\n\n我们创建三个文件夹，名字分别叫7001、7002、7003：\n\n# 进入/tmp目录\ncd /tmp\n# 创建目录\nmkdir 7001 7002 7003\n\n\n如图：\n\n\n\n2）恢复原始配置\n\n修改redis-6.2.4/redis.conf文件，将其中的持久化模式改为默认的rdb模式，aof保持关闭状态。\n\n# 开启rdb\n# save \"\"\nsave 3600 1\nsave 300 100\nsave 60 10000\n\n# 关闭aof\nappendonly no\n\n\n3）拷贝配置文件到每个实例目录\n\n然后将redis-6.2.4/redis.conf文件拷贝到三个目录中（在/tmp目录执行下列命令）：\n\n# 方式一：逐个拷贝\ncp redis-6.2.4/redis.conf 7001\ncp redis-6.2.4/redis.conf 7002\ncp redis-6.2.4/redis.conf 7003\n\n# 方式二：管道组合命令，一键拷贝\necho 7001 7002 7003 | xargs -t -n 1 cp redis-6.2.4/redis.conf\n\n\n4）修改每个实例的端口、工作目录\n\n修改每个文件夹内的配置文件，将端口分别修改为7001、7002、7003，将rdb文件保存位置都修改为自己所在目录（在/tmp目录执行下列命令）：\n\nsed -i -e 's/6379/7001/g' -e 's/dir .\\//dir \\/tmp\\/7001\\//g' 7001/redis.conf\nsed -i -e 's/6379/7002/g' -e 's/dir .\\//dir \\/tmp\\/7002\\//g' 7002/redis.conf\nsed -i -e 's/6379/7003/g' -e 's/dir .\\//dir \\/tmp\\/7003\\//g' 7003/redis.conf\n\n\n5）修改每个实例的声明ip\n\n虚拟机本身有多个ip，为了避免将来混乱，我们需要在redis.conf文件中指定每一个实例的绑定ip信息，格式如下：\n\n# redis实例的声明 ip\nreplica-announce-ip 192.168.150.101\n\n\n每个目录都要改，我们一键完成修改（在/tmp目录执行下列命令）：\n\n# 逐一执行\nsed -i '1a replica-announce-ip 192.168.150.101' 7001/redis.conf\nsed -i '1a replica-announce-ip 192.168.150.101' 7002/redis.conf\nsed -i '1a replica-announce-ip 192.168.150.101' 7003/redis.conf\n\n# 或者一键修改\nprintf '%s\\n' 7001 7002 7003 | xargs -i{} -t sed -i '1a replica-announce-ip 192.168.150.101' {}/redis.conf\n\n\n\n# 启动\n\n为了方便查看日志，我们打开3个ssh窗口，分别启动3个redis实例，启动命令：\n\n# 第1个\nredis-server 7001/redis.conf\n# 第2个\nredis-server 7002/redis.conf\n# 第3个\nredis-server 7003/redis.conf\n\n\n启动后：\n\n\n\n如果要一键停止，可以运行下面命令：\n\nprintf '%s\\n' 7001 7002 7003 | xargs -i{} -t redis-cli -p {} shutdown\n\n\n\n# 开启主从关系\n\n现在三个实例还没有任何关系，要配置主从可以使用replicaof 或者slaveof（5.0以前）命令。\n\n有临时和永久两种模式：\n\n * 修改配置文件（永久生效）\n   \n   * 在redis.conf中添加一行配置：slaveof <masterip> <masterport>\n\n * 使用redis-cli客户端连接到redis服务，执行slaveof命令（重启后失效）：\n   \n   slaveof <masterip> <masterport>\n   \n\n注意：在5.0以后新增命令replicaof，与salveof效果一致。\n\n这里我们为了演示方便，使用方式二。\n\n通过redis-cli命令连接7002，执行下面命令：\n\n# 连接 7002\nredis-cli -p 7002\n# 执行slaveof\nslaveof 192.168.150.101 7001\n\n\n通过redis-cli命令连接7003，执行下面命令：\n\n# 连接 7003\nredis-cli -p 7003\n# 执行slaveof\nslaveof 192.168.150.101 7001\n\n\n然后连接 7001节点，查看集群状态：\n\n# 连接 7001\nredis-cli -p 7001\n# 查看状态\ninfo replication\n\n\n结果：\n\n\n\n\n# 测试\n\n执行下列操作以测试：\n\n * 利用redis-cli连接7001，执行set num 123\n\n * 利用redis-cli连接7002，执行get num，再执行set num 666\n\n * 利用redis-cli连接7003，执行get num，再执行set num 888\n\n可以发现，只有在7001这个master节点上可以执行写操作，7002和7003这两个slave节点只能执行读操作。",charsets:{cjk:!0}},{title:"哨兵集群搭建",frontmatter:{autoSort:97,title:"哨兵集群搭建",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/ca02f2/",categories:["后端","数据库","Redis"],tags:["知识","数据库","Redis"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/05.Redis/103.%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA.html",relativePath:"01.后端/30.数据库/05.Redis/103.哨兵集群搭建.md",key:"v-360acd55",path:"/pages/ca02f2/",headers:[{level:2,title:"集群结构",slug:"集群结构",normalizedTitle:"集群结构",charIndex:2},{level:2,title:"准备实例和配置",slug:"准备实例和配置",normalizedTitle:"准备实例和配置",charIndex:194},{level:2,title:"启动",slug:"启动",normalizedTitle:"启动",charIndex:1097},{level:2,title:"测试",slug:"测试",normalizedTitle:"测试",charIndex:1270}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"集群结构 准备实例和配置 启动 测试",content:"# 集群结构\n\n这里我们搭建一个三节点形成的Sentinel集群，来监管之前的Redis主从集群。如图：\n\n\n\n三个sentinel实例信息如下：\n\n节点   IP                PORT\ns1   192.168.150.101   27001\ns2   192.168.150.101   27002\ns3   192.168.150.101   27003\n\n\n# 准备实例和配置\n\n要在同一台虚拟机开启3个实例，必须准备三份不同的配置文件和目录，配置文件所在目录也就是工作目录。\n\n我们创建三个文件夹，名字分别叫s1、s2、s3：\n\n# 进入/tmp目录\ncd /tmp\n# 创建目录\nmkdir s1 s2 s3\n\n\n如图：\n\n\n\n然后我们在s1目录创建一个sentinel.conf文件，添加下面的内容：\n\nport 27001\nsentinel announce-ip 192.168.150.101\nsentinel monitor mymaster 192.168.150.101 7001 2\nsentinel down-after-milliseconds mymaster 5000\nsentinel failover-timeout mymaster 60000\ndir \"/tmp/s1\"\n\n\n解读：\n\n * port 27001：是当前sentinel实例的端口\n * sentinel monitor mymaster 192.168.150.101 7001 2：指定主节点信息\n   * mymaster：主节点名称，自定义，任意写\n   * 192.168.150.101 7001：主节点的ip和端口\n   * 2：选举master时的quorum值\n\n然后将s1/sentinel.conf文件拷贝到s2、s3两个目录中（在/tmp目录执行下列命令）：\n\n# 方式一：逐个拷贝\ncp s1/sentinel.conf s2\ncp s1/sentinel.conf s3\n# 方式二：管道组合命令，一键拷贝\necho s2 s3 | xargs -t -n 1 cp s1/sentinel.conf\n\n\n修改s2、s3两个文件夹内的配置文件，将端口分别修改为27002、27003：\n\nsed -i -e 's/27001/27002/g' -e 's/s1/s2/g' s2/sentinel.conf\nsed -i -e 's/27001/27003/g' -e 's/s1/s3/g' s3/sentinel.conf\n\n\n\n# 启动\n\n为了方便查看日志，我们打开3个ssh窗口，分别启动3个redis实例，启动命令：\n\n# 第1个\nredis-sentinel s1/sentinel.conf\n# 第2个\nredis-sentinel s2/sentinel.conf\n# 第3个\nredis-sentinel s3/sentinel.conf\n\n\n启动后：\n\n\n\n\n# 测试\n\n尝试让master节点7001宕机，查看sentinel日志：\n\n\n\n查看7003的日志：\n\n\n\n查看7002的日志：\n\n",normalizedContent:"# 集群结构\n\n这里我们搭建一个三节点形成的sentinel集群，来监管之前的redis主从集群。如图：\n\n\n\n三个sentinel实例信息如下：\n\n节点   ip                port\ns1   192.168.150.101   27001\ns2   192.168.150.101   27002\ns3   192.168.150.101   27003\n\n\n# 准备实例和配置\n\n要在同一台虚拟机开启3个实例，必须准备三份不同的配置文件和目录，配置文件所在目录也就是工作目录。\n\n我们创建三个文件夹，名字分别叫s1、s2、s3：\n\n# 进入/tmp目录\ncd /tmp\n# 创建目录\nmkdir s1 s2 s3\n\n\n如图：\n\n\n\n然后我们在s1目录创建一个sentinel.conf文件，添加下面的内容：\n\nport 27001\nsentinel announce-ip 192.168.150.101\nsentinel monitor mymaster 192.168.150.101 7001 2\nsentinel down-after-milliseconds mymaster 5000\nsentinel failover-timeout mymaster 60000\ndir \"/tmp/s1\"\n\n\n解读：\n\n * port 27001：是当前sentinel实例的端口\n * sentinel monitor mymaster 192.168.150.101 7001 2：指定主节点信息\n   * mymaster：主节点名称，自定义，任意写\n   * 192.168.150.101 7001：主节点的ip和端口\n   * 2：选举master时的quorum值\n\n然后将s1/sentinel.conf文件拷贝到s2、s3两个目录中（在/tmp目录执行下列命令）：\n\n# 方式一：逐个拷贝\ncp s1/sentinel.conf s2\ncp s1/sentinel.conf s3\n# 方式二：管道组合命令，一键拷贝\necho s2 s3 | xargs -t -n 1 cp s1/sentinel.conf\n\n\n修改s2、s3两个文件夹内的配置文件，将端口分别修改为27002、27003：\n\nsed -i -e 's/27001/27002/g' -e 's/s1/s2/g' s2/sentinel.conf\nsed -i -e 's/27001/27003/g' -e 's/s1/s3/g' s3/sentinel.conf\n\n\n\n# 启动\n\n为了方便查看日志，我们打开3个ssh窗口，分别启动3个redis实例，启动命令：\n\n# 第1个\nredis-sentinel s1/sentinel.conf\n# 第2个\nredis-sentinel s2/sentinel.conf\n# 第3个\nredis-sentinel s3/sentinel.conf\n\n\n启动后：\n\n\n\n\n# 测试\n\n尝试让master节点7001宕机，查看sentinel日志：\n\n\n\n查看7003的日志：\n\n\n\n查看7002的日志：\n\n",charsets:{cjk:!0}},{title:"Redis持久化",frontmatter:{autoSort:96,title:"Redis持久化",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/7c7a2d/",categories:["后端","数据库","Redis"],tags:["知识","数据库","Redis"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/05.Redis/23.Redis%E6%8C%81%E4%B9%85%E5%8C%96.html",relativePath:"01.后端/30.数据库/05.Redis/23.Redis持久化.md",key:"v-3bfd7f0c",path:"/pages/7c7a2d/",headers:[{level:2,title:"RDB持久化",slug:"rdb持久化",normalizedTitle:"rdb持久化",charIndex:19},{level:3,title:"执行时机",slug:"执行时机",normalizedTitle:"执行时机",charIndex:189},{level:3,title:"RDB原理",slug:"rdb原理",normalizedTitle:"rdb原理",charIndex:783},{level:3,title:"小结",slug:"小结",normalizedTitle:"小结",charIndex:937},{level:2,title:"AOF持久化",slug:"aof持久化",normalizedTitle:"aof持久化",charIndex:29},{level:3,title:"AOF原理",slug:"aof原理",normalizedTitle:"aof原理",charIndex:1207},{level:3,title:"AOF配置",slug:"aof配置",normalizedTitle:"aof配置",charIndex:1287},{level:3,title:"AOF文件重写",slug:"aof文件重写",normalizedTitle:"aof文件重写",charIndex:1623},{level:2,title:"RDB与AOF对比",slug:"rdb与aof对比",normalizedTitle:"rdb与aof对比",charIndex:2032}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"RDB持久化 执行时机 RDB原理 小结 AOF持久化 AOF原理 AOF配置 AOF文件重写 RDB与AOF对比",content:'Redis有两种持久化方案：\n\n * RDB持久化\n * AOF持久化\n\n\n# RDB持久化\n\nRDB全称Redis Database Backup file（Redis数据备份文件），也被叫做Redis数据快照。简单来说就是把内存中的所有数据都记录到磁盘中。当Redis实例故障重启后，从磁盘读取快照文件，恢复数据。快照文件称为RDB文件，默认是保存在当前运行目录。\n\n\n# 执行时机\n\nRDB持久化在四种情况下会执行：\n\n * 执行save命令\n * 执行bgsave命令\n * Redis停机时\n * 触发RDB条件时\n\n默认存储地方就是当前执行命令的地方./，名称为dump.rdb\n\n1）save命令\n\n执行下面的命令，可以立即执行一次RDB：\n\n\n\nsave命令会导致主进程执行RDB，这个过程中其它所有命令都会被阻塞。只有在数据迁移时可能用到。\n\n2）bgsave命令\n\n下面的命令可以异步执行RDB：\n\n\n\n这个命令执行后会开启独立进程完成RDB，主进程可以持续处理用户请求，不受影响。\n\n3）停机时\n\nRedis停机时会执行一次save命令，实现RDB持久化。\n\n4）触发RDB条件\n\nRedis内部有触发RDB的机制，可以在redis.conf文件中找到，格式如下：\n\n# 900秒内，如果至少有1个key被修改，则执行bgsave ， 如果是save "" 则表示禁用RDB\nsave 900 1  \nsave 300 10  \nsave 60 10000 \n\n\nRDB的其它配置也可以在redis.conf文件中设置：\n\n# 是否压缩 ,建议不开启，压缩也会消耗cpu，磁盘的话不值钱\nrdbcompression yes\n\n# RDB文件名称\ndbfilename dump.rdb  \n\n# 文件保存的路径目录\ndir ./ \n\n\n\n# RDB原理\n\nbgsave开始时会fork主进程得到子进程，子进程共享主进程的内存数据。完成fork后读取内存数据并写入 RDB 文件。\n\nfork采用的是copy-on-write技术：\n\n * 当主进程执行读操作时，访问共享内存；\n * 当主进程执行写操作时，则会拷贝一份数据，执行写操作。\n\n\n\n\n# 小结\n\nRDB方式bgsave的基本流程？\n\n * fork主进程得到一个子进程，共享内存空间\n * 子进程读取内存数据并写入新的RDB文件\n * 用新RDB文件替换旧的RDB文件\n\nRDB会在什么时候执行？save 60 1000代表什么含义？\n\n * 默认是服务停止时**(save)**\n * 代表60秒内至少执行1000次修改则触发RDB**(bgsave)**\n\nRDB的缺点？\n\n * RDB执行间隔时间长，两次RDB之间写入数据有丢失的风险\n * fork子进程、压缩、写出RDB文件都比较耗时\n\n\n# AOF持久化\n\n\n# AOF原理\n\nAOF全称为Append Only File（追加文件）。Redis处理的每一个写命令都会记录在AOF文件，可以看做是命令日志文件。\n\n\n\n\n# AOF配置\n\nAOF默认是关闭的，需要修改redis.conf配置文件来开启AOF：\n\n# 是否开启AOF功能，默认是no\nappendonly yes\n# AOF文件的名称\nappendfilename "appendonly.aof"\n\n\nAOF的命令记录的频率也可以通过redis.conf文件来配：\n\n# 表示每执行一次写命令，立即记录到AOF文件\nappendfsync always \n# 写命令执行完先放入AOF缓冲区，然后表示每隔1秒将缓冲区数据写到AOF文件，是默认方案\nappendfsync everysec \n# 写命令执行完先放入AOF缓冲区，由操作系统决定何时将缓冲区内容写回磁盘\nappendfsync no\n\n\n三种策略对比：\n\n\n\n\n# AOF文件重写\n\n因为是记录命令，AOF文件会比RDB文件大的多。而且AOF会记录对同一个key的多次写操作，但只有最后一次写操作才有意义。通过执行bgrewriteaof命令，可以让AOF文件执行重写功能，用最少的命令达到相同效果。\n\n\n\n如图，AOF原本有三个命令，但是set num 123 和 set num 666都是对num的操作，第二次会覆盖第一次的值，因此第一个命令记录下来没有意义。\n\n所以重写命令后，AOF文件内容就是：mset name jack num 666\n\nRedis也会在触发阈值时自动去重写AOF文件。阈值也可以在redis.conf中配置：\n\n# AOF文件比上次文件 增长超过多少百分比则触发重写\nauto-aof-rewrite-percentage 100\n# AOF文件体积最小多大以上才触发重写 \nauto-aof-rewrite-min-size 64mb \n\n\n\n# RDB与AOF对比\n\nRDB和AOF各有自己的优缺点，如果对数据安全性要求较高，在实际开发中往往会结合两者来使用。\n\n',normalizedContent:'redis有两种持久化方案：\n\n * rdb持久化\n * aof持久化\n\n\n# rdb持久化\n\nrdb全称redis database backup file（redis数据备份文件），也被叫做redis数据快照。简单来说就是把内存中的所有数据都记录到磁盘中。当redis实例故障重启后，从磁盘读取快照文件，恢复数据。快照文件称为rdb文件，默认是保存在当前运行目录。\n\n\n# 执行时机\n\nrdb持久化在四种情况下会执行：\n\n * 执行save命令\n * 执行bgsave命令\n * redis停机时\n * 触发rdb条件时\n\n默认存储地方就是当前执行命令的地方./，名称为dump.rdb\n\n1）save命令\n\n执行下面的命令，可以立即执行一次rdb：\n\n\n\nsave命令会导致主进程执行rdb，这个过程中其它所有命令都会被阻塞。只有在数据迁移时可能用到。\n\n2）bgsave命令\n\n下面的命令可以异步执行rdb：\n\n\n\n这个命令执行后会开启独立进程完成rdb，主进程可以持续处理用户请求，不受影响。\n\n3）停机时\n\nredis停机时会执行一次save命令，实现rdb持久化。\n\n4）触发rdb条件\n\nredis内部有触发rdb的机制，可以在redis.conf文件中找到，格式如下：\n\n# 900秒内，如果至少有1个key被修改，则执行bgsave ， 如果是save "" 则表示禁用rdb\nsave 900 1  \nsave 300 10  \nsave 60 10000 \n\n\nrdb的其它配置也可以在redis.conf文件中设置：\n\n# 是否压缩 ,建议不开启，压缩也会消耗cpu，磁盘的话不值钱\nrdbcompression yes\n\n# rdb文件名称\ndbfilename dump.rdb  \n\n# 文件保存的路径目录\ndir ./ \n\n\n\n# rdb原理\n\nbgsave开始时会fork主进程得到子进程，子进程共享主进程的内存数据。完成fork后读取内存数据并写入 rdb 文件。\n\nfork采用的是copy-on-write技术：\n\n * 当主进程执行读操作时，访问共享内存；\n * 当主进程执行写操作时，则会拷贝一份数据，执行写操作。\n\n\n\n\n# 小结\n\nrdb方式bgsave的基本流程？\n\n * fork主进程得到一个子进程，共享内存空间\n * 子进程读取内存数据并写入新的rdb文件\n * 用新rdb文件替换旧的rdb文件\n\nrdb会在什么时候执行？save 60 1000代表什么含义？\n\n * 默认是服务停止时**(save)**\n * 代表60秒内至少执行1000次修改则触发rdb**(bgsave)**\n\nrdb的缺点？\n\n * rdb执行间隔时间长，两次rdb之间写入数据有丢失的风险\n * fork子进程、压缩、写出rdb文件都比较耗时\n\n\n# aof持久化\n\n\n# aof原理\n\naof全称为append only file（追加文件）。redis处理的每一个写命令都会记录在aof文件，可以看做是命令日志文件。\n\n\n\n\n# aof配置\n\naof默认是关闭的，需要修改redis.conf配置文件来开启aof：\n\n# 是否开启aof功能，默认是no\nappendonly yes\n# aof文件的名称\nappendfilename "appendonly.aof"\n\n\naof的命令记录的频率也可以通过redis.conf文件来配：\n\n# 表示每执行一次写命令，立即记录到aof文件\nappendfsync always \n# 写命令执行完先放入aof缓冲区，然后表示每隔1秒将缓冲区数据写到aof文件，是默认方案\nappendfsync everysec \n# 写命令执行完先放入aof缓冲区，由操作系统决定何时将缓冲区内容写回磁盘\nappendfsync no\n\n\n三种策略对比：\n\n\n\n\n# aof文件重写\n\n因为是记录命令，aof文件会比rdb文件大的多。而且aof会记录对同一个key的多次写操作，但只有最后一次写操作才有意义。通过执行bgrewriteaof命令，可以让aof文件执行重写功能，用最少的命令达到相同效果。\n\n\n\n如图，aof原本有三个命令，但是set num 123 和 set num 666都是对num的操作，第二次会覆盖第一次的值，因此第一个命令记录下来没有意义。\n\n所以重写命令后，aof文件内容就是：mset name jack num 666\n\nredis也会在触发阈值时自动去重写aof文件。阈值也可以在redis.conf中配置：\n\n# aof文件比上次文件 增长超过多少百分比则触发重写\nauto-aof-rewrite-percentage 100\n# aof文件体积最小多大以上才触发重写 \nauto-aof-rewrite-min-size 64mb \n\n\n\n# rdb与aof对比\n\nrdb和aof各有自己的优缺点，如果对数据安全性要求较高，在实际开发中往往会结合两者来使用。\n\n',charsets:{cjk:!0}},{title:"分片集群搭建",frontmatter:{autoSort:96,title:"分片集群搭建",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/fdb88d/",categories:["后端","数据库","Redis"],tags:["知识","数据库","Redis"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/05.Redis/104.%E5%88%86%E7%89%87%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA.html",relativePath:"01.后端/30.数据库/05.Redis/104.分片集群搭建.md",key:"v-3bf8a54c",path:"/pages/fdb88d/",headers:[{level:2,title:"集群结构",slug:"集群结构",normalizedTitle:"集群结构",charIndex:2},{level:2,title:"准备实例和配置",slug:"准备实例和配置",normalizedTitle:"准备实例和配置",charIndex:340},{level:2,title:"启动",slug:"启动",normalizedTitle:"启动",charIndex:1170},{level:2,title:"创建集群",slug:"创建集群",normalizedTitle:"创建集群",charIndex:1545},{level:2,title:"测试",slug:"测试",normalizedTitle:"测试",charIndex:2581}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"集群结构 准备实例和配置 启动 创建集群 测试",content:"# 集群结构\n\n分片集群需要的节点数量较多，这里我们搭建一个最小的分片集群，包含3个master节点，每个master包含一个slave节点，结构如下：\n\n\n\n这里我们会在同一台虚拟机中开启6个redis实例，模拟分片集群，信息如下：\n\nIP                PORT   角色\n192.168.150.101   7001   master\n192.168.150.101   7002   master\n192.168.150.101   7003   master\n192.168.150.101   8001   slave\n192.168.150.101   8002   slave\n192.168.150.101   8003   slave\n\n\n# 准备实例和配置\n\n删除之前的7001、7002、7003这几个目录，重新创建出7001、7002、7003、8001、8002、8003目录：\n\n# 进入/tmp目录\ncd /tmp\n# 删除旧的，避免配置干扰\nrm -rf 7001 7002 7003\n# 创建目录\nmkdir 7001 7002 7003 8001 8002 8003\n\n\n在/tmp下准备一个新的redis.conf文件，内容如下：\n\nport 6379\n# 开启集群功能\ncluster-enabled yes\n# 集群的配置文件名称，不需要我们创建，由redis自己维护\ncluster-config-file /tmp/6379/nodes.conf\n# 节点心跳失败的超时时间\ncluster-node-timeout 5000\n# 持久化文件存放目录\ndir /tmp/6379\n# 绑定地址\nbind 0.0.0.0\n# 让redis后台运行\ndaemonize yes\n# 注册的实例ip\nreplica-announce-ip 192.168.150.101\n# 保护模式\nprotected-mode no\n# 数据库数量\ndatabases 1\n# 日志\nlogfile /tmp/6379/run.log\n\n\n将这个文件拷贝到每个目录下：\n\n# 进入/tmp目录\ncd /tmp\n# 执行拷贝\necho 7001 7002 7003 8001 8002 8003 | xargs -t -n 1 cp redis.conf\n\n\n修改每个目录下的redis.conf，将其中的6379修改为与所在目录一致：\n\n# 进入/tmp目录\ncd /tmp\n# 修改配置文件\nprintf '%s\\n' 7001 7002 7003 8001 8002 8003 | xargs -I{} -t sed -i 's/6379/{}/g' {}/redis.conf\n\n\n\n# 启动\n\n因为已经配置了后台启动模式，所以可以直接启动服务：\n\n# 进入/tmp目录\ncd /tmp\n# 一键启动所有服务\nprintf '%s\\n' 7001 7002 7003 8001 8002 8003 | xargs -I{} -t redis-server {}/redis.conf\n\n\n通过ps查看状态：\n\nps -ef | grep redis\n\n\n发现服务都已经正常启动：\n\n\n\n如果要关闭所有进程，可以执行命令：\n\nps -ef | grep redis | awk '{print $2}' | xargs kill\n\n\n或者（推荐这种方式）：\n\nprintf '%s\\n' 7001 7002 7003 8001 8002 8003 | xargs -I{} -t redis-cli -p {} shutdown\n\n\n\n# 创建集群\n\n虽然服务启动了，但是目前每个服务之间都是独立的，没有任何关联。\n\n我们需要执行命令来创建集群，在Redis5.0之前创建集群比较麻烦，5.0之后集群管理命令都集成到了redis-cli中。\n\n1）Redis5.0之前\n\nRedis5.0之前集群命令都是用redis安装包下的src/redis-trib.rb来实现的。因为redis-trib.rb是有ruby语言编写的所以需要安装ruby环境。\n\n# 安装依赖\nyum -y install zlib ruby rubygems\ngem install redis\n\n\n然后通过命令来管理集群：\n\n# 进入redis的src目录\ncd /tmp/redis-6.2.4/src\n# 创建集群\n./redis-trib.rb create --replicas 1 192.168.150.101:7001 192.168.150.101:7002 192.168.150.101:7003 192.168.150.101:8001 192.168.150.101:8002 192.168.150.101:8003\n\n\n2）Redis5.0以后\n\n我们使用的是Redis6.2.4版本，集群管理以及集成到了redis-cli中，格式如下：\n\nredis-cli --cluster create --cluster-replicas 1 192.168.150.101:7001 192.168.150.101:7002 192.168.150.101:7003 192.168.150.101:8001 192.168.150.101:8002 192.168.150.101:8003\n\n\n命令说明：\n\n * redis-cli --cluster或者./redis-trib.rb：代表集群操作命令\n * create：代表是创建集群\n * --replicas 1或者--cluster-replicas 1 ：指定集群中每个master的副本个数为1，此时节点总数 ÷ (replicas + 1) 得到的就是master的数量。因此节点列表中的前n个就是master，其它节点都是slave节点，随机分配到不同master\n\n运行后的样子：\n\n\n\n这里输入yes，则集群开始创建：\n\n\n\n通过命令可以查看集群状态：\n\nredis-cli -p 7001 cluster nodes\n\n\n\n\n\n# 测试\n\n尝试连接7001节点，存储一个数据：\n\n# 连接\nredis-cli -p 7001\n# 存储数据\nset num 123\n# 读取数据\nget num\n# 再次存储\nset a 1\n\n\n结果悲剧了：\n\n\n\n集群操作时，需要给redis-cli加上-c参数才可以：\n\nredis-cli -c -p 7001\n\n\n这次可以了：\n\n",normalizedContent:"# 集群结构\n\n分片集群需要的节点数量较多，这里我们搭建一个最小的分片集群，包含3个master节点，每个master包含一个slave节点，结构如下：\n\n\n\n这里我们会在同一台虚拟机中开启6个redis实例，模拟分片集群，信息如下：\n\nip                port   角色\n192.168.150.101   7001   master\n192.168.150.101   7002   master\n192.168.150.101   7003   master\n192.168.150.101   8001   slave\n192.168.150.101   8002   slave\n192.168.150.101   8003   slave\n\n\n# 准备实例和配置\n\n删除之前的7001、7002、7003这几个目录，重新创建出7001、7002、7003、8001、8002、8003目录：\n\n# 进入/tmp目录\ncd /tmp\n# 删除旧的，避免配置干扰\nrm -rf 7001 7002 7003\n# 创建目录\nmkdir 7001 7002 7003 8001 8002 8003\n\n\n在/tmp下准备一个新的redis.conf文件，内容如下：\n\nport 6379\n# 开启集群功能\ncluster-enabled yes\n# 集群的配置文件名称，不需要我们创建，由redis自己维护\ncluster-config-file /tmp/6379/nodes.conf\n# 节点心跳失败的超时时间\ncluster-node-timeout 5000\n# 持久化文件存放目录\ndir /tmp/6379\n# 绑定地址\nbind 0.0.0.0\n# 让redis后台运行\ndaemonize yes\n# 注册的实例ip\nreplica-announce-ip 192.168.150.101\n# 保护模式\nprotected-mode no\n# 数据库数量\ndatabases 1\n# 日志\nlogfile /tmp/6379/run.log\n\n\n将这个文件拷贝到每个目录下：\n\n# 进入/tmp目录\ncd /tmp\n# 执行拷贝\necho 7001 7002 7003 8001 8002 8003 | xargs -t -n 1 cp redis.conf\n\n\n修改每个目录下的redis.conf，将其中的6379修改为与所在目录一致：\n\n# 进入/tmp目录\ncd /tmp\n# 修改配置文件\nprintf '%s\\n' 7001 7002 7003 8001 8002 8003 | xargs -i{} -t sed -i 's/6379/{}/g' {}/redis.conf\n\n\n\n# 启动\n\n因为已经配置了后台启动模式，所以可以直接启动服务：\n\n# 进入/tmp目录\ncd /tmp\n# 一键启动所有服务\nprintf '%s\\n' 7001 7002 7003 8001 8002 8003 | xargs -i{} -t redis-server {}/redis.conf\n\n\n通过ps查看状态：\n\nps -ef | grep redis\n\n\n发现服务都已经正常启动：\n\n\n\n如果要关闭所有进程，可以执行命令：\n\nps -ef | grep redis | awk '{print $2}' | xargs kill\n\n\n或者（推荐这种方式）：\n\nprintf '%s\\n' 7001 7002 7003 8001 8002 8003 | xargs -i{} -t redis-cli -p {} shutdown\n\n\n\n# 创建集群\n\n虽然服务启动了，但是目前每个服务之间都是独立的，没有任何关联。\n\n我们需要执行命令来创建集群，在redis5.0之前创建集群比较麻烦，5.0之后集群管理命令都集成到了redis-cli中。\n\n1）redis5.0之前\n\nredis5.0之前集群命令都是用redis安装包下的src/redis-trib.rb来实现的。因为redis-trib.rb是有ruby语言编写的所以需要安装ruby环境。\n\n# 安装依赖\nyum -y install zlib ruby rubygems\ngem install redis\n\n\n然后通过命令来管理集群：\n\n# 进入redis的src目录\ncd /tmp/redis-6.2.4/src\n# 创建集群\n./redis-trib.rb create --replicas 1 192.168.150.101:7001 192.168.150.101:7002 192.168.150.101:7003 192.168.150.101:8001 192.168.150.101:8002 192.168.150.101:8003\n\n\n2）redis5.0以后\n\n我们使用的是redis6.2.4版本，集群管理以及集成到了redis-cli中，格式如下：\n\nredis-cli --cluster create --cluster-replicas 1 192.168.150.101:7001 192.168.150.101:7002 192.168.150.101:7003 192.168.150.101:8001 192.168.150.101:8002 192.168.150.101:8003\n\n\n命令说明：\n\n * redis-cli --cluster或者./redis-trib.rb：代表集群操作命令\n * create：代表是创建集群\n * --replicas 1或者--cluster-replicas 1 ：指定集群中每个master的副本个数为1，此时节点总数 ÷ (replicas + 1) 得到的就是master的数量。因此节点列表中的前n个就是master，其它节点都是slave节点，随机分配到不同master\n\n运行后的样子：\n\n\n\n这里输入yes，则集群开始创建：\n\n\n\n通过命令可以查看集群状态：\n\nredis-cli -p 7001 cluster nodes\n\n\n\n\n\n# 测试\n\n尝试连接7001节点，存储一个数据：\n\n# 连接\nredis-cli -p 7001\n# 存储数据\nset num 123\n# 读取数据\nget num\n# 再次存储\nset a 1\n\n\n结果悲剧了：\n\n\n\n集群操作时，需要给redis-cli加上-c参数才可以：\n\nredis-cli -c -p 7001\n\n\n这次可以了：\n\n",charsets:{cjk:!0}},{title:"RedisAPI",frontmatter:{autoSort:98,title:"RedisAPI",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/94a219/",categories:["后端","数据库","Redis"],tags:["知识","数据库","Redis"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/05.Redis/15.Redis-API.html",relativePath:"01.后端/30.数据库/05.Redis/15.Redis-API.md",key:"v-a0cd1b6e",path:"/pages/94a219/",headers:[{level:2,title:"Jedis",slug:"jedis",normalizedTitle:"jedis",charIndex:2},{level:3,title:"Jedis快速入门",slug:"jedis快速入门",normalizedTitle:"jedis快速入门",charIndex:328},{level:3,title:"Jedis连接池",slug:"jedis连接池",normalizedTitle:"jedis连接池",charIndex:1543},{level:2,title:"SpringDataRedis",slug:"springdataredis",normalizedTitle:"springdataredis",charIndex:164},{level:3,title:"快速入门",slug:"快速入门",normalizedTitle:"快速入门",charIndex:333},{level:3,title:"数据序列化器",slug:"数据序列化器",normalizedTitle:"数据序列化器",charIndex:6482},{level:3,title:"StringRedisTemplate",slug:"stringredistemplate",normalizedTitle:"stringredistemplate",charIndex:7611},{level:3,title:"Hash结构操作",slug:"hash结构操作",normalizedTitle:"hash结构操作",charIndex:9263}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"Jedis Jedis快速入门 Jedis连接池 SpringDataRedis 快速入门 数据序列化器 StringRedisTemplate Hash结构操作",content:'# Jedis\n\n在Redis官网中提供了各种语言的客户端，地址：https://redis.io/docs/clients/\n\n\n\n其中Java客户端也包含很多：\n\n\n\n标记为❤的就是推荐使用的java客户端，包括：\n\n * Jedis和Lettuce：这两个主要是提供了Redis命令对应的API，方便我们操作Redis，而SpringDataRedis又对这两种做了抽象和封装，因此我们后期会直接以SpringDataRedis来学习。\n * Redisson：是在Redis基础上实现了分布式的可伸缩的java数据结构，例如Map.Queue等，而且支持跨进程的同步机制：Lock.Semaphore等待，比较适合用来实现特殊的功能需求。\n\n\n# Jedis快速入门\n\njedis优势\n\n> 所有的命令与redis原生命令基本一致\n> \n> 没有多余的学习成本\n\n入门案例详细步骤\n\n案例分析：\n\n1）引入依赖：\n\n\x3c!--jedis--\x3e\n<dependency>\n    <groupId>redis.clients</groupId>\n    <artifactId>jedis</artifactId>\n    <version>3.7.0</version>\n</dependency>\n\x3c!--单元测试--\x3e\n<dependency>\n    <groupId>org.junit.jupiter</groupId>\n    <artifactId>junit-jupiter</artifactId>\n    <version>5.7.0</version>\n    <scope>test</scope>\n</dependency>\n\n\n2）建立连接\n\n新建一个单元测试类，内容如下：\n\nprivate Jedis jedis;\n\n@BeforeEach\nvoid setUp() {\n    // 1.建立连接\n    // jedis = new Jedis("192.168.150.101", 6379);\n    jedis = JedisConnectionFactory.getJedis();\n    // 2.设置密码\n    jedis.auth("123321");\n    // 3.选择库\n    jedis.select(0);\n}\n\n\n3）测试：\n\n@Test\nvoid testString() {\n    // 存入数据\n    String result = jedis.set("name", "虎哥");\n    System.out.println("result = " + result);\n    // 获取数据\n    String name = jedis.get("name");\n    System.out.println("name = " + name);\n}\n\n@Test\nvoid testHash() {\n    // 插入hash数据\n    jedis.hset("user:1", "name", "Jack");\n    jedis.hset("user:1", "age", "21");\n\n    // 获取\n    Map<String, String> map = jedis.hgetAll("user:1");\n    System.out.println(map);\n}\n\n\n4）释放资源\n\n@AfterEach\nvoid tearDown() {\n    if (jedis != null) {\n        jedis.close();\n    }\n}\n\n\n\n# Jedis连接池\n\nJedis本身是线程不安全的，并且频繁的创建和销毁连接会有性能损耗，因此我们推荐大家使用Jedis连接池代替Jedis的直连方式\n\n有关池化思想，并不仅仅是这里会使用，很多地方都有，比如说我们的数据库连接池，比如我们tomcat中的线程池，这些都是池化思想的体现。\n\n# 创建Jedis的连接池\n\npublic class JedisConnectionFacotry {\n\n     private static final JedisPool jedisPool;\n\n     static {\n         //配置连接池\n         JedisPoolConfig poolConfig = new JedisPoolConfig();\n         poolConfig.setMaxTotal(8);\n         poolConfig.setMaxIdle(8);\n         poolConfig.setMinIdle(0);\n         poolConfig.setMaxWaitMillis(1000);\n         //创建连接池对象\n         jedisPool = new JedisPool(poolConfig,\n                 "192.168.150.101",6379,1000,"123321");\n     }\n\n     public static Jedis getJedis(){\n          return jedisPool.getResource();\n     }\n}\n\n\n代码说明：\n\n * 1） JedisConnectionFacotry：工厂设计模式是实际开发中非常常用的一种设计模式，我们可以使用工厂，去降低代的耦合，比如Spring中的Bean的创建，就用到了工厂设计模式\n\n * 2）静态代码块：随着类的加载而加载，确保只能执行一次，我们在加载当前工厂类的时候，就可以执行static的操作完成对 连接池的初始化\n\n * 3）最后提供返回连接池中连接的方法.\n\n# 改造原始代码\n\n代码说明:\n\n1.在我们完成了使用工厂设计模式来完成代码的编写之后，我们在获得连接时，就可以通过工厂来获得。\n\n，而不用直接去new对象，降低耦合，并且使用的还是连接池对象。\n\n2.当我们使用了连接池后，当我们关闭连接其实并不是关闭，而是将Jedis还回连接池的。\n\n    @BeforeEach\n    void setUp(){\n        //建立连接\n        /*jedis = new Jedis("127.0.0.1",6379);*/\n        jedis = JedisConnectionFacotry.getJedis();\n         //选择库\n        jedis.select(0);\n    }\n\n   @AfterEach\n    void tearDown() {\n        if (jedis != null) {\n            jedis.close();\n        }\n    }\n\n\n\n# SpringDataRedis\n\nSpringData是Spring中数据操作的模块，包含对各种数据库的集成，其中对Redis的集成模块就叫做SpringDataRedis，官网地址：https://spring.io/projects/spring-data-redis\n\n * 提供了对不同Redis客户端的整合（Lettuce和Jedis）\n * 提供了RedisTemplate统一API来操作Redis\n * 支持Redis的发布订阅模型\n * 支持Redis哨兵和Redis集群\n * 支持基于Lettuce的响应式编程\n * 支持基于JDK.JSON.字符串.Spring对象的数据序列化及反序列化\n * 支持基于Redis的JDKCollection实现\n\nSpringDataRedis中提供了RedisTemplate工具类，其中封装了各种对Redis的操作。并且将不同数据类型的操作API封装到了不同的类型中：\n\n\n\n\n# 快速入门\n\nSpringBoot已经提供了对SpringDataRedis的支持，使用非常简单：\n\n# 导入pom坐标\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.5.7</version>\n        <relativePath/> \x3c!-- lookup parent from repository --\x3e\n    </parent>\n    <groupId>com.heima</groupId>\n    <artifactId>redis-demo</artifactId>\n    <version>0.0.1-SNAPSHOT</version>\n    <name>redis-demo</name>\n    <description>Demo project for Spring Boot</description>\n    <properties>\n        <java.version>1.8</java.version>\n    </properties>\n    <dependencies>\n        \x3c!--redis依赖--\x3e\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-data-redis</artifactId>\n        </dependency>\n        \x3c!--common-pool--\x3e\n        <dependency>\n            <groupId>org.apache.commons</groupId>\n            <artifactId>commons-pool2</artifactId>\n        </dependency>\n        \x3c!--Jackson依赖--\x3e\n        <dependency>\n            <groupId>com.fasterxml.jackson.core</groupId>\n            <artifactId>jackson-databind</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.projectlombok</groupId>\n            <artifactId>lombok</artifactId>\n            <optional>true</optional>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n                <configuration>\n                    <excludes>\n                        <exclude>\n                            <groupId>org.projectlombok</groupId>\n                            <artifactId>lombok</artifactId>\n                        </exclude>\n                    </excludes>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>\n\n\n# 配置文件\n\nspring:\n  redis:\n    host: 192.168.159.100\n    port: 6379\n    #password: 123321\n    lettuce:\n      pool:\n        max-active: 8  #最大连接\n        max-idle: 8   #最大空闲连接\n        min-idle: 0   #最小空闲连接\n        max-wait: 100ms #连接等待时间\n\n\n# 测试代码\n\n@SpringBootTest\nclass RedisDemoApplicationTests {\n\n    @Autowired\n    private RedisTemplate<String, Object> redisTemplate;\n\n    @Test\n    void testString() {\n        // 写入一条String数据\n        redisTemplate.opsForValue().set("name", "虎哥");\n        // 获取string数据\n        Object name = redisTemplate.opsForValue().get("name");\n        System.out.println("name = " + name);\n    }\n}\n\n\n贴心小提示：SpringDataJpa使用起来非常简单，记住如下几个步骤即可\n\nSpringDataRedis的使用步骤：\n\n * 引入spring-boot-starter-data-redis依赖\n * 在application.yml配置Redis信息\n * 注入RedisTemplate\n\n\n# 数据序列化器\n\nRedisTemplate可以接收任意Object作为值写入Redis：\n\n\n\n只不过写入前会把Object序列化为字节形式，默认是采用JDK序列化，得到的结果是这样的：\n\n\n\n缺点：\n\n * 可读性差\n * 内存占用较大\n\n我们可以自定义RedisTemplate的序列化方式，代码如下：\n\n@Configuration\npublic class RedisConfig {\n\n    @Bean\n    public RedisTemplate<String, Object> redisTemplate(RedisConnectionFactory connectionFactory){\n        // 创建RedisTemplate对象\n        RedisTemplate<String, Object> template = new RedisTemplate<>();\n        // 设置连接工厂\n        template.setConnectionFactory(connectionFactory);\n        // 创建JSON序列化工具\n        GenericJackson2JsonRedisSerializer jsonRedisSerializer = \n            \t\t\t\t\t\t\tnew GenericJackson2JsonRedisSerializer();\n        // 设置Key的序列化\n        template.setKeySerializer(RedisSerializer.string());\n        template.setHashKeySerializer(RedisSerializer.string());\n        // 设置Value的序列化\n        template.setValueSerializer(jsonRedisSerializer);\n        template.setHashValueSerializer(jsonRedisSerializer);\n        // 返回\n        return template;\n    }\n}\n\n\n这里采用了JSON序列化来代替默认的JDK序列化方式。最终结果如图：\n\n\n\n整体可读性有了很大提升，并且能将Java对象自动的序列化为JSON字符串，并且查询时能自动把JSON反序列化为Java对象。不过，其中记录了序列化时对应的class名称，目的是为了查询时实现自动反序列化。这会带来额外的内存开销。\n\n\n# StringRedisTemplate\n\n尽管JSON的序列化方式可以满足我们的需求，但依然存在一些问题，如图：\n\n\n\n为了在反序列化时知道对象的类型，JSON序列化器会将类的class类型写入json结果中，存入Redis，会带来额外的内存开销。\n\n为了减少内存的消耗，我们可以采用手动序列化的方式，换句话说，就是不借助默认的序列化器，而是我们自己来控制序列化的动作，同时，我们只采用String的序列化器，这样，在存储value时，我们就不需要在内存中就不用多存储数据，从而节约我们的内存空间\n\n\n\n这种用法比较普遍，因此SpringDataRedis就提供了RedisTemplate的子类：StringRedisTemplate，它的key和value的序列化方式默认就是String方式。\n\n\n\n省去了我们自定义RedisTemplate的序列化方式的步骤，而是直接使用：\n\n@SpringBootTest\nclass RedisStringTests {\n\n    @Autowired\n    private StringRedisTemplate stringRedisTemplate;\n\n    @Test\n    void testString() {\n        // 写入一条String数据\n        stringRedisTemplate.opsForValue().set("verify:phone:13600527634", "124143");\n        // 获取string数据\n        Object name = stringRedisTemplate.opsForValue().get("name");\n        System.out.println("name = " + name);\n    }\n\n    private static final ObjectMapper mapper = new ObjectMapper();\n\n    @Test\n    void testSaveUser() throws JsonProcessingException {\n        // 创建对象\n        User user = new User("虎哥", 21);\n        // 手动序列化\n        String json = mapper.writeValueAsString(user);\n        // 写入数据\n        stringRedisTemplate.opsForValue().set("user:200", json);\n\n        // 获取数据\n        String jsonUser = stringRedisTemplate.opsForValue().get("user:200");\n        // 手动反序列化\n        User user1 = mapper.readValue(jsonUser, User.class);\n        System.out.println("user1 = " + user1);\n    }\n\n}\n\n\n此时我们再来看一看存储的数据，小伙伴们就会发现那个class数据已经不在了，节约了我们的空间~\n\n\n\n最后小总结：\n\nRedisTemplate的两种序列化实践方案：\n\n * 方案一：\n   \n   * 自定义RedisTemplate\n   * 修改RedisTemplate的序列化器为GenericJackson2JsonRedisSerializer\n\n * 方案二：\n   \n   * 使用StringRedisTemplate\n   * 写入Redis时，手动把对象序列化为JSON\n   * 读取Redis时，手动把读取到的JSON反序列化为对象\n\n\n# Hash结构操作\n\n在基础篇的最后，咱们对Hash结构操作一下，收一个小尾巴，这个代码咱们就不再解释啦\n\n马上就开始新的篇章~~~进入到我们的Redis实战篇\n\n@SpringBootTest\nclass RedisStringTests {\n\n    @Autowired\n    private StringRedisTemplate stringRedisTemplate;\n\n\n    @Test\n    void testHash() {\n        stringRedisTemplate.opsForHash().put("user:400", "name", "虎哥");\n        stringRedisTemplate.opsForHash().put("user:400", "age", "21");\n\n        Map<Object, Object> entries = stringRedisTemplate.opsForHash().entries("user:400");\n        System.out.println("entries = " + entries);\n    }\n}\n',normalizedContent:'# jedis\n\n在redis官网中提供了各种语言的客户端，地址：https://redis.io/docs/clients/\n\n\n\n其中java客户端也包含很多：\n\n\n\n标记为❤的就是推荐使用的java客户端，包括：\n\n * jedis和lettuce：这两个主要是提供了redis命令对应的api，方便我们操作redis，而springdataredis又对这两种做了抽象和封装，因此我们后期会直接以springdataredis来学习。\n * redisson：是在redis基础上实现了分布式的可伸缩的java数据结构，例如map.queue等，而且支持跨进程的同步机制：lock.semaphore等待，比较适合用来实现特殊的功能需求。\n\n\n# jedis快速入门\n\njedis优势\n\n> 所有的命令与redis原生命令基本一致\n> \n> 没有多余的学习成本\n\n入门案例详细步骤\n\n案例分析：\n\n1）引入依赖：\n\n\x3c!--jedis--\x3e\n<dependency>\n    <groupid>redis.clients</groupid>\n    <artifactid>jedis</artifactid>\n    <version>3.7.0</version>\n</dependency>\n\x3c!--单元测试--\x3e\n<dependency>\n    <groupid>org.junit.jupiter</groupid>\n    <artifactid>junit-jupiter</artifactid>\n    <version>5.7.0</version>\n    <scope>test</scope>\n</dependency>\n\n\n2）建立连接\n\n新建一个单元测试类，内容如下：\n\nprivate jedis jedis;\n\n@beforeeach\nvoid setup() {\n    // 1.建立连接\n    // jedis = new jedis("192.168.150.101", 6379);\n    jedis = jedisconnectionfactory.getjedis();\n    // 2.设置密码\n    jedis.auth("123321");\n    // 3.选择库\n    jedis.select(0);\n}\n\n\n3）测试：\n\n@test\nvoid teststring() {\n    // 存入数据\n    string result = jedis.set("name", "虎哥");\n    system.out.println("result = " + result);\n    // 获取数据\n    string name = jedis.get("name");\n    system.out.println("name = " + name);\n}\n\n@test\nvoid testhash() {\n    // 插入hash数据\n    jedis.hset("user:1", "name", "jack");\n    jedis.hset("user:1", "age", "21");\n\n    // 获取\n    map<string, string> map = jedis.hgetall("user:1");\n    system.out.println(map);\n}\n\n\n4）释放资源\n\n@aftereach\nvoid teardown() {\n    if (jedis != null) {\n        jedis.close();\n    }\n}\n\n\n\n# jedis连接池\n\njedis本身是线程不安全的，并且频繁的创建和销毁连接会有性能损耗，因此我们推荐大家使用jedis连接池代替jedis的直连方式\n\n有关池化思想，并不仅仅是这里会使用，很多地方都有，比如说我们的数据库连接池，比如我们tomcat中的线程池，这些都是池化思想的体现。\n\n# 创建jedis的连接池\n\npublic class jedisconnectionfacotry {\n\n     private static final jedispool jedispool;\n\n     static {\n         //配置连接池\n         jedispoolconfig poolconfig = new jedispoolconfig();\n         poolconfig.setmaxtotal(8);\n         poolconfig.setmaxidle(8);\n         poolconfig.setminidle(0);\n         poolconfig.setmaxwaitmillis(1000);\n         //创建连接池对象\n         jedispool = new jedispool(poolconfig,\n                 "192.168.150.101",6379,1000,"123321");\n     }\n\n     public static jedis getjedis(){\n          return jedispool.getresource();\n     }\n}\n\n\n代码说明：\n\n * 1） jedisconnectionfacotry：工厂设计模式是实际开发中非常常用的一种设计模式，我们可以使用工厂，去降低代的耦合，比如spring中的bean的创建，就用到了工厂设计模式\n\n * 2）静态代码块：随着类的加载而加载，确保只能执行一次，我们在加载当前工厂类的时候，就可以执行static的操作完成对 连接池的初始化\n\n * 3）最后提供返回连接池中连接的方法.\n\n# 改造原始代码\n\n代码说明:\n\n1.在我们完成了使用工厂设计模式来完成代码的编写之后，我们在获得连接时，就可以通过工厂来获得。\n\n，而不用直接去new对象，降低耦合，并且使用的还是连接池对象。\n\n2.当我们使用了连接池后，当我们关闭连接其实并不是关闭，而是将jedis还回连接池的。\n\n    @beforeeach\n    void setup(){\n        //建立连接\n        /*jedis = new jedis("127.0.0.1",6379);*/\n        jedis = jedisconnectionfacotry.getjedis();\n         //选择库\n        jedis.select(0);\n    }\n\n   @aftereach\n    void teardown() {\n        if (jedis != null) {\n            jedis.close();\n        }\n    }\n\n\n\n# springdataredis\n\nspringdata是spring中数据操作的模块，包含对各种数据库的集成，其中对redis的集成模块就叫做springdataredis，官网地址：https://spring.io/projects/spring-data-redis\n\n * 提供了对不同redis客户端的整合（lettuce和jedis）\n * 提供了redistemplate统一api来操作redis\n * 支持redis的发布订阅模型\n * 支持redis哨兵和redis集群\n * 支持基于lettuce的响应式编程\n * 支持基于jdk.json.字符串.spring对象的数据序列化及反序列化\n * 支持基于redis的jdkcollection实现\n\nspringdataredis中提供了redistemplate工具类，其中封装了各种对redis的操作。并且将不同数据类型的操作api封装到了不同的类型中：\n\n\n\n\n# 快速入门\n\nspringboot已经提供了对springdataredis的支持，使用非常简单：\n\n# 导入pom坐标\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0" xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n    <parent>\n        <groupid>org.springframework.boot</groupid>\n        <artifactid>spring-boot-starter-parent</artifactid>\n        <version>2.5.7</version>\n        <relativepath/> \x3c!-- lookup parent from repository --\x3e\n    </parent>\n    <groupid>com.heima</groupid>\n    <artifactid>redis-demo</artifactid>\n    <version>0.0.1-snapshot</version>\n    <name>redis-demo</name>\n    <description>demo project for spring boot</description>\n    <properties>\n        <java.version>1.8</java.version>\n    </properties>\n    <dependencies>\n        \x3c!--redis依赖--\x3e\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-data-redis</artifactid>\n        </dependency>\n        \x3c!--common-pool--\x3e\n        <dependency>\n            <groupid>org.apache.commons</groupid>\n            <artifactid>commons-pool2</artifactid>\n        </dependency>\n        \x3c!--jackson依赖--\x3e\n        <dependency>\n            <groupid>com.fasterxml.jackson.core</groupid>\n            <artifactid>jackson-databind</artifactid>\n        </dependency>\n        <dependency>\n            <groupid>org.projectlombok</groupid>\n            <artifactid>lombok</artifactid>\n            <optional>true</optional>\n        </dependency>\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-test</artifactid>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupid>org.springframework.boot</groupid>\n                <artifactid>spring-boot-maven-plugin</artifactid>\n                <configuration>\n                    <excludes>\n                        <exclude>\n                            <groupid>org.projectlombok</groupid>\n                            <artifactid>lombok</artifactid>\n                        </exclude>\n                    </excludes>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>\n\n\n# 配置文件\n\nspring:\n  redis:\n    host: 192.168.159.100\n    port: 6379\n    #password: 123321\n    lettuce:\n      pool:\n        max-active: 8  #最大连接\n        max-idle: 8   #最大空闲连接\n        min-idle: 0   #最小空闲连接\n        max-wait: 100ms #连接等待时间\n\n\n# 测试代码\n\n@springboottest\nclass redisdemoapplicationtests {\n\n    @autowired\n    private redistemplate<string, object> redistemplate;\n\n    @test\n    void teststring() {\n        // 写入一条string数据\n        redistemplate.opsforvalue().set("name", "虎哥");\n        // 获取string数据\n        object name = redistemplate.opsforvalue().get("name");\n        system.out.println("name = " + name);\n    }\n}\n\n\n贴心小提示：springdatajpa使用起来非常简单，记住如下几个步骤即可\n\nspringdataredis的使用步骤：\n\n * 引入spring-boot-starter-data-redis依赖\n * 在application.yml配置redis信息\n * 注入redistemplate\n\n\n# 数据序列化器\n\nredistemplate可以接收任意object作为值写入redis：\n\n\n\n只不过写入前会把object序列化为字节形式，默认是采用jdk序列化，得到的结果是这样的：\n\n\n\n缺点：\n\n * 可读性差\n * 内存占用较大\n\n我们可以自定义redistemplate的序列化方式，代码如下：\n\n@configuration\npublic class redisconfig {\n\n    @bean\n    public redistemplate<string, object> redistemplate(redisconnectionfactory connectionfactory){\n        // 创建redistemplate对象\n        redistemplate<string, object> template = new redistemplate<>();\n        // 设置连接工厂\n        template.setconnectionfactory(connectionfactory);\n        // 创建json序列化工具\n        genericjackson2jsonredisserializer jsonredisserializer = \n            \t\t\t\t\t\t\tnew genericjackson2jsonredisserializer();\n        // 设置key的序列化\n        template.setkeyserializer(redisserializer.string());\n        template.sethashkeyserializer(redisserializer.string());\n        // 设置value的序列化\n        template.setvalueserializer(jsonredisserializer);\n        template.sethashvalueserializer(jsonredisserializer);\n        // 返回\n        return template;\n    }\n}\n\n\n这里采用了json序列化来代替默认的jdk序列化方式。最终结果如图：\n\n\n\n整体可读性有了很大提升，并且能将java对象自动的序列化为json字符串，并且查询时能自动把json反序列化为java对象。不过，其中记录了序列化时对应的class名称，目的是为了查询时实现自动反序列化。这会带来额外的内存开销。\n\n\n# stringredistemplate\n\n尽管json的序列化方式可以满足我们的需求，但依然存在一些问题，如图：\n\n\n\n为了在反序列化时知道对象的类型，json序列化器会将类的class类型写入json结果中，存入redis，会带来额外的内存开销。\n\n为了减少内存的消耗，我们可以采用手动序列化的方式，换句话说，就是不借助默认的序列化器，而是我们自己来控制序列化的动作，同时，我们只采用string的序列化器，这样，在存储value时，我们就不需要在内存中就不用多存储数据，从而节约我们的内存空间\n\n\n\n这种用法比较普遍，因此springdataredis就提供了redistemplate的子类：stringredistemplate，它的key和value的序列化方式默认就是string方式。\n\n\n\n省去了我们自定义redistemplate的序列化方式的步骤，而是直接使用：\n\n@springboottest\nclass redisstringtests {\n\n    @autowired\n    private stringredistemplate stringredistemplate;\n\n    @test\n    void teststring() {\n        // 写入一条string数据\n        stringredistemplate.opsforvalue().set("verify:phone:13600527634", "124143");\n        // 获取string数据\n        object name = stringredistemplate.opsforvalue().get("name");\n        system.out.println("name = " + name);\n    }\n\n    private static final objectmapper mapper = new objectmapper();\n\n    @test\n    void testsaveuser() throws jsonprocessingexception {\n        // 创建对象\n        user user = new user("虎哥", 21);\n        // 手动序列化\n        string json = mapper.writevalueasstring(user);\n        // 写入数据\n        stringredistemplate.opsforvalue().set("user:200", json);\n\n        // 获取数据\n        string jsonuser = stringredistemplate.opsforvalue().get("user:200");\n        // 手动反序列化\n        user user1 = mapper.readvalue(jsonuser, user.class);\n        system.out.println("user1 = " + user1);\n    }\n\n}\n\n\n此时我们再来看一看存储的数据，小伙伴们就会发现那个class数据已经不在了，节约了我们的空间~\n\n\n\n最后小总结：\n\nredistemplate的两种序列化实践方案：\n\n * 方案一：\n   \n   * 自定义redistemplate\n   * 修改redistemplate的序列化器为genericjackson2jsonredisserializer\n\n * 方案二：\n   \n   * 使用stringredistemplate\n   * 写入redis时，手动把对象序列化为json\n   * 读取redis时，手动把读取到的json反序列化为对象\n\n\n# hash结构操作\n\n在基础篇的最后，咱们对hash结构操作一下，收一个小尾巴，这个代码咱们就不再解释啦\n\n马上就开始新的篇章~~~进入到我们的redis实战篇\n\n@springboottest\nclass redisstringtests {\n\n    @autowired\n    private stringredistemplate stringredistemplate;\n\n\n    @test\n    void testhash() {\n        stringredistemplate.opsforhash().put("user:400", "name", "虎哥");\n        stringredistemplate.opsforhash().put("user:400", "age", "21");\n\n        map<object, object> entries = stringredistemplate.opsforhash().entries("user:400");\n        system.out.println("entries = " + entries);\n    }\n}\n',charsets:{cjk:!0}},{title:"Redis主从",frontmatter:{autoSort:95,title:"Redis主从",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/ef6419/",categories:["后端","数据库","Redis"],tags:["知识","数据库","Redis"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/05.Redis/25.Redis%E4%B8%BB%E4%BB%8E.html",relativePath:"01.后端/30.数据库/05.Redis/25.Redis主从.md",key:"v-6229bca2",path:"/pages/ef6419/",headers:[{level:2,title:"搭建主从架构",slug:"搭建主从架构",normalizedTitle:"搭建主从架构",charIndex:2},{level:2,title:"主从数据同步原理",slug:"主从数据同步原理",normalizedTitle:"主从数据同步原理",charIndex:332},{level:3,title:"全量同步",slug:"全量同步",normalizedTitle:"全量同步",charIndex:345},{level:3,title:"增量同步",slug:"增量同步",normalizedTitle:"增量同步",charIndex:1055},{level:3,title:"repl_backlog原理",slug:"repl-backlog原理",normalizedTitle:"repl_backlog原理",charIndex:1457},{level:2,title:"主从同步优化",slug:"主从同步优化",normalizedTitle:"主从同步优化",charIndex:2075},{level:2,title:"小结",slug:"小结",normalizedTitle:"小结",charIndex:2356}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"搭建主从架构 主从数据同步原理 全量同步 增量同步 repl_backlog原理 主从同步优化 小结",content:"# 搭建主从架构\n\n单节点Redis的并发能力是有上限的，要进一步提高Redis的并发能力，就需要搭建主从集群，实现读写分离。\n\n默认是 主节点进行读写操作，从节点是只读操作\n\n\n\n具体搭建流程\n\n基本命令\n\n# 连接 7002\nredis-cli -p 7002\n# 执行slaveof\nslaveof 192.168.150.101 7001\n\n# 连接 7003\nredis-cli -p 7003\n# 执行slaveof\nslaveof 192.168.150.101 7001\n\n#修改配置文件-conf ————bind 0.0.0.0\n# 连接 7001\nredis-cli -p 7001\n# 查看状态\ninfo replication\n\n\n\n# 主从数据同步原理\n\n\n# 全量同步\n\n主从第一次建立连接时，会执行全量同步，将master节点的所有数据都拷贝给slave节点，流程：\n\n\n\n这里有一个问题，master如何得知salve是第一次来连接呢？？\n\n有几个概念，可以作为判断依据：\n\n * Replication Id：简称replid，是数据集的标记，id一致则说明是同一数据集。每一个master都有唯一的replid，slave则会继承master节点的replid\n * offset：偏移量，随着记录在repl_baklog中的数据增多而逐渐增大。slave完成同步时也会记录当前同步的offset。如果slave的offset小于master的offset，说明slave数据落后于master，需要更新。\n\n因此slave做数据同步，必须向master声明自己的replication id 和offset，master才可以判断到底需要同步哪些数据。\n\n因为slave原本也是一个master，有自己的replid和offset，当第一次变成slave，与master建立连接时，发送的replid和offset是自己的replid和offset。\n\nmaster判断发现slave发送来的replid与自己的不一致，说明这是一个全新的slave，就知道要做全量同步了。\n\nmaster会将自己的replid和offset都发送给这个slave，slave保存这些信息。以后slave的replid就与master一致了。\n\n因此，master判断一个节点是否是第一次同步的依据，就是看replid是否一致。\n\n如图：\n\n\n\n完整流程描述：\n\n * slave节点请求增量同步\n * master节点判断replid，发现不一致，拒绝增量同步\n * master将完整内存数据生成RDB，发送RDB到slave\n * slave清空本地数据，加载master的RDB\n * master将RDB期间的命令记录在repl_baklog，并持续将log中的命令发送给slave\n * slave执行接收到的命令，保持与master之间的同步\n\n * 从节点日志\n\n\n\n * 主节点日志\n\n\n\n\n# 增量同步\n\n全量同步需要先做RDB，然后将RDB文件通过网络传输个slave，成本太高了。因此除了第一次做全量同步，其它大多数时候slave与master都是做增量同步。\n\n * 例如 slave 重启后，则执行增量同步\n\n什么是增量同步？就是只更新slave与master存在差异的部分数据。如图：\n\n\n\n那么master怎么知道slave与自己的数据差异在哪里呢?\n\n\n# repl_backlog原理\n\nmaster怎么知道slave与自己的数据差异在哪里呢?\n\n这就要说到全量同步时的repl_baklog文件了。\n\n这个文件是一个固定大小的数组，只不过数组是环形，也就是说角标到达数组末尾后，会再次从0开始读写，这样数组头部的数据就会被覆盖。\n\nrepl_baklog中会记录Redis处理过的命令日志及offset，包括master当前的offset，和slave已经拷贝到的offset：\n\n\n\nslave与master的offset之间的差异，就是salve需要增量拷贝的数据了。\n\n随着不断有数据写入，master的offset逐渐变大，slave也不断的拷贝，追赶master的offset：\n\n\n\n直到数组被填满：\n\n\n\n此时，如果有新的数据写入，就会覆盖数组中的旧数据。不过，旧的数据只要是绿色的，说明是已经被同步到slave的数据，即便被覆盖了也没什么影响。因为未同步的仅仅是红色部分。\n\n但是，如果slave出现网络阻塞，导致master的offset远远超过了slave的offset：\n\n\n\n如果master继续写入新数据，其offset就会覆盖旧的数据，直到将slave现在的offset也覆盖：\n\n\n\n棕色框中的红色部分，就是尚未同步，但是却已经被覆盖的数据。此时如果slave恢复，需要同步，却发现自己的offset都没有了，无法完成增量同步了。只能做全量同步。\n\n\n\n\n# 主从同步优化\n\n主从同步可以保证主从数据的一致性，非常重要。\n\n可以从以下几个方面来优化Redis主从就集群：\n\n * 在master中配置repl-diskless-sync yes启用无磁盘复制，避免全量同步时的磁盘IO。\n * Redis单节点上的内存占用不要太大，减少RDB导致的过多磁盘IO\n * 适当提高repl_baklog的大小，发现slave宕机时尽快实现故障恢复，尽可能避免全量同步\n * 限制一个master上的slave节点数量，如果实在是太多slave，则可以采用主-从-从链式结构，减少master压力\n\n主从从架构图：\n\n\n\n\n# 小结\n\n简述全量同步和增量同步区别？\n\n * 全量同步：master将完整内存数据生成RDB，发送RDB到slave。后续命令则记录在repl_baklog，逐个发送给slave。\n * 增量同步：slave提交自己的offset到master，master获取repl_baklog中从offset之后的命令给slave\n\n什么时候执行全量同步？\n\n * slave节点第一次连接master节点时\n * slave节点断开时间太久，repl_baklog中的offset已经被覆盖时\n\n什么时候执行增量同步？\n\n * slave节点断开又恢复，并且在repl_baklog中能找到offset时",normalizedContent:"# 搭建主从架构\n\n单节点redis的并发能力是有上限的，要进一步提高redis的并发能力，就需要搭建主从集群，实现读写分离。\n\n默认是 主节点进行读写操作，从节点是只读操作\n\n\n\n具体搭建流程\n\n基本命令\n\n# 连接 7002\nredis-cli -p 7002\n# 执行slaveof\nslaveof 192.168.150.101 7001\n\n# 连接 7003\nredis-cli -p 7003\n# 执行slaveof\nslaveof 192.168.150.101 7001\n\n#修改配置文件-conf ————bind 0.0.0.0\n# 连接 7001\nredis-cli -p 7001\n# 查看状态\ninfo replication\n\n\n\n# 主从数据同步原理\n\n\n# 全量同步\n\n主从第一次建立连接时，会执行全量同步，将master节点的所有数据都拷贝给slave节点，流程：\n\n\n\n这里有一个问题，master如何得知salve是第一次来连接呢？？\n\n有几个概念，可以作为判断依据：\n\n * replication id：简称replid，是数据集的标记，id一致则说明是同一数据集。每一个master都有唯一的replid，slave则会继承master节点的replid\n * offset：偏移量，随着记录在repl_baklog中的数据增多而逐渐增大。slave完成同步时也会记录当前同步的offset。如果slave的offset小于master的offset，说明slave数据落后于master，需要更新。\n\n因此slave做数据同步，必须向master声明自己的replication id 和offset，master才可以判断到底需要同步哪些数据。\n\n因为slave原本也是一个master，有自己的replid和offset，当第一次变成slave，与master建立连接时，发送的replid和offset是自己的replid和offset。\n\nmaster判断发现slave发送来的replid与自己的不一致，说明这是一个全新的slave，就知道要做全量同步了。\n\nmaster会将自己的replid和offset都发送给这个slave，slave保存这些信息。以后slave的replid就与master一致了。\n\n因此，master判断一个节点是否是第一次同步的依据，就是看replid是否一致。\n\n如图：\n\n\n\n完整流程描述：\n\n * slave节点请求增量同步\n * master节点判断replid，发现不一致，拒绝增量同步\n * master将完整内存数据生成rdb，发送rdb到slave\n * slave清空本地数据，加载master的rdb\n * master将rdb期间的命令记录在repl_baklog，并持续将log中的命令发送给slave\n * slave执行接收到的命令，保持与master之间的同步\n\n * 从节点日志\n\n\n\n * 主节点日志\n\n\n\n\n# 增量同步\n\n全量同步需要先做rdb，然后将rdb文件通过网络传输个slave，成本太高了。因此除了第一次做全量同步，其它大多数时候slave与master都是做增量同步。\n\n * 例如 slave 重启后，则执行增量同步\n\n什么是增量同步？就是只更新slave与master存在差异的部分数据。如图：\n\n\n\n那么master怎么知道slave与自己的数据差异在哪里呢?\n\n\n# repl_backlog原理\n\nmaster怎么知道slave与自己的数据差异在哪里呢?\n\n这就要说到全量同步时的repl_baklog文件了。\n\n这个文件是一个固定大小的数组，只不过数组是环形，也就是说角标到达数组末尾后，会再次从0开始读写，这样数组头部的数据就会被覆盖。\n\nrepl_baklog中会记录redis处理过的命令日志及offset，包括master当前的offset，和slave已经拷贝到的offset：\n\n\n\nslave与master的offset之间的差异，就是salve需要增量拷贝的数据了。\n\n随着不断有数据写入，master的offset逐渐变大，slave也不断的拷贝，追赶master的offset：\n\n\n\n直到数组被填满：\n\n\n\n此时，如果有新的数据写入，就会覆盖数组中的旧数据。不过，旧的数据只要是绿色的，说明是已经被同步到slave的数据，即便被覆盖了也没什么影响。因为未同步的仅仅是红色部分。\n\n但是，如果slave出现网络阻塞，导致master的offset远远超过了slave的offset：\n\n\n\n如果master继续写入新数据，其offset就会覆盖旧的数据，直到将slave现在的offset也覆盖：\n\n\n\n棕色框中的红色部分，就是尚未同步，但是却已经被覆盖的数据。此时如果slave恢复，需要同步，却发现自己的offset都没有了，无法完成增量同步了。只能做全量同步。\n\n\n\n\n# 主从同步优化\n\n主从同步可以保证主从数据的一致性，非常重要。\n\n可以从以下几个方面来优化redis主从就集群：\n\n * 在master中配置repl-diskless-sync yes启用无磁盘复制，避免全量同步时的磁盘io。\n * redis单节点上的内存占用不要太大，减少rdb导致的过多磁盘io\n * 适当提高repl_baklog的大小，发现slave宕机时尽快实现故障恢复，尽可能避免全量同步\n * 限制一个master上的slave节点数量，如果实在是太多slave，则可以采用主-从-从链式结构，减少master压力\n\n主从从架构图：\n\n\n\n\n# 小结\n\n简述全量同步和增量同步区别？\n\n * 全量同步：master将完整内存数据生成rdb，发送rdb到slave。后续命令则记录在repl_baklog，逐个发送给slave。\n * 增量同步：slave提交自己的offset到master，master获取repl_baklog中从offset之后的命令给slave\n\n什么时候执行全量同步？\n\n * slave节点第一次连接master节点时\n * slave节点断开时间太久，repl_baklog中的offset已经被覆盖时\n\n什么时候执行增量同步？\n\n * slave节点断开又恢复，并且在repl_baklog中能找到offset时",charsets:{cjk:!0}},{title:"多表查询",frontmatter:{autoSort:96,title:"多表查询",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/a367f0/",categories:["后端","数据库","MySQL"],tags:["知识","数据库","MySQL"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/01.MySQL/15.%E5%A4%9A%E8%A1%A8%E6%9F%A5%E8%AF%A2.html",relativePath:"01.后端/30.数据库/01.MySQL/15.多表查询.md",key:"v-6daf2e3a",path:"/pages/a367f0/",headers:[{level:2,title:"多表关系",slug:"多表关系",normalizedTitle:"多表关系",charIndex:2},{level:2,title:"多表查询概述",slug:"多表查询概述",normalizedTitle:"多表查询概述",charIndex:3912},{level:2,title:"内连接",slug:"内连接",normalizedTitle:"内连接",charIndex:6028},{level:2,title:"外连接",slug:"外连接",normalizedTitle:"外连接",charIndex:6081},{level:2,title:"自连接",slug:"自连接",normalizedTitle:"自连接",charIndex:6236},{level:2,title:"联合查询",slug:"联合查询",normalizedTitle:"联合查询",charIndex:7892},{level:2,title:"子查询",slug:"子查询",normalizedTitle:"子查询",charIndex:6293},{level:3,title:"标量子查询",slug:"标量子查询",normalizedTitle:"标量子查询",charIndex:8428},{level:3,title:"列子查询",slug:"列子查询",normalizedTitle:"列子查询",charIndex:8459},{level:3,title:"行子查询",slug:"行子查询",normalizedTitle:"行子查询",charIndex:8489},{level:3,title:"表子查询",slug:"表子查询",normalizedTitle:"表子查询",charIndex:8519},{level:2,title:"多表查询案例",slug:"多表查询案例",normalizedTitle:"多表查询案例",charIndex:11384}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"多表关系 多表查询概述 内连接 外连接 自连接 联合查询 子查询 标量子查询 列子查询 行子查询 表子查询 多表查询案例",content:"# 多表关系\n\n 1. 一对多\n    \n    实现: 在多的一方建立外键，指向一的一方的主键\n    \n    \n    \n    create table dept(\n        id   int auto_increment comment 'ID' primary key,\n        name varchar(50) not null comment '部门名称'\n    )comment '部门表';\n    INSERT INTO dept (id, name) VALUES (1, '研发部'), (2, '市场部'),(3, '财务部'), (4, '销售部'), (5, '总经办');\n    \n    \n    create table emp(\n        id  int auto_increment comment 'ID' primary key,\n        name varchar(50) not null comment '姓名',\n        age  int comment '年龄',\n        job varchar(20) comment '职位',\n        salary int comment '薪资',\n        entrydate date comment '入职时间',\n        managerid int comment '直属领导ID',\n        dept_id int comment '部门ID'\n    )comment '员工表';\n    \n    INSERT INTO emp (id, name, age, job,salary, entrydate, managerid, dept_id) \n    VALUES(1, '金庸', 66, '总裁',20000, '2000-01-01', null,5),\n    (2, '张无忌', 20, '项目经理',12500, '2005-12-05', 1,1),\n    (3, '杨逍', 33, '开发', 8400,'2000-11-03', 2,1),\n    (4, '韦一笑', 48, '开发',11000, '2002-02-05', 2,1),\n    (5, '常遇春', 43, '开发',10500, '2004-09-07', 3,1),\n    (6, '小昭', 19, '程序员鼓励师',6600, '2004-10-12', 2,1);\n    \n    -- 添加外键\n    alter table emp add constraint fk_emp_dept_id foreign key (dept_id) references dept(id);\n    \n\n 2. 多对多\n    \n    实现: 建立第三张中间表，中间表至少包含两个外键，分别关联两方主键\n    \n    \n    \n    -- 多对多 ----------------\n    create table student(\n        id int auto_increment primary key comment '主键ID',\n        name varchar(10) comment '姓名',\n        no varchar(10) comment '学号'\n    ) comment '学生表';\n    insert into student values (null, '黛绮丝', '2000100101'),(null, '谢逊', '2000100102'),(null, '殷天正', '2000100103'),(null, '韦一笑', '2000100104');\n    \n    \n    create table course(\n        id int auto_increment primary key comment '主键ID',\n        name varchar(10) comment '课程名称'\n    ) comment '课程表';\n    insert into course values (null, 'Java'), (null, 'PHP'), (null , 'MySQL') , (null, 'Hadoop');\n    \n    \n    create table student_course(\n        id int auto_increment comment '主键' primary key,\n        studentid int not null comment '学生ID',\n        courseid  int not null comment '课程ID',\n        constraint fk_courseid foreign key (courseid) references course (id),\n        constraint fk_studentid foreign key (studentid) references student (id)\n    )comment '学生课程中间表';\n    \n    insert into student_course values (null,1,1),(null,1,2),(null,1,3),(null,2,2),(null,2,3),(null,3,4);\n    \n\n 3. 一对一\n    \n    * 一对一关系，多用于单表拆分，将一张表的基础字段放在一张表中，其他详情字段放在另 一张表中，以提升操作效率\n    * 实现: 在任意一方加入外键，关联另外一方的主键，并且设置外键为唯一的(UNIQUE)\n    \n    \n    \n    -- --------------------------------- 一对一 ---------------------------\n    create table tb_user(\n        id int auto_increment primary key comment '主键ID',\n        name varchar(10) comment '姓名',\n        age int comment '年龄',\n        gender char(1) comment '1: 男 , 2: 女',\n        phone char(11) comment '手机号'\n    ) comment '用户基本信息表';\n    \n    create table tb_user_edu(\n        id int auto_increment primary key comment '主键ID',\n        degree varchar(20) comment '学历',\n        major varchar(50) comment '专业',\n        primaryschool varchar(50) comment '小学',\n        middleschool varchar(50) comment '中学',\n        university varchar(50) comment '大学',\n        userid int unique comment '用户ID',\n        constraint fk_userid foreign key (userid) references tb_user(id)\n    ) comment '用户教育信息表';\n    \n    \n    insert into tb_user(id, name, age, gender, phone) values\n            (null,'黄渤',45,'1','18800001111'),\n            (null,'冰冰',35,'2','18800002222'),\n            (null,'码云',55,'1','18800008888'),\n            (null,'李彦宏',50,'1','18800009999');\n    \n    insert into tb_user_edu(id, degree, major, primaryschool, middleschool, university, userid) values\n            (null,'本科','舞蹈','静安区第一小学','静安区第一中学','北京舞蹈学院',1),\n            (null,'硕士','表演','朝阳区第一小学','朝阳区第一中学','北京电影学院',2),\n            (null,'本科','英语','杭州市第一小学','杭州市第一中学','杭州师范大学',3),\n            (null,'本科','应用数学','阳泉第一小学','阳泉区第一中学','清华大学',4);\n    \n    \n    \n\n\n# 多表查询概述\n\n 1. 准备数据\n    \n    create table dept(\n        id   int auto_increment comment 'ID' primary key,\n        name varchar(50) not null comment '部门名称'\n    )comment '部门表';\n    \n    create table emp(\n        id  int auto_increment comment 'ID' primary key,\n        name varchar(50) not null comment '姓名',\n        age  int comment '年龄',\n        job varchar(20) comment '职位',\n        salary int comment '薪资',\n        entrydate date comment '入职时间',\n        managerid int comment '直属领导ID',\n        dept_id int comment '部门ID'\n    )comment '员工表';\n    \n    -- 添加外键\n    alter table emp add constraint fk_emp_dept_id foreign key (dept_id) references dept(id);\n    \n    INSERT INTO dept (id, name) VALUES (1, '研发部'), (2, '市场部'),(3, '财务部'), (4, '销售部'), (5, '总经办'), (6, '人事部');\n    INSERT INTO emp (id, name, age, job,salary, entrydate, managerid, dept_id) VALUES\n                (1, '金庸', 66, '总裁',20000, '2000-01-01', null,5),\n    \n                (2, '张无忌', 20, '项目经理',12500, '2005-12-05', 1,1),\n                (3, '杨逍', 33, '开发', 8400,'2000-11-03', 2,1),\n                (4, '韦一笑', 48, '开发',11000, '2002-02-05', 2,1),\n                (5, '常遇春', 43, '开发',10500, '2004-09-07', 3,1),\n                (6, '小昭', 19, '程序员鼓励师',6600, '2004-10-12', 2,1),\n    \n                (7, '灭绝', 60, '财务总监',8500, '2002-09-12', 1,3),\n                (8, '周芷若', 19, '会计',48000, '2006-06-02', 7,3),\n                (9, '丁敏君', 23, '出纳',5250, '2009-05-13', 7,3),\n    \n                (10, '赵敏', 20, '市场部总监',12500, '2004-10-12', 1,2),\n                (11, '鹿杖客', 56, '职员',3750, '2006-10-03', 10,2),\n                (12, '鹤笔翁', 19, '职员',3750, '2007-05-09', 10,2),\n                (13, '方东白', 19, '职员',5500, '2009-02-12', 10,2),\n    \n                (14, '张三丰', 88, '销售总监',14000, '2004-10-12', 1,4),\n                (15, '俞莲舟', 38, '销售',4600, '2004-10-12', 14,4),\n                (16, '宋远桥', 40, '销售',4600, '2004-10-12', 14,4),\n                (17, '陈友谅', 42, null,2000, '2011-10-12', 1,null);\n    \n\n 2. 查询所有，笛卡尔积\n    \n    -- 多表查询 -- 笛卡尔积\n    select * from emp , dept where emp.dept_id = dept.id;\n    \n\n 3. 分类\n    \n    \n    \n    * 连接查询\n      \n      * 内连接\n        \n        > 相当于查询A、B交集部分数据\n      \n      * 外连接\n        \n        * 左外连接\n          \n          > 查询左表所有数据，以及两张表交集部分数据\n        \n        * 右外连接\n          \n          > 查询右表所有数据，以及两张表交集部分数据\n      \n      * 自连接\n        \n        > 当前表与自身的连接查询，自连接必须使用表别名\n    \n    * 子查询\n      \n      > 将上一次查询的结果作为新的表，进行嵌套查询。\n\n\n# 内连接\n\n查询每一个员工的姓名 , 及关联的部门的名称\n\n * 隐式内连接\n   \n   SELECT 字段列表 FROM 表1 , 表2 WHERE 条件 ... ;\n   \n   select emp.name , dept.name from emp , dept where emp.dept_id = dept.id;\n   \n   -- 起别名\n   select e.name,d.name from emp e , dept d where e.dept_id = d.id;\n   \n\n * 显示内连接\n   \n   SELECT 字段列表 FROM 表1 [ INNER ] JOIN 表2 ON 连接条件 ... ;\n   \n   select e.name, d.name from emp e inner join dept d  on e.dept_id = d.id;\n   \n   -- 省略 inner\n   select e.name, d.name from emp e join dept d  on e.dept_id = d.id;\n   \n\n\n# 外连接\n\n * 左外连接\n   \n   查询emp表的所有数据, 和对应的部门信息\n   \n   SELECT 字段列表 FROM 表1 LEFT [ OUTER ] JOIN 表2 ON 条件 ... ;\n   \n   select e.*, d.name from emp e left outer join dept d on e.dept_id = d.id;\n   -- 省略 outer\n   select e.*, d.name from emp e left join dept d on e.dept_id = d.id;\n   \n\n * 右外连接\n   \n   查询dept表的所有数据, 和对应的员工信息(右外连接)\n   \n   SELECT 字段列表 FROM 表1 RIGHT [ OUTER ] JOIN 表2 ON 条件 ... ;\n   \n   select d.*, e.* from emp e right outer join dept d on e.dept_id = d.id;\n   -- 省略 outer\n   select d.*, e.* from emp e right join dept d on e.dept_id = d.id;\n   \n   -- 用左外连接代替右外连接实现\n   select d.*, e.* from dept d left outer join emp e on e.dept_id = d.id;\n   \n\n\n# 自连接\n\n自连接查询，顾名思义，就是自己连接自己，也就是把一张表连接查询多次。\n\n在自连接查询中，必须要为表起别名，要不然我们不清楚所指定的条件、返回的字段，到底 是哪一张表的字段。\n\n 1. 查询员工 及其 所属领导的名字\n\nSELECT 字段列表 FROM 表A 别名A JOIN 表A 别名B ON 条件 ... ;\n\n-- 自内连接\nselect a.name , b.name from emp a , emp b where a.managerid = b.id;\n\n\n 2. 查询所有员工 emp 及其领导的名字 emp , 如果员工没有领导, 也需要查询出来\n    \n    -- 自外连接\n    select a.name '员工', b.name '领导' from emp a left join emp b on a.managerid =\n    b.id;\n    \n\n\n# 联合查询\n\n对于union查询，就是把多次查询的结果合并起来，形成一个新的查询结果集。\n\n如果多条查询语句查询出来的结果，字段数量不一致， 在进行union/union all联合查询时，会报错。\n\ne.g. 将薪资低于 5000 的员工 , 和 年龄大于 50 岁的员工全部查询出来.\n\nselect * from emp where salary < 5000\nunion all\nselect * from emp where age > 50;\n\n-- union 相比于 union all 可以 去重\nselect * from emp where salary < 5000\nunion \nselect * from emp where age > 50;\n\n\n-- 用or代替联合查询\nselect * from emp where salary < 5000 or age > 50;\n\n\n\n# 子查询\n\nSQL语句中嵌套SELECT语句，称为嵌套查询，又称子查询。\n\nSELECT * FROM t1 WHERE column1 = ( SELECT column1 FROM t2 );\n\n\n * 根据子查询结果不同，分为：\n   \n   > A. 标量子查询（子查询结果为单个值）\n   > \n   > B. 列子查询 (子查询结果为一列)\n   > \n   > C. 行子查询 (子查询结果为一行)\n   > \n   > D. 表子查询 (子查询结果为多行多列)\n\n * 根据子查询位置，分为：\n   \n   > A. WHERE之后\n   > \n   > B. FROM之后\n   > \n   > C. SELECT之后\n\n\n# 标量子查询\n\n子查询返回的结果是单个值（数字、字符串、日期等），最简单的形式，这种子查询称为标量子查询。 常用的操作符：= <> > >= < <=\n\n 1. 查询 \"销售部\" 的所有员工信息\n    \n    -- a. 查询 \"销售部\" 部门ID--(4)\n    select id from dept where name = '销售部';\n    -- b. 根据销售部部门ID, 查询员工信息\n    select * from emp where dept_id = 4;\n    \n    -- 合并\n    select * from emp where dept_id = (select id from dept where name = '销售部');\n    \n\n 2. 查询在 \"方东白\" 入职之后的员工信息\n    \n    -- a. 查询 方东白 的入职日期--(2009-02-12)\n    select entrydate from emp where name = '方东白';\n    -- b. 查询指定入职日期之后入职的员工信息\n    select * from emp where entrydate > '2009-02-12';\n    -- 合并\n    select * from emp where entrydate > (select entrydate from emp where name = '方东白');\n    \n\n\n# 列子查询\n\n子查询返回的结果是一列（可以是多行），这种子查询称为列子查询。 常用的操作符：IN 、NOT IN 、 ANY 、SOME 、 ALL\n\n 1. 查询 \"销售部\" 和 \"市场部\" 的所有员工信息\n    \n    -- a. 查询 \"销售部\" 和 \"市场部\" 的部门ID\n    select id from dept where name in ('销售部', '市场部');\n    -- b. 根据部门ID, 查询员工信息\n    select * from emp where dept_id in (select id from dept where name in ('销售部', '市场部'));\n    \n\n 2. 查询比 财务部 所有人(all)工资都高的员工信息\n    \n    -- a. 查询所有 财务部 人员工资\n    select id from dept where name = '财务部';\n    select salary from emp where dept_id = (select id from dept where name = '财务部');\n    -- b. 比 财务部 所有人工资都高的员工信息\n    select * from emp where salary > all(select salary from emp where dept_id = (select id from dept where name = '财务部'));\n    \n\n 3. 查询比研发部其中任意一人(any,some)工资高的员工信息\n    \n    -- a. 查询研发部所有人工资\n    select salary from emp where dept_id = (select id from dept where name = '研发部');\n    -- b. 比研发部其中任意一人工资高的员工信息\n    select * from emp where salary > any(select salary from emp where dept_id = (select id from dept where name = '研发部'));\n    \n\n\n# 行子查询\n\n子查询返回的结果是一行（可以是多列），这种子查询称为行子查询。 常用的操作符：= 、<> 、IN 、NOT IN\n\n 1. 查询与 \"张无忌\" 的薪资及直属领导相同的员工信息\n    \n    -- a. 查询 \"张无忌\" 的薪资及直属领导\n    select salary, managerid from emp where name = '张无忌';\n    \n    -- b. 查询与 \"张无忌\" 的薪资及直属领导相同的员工信息;\n    select * from emp where (salary, managerid) = (12500, 1);\n    select * from emp where (salary, managerid) = (select salary, managerid from emp where name = '张无忌');\n    \n\n\n# 表子查询\n\n子查询返回的结果是多行多列，这种子查询称为表子查询。 常用的操作符：IN\n\n 1. 查询与 \"鹿杖客\" , \"宋远桥\" 的职位和薪资相同的员工信息\n    \n    -- a. 查询 \"鹿杖客\" , \"宋远桥\" 的职位和薪资\n    select job, salary from emp where name in ('鹿杖客', '宋远桥');\n    \n    -- b. 查询与 \"鹿杖客\" , \"宋远桥\" 的职位和薪资相同的员工信息\n    select * from emp where job in ('职员', '销售') and salary in (3750, 4600);\n    select * from emp where (job, salary) in (select job, salary from emp where name in ('鹿杖客', '宋远桥'));\n    \n\n 2. 查询入职日期是 \"2006-01-01\" 之后的员工信息 , 及其部门信息\n    \n    -- a. 入职日期是 \"2006-01-01\" 之后的员工信息\n    select * from emp where entrydate > '2006-01-01';\n    \n    -- b. 查询这部分员工, 对应的部门信息;\n    -- 把查询出的信息当成一个新表来对待\n    select e.*, d.name from (select * from emp where entrydate > '2006-01-01') e left join dept d on e.dept_id = d.id;\n    \n\n\n# 多表查询案例\n\n-- 数据准备\ncreate table salgrade(\n    grade int,\n    losal int,\n    hisal int\n) comment '薪资等级表';\n\ninsert into salgrade values (1,0,3000);\ninsert into salgrade values (2,3001,5000);\ninsert into salgrade values (3,5001,8000);\ninsert into salgrade values (4,8001,10000);\ninsert into salgrade values (5,10001,15000);\ninsert into salgrade values (6,15001,20000);\ninsert into salgrade values (7,20001,25000);\ninsert into salgrade values (8,25001,30000);\n\n\n-- 1. 查询员工的姓名、年龄、职位、部门信息 （隐式内连接）\n-- 表: emp , dept\n-- 连接条件: emp.dept_id = dept.id\nselect e.name, e.age, e.job, d.name from emp e, dept d where e.dept_id = d.id;\n\n\n-- 2. 查询年龄小于30岁的员工的姓名、年龄、职位、部门信息（显式内连接）\n-- 表: emp , dept\n-- 连接条件: emp.dept_id = dept.id\nselect e.name, e.age, e.job, d.name from emp e join dept d on e.dept_id = d.id where e.age < 30;\n\n\n-- 3. 查询拥有员工的部门ID、部门名称(显示内连接)\n-- distinct 去重\n-- 表: emp , dept\n-- 连接条件: emp.dept_id = dept.id\nselect distinct d.* from emp e join dept d on d.id = e.dept_id;\n\n\n\n\n-- 4. 查询所有年龄大于40岁的员工, 及其归属的部门名称; 如果员工没有分配部门, 也需要展示出来(左外连接)\n-- 表: emp , dept\n-- 连接条件: emp.dept_id = dept.id\n-- 外连接\nselect e.*, d.name from emp e left join dept d on d.id = e.dept_id where e.age > 40;\n\n\n\n\n-- 5. 查询所有员工的工资等级\n-- 表: emp , salgrade\n-- 连接条件 : emp.salary >= salgrade.losal and emp.salary <= salgrade.hisal\nselect e.name, e.salary, s.grade, s.losal, s.hisal from emp e, salgrade s where e.salary >= s.losal and e.salary <= s.hisal;\nselect e.name, e.salary, s.grade, s.losal, s.hisal from emp e, salgrade s where e.salary between s.losal and s.hisal;\n\n\n\n\n-- 6. 查询 \"研发部\" 所有员工的信息及 工资等级\n-- 表: emp , salgrade , dept\n-- 连接条件 : emp.salary between salgrade.losal and salgrade.hisal , emp.dept_id = dept.id\n-- 查询条件 : dept.name = '研发部'\n\n\nselect e.* , s.grade from (select e.* from emp e where e.dept_id = (select id from dept where name = '研发部')) e, salgrade s where e.salary between s.losal and s.hisal;\nselect e.* , s.grade from emp e, salgrade s where e.dept_id = (select id from dept where name = '研发部') and e.salary between s.losal and s.hisal;\nselect e.*, s.grade\nfrom emp e,\n     salgrade s,\n     dept d\nwhere (e.dept_id = d.id)\n  and (d.name = '研发部')\n  and (e.salary between s.losal and s.hisal);\n\n\n-- 7. 查询 \"研发部\" 员工的平均工资\n-- 表: emp , dept\n-- 连接条件 :  emp.dept_id = dept.id\n\nselect avg(e.salary) from emp e, dept d where e.dept_id = d.id and d.name = '研发部';\n\n\n-- 8. 查询工资比 \"灭绝\" 高的员工信息。\n-- a. 查询 \"灭绝\" 的薪资\nselect salary from emp where name = '灭绝';\n-- b. 查询比她工资高的员工数据\nselect * from emp where salary > (select salary from emp where name = '灭绝');\n\n\n\n-- 9. 查询比平均薪资高的员工信息\n-- a. 查询员工的平均薪资\nselect avg(salary) from emp;\n\n-- b. 查询比平均薪资高的员工信息\nselect * from emp where salary > (select avg(salary) from emp);\n\n\n\n\n\n-- 10. 查询低于本部门平均工资的员工信息\n-- a. 查询指定部门平均薪资\nselect avg(salary), dept_id from emp group by dept_id;\n-- b. 查询低于本部门平均工资的员工信息\nselect * from emp e, (select avg(salary) sa, dept_id from emp group by dept_id) s where e.dept_id = s.dept_id and e.salary < s.sa;\n\n\n-- a. 查询指定部门平均薪资  1\nselect avg(salary) from emp e1 where e1.dept_id = 1;\nselect avg(salary) from emp e1 where e1.dept_id = 2;\n-- ……\n-- b. 查询低于本部门平均工资的员工信息\nselect * from emp e2 where e2.salary < (select avg(salary) from emp e1 where e1.dept_id = e2.dept_id);\n\n\n\n\n-- 11. 查询所有的部门信息, 并统计部门的员工人数\nselect d.* from dept d;\nselect count(*) from emp where dept_id = 1;\nselect count(*) from emp where dept_id = 2;\n\nselect d.* , (select count(*) from emp e where e.dept_id = d.id) '员工人数' from dept d;\n\n\n-- 12. 查询所有学生的选课情况, 展示出学生名称, 学号, 课程名称\n-- 表: student , course , student_course\n-- 连接条件: student.id = student_course.studentid , course.id = student_course.courseid\n\nselect s.name, s.no, c.name from student s, student_course sc, course c where s.id = sc.studentid and c.id = sc.courseid;\n",normalizedContent:"# 多表关系\n\n 1. 一对多\n    \n    实现: 在多的一方建立外键，指向一的一方的主键\n    \n    \n    \n    create table dept(\n        id   int auto_increment comment 'id' primary key,\n        name varchar(50) not null comment '部门名称'\n    )comment '部门表';\n    insert into dept (id, name) values (1, '研发部'), (2, '市场部'),(3, '财务部'), (4, '销售部'), (5, '总经办');\n    \n    \n    create table emp(\n        id  int auto_increment comment 'id' primary key,\n        name varchar(50) not null comment '姓名',\n        age  int comment '年龄',\n        job varchar(20) comment '职位',\n        salary int comment '薪资',\n        entrydate date comment '入职时间',\n        managerid int comment '直属领导id',\n        dept_id int comment '部门id'\n    )comment '员工表';\n    \n    insert into emp (id, name, age, job,salary, entrydate, managerid, dept_id) \n    values(1, '金庸', 66, '总裁',20000, '2000-01-01', null,5),\n    (2, '张无忌', 20, '项目经理',12500, '2005-12-05', 1,1),\n    (3, '杨逍', 33, '开发', 8400,'2000-11-03', 2,1),\n    (4, '韦一笑', 48, '开发',11000, '2002-02-05', 2,1),\n    (5, '常遇春', 43, '开发',10500, '2004-09-07', 3,1),\n    (6, '小昭', 19, '程序员鼓励师',6600, '2004-10-12', 2,1);\n    \n    -- 添加外键\n    alter table emp add constraint fk_emp_dept_id foreign key (dept_id) references dept(id);\n    \n\n 2. 多对多\n    \n    实现: 建立第三张中间表，中间表至少包含两个外键，分别关联两方主键\n    \n    \n    \n    -- 多对多 ----------------\n    create table student(\n        id int auto_increment primary key comment '主键id',\n        name varchar(10) comment '姓名',\n        no varchar(10) comment '学号'\n    ) comment '学生表';\n    insert into student values (null, '黛绮丝', '2000100101'),(null, '谢逊', '2000100102'),(null, '殷天正', '2000100103'),(null, '韦一笑', '2000100104');\n    \n    \n    create table course(\n        id int auto_increment primary key comment '主键id',\n        name varchar(10) comment '课程名称'\n    ) comment '课程表';\n    insert into course values (null, 'java'), (null, 'php'), (null , 'mysql') , (null, 'hadoop');\n    \n    \n    create table student_course(\n        id int auto_increment comment '主键' primary key,\n        studentid int not null comment '学生id',\n        courseid  int not null comment '课程id',\n        constraint fk_courseid foreign key (courseid) references course (id),\n        constraint fk_studentid foreign key (studentid) references student (id)\n    )comment '学生课程中间表';\n    \n    insert into student_course values (null,1,1),(null,1,2),(null,1,3),(null,2,2),(null,2,3),(null,3,4);\n    \n\n 3. 一对一\n    \n    * 一对一关系，多用于单表拆分，将一张表的基础字段放在一张表中，其他详情字段放在另 一张表中，以提升操作效率\n    * 实现: 在任意一方加入外键，关联另外一方的主键，并且设置外键为唯一的(unique)\n    \n    \n    \n    -- --------------------------------- 一对一 ---------------------------\n    create table tb_user(\n        id int auto_increment primary key comment '主键id',\n        name varchar(10) comment '姓名',\n        age int comment '年龄',\n        gender char(1) comment '1: 男 , 2: 女',\n        phone char(11) comment '手机号'\n    ) comment '用户基本信息表';\n    \n    create table tb_user_edu(\n        id int auto_increment primary key comment '主键id',\n        degree varchar(20) comment '学历',\n        major varchar(50) comment '专业',\n        primaryschool varchar(50) comment '小学',\n        middleschool varchar(50) comment '中学',\n        university varchar(50) comment '大学',\n        userid int unique comment '用户id',\n        constraint fk_userid foreign key (userid) references tb_user(id)\n    ) comment '用户教育信息表';\n    \n    \n    insert into tb_user(id, name, age, gender, phone) values\n            (null,'黄渤',45,'1','18800001111'),\n            (null,'冰冰',35,'2','18800002222'),\n            (null,'码云',55,'1','18800008888'),\n            (null,'李彦宏',50,'1','18800009999');\n    \n    insert into tb_user_edu(id, degree, major, primaryschool, middleschool, university, userid) values\n            (null,'本科','舞蹈','静安区第一小学','静安区第一中学','北京舞蹈学院',1),\n            (null,'硕士','表演','朝阳区第一小学','朝阳区第一中学','北京电影学院',2),\n            (null,'本科','英语','杭州市第一小学','杭州市第一中学','杭州师范大学',3),\n            (null,'本科','应用数学','阳泉第一小学','阳泉区第一中学','清华大学',4);\n    \n    \n    \n\n\n# 多表查询概述\n\n 1. 准备数据\n    \n    create table dept(\n        id   int auto_increment comment 'id' primary key,\n        name varchar(50) not null comment '部门名称'\n    )comment '部门表';\n    \n    create table emp(\n        id  int auto_increment comment 'id' primary key,\n        name varchar(50) not null comment '姓名',\n        age  int comment '年龄',\n        job varchar(20) comment '职位',\n        salary int comment '薪资',\n        entrydate date comment '入职时间',\n        managerid int comment '直属领导id',\n        dept_id int comment '部门id'\n    )comment '员工表';\n    \n    -- 添加外键\n    alter table emp add constraint fk_emp_dept_id foreign key (dept_id) references dept(id);\n    \n    insert into dept (id, name) values (1, '研发部'), (2, '市场部'),(3, '财务部'), (4, '销售部'), (5, '总经办'), (6, '人事部');\n    insert into emp (id, name, age, job,salary, entrydate, managerid, dept_id) values\n                (1, '金庸', 66, '总裁',20000, '2000-01-01', null,5),\n    \n                (2, '张无忌', 20, '项目经理',12500, '2005-12-05', 1,1),\n                (3, '杨逍', 33, '开发', 8400,'2000-11-03', 2,1),\n                (4, '韦一笑', 48, '开发',11000, '2002-02-05', 2,1),\n                (5, '常遇春', 43, '开发',10500, '2004-09-07', 3,1),\n                (6, '小昭', 19, '程序员鼓励师',6600, '2004-10-12', 2,1),\n    \n                (7, '灭绝', 60, '财务总监',8500, '2002-09-12', 1,3),\n                (8, '周芷若', 19, '会计',48000, '2006-06-02', 7,3),\n                (9, '丁敏君', 23, '出纳',5250, '2009-05-13', 7,3),\n    \n                (10, '赵敏', 20, '市场部总监',12500, '2004-10-12', 1,2),\n                (11, '鹿杖客', 56, '职员',3750, '2006-10-03', 10,2),\n                (12, '鹤笔翁', 19, '职员',3750, '2007-05-09', 10,2),\n                (13, '方东白', 19, '职员',5500, '2009-02-12', 10,2),\n    \n                (14, '张三丰', 88, '销售总监',14000, '2004-10-12', 1,4),\n                (15, '俞莲舟', 38, '销售',4600, '2004-10-12', 14,4),\n                (16, '宋远桥', 40, '销售',4600, '2004-10-12', 14,4),\n                (17, '陈友谅', 42, null,2000, '2011-10-12', 1,null);\n    \n\n 2. 查询所有，笛卡尔积\n    \n    -- 多表查询 -- 笛卡尔积\n    select * from emp , dept where emp.dept_id = dept.id;\n    \n\n 3. 分类\n    \n    \n    \n    * 连接查询\n      \n      * 内连接\n        \n        > 相当于查询a、b交集部分数据\n      \n      * 外连接\n        \n        * 左外连接\n          \n          > 查询左表所有数据，以及两张表交集部分数据\n        \n        * 右外连接\n          \n          > 查询右表所有数据，以及两张表交集部分数据\n      \n      * 自连接\n        \n        > 当前表与自身的连接查询，自连接必须使用表别名\n    \n    * 子查询\n      \n      > 将上一次查询的结果作为新的表，进行嵌套查询。\n\n\n# 内连接\n\n查询每一个员工的姓名 , 及关联的部门的名称\n\n * 隐式内连接\n   \n   select 字段列表 from 表1 , 表2 where 条件 ... ;\n   \n   select emp.name , dept.name from emp , dept where emp.dept_id = dept.id;\n   \n   -- 起别名\n   select e.name,d.name from emp e , dept d where e.dept_id = d.id;\n   \n\n * 显示内连接\n   \n   select 字段列表 from 表1 [ inner ] join 表2 on 连接条件 ... ;\n   \n   select e.name, d.name from emp e inner join dept d  on e.dept_id = d.id;\n   \n   -- 省略 inner\n   select e.name, d.name from emp e join dept d  on e.dept_id = d.id;\n   \n\n\n# 外连接\n\n * 左外连接\n   \n   查询emp表的所有数据, 和对应的部门信息\n   \n   select 字段列表 from 表1 left [ outer ] join 表2 on 条件 ... ;\n   \n   select e.*, d.name from emp e left outer join dept d on e.dept_id = d.id;\n   -- 省略 outer\n   select e.*, d.name from emp e left join dept d on e.dept_id = d.id;\n   \n\n * 右外连接\n   \n   查询dept表的所有数据, 和对应的员工信息(右外连接)\n   \n   select 字段列表 from 表1 right [ outer ] join 表2 on 条件 ... ;\n   \n   select d.*, e.* from emp e right outer join dept d on e.dept_id = d.id;\n   -- 省略 outer\n   select d.*, e.* from emp e right join dept d on e.dept_id = d.id;\n   \n   -- 用左外连接代替右外连接实现\n   select d.*, e.* from dept d left outer join emp e on e.dept_id = d.id;\n   \n\n\n# 自连接\n\n自连接查询，顾名思义，就是自己连接自己，也就是把一张表连接查询多次。\n\n在自连接查询中，必须要为表起别名，要不然我们不清楚所指定的条件、返回的字段，到底 是哪一张表的字段。\n\n 1. 查询员工 及其 所属领导的名字\n\nselect 字段列表 from 表a 别名a join 表a 别名b on 条件 ... ;\n\n-- 自内连接\nselect a.name , b.name from emp a , emp b where a.managerid = b.id;\n\n\n 2. 查询所有员工 emp 及其领导的名字 emp , 如果员工没有领导, 也需要查询出来\n    \n    -- 自外连接\n    select a.name '员工', b.name '领导' from emp a left join emp b on a.managerid =\n    b.id;\n    \n\n\n# 联合查询\n\n对于union查询，就是把多次查询的结果合并起来，形成一个新的查询结果集。\n\n如果多条查询语句查询出来的结果，字段数量不一致， 在进行union/union all联合查询时，会报错。\n\ne.g. 将薪资低于 5000 的员工 , 和 年龄大于 50 岁的员工全部查询出来.\n\nselect * from emp where salary < 5000\nunion all\nselect * from emp where age > 50;\n\n-- union 相比于 union all 可以 去重\nselect * from emp where salary < 5000\nunion \nselect * from emp where age > 50;\n\n\n-- 用or代替联合查询\nselect * from emp where salary < 5000 or age > 50;\n\n\n\n# 子查询\n\nsql语句中嵌套select语句，称为嵌套查询，又称子查询。\n\nselect * from t1 where column1 = ( select column1 from t2 );\n\n\n * 根据子查询结果不同，分为：\n   \n   > a. 标量子查询（子查询结果为单个值）\n   > \n   > b. 列子查询 (子查询结果为一列)\n   > \n   > c. 行子查询 (子查询结果为一行)\n   > \n   > d. 表子查询 (子查询结果为多行多列)\n\n * 根据子查询位置，分为：\n   \n   > a. where之后\n   > \n   > b. from之后\n   > \n   > c. select之后\n\n\n# 标量子查询\n\n子查询返回的结果是单个值（数字、字符串、日期等），最简单的形式，这种子查询称为标量子查询。 常用的操作符：= <> > >= < <=\n\n 1. 查询 \"销售部\" 的所有员工信息\n    \n    -- a. 查询 \"销售部\" 部门id--(4)\n    select id from dept where name = '销售部';\n    -- b. 根据销售部部门id, 查询员工信息\n    select * from emp where dept_id = 4;\n    \n    -- 合并\n    select * from emp where dept_id = (select id from dept where name = '销售部');\n    \n\n 2. 查询在 \"方东白\" 入职之后的员工信息\n    \n    -- a. 查询 方东白 的入职日期--(2009-02-12)\n    select entrydate from emp where name = '方东白';\n    -- b. 查询指定入职日期之后入职的员工信息\n    select * from emp where entrydate > '2009-02-12';\n    -- 合并\n    select * from emp where entrydate > (select entrydate from emp where name = '方东白');\n    \n\n\n# 列子查询\n\n子查询返回的结果是一列（可以是多行），这种子查询称为列子查询。 常用的操作符：in 、not in 、 any 、some 、 all\n\n 1. 查询 \"销售部\" 和 \"市场部\" 的所有员工信息\n    \n    -- a. 查询 \"销售部\" 和 \"市场部\" 的部门id\n    select id from dept where name in ('销售部', '市场部');\n    -- b. 根据部门id, 查询员工信息\n    select * from emp where dept_id in (select id from dept where name in ('销售部', '市场部'));\n    \n\n 2. 查询比 财务部 所有人(all)工资都高的员工信息\n    \n    -- a. 查询所有 财务部 人员工资\n    select id from dept where name = '财务部';\n    select salary from emp where dept_id = (select id from dept where name = '财务部');\n    -- b. 比 财务部 所有人工资都高的员工信息\n    select * from emp where salary > all(select salary from emp where dept_id = (select id from dept where name = '财务部'));\n    \n\n 3. 查询比研发部其中任意一人(any,some)工资高的员工信息\n    \n    -- a. 查询研发部所有人工资\n    select salary from emp where dept_id = (select id from dept where name = '研发部');\n    -- b. 比研发部其中任意一人工资高的员工信息\n    select * from emp where salary > any(select salary from emp where dept_id = (select id from dept where name = '研发部'));\n    \n\n\n# 行子查询\n\n子查询返回的结果是一行（可以是多列），这种子查询称为行子查询。 常用的操作符：= 、<> 、in 、not in\n\n 1. 查询与 \"张无忌\" 的薪资及直属领导相同的员工信息\n    \n    -- a. 查询 \"张无忌\" 的薪资及直属领导\n    select salary, managerid from emp where name = '张无忌';\n    \n    -- b. 查询与 \"张无忌\" 的薪资及直属领导相同的员工信息;\n    select * from emp where (salary, managerid) = (12500, 1);\n    select * from emp where (salary, managerid) = (select salary, managerid from emp where name = '张无忌');\n    \n\n\n# 表子查询\n\n子查询返回的结果是多行多列，这种子查询称为表子查询。 常用的操作符：in\n\n 1. 查询与 \"鹿杖客\" , \"宋远桥\" 的职位和薪资相同的员工信息\n    \n    -- a. 查询 \"鹿杖客\" , \"宋远桥\" 的职位和薪资\n    select job, salary from emp where name in ('鹿杖客', '宋远桥');\n    \n    -- b. 查询与 \"鹿杖客\" , \"宋远桥\" 的职位和薪资相同的员工信息\n    select * from emp where job in ('职员', '销售') and salary in (3750, 4600);\n    select * from emp where (job, salary) in (select job, salary from emp where name in ('鹿杖客', '宋远桥'));\n    \n\n 2. 查询入职日期是 \"2006-01-01\" 之后的员工信息 , 及其部门信息\n    \n    -- a. 入职日期是 \"2006-01-01\" 之后的员工信息\n    select * from emp where entrydate > '2006-01-01';\n    \n    -- b. 查询这部分员工, 对应的部门信息;\n    -- 把查询出的信息当成一个新表来对待\n    select e.*, d.name from (select * from emp where entrydate > '2006-01-01') e left join dept d on e.dept_id = d.id;\n    \n\n\n# 多表查询案例\n\n-- 数据准备\ncreate table salgrade(\n    grade int,\n    losal int,\n    hisal int\n) comment '薪资等级表';\n\ninsert into salgrade values (1,0,3000);\ninsert into salgrade values (2,3001,5000);\ninsert into salgrade values (3,5001,8000);\ninsert into salgrade values (4,8001,10000);\ninsert into salgrade values (5,10001,15000);\ninsert into salgrade values (6,15001,20000);\ninsert into salgrade values (7,20001,25000);\ninsert into salgrade values (8,25001,30000);\n\n\n-- 1. 查询员工的姓名、年龄、职位、部门信息 （隐式内连接）\n-- 表: emp , dept\n-- 连接条件: emp.dept_id = dept.id\nselect e.name, e.age, e.job, d.name from emp e, dept d where e.dept_id = d.id;\n\n\n-- 2. 查询年龄小于30岁的员工的姓名、年龄、职位、部门信息（显式内连接）\n-- 表: emp , dept\n-- 连接条件: emp.dept_id = dept.id\nselect e.name, e.age, e.job, d.name from emp e join dept d on e.dept_id = d.id where e.age < 30;\n\n\n-- 3. 查询拥有员工的部门id、部门名称(显示内连接)\n-- distinct 去重\n-- 表: emp , dept\n-- 连接条件: emp.dept_id = dept.id\nselect distinct d.* from emp e join dept d on d.id = e.dept_id;\n\n\n\n\n-- 4. 查询所有年龄大于40岁的员工, 及其归属的部门名称; 如果员工没有分配部门, 也需要展示出来(左外连接)\n-- 表: emp , dept\n-- 连接条件: emp.dept_id = dept.id\n-- 外连接\nselect e.*, d.name from emp e left join dept d on d.id = e.dept_id where e.age > 40;\n\n\n\n\n-- 5. 查询所有员工的工资等级\n-- 表: emp , salgrade\n-- 连接条件 : emp.salary >= salgrade.losal and emp.salary <= salgrade.hisal\nselect e.name, e.salary, s.grade, s.losal, s.hisal from emp e, salgrade s where e.salary >= s.losal and e.salary <= s.hisal;\nselect e.name, e.salary, s.grade, s.losal, s.hisal from emp e, salgrade s where e.salary between s.losal and s.hisal;\n\n\n\n\n-- 6. 查询 \"研发部\" 所有员工的信息及 工资等级\n-- 表: emp , salgrade , dept\n-- 连接条件 : emp.salary between salgrade.losal and salgrade.hisal , emp.dept_id = dept.id\n-- 查询条件 : dept.name = '研发部'\n\n\nselect e.* , s.grade from (select e.* from emp e where e.dept_id = (select id from dept where name = '研发部')) e, salgrade s where e.salary between s.losal and s.hisal;\nselect e.* , s.grade from emp e, salgrade s where e.dept_id = (select id from dept where name = '研发部') and e.salary between s.losal and s.hisal;\nselect e.*, s.grade\nfrom emp e,\n     salgrade s,\n     dept d\nwhere (e.dept_id = d.id)\n  and (d.name = '研发部')\n  and (e.salary between s.losal and s.hisal);\n\n\n-- 7. 查询 \"研发部\" 员工的平均工资\n-- 表: emp , dept\n-- 连接条件 :  emp.dept_id = dept.id\n\nselect avg(e.salary) from emp e, dept d where e.dept_id = d.id and d.name = '研发部';\n\n\n-- 8. 查询工资比 \"灭绝\" 高的员工信息。\n-- a. 查询 \"灭绝\" 的薪资\nselect salary from emp where name = '灭绝';\n-- b. 查询比她工资高的员工数据\nselect * from emp where salary > (select salary from emp where name = '灭绝');\n\n\n\n-- 9. 查询比平均薪资高的员工信息\n-- a. 查询员工的平均薪资\nselect avg(salary) from emp;\n\n-- b. 查询比平均薪资高的员工信息\nselect * from emp where salary > (select avg(salary) from emp);\n\n\n\n\n\n-- 10. 查询低于本部门平均工资的员工信息\n-- a. 查询指定部门平均薪资\nselect avg(salary), dept_id from emp group by dept_id;\n-- b. 查询低于本部门平均工资的员工信息\nselect * from emp e, (select avg(salary) sa, dept_id from emp group by dept_id) s where e.dept_id = s.dept_id and e.salary < s.sa;\n\n\n-- a. 查询指定部门平均薪资  1\nselect avg(salary) from emp e1 where e1.dept_id = 1;\nselect avg(salary) from emp e1 where e1.dept_id = 2;\n-- ……\n-- b. 查询低于本部门平均工资的员工信息\nselect * from emp e2 where e2.salary < (select avg(salary) from emp e1 where e1.dept_id = e2.dept_id);\n\n\n\n\n-- 11. 查询所有的部门信息, 并统计部门的员工人数\nselect d.* from dept d;\nselect count(*) from emp where dept_id = 1;\nselect count(*) from emp where dept_id = 2;\n\nselect d.* , (select count(*) from emp e where e.dept_id = d.id) '员工人数' from dept d;\n\n\n-- 12. 查询所有学生的选课情况, 展示出学生名称, 学号, 课程名称\n-- 表: student , course , student_course\n-- 连接条件: student.id = student_course.studentid , course.id = student_course.courseid\n\nselect s.name, s.no, c.name from student s, student_course sc, course c where s.id = sc.studentid and c.id = sc.courseid;\n",charsets:{cjk:!0}},{title:"Redis优化",frontmatter:{autoSort:97,title:"Redis优化",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/444956/",categories:["后端","数据库","Redis"],tags:["知识","数据库","Redis"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/05.Redis/20.Redis%E4%BC%98%E5%8C%96.html",relativePath:"01.后端/30.数据库/05.Redis/20.Redis优化.md",key:"v-1fa3135c",path:"/pages/444956/",headers:[{level:2,title:"Redis键值设计",slug:"redis键值设计",normalizedTitle:"redis键值设计",charIndex:2},{level:3,title:"优雅的key结构",slug:"优雅的key结构",normalizedTitle:"优雅的key结构",charIndex:16},{level:3,title:"拒绝BigKey",slug:"拒绝bigkey",normalizedTitle:"拒绝bigkey",charIndex:381},{level:3,title:"恰当的数据类型",slug:"恰当的数据类型",normalizedTitle:"恰当的数据类型",charIndex:4134},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:7086},{level:2,title:"批处理优化",slug:"批处理优化",normalizedTitle:"批处理优化",charIndex:7258},{level:3,title:"Pipeline",slug:"pipeline",normalizedTitle:"pipeline",charIndex:5521},{level:3,title:"集群下的批处理",slug:"集群下的批处理",normalizedTitle:"集群下的批处理",charIndex:8375},{level:2,title:"服务器端优化-持久化配置",slug:"服务器端优化-持久化配置",normalizedTitle:"服务器端优化-持久化配置",charIndex:12267},{level:2,title:"服务器端优化-慢查询优化",slug:"服务器端优化-慢查询优化",normalizedTitle:"服务器端优化-慢查询优化",charIndex:12675},{level:3,title:"什么是慢查询",slug:"什么是慢查询",normalizedTitle:"什么是慢查询",charIndex:12692},{level:3,title:"如何查看慢查询",slug:"如何查看慢查询",normalizedTitle:"如何查看慢查询",charIndex:13039},{level:2,title:"服务器端优化-命令及安全配置",slug:"服务器端优化-命令及安全配置",normalizedTitle:"服务器端优化-命令及安全配置",charIndex:13163},{level:2,title:"服务器端优化-Redis内存划分和内存配置",slug:"服务器端优化-redis内存划分和内存配置",normalizedTitle:"服务器端优化-redis内存划分和内存配置",charIndex:13922},{level:2,title:"服务器端集群优化-集群还是主从",slug:"服务器端集群优化-集群还是主从",normalizedTitle:"服务器端集群优化-集群还是主从",charIndex:15232}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"Redis键值设计 优雅的key结构 拒绝BigKey 恰当的数据类型 总结 批处理优化 Pipeline 集群下的批处理 服务器端优化-持久化配置 服务器端优化-慢查询优化 什么是慢查询 如何查看慢查询 服务器端优化-命令及安全配置 服务器端优化-Redis内存划分和内存配置 服务器端集群优化-集群还是主从",content:'# Redis键值设计\n\n\n# 优雅的key结构\n\nRedis的Key虽然可以自定义，但最好遵循下面的几个最佳实践约定：\n\n * 遵循基本格式：[业务名称]:[数据名]:[id]\n * 长度不超过44字节\n * 不包含特殊字符\n\n例如：我们的登录业务，保存用户信息，其key可以设计成如下格式：\n\n\n\n这样设计的好处：\n\n * 可读性强\n * 避免key冲突\n * 方便管理\n * 更节省内存： key是string类型，底层编码包含int、embstr和raw三种。embstr在小于44字节使用，采用连续内存空间，内存占用更小。当字节数大于44字节时，会转为raw模式存储，在raw模式下，内存空间不是连续的，而是采用一个指针指向了另外一段内存空间，在这段空间里存储SDS内容，这样空间不连续，访问的时候性能也就会收到影响，还有可能产生内存碎片\n\n\n\n\n# 拒绝BigKey\n\nBigKey通常以Key的大小和Key中成员的数量来综合判定，例如：\n\n * Key本身的数据量过大：一个String类型的Key，它的值为5 MB\n * Key中的成员数过多：一个ZSET类型的Key，它的成员数量为10,000个\n * Key中成员的数据量过大：一个Hash类型的Key，它的成员数量虽然只有1,000个但这些成员的Value（值）总大小为100 MB\n\n那么如何判断元素的大小呢？redis也给我们提供了命令\n\n\n\n推荐值：\n\n * 单个key的value小于10KB\n * 对于集合类型的key，建议元素数量小于1000\n\n# BigKey的危害\n\n * 网络阻塞\n   * 对BigKey执行读请求时，少量的QPS就可能导致带宽使用率被占满，导致Redis实例，乃至所在物理机变慢\n * 数据倾斜\n   * BigKey所在的Redis实例内存使用率远超其他实例，无法使数据分片的内存资源达到均衡\n * Redis阻塞\n   * 对元素较多的hash、list、zset等做运算会耗时较旧，使主线程被阻塞\n * CPU压力\n   * 对BigKey的数据序列化和反序列化会导致CPU的使用率飙升，影响Redis实例和本机其它应用\n\n# 如何发现BigKey\n\n# ①redis-cli --bigkeys\n\n利用redis-cli提供的--bigkeys参数，可以遍历分析所有key，并返回Key的整体统计信息与每个数据的Top1的big key\n\n命令：redis-cli -a 密码 --bigkeys\n\n\n\n# ②scan扫描\n\n自己编程，利用scan扫描Redis中的所有key，利用strlen、hlen等命令判断key的长度（此处不建议使用MEMORY USAGE）\n\n\n\nscan 命令调用完后每次会返回2个元素，第一个是下一次迭代的光标，第一次光标会设置为0，当最后一次scan 返回的光标等于0时，表示整个scan遍历结束了，第二个返回的是List，一个匹配的key的数组\n\nimport com.heima.jedis.util.JedisConnectionFactory;\nimport org.junit.jupiter.api.AfterEach;\nimport org.junit.jupiter.api.BeforeEach;\nimport org.junit.jupiter.api.Test;\nimport redis.clients.jedis.Jedis;\nimport redis.clients.jedis.ScanResult;\n\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\n\npublic class JedisTest {\n    private Jedis jedis;\n\n    @BeforeEach\n    void setUp() {\n        // 1.建立连接\n        // jedis = new Jedis("192.168.150.101", 6379);\n        jedis = JedisConnectionFactory.getJedis();\n        // 2.设置密码\n        jedis.auth("123321");\n        // 3.选择库\n        jedis.select(0);\n    }\n\n    final static int STR_MAX_LEN = 10 * 1024;\n    final static int HASH_MAX_LEN = 500;\n\n    @Test\n    void testScan() {\n        int maxLen = 0;\n        long len = 0;\n\n        String cursor = "0";\n        do {\n            // 扫描并获取一部分key\n            ScanResult<String> result = jedis.scan(cursor);\n            // 记录cursor\n            cursor = result.getCursor();\n            List<String> list = result.getResult();\n            if (list == null || list.isEmpty()) {\n                break;\n            }\n            // 遍历\n            for (String key : list) {\n                // 判断key的类型\n                String type = jedis.type(key);\n                switch (type) {\n                    case "string":\n                        len = jedis.strlen(key);\n                        maxLen = STR_MAX_LEN;\n                        break;\n                    case "hash":\n                        len = jedis.hlen(key);\n                        maxLen = HASH_MAX_LEN;\n                        break;\n                    case "list":\n                        len = jedis.llen(key);\n                        maxLen = HASH_MAX_LEN;\n                        break;\n                    case "set":\n                        len = jedis.scard(key);\n                        maxLen = HASH_MAX_LEN;\n                        break;\n                    case "zset":\n                        len = jedis.zcard(key);\n                        maxLen = HASH_MAX_LEN;\n                        break;\n                    default:\n                        break;\n                }\n                if (len >= maxLen) {\n                    System.out.printf("Found big key : %s, type: %s, length or size: %d %n", key, type, len);\n                }\n            }\n        } while (!cursor.equals("0"));\n    }\n    \n    @AfterEach\n    void tearDown() {\n        if (jedis != null) {\n            jedis.close();\n        }\n    }\n\n}\n\n\n# ③第三方工具\n\n * 利用第三方工具，如 Redis-Rdb-Tools 分析RDB快照文件，全面分析内存使用情况\n * https://github.com/sripathikrishnan/redis-rdb-tools\n\n# ④网络监控\n\n * 自定义工具，监控进出Redis的网络数据，超出预警值时主动告警\n * 一般阿里云搭建的云服务器就有相关监控页面\n\n\n\n# 1.2.3、如何删除BigKey\n\nBigKey内存占用较多，即便时删除这样的key也需要耗费很长时间，导致Redis主线程阻塞，引发一系列问题。\n\n * redis 3.0 及以下版本\n   * 如果是集合类型，则遍历BigKey的元素，先逐个删除子元素，最后删除BigKey\n\n\n\n * Redis 4.0以后\n   * Redis在4.0后提供了异步删除的命令：unlink\n\n\n# 恰当的数据类型\n\n# 例1：比如存储一个User对象，我们有三种存储方式：\n\n# ①方式一：json字符串\n\nUSER:1   {"NAME": "JACK", "AGE": 21}\n\n优点：实现简单粗暴\n\n缺点：数据耦合，不够灵活\n\n# ②方式二：字段打散\n\nUSER:1:NAME   JACK\nuser:1:age    21\n\n优点：可以灵活访问对象任意字段\n\n缺点：占用空间大、没办法做统一控制\n\n# ③方式三：hash（推荐）\n\nuser:1   name   jack\n         age    21\n\n优点：底层使用ziplist，空间占用小，可以灵活访问对象的任意字段\n\n缺点：代码相对复杂\n\n# 例2：假如有hash类型的key，其中有100万对field和value，field是自增id，这个key存在什么问题？如何优化？\n\nkey       field       value\nsomeKey   id:0        value0\n          .....       .....\n          id:999999   value999999\n\n存在的问题：\n\n * hash的entry数量超过500时，会使用哈希表而不是ZipList，内存占用较多\n   * \n * 可以通过hash-max-ziplist-entries配置entry上限。但是如果entry过多就会导致BigKey问题\n\n# 方案一\n\n拆分为string类型\n\nkey         value\nid:0        value0\n.....       .....\nid:999999   value999999\n\n存在的问题：\n\n * string结构底层没有太多内存优化，内存占用较多\n\n\n\n * 想要批量获取这些数据比较麻烦\n\n# 方案二\n\n拆分为小的hash，将 id / 100 作为key， 将id % 100 作为field，这样每100个元素为一个Hash\n\nkey        field   value\nkey:0      id:00   value0\n           .....   .....\n           id:99   value99\nkey:1      id:00   value100\n           .....   .....\n           id:99   value199\n....\nkey:9999   id:00   value999900\n           .....   .....\n           id:99   value999999\n\n\n\npackage com.heima.test;\n\nimport com.heima.jedis.util.JedisConnectionFactory;\nimport org.junit.jupiter.api.AfterEach;\nimport org.junit.jupiter.api.BeforeEach;\nimport org.junit.jupiter.api.Test;\nimport redis.clients.jedis.Jedis;\nimport redis.clients.jedis.Pipeline;\nimport redis.clients.jedis.ScanResult;\n\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\n\npublic class JedisTest {\n    private Jedis jedis;\n\n    @BeforeEach\n    void setUp() {\n        // 1.建立连接\n        // jedis = new Jedis("192.168.150.101", 6379);\n        jedis = JedisConnectionFactory.getJedis();\n        // 2.设置密码\n        jedis.auth("123321");\n        // 3.选择库\n        jedis.select(0);\n    }\n\n    @Test\n    void testSetBigKey() {\n        Map<String, String> map = new HashMap<>();\n        for (int i = 1; i <= 650; i++) {\n            map.put("hello_" + i, "world!");\n        }\n        jedis.hmset("m2", map);\n    }\n\n    @Test\n    void testBigHash() {\n        Map<String, String> map = new HashMap<>();\n        for (int i = 1; i <= 100000; i++) {\n            map.put("key_" + i, "value_" + i);\n        }\n        jedis.hmset("test:big:hash", map);\n    }\n\n    @Test\n    void testBigString() {\n        for (int i = 1; i <= 100000; i++) {\n            jedis.set("test:str:key_" + i, "value_" + i);\n        }\n    }\n\n    @Test\n    void testSmallHash() {\n        int hashSize = 100;\n        Map<String, String> map = new HashMap<>(hashSize);\n        for (int i = 1; i <= 100000; i++) {\n            int k = (i - 1) / hashSize;\n            int v = i % hashSize;\n            map.put("key_" + v, "value_" + v);\n            if (v == 0) {\n                jedis.hmset("test:small:hash_" + k, map);\n            }\n        }\n    }\n\n    @AfterEach\n    void tearDown() {\n        if (jedis != null) {\n            jedis.close();\n        }\n    }\n}\n\n\n\n# 总结\n\n * Key的最佳实践\n   * 固定格式：[业务名]:[数据名]:[id]\n   * 足够简短：不超过44字节\n   * 不包含特殊字符\n * Value的最佳实践：\n   * 合理的拆分数据，拒绝BigKey\n   * 选择合适数据结构\n   * Hash结构的entry数量不要超过1000\n   * 设置合理的超时时间\n\n\n# 批处理优化\n\n\n# Pipeline\n\n# 我们的客户端与redis服务器是这样交互的\n\n单个命令的执行流程\n\n\n\nN条命令的执行流程\n\n\n\nredis处理指令是很快的，主要花费的时候在于网络传输。于是乎很容易想到将多条指令批量的传输给redis\n\n\n\n# MSet\n\nRedis提供了很多Mxxx这样的命令，可以实现批量插入数据，例如：\n\n * mset\n * hmset\n\n利用mset批量插入10万条数据\n\n@Test\nvoid testMxx() {\n    String[] arr = new String[2000];\n    int j;\n    long b = System.currentTimeMillis();\n    for (int i = 1; i <= 100000; i++) {\n        j = (i % 1000) << 1;\n        arr[j] = "test:key_" + i;\n        arr[j + 1] = "value_" + i;\n        if (j == 0) {\n            jedis.mset(arr);\n        }\n    }\n    long e = System.currentTimeMillis();\n    System.out.println("time: " + (e - b));\n}\n\n\n# Pipeline\n\nMSET虽然可以批处理，但是却只能操作部分数据类型，因此如果有对复杂数据类型的批处理需要，建议使用Pipeline\n\n@Test\nvoid testPipeline() {\n    // 创建管道\n    Pipeline pipeline = jedis.pipelined();\n    long b = System.currentTimeMillis();\n    for (int i = 1; i <= 100000; i++) {\n        // 放入命令到管道\n        pipeline.set("test:key_" + i, "value_" + i);\n        if (i % 1000 == 0) {\n            // 每放入1000条命令，批量执行\n            pipeline.sync();\n        }\n    }\n    long e = System.currentTimeMillis();\n    System.out.println("time: " + (e - b));\n}\n\n\n\n# 集群下的批处理\n\n如MSET或Pipeline这样的批处理需要在一次请求中携带多条命令，而此时如果Redis是一个集群，那批处理命令的多个key必须落在一个插槽中，否则就会导致执行失败。大家可以想一想这样的要求其实很难实现，因为我们在批处理时，可能一次要插入很多条数据，这些数据很有可能不会都落在相同的节点上，这就会导致报错了\n\n这个时候，我们可以找到4种解决方案\n\n\n\n第一种方案：串行执行，所以这种方式没有什么意义，当然，执行起来就很简单了，缺点就是耗时过久。\n\n第二种方案：串行slot，简单来说，就是执行前，客户端先计算一下对应的key的slot，一样slot的key就放到一个组里边，不同的，就放到不同的组里边，然后对每个组执行pipeline的批处理，他就能串行执行各个组的命令，这种做法比第一种方法耗时要少，但是缺点呢，相对来说复杂一点，所以这种方案还需要优化一下\n\n第三种方案：并行slot，相较于第二种方案，在分组完成后串行执行，第三种方案，就变成了并行执行各个命令，所以他的耗时就非常短，但是实现呢，也更加复杂。\n\n第四种：hash_tag，redis计算key的slot的时候，其实是根据key的有效部分来计算的，通过这种方式就能一次处理所有的key，这种方式耗时最短，实现也简单，但是如果通过操作key的有效部分，那么就会导致所有的key都落在一个节点上，产生数据倾斜的问题，所以我们推荐使用第三种方式。\n\n# 串行化执行代码实践\n\npublic class JedisClusterTest {\n\n    private JedisCluster jedisCluster;\n\n    @BeforeEach\n    void setUp() {\n        // 配置连接池\n        JedisPoolConfig poolConfig = new JedisPoolConfig();\n        poolConfig.setMaxTotal(8);\n        poolConfig.setMaxIdle(8);\n        poolConfig.setMinIdle(0);\n        poolConfig.setMaxWaitMillis(1000);\n        HashSet<HostAndPort> nodes = new HashSet<>();\n        nodes.add(new HostAndPort("192.168.150.101", 7001));\n        nodes.add(new HostAndPort("192.168.150.101", 7002));\n        nodes.add(new HostAndPort("192.168.150.101", 7003));\n        nodes.add(new HostAndPort("192.168.150.101", 8001));\n        nodes.add(new HostAndPort("192.168.150.101", 8002));\n        nodes.add(new HostAndPort("192.168.150.101", 8003));\n        jedisCluster = new JedisCluster(nodes, poolConfig);\n    }\n\n    @Test\n    void testMSet() {\n        jedisCluster.mset("name", "Jack", "age", "21", "sex", "male");\n\n    }\n\n    @Test\n    void testMSet2() {\n        Map<String, String> map = new HashMap<>(3);\n        map.put("name", "Jack");\n        map.put("age", "21");\n        map.put("sex", "Male");\n        //对Map数据进行分组。根据相同的slot放在一个分组\n        //key就是slot，value就是一个组\n        Map<Integer, List<Map.Entry<String, String>>> result = map.entrySet()\n                .stream()\n                .collect(Collectors.groupingBy(\n                        entry -> ClusterSlotHashUtil.calculateSlot(entry.getKey()))\n                );\n        //串行的去执行mset的逻辑\n        for (List<Map.Entry<String, String>> list : result.values()) {\n            String[] arr = new String[list.size() * 2];\n            int j = 0;\n            for (int i = 0; i < list.size(); i++) {\n                j = i<<2;\n                Map.Entry<String, String> e = list.get(0);\n                arr[j] = e.getKey();\n                arr[j + 1] = e.getValue();\n            }\n            jedisCluster.mset(arr);\n        }\n    }\n\n    @AfterEach\n    void tearDown() {\n        if (jedisCluster != null) {\n            jedisCluster.close();\n        }\n    }\n}\n\n\n# Spring集群环境下批处理代码\n\n   @Test\n    void testMSetInCluster() {\n        Map<String, String> map = new HashMap<>(3);\n        map.put("name", "Rose");\n        map.put("age", "21");\n        map.put("sex", "Female");\n        stringRedisTemplate.opsForValue().multiSet(map);\n\n\n        List<String> strings = stringRedisTemplate.opsForValue().multiGet(Arrays.asList("name", "age", "sex"));\n        strings.forEach(System.out::println);\n\n    }\n\n\n原理分析\n\n在RedisAdvancedClusterAsyncCommandsImpl 类中\n\n首先根据slotHash算出来一个partitioned的map，map中的key就是slot，而他的value就是对应的对应相同slot的key对应的数据\n\n通过 RedisFuture<String> mset = super.mset(op);进行异步的消息发送\n\n@Override\npublic RedisFuture<String> mset(Map<K, V> map) {\n\n    Map<Integer, List<K>> partitioned = SlotHash.partition(codec, map.keySet());\n\n    if (partitioned.size() < 2) {\n        return super.mset(map);\n    }\n\n    Map<Integer, RedisFuture<String>> executions = new HashMap<>();\n\n    for (Map.Entry<Integer, List<K>> entry : partitioned.entrySet()) {\n\n        Map<K, V> op = new HashMap<>();\n        entry.getValue().forEach(k -> op.put(k, map.get(k)));\n\n        RedisFuture<String> mset = super.mset(op);\n        executions.put(entry.getKey(), mset);\n    }\n\n    return MultiNodeExecution.firstOfAsync(executions);\n}\n\n\n\n# 服务器端优化-持久化配置\n\nRedis的持久化虽然可以保证数据安全，但也会带来很多额外的开销，因此持久化请遵循下列建议：\n\n * 用来做缓存的Redis实例尽量不要开启持久化功能\n * 建议关闭RDB持久化功能，使用AOF持久化\n * 利用脚本定期在slave节点做RDB，实现数据备份\n * 设置合理的rewrite阈值，避免频繁的bgrewrite\n * 配置no-appendfsync-on-rewrite = yes，禁止在rewrite期间做aof，避免因AOF引起的阻塞\n * 部署有关建议：\n   * Redis实例的物理机要预留足够内存，应对fork和rewrite\n   * 单个Redis实例内存上限不要太大，例如4G或8G。可以加快fork的速度、减少主从同步、数据迁移压力\n   * 不要与CPU密集型应用部署在一起\n   * 不要与高硬盘负载应用一起部署。例如：数据库、消息队列\n\n\n# 服务器端优化-慢查询优化\n\n\n# 什么是慢查询\n\n并不是很慢的查询才是慢查询，而是：在Redis执行时耗时超过某个阈值的命令，称为慢查询。\n\n慢查询的危害：由于Redis是单线程的，所以当客户端发出指令后，他们都会进入到redis底层的queue来执行，如果此时有一些慢查询的数据，就会导致大量请求阻塞，从而引起报错，所以我们需要解决慢查询问题。\n\n\n\n慢查询的阈值可以通过配置指定：\n\nslowlog-log-slower-than：慢查询阈值，单位是微秒。默认是10000，建议1000\n\n慢查询会被放入慢查询日志中，日志的长度有上限，可以通过配置指定：\n\nslowlog-max-len：慢查询日志（本质是一个队列）的长度。默认是128，建议1000\n\n\n\n修改这两个配置可以使用：config set命令：\n\n\n\n\n# 如何查看慢查询\n\n知道了以上内容之后，那么咱们如何去查看慢查询日志列表呢：\n\n * slowlog len：查询慢查询日志长度\n * slowlog get [n]：读取n条慢查询日志\n * slowlog reset：清空慢查询列表\n\n\n\n\n# 服务器端优化-命令及安全配置\n\n安全可以说是服务器端一个非常重要的话题，如果安全出现了问题，那么一旦这个漏洞被一些坏人知道了之后，并且进行攻击，那么这就会给咱们的系统带来很多的损失，所以我们这节课就来解决这个问题。\n\nRedis会绑定在0.0.0.0:6379，这样将会将Redis服务暴露到公网上，而Redis如果没有做身份认证，会出现严重的安全漏洞. 漏洞重现方式：https://cloud.tencent.com/developer/article/1039000\n\n为什么会出现不需要密码也能够登录呢，主要是Redis考虑到每次登录都比较麻烦，所以Redis就有一种ssh免秘钥登录的方式，生成一对公钥和私钥，私钥放在本地，公钥放在redis端，当我们登录时服务器，再登录时候，他会去解析公钥和私钥，如果没有问题，则不需要利用redis的登录也能访问，这种做法本身也很常见，但是这里有一个前提，前提就是公钥必须保存在服务器上，才行，但是Redis的漏洞在于在不登录的情况下，也能把秘钥送到Linux服务器，从而产生漏洞\n\n漏洞出现的核心的原因有以下几点：\n\n * Redis未设置密码\n * 利用了Redis的config set命令动态修改Redis配置\n * 使用了Root账号权限启动Redis\n\n所以：如何解决呢？我们可以采用如下几种方案\n\n为了避免这样的漏洞，这里给出一些建议：\n\n * Redis一定要设置密码\n * 禁止线上使用下面命令：keys、flushall、flushdb、config set等命令。可以利用rename-command禁用。\n * bind：限制网卡，禁止外网网卡访问\n * 开启防火墙\n * 不要使用Root账户启动Redis\n * 尽量不是有默认的端口\n\n\n# 服务器端优化-Redis内存划分和内存配置\n\n当Redis内存不足时，可能导致Key频繁被删除、响应时间变长、QPS不稳定等问题。当内存使用率达到90%以上时就需要我们警惕，并快速定位到内存占用的原因。\n\n有关碎片问题分析\n\nRedis底层分配并不是这个key有多大，他就会分配多大，而是有他自己的分配策略，比如8,16,20等等，假定当前key只需要10个字节，此时分配8肯定不够，那么他就会分配16个字节，多出来的6个字节就不能被使用，这就是我们常说的 碎片问题\n\n进程内存问题分析：\n\n这片内存，通常我们都可以忽略不计\n\n缓冲区内存问题分析：\n\n一般包括客户端缓冲区、AOF缓冲区、复制缓冲区等。客户端缓冲区又包括输入缓冲区和输出缓冲区两种。这部分内存占用波动较大，所以这片内存也是我们需要重点分析的内存问题。\n\n内存占用    说明\n数据内存    是Redis最主要的部分，存储Redis的键值信息。主要问题是BigKey问题、内存碎片问题\n进程内存    Redis主进程本身运⾏肯定需要占⽤内存，如代码、常量池等等；这部分内存⼤约⼏兆，在⼤多数⽣产环境中与Redis数据占⽤的内存相⽐可以忽略。\n缓冲区内存   一般包括客户端缓冲区、AOF缓冲区、复制缓冲区等。客户端缓冲区又包括输入缓冲区和输出缓冲区两种。这部分内存占用波动较大，不当使用BigKey，可能导致内存溢出。\n\n于是我们就需要通过一些命令，可以查看到Redis目前的内存分配状态：\n\n * info memory：查看内存分配的情况\n\n\n\n * memory xxx：查看key的主要占用情况\n\n\n\n接下来我们看到了这些配置，最关键的缓存区内存如何定位和解决呢？\n\n内存缓冲区常见的有三种：\n\n * 复制缓冲区：主从复制的repl_backlog_buf，如果太小可能导致频繁的全量复制，影响性能。通过replbacklog-size来设置，默认1mb\n * AOF缓冲区：AOF刷盘之前的缓存区域，AOF执行rewrite的缓冲区。无法设置容量上限\n * 客户端缓冲区：分为输入缓冲区和输出缓冲区，输入缓冲区最大1G且不能设置。输出缓冲区可以设置\n\n以上复制缓冲区和AOF缓冲区 不会有问题，最关键就是客户端缓冲区的问题\n\n客户端缓冲区：指的就是我们发送命令时，客户端用来缓存命令的一个缓冲区，也就是我们向redis输入数据的输入端缓冲区和redis向客户端返回数据的响应缓存区，输入缓冲区最大1G且不能设置，所以这一块我们根本不用担心，如果超过了这个空间，redis会直接断开，因为本来此时此刻就代表着redis处理不过来了，我们需要担心的就是输出端缓冲区\n\n\n\n我们在使用redis过程中，处理大量的big value，那么会导致我们的输出结果过多，如果输出缓存区过大，会导致redis直接断开，而默认配置的情况下， 其实他是没有大小的，这就比较坑了，内存可能一下子被占满，会直接导致咱们的redis断开，所以解决方案有两个\n\n1、设置一个大小\n\n2、增加我们带宽的大小，避免我们出现大量数据从而直接超过了redis的承受能力\n\n\n# 服务器端集群优化-集群还是主从\n\n集群虽然具备高可用特性，能实现自动故障恢复，但是如果使用不当，也会存在一些问题：\n\n * 集群完整性问题\n * 集群带宽问题\n * 数据倾斜问题\n * 客户端性能问题\n * 命令的集群兼容性问题\n * lua和事务问题\n\n问题1、在Redis的默认配置中，如果发现任意一个插槽不可用，则整个集群都会停止对外服务：\n\n大家可以设想一下，如果有几个slot不能使用，那么此时整个集群都不能用了，我们在开发中，其实最重要的是可用性，所以需要把如下配置修改成no，即有slot不能使用时，我们的redis集群还是可以对外提供服务\n\n\n\n问题2、集群带宽问题\n\n集群节点之间会不断的互相Ping来确定集群中其它节点的状态。每次Ping携带的信息至少包括：\n\n * 插槽信息\n * 集群状态信息\n\n集群中节点越多，集群状态信息数据量也越大，10个节点的相关信息可能达到1kb，此时每次集群互通需要的带宽会非常高，这样会导致集群中大量的带宽都会被ping信息所占用，这是一个非常可怕的问题，所以我们需要去解决这样的问题\n\n解决途径：\n\n * 避免大集群，集群节点数不要太多，最好少于1000，如果业务庞大，则建立多个集群。\n * 避免在单个物理机中运行太多Redis实例\n * 配置合适的cluster-node-timeout值\n\n问题3、命令的集群兼容性问题\n\n有关这个问题咱们已经探讨过了，当我们使用批处理的命令时，redis要求我们的key必须落在相同的slot上，然后大量的key同时操作时，是无法完成的，所以客户端必须要对这样的数据进行处理，这些方案我们之前已经探讨过了，所以不再这个地方赘述了。\n\n问题4、lua和事务的问题\n\nlua和事务都是要保证原子性问题，如果你的key不在一个节点，那么是无法保证lua的执行和事务的特性的，所以在集群模式是没有办法执行lua和事务的\n\n那我们到底是集群还是主从\n\n单体Redis（主从Redis）已经能达到万级别的QPS，并且也具备很强的高可用特性。如果主从能满足业务需求的情况下，所以如果不是在万不得已的情况下，尽量不搭建Redis集群',normalizedContent:'# redis键值设计\n\n\n# 优雅的key结构\n\nredis的key虽然可以自定义，但最好遵循下面的几个最佳实践约定：\n\n * 遵循基本格式：[业务名称]:[数据名]:[id]\n * 长度不超过44字节\n * 不包含特殊字符\n\n例如：我们的登录业务，保存用户信息，其key可以设计成如下格式：\n\n\n\n这样设计的好处：\n\n * 可读性强\n * 避免key冲突\n * 方便管理\n * 更节省内存： key是string类型，底层编码包含int、embstr和raw三种。embstr在小于44字节使用，采用连续内存空间，内存占用更小。当字节数大于44字节时，会转为raw模式存储，在raw模式下，内存空间不是连续的，而是采用一个指针指向了另外一段内存空间，在这段空间里存储sds内容，这样空间不连续，访问的时候性能也就会收到影响，还有可能产生内存碎片\n\n\n\n\n# 拒绝bigkey\n\nbigkey通常以key的大小和key中成员的数量来综合判定，例如：\n\n * key本身的数据量过大：一个string类型的key，它的值为5 mb\n * key中的成员数过多：一个zset类型的key，它的成员数量为10,000个\n * key中成员的数据量过大：一个hash类型的key，它的成员数量虽然只有1,000个但这些成员的value（值）总大小为100 mb\n\n那么如何判断元素的大小呢？redis也给我们提供了命令\n\n\n\n推荐值：\n\n * 单个key的value小于10kb\n * 对于集合类型的key，建议元素数量小于1000\n\n# bigkey的危害\n\n * 网络阻塞\n   * 对bigkey执行读请求时，少量的qps就可能导致带宽使用率被占满，导致redis实例，乃至所在物理机变慢\n * 数据倾斜\n   * bigkey所在的redis实例内存使用率远超其他实例，无法使数据分片的内存资源达到均衡\n * redis阻塞\n   * 对元素较多的hash、list、zset等做运算会耗时较旧，使主线程被阻塞\n * cpu压力\n   * 对bigkey的数据序列化和反序列化会导致cpu的使用率飙升，影响redis实例和本机其它应用\n\n# 如何发现bigkey\n\n# ①redis-cli --bigkeys\n\n利用redis-cli提供的--bigkeys参数，可以遍历分析所有key，并返回key的整体统计信息与每个数据的top1的big key\n\n命令：redis-cli -a 密码 --bigkeys\n\n\n\n# ②scan扫描\n\n自己编程，利用scan扫描redis中的所有key，利用strlen、hlen等命令判断key的长度（此处不建议使用memory usage）\n\n\n\nscan 命令调用完后每次会返回2个元素，第一个是下一次迭代的光标，第一次光标会设置为0，当最后一次scan 返回的光标等于0时，表示整个scan遍历结束了，第二个返回的是list，一个匹配的key的数组\n\nimport com.heima.jedis.util.jedisconnectionfactory;\nimport org.junit.jupiter.api.aftereach;\nimport org.junit.jupiter.api.beforeeach;\nimport org.junit.jupiter.api.test;\nimport redis.clients.jedis.jedis;\nimport redis.clients.jedis.scanresult;\n\nimport java.util.hashmap;\nimport java.util.list;\nimport java.util.map;\n\npublic class jedistest {\n    private jedis jedis;\n\n    @beforeeach\n    void setup() {\n        // 1.建立连接\n        // jedis = new jedis("192.168.150.101", 6379);\n        jedis = jedisconnectionfactory.getjedis();\n        // 2.设置密码\n        jedis.auth("123321");\n        // 3.选择库\n        jedis.select(0);\n    }\n\n    final static int str_max_len = 10 * 1024;\n    final static int hash_max_len = 500;\n\n    @test\n    void testscan() {\n        int maxlen = 0;\n        long len = 0;\n\n        string cursor = "0";\n        do {\n            // 扫描并获取一部分key\n            scanresult<string> result = jedis.scan(cursor);\n            // 记录cursor\n            cursor = result.getcursor();\n            list<string> list = result.getresult();\n            if (list == null || list.isempty()) {\n                break;\n            }\n            // 遍历\n            for (string key : list) {\n                // 判断key的类型\n                string type = jedis.type(key);\n                switch (type) {\n                    case "string":\n                        len = jedis.strlen(key);\n                        maxlen = str_max_len;\n                        break;\n                    case "hash":\n                        len = jedis.hlen(key);\n                        maxlen = hash_max_len;\n                        break;\n                    case "list":\n                        len = jedis.llen(key);\n                        maxlen = hash_max_len;\n                        break;\n                    case "set":\n                        len = jedis.scard(key);\n                        maxlen = hash_max_len;\n                        break;\n                    case "zset":\n                        len = jedis.zcard(key);\n                        maxlen = hash_max_len;\n                        break;\n                    default:\n                        break;\n                }\n                if (len >= maxlen) {\n                    system.out.printf("found big key : %s, type: %s, length or size: %d %n", key, type, len);\n                }\n            }\n        } while (!cursor.equals("0"));\n    }\n    \n    @aftereach\n    void teardown() {\n        if (jedis != null) {\n            jedis.close();\n        }\n    }\n\n}\n\n\n# ③第三方工具\n\n * 利用第三方工具，如 redis-rdb-tools 分析rdb快照文件，全面分析内存使用情况\n * https://github.com/sripathikrishnan/redis-rdb-tools\n\n# ④网络监控\n\n * 自定义工具，监控进出redis的网络数据，超出预警值时主动告警\n * 一般阿里云搭建的云服务器就有相关监控页面\n\n\n\n# 1.2.3、如何删除bigkey\n\nbigkey内存占用较多，即便时删除这样的key也需要耗费很长时间，导致redis主线程阻塞，引发一系列问题。\n\n * redis 3.0 及以下版本\n   * 如果是集合类型，则遍历bigkey的元素，先逐个删除子元素，最后删除bigkey\n\n\n\n * redis 4.0以后\n   * redis在4.0后提供了异步删除的命令：unlink\n\n\n# 恰当的数据类型\n\n# 例1：比如存储一个user对象，我们有三种存储方式：\n\n# ①方式一：json字符串\n\nuser:1   {"name": "jack", "age": 21}\n\n优点：实现简单粗暴\n\n缺点：数据耦合，不够灵活\n\n# ②方式二：字段打散\n\nuser:1:name   jack\nuser:1:age    21\n\n优点：可以灵活访问对象任意字段\n\n缺点：占用空间大、没办法做统一控制\n\n# ③方式三：hash（推荐）\n\nuser:1   name   jack\n         age    21\n\n优点：底层使用ziplist，空间占用小，可以灵活访问对象的任意字段\n\n缺点：代码相对复杂\n\n# 例2：假如有hash类型的key，其中有100万对field和value，field是自增id，这个key存在什么问题？如何优化？\n\nkey       field       value\nsomekey   id:0        value0\n          .....       .....\n          id:999999   value999999\n\n存在的问题：\n\n * hash的entry数量超过500时，会使用哈希表而不是ziplist，内存占用较多\n   * \n * 可以通过hash-max-ziplist-entries配置entry上限。但是如果entry过多就会导致bigkey问题\n\n# 方案一\n\n拆分为string类型\n\nkey         value\nid:0        value0\n.....       .....\nid:999999   value999999\n\n存在的问题：\n\n * string结构底层没有太多内存优化，内存占用较多\n\n\n\n * 想要批量获取这些数据比较麻烦\n\n# 方案二\n\n拆分为小的hash，将 id / 100 作为key， 将id % 100 作为field，这样每100个元素为一个hash\n\nkey        field   value\nkey:0      id:00   value0\n           .....   .....\n           id:99   value99\nkey:1      id:00   value100\n           .....   .....\n           id:99   value199\n....\nkey:9999   id:00   value999900\n           .....   .....\n           id:99   value999999\n\n\n\npackage com.heima.test;\n\nimport com.heima.jedis.util.jedisconnectionfactory;\nimport org.junit.jupiter.api.aftereach;\nimport org.junit.jupiter.api.beforeeach;\nimport org.junit.jupiter.api.test;\nimport redis.clients.jedis.jedis;\nimport redis.clients.jedis.pipeline;\nimport redis.clients.jedis.scanresult;\n\nimport java.util.hashmap;\nimport java.util.list;\nimport java.util.map;\n\npublic class jedistest {\n    private jedis jedis;\n\n    @beforeeach\n    void setup() {\n        // 1.建立连接\n        // jedis = new jedis("192.168.150.101", 6379);\n        jedis = jedisconnectionfactory.getjedis();\n        // 2.设置密码\n        jedis.auth("123321");\n        // 3.选择库\n        jedis.select(0);\n    }\n\n    @test\n    void testsetbigkey() {\n        map<string, string> map = new hashmap<>();\n        for (int i = 1; i <= 650; i++) {\n            map.put("hello_" + i, "world!");\n        }\n        jedis.hmset("m2", map);\n    }\n\n    @test\n    void testbighash() {\n        map<string, string> map = new hashmap<>();\n        for (int i = 1; i <= 100000; i++) {\n            map.put("key_" + i, "value_" + i);\n        }\n        jedis.hmset("test:big:hash", map);\n    }\n\n    @test\n    void testbigstring() {\n        for (int i = 1; i <= 100000; i++) {\n            jedis.set("test:str:key_" + i, "value_" + i);\n        }\n    }\n\n    @test\n    void testsmallhash() {\n        int hashsize = 100;\n        map<string, string> map = new hashmap<>(hashsize);\n        for (int i = 1; i <= 100000; i++) {\n            int k = (i - 1) / hashsize;\n            int v = i % hashsize;\n            map.put("key_" + v, "value_" + v);\n            if (v == 0) {\n                jedis.hmset("test:small:hash_" + k, map);\n            }\n        }\n    }\n\n    @aftereach\n    void teardown() {\n        if (jedis != null) {\n            jedis.close();\n        }\n    }\n}\n\n\n\n# 总结\n\n * key的最佳实践\n   * 固定格式：[业务名]:[数据名]:[id]\n   * 足够简短：不超过44字节\n   * 不包含特殊字符\n * value的最佳实践：\n   * 合理的拆分数据，拒绝bigkey\n   * 选择合适数据结构\n   * hash结构的entry数量不要超过1000\n   * 设置合理的超时时间\n\n\n# 批处理优化\n\n\n# pipeline\n\n# 我们的客户端与redis服务器是这样交互的\n\n单个命令的执行流程\n\n\n\nn条命令的执行流程\n\n\n\nredis处理指令是很快的，主要花费的时候在于网络传输。于是乎很容易想到将多条指令批量的传输给redis\n\n\n\n# mset\n\nredis提供了很多mxxx这样的命令，可以实现批量插入数据，例如：\n\n * mset\n * hmset\n\n利用mset批量插入10万条数据\n\n@test\nvoid testmxx() {\n    string[] arr = new string[2000];\n    int j;\n    long b = system.currenttimemillis();\n    for (int i = 1; i <= 100000; i++) {\n        j = (i % 1000) << 1;\n        arr[j] = "test:key_" + i;\n        arr[j + 1] = "value_" + i;\n        if (j == 0) {\n            jedis.mset(arr);\n        }\n    }\n    long e = system.currenttimemillis();\n    system.out.println("time: " + (e - b));\n}\n\n\n# pipeline\n\nmset虽然可以批处理，但是却只能操作部分数据类型，因此如果有对复杂数据类型的批处理需要，建议使用pipeline\n\n@test\nvoid testpipeline() {\n    // 创建管道\n    pipeline pipeline = jedis.pipelined();\n    long b = system.currenttimemillis();\n    for (int i = 1; i <= 100000; i++) {\n        // 放入命令到管道\n        pipeline.set("test:key_" + i, "value_" + i);\n        if (i % 1000 == 0) {\n            // 每放入1000条命令，批量执行\n            pipeline.sync();\n        }\n    }\n    long e = system.currenttimemillis();\n    system.out.println("time: " + (e - b));\n}\n\n\n\n# 集群下的批处理\n\n如mset或pipeline这样的批处理需要在一次请求中携带多条命令，而此时如果redis是一个集群，那批处理命令的多个key必须落在一个插槽中，否则就会导致执行失败。大家可以想一想这样的要求其实很难实现，因为我们在批处理时，可能一次要插入很多条数据，这些数据很有可能不会都落在相同的节点上，这就会导致报错了\n\n这个时候，我们可以找到4种解决方案\n\n\n\n第一种方案：串行执行，所以这种方式没有什么意义，当然，执行起来就很简单了，缺点就是耗时过久。\n\n第二种方案：串行slot，简单来说，就是执行前，客户端先计算一下对应的key的slot，一样slot的key就放到一个组里边，不同的，就放到不同的组里边，然后对每个组执行pipeline的批处理，他就能串行执行各个组的命令，这种做法比第一种方法耗时要少，但是缺点呢，相对来说复杂一点，所以这种方案还需要优化一下\n\n第三种方案：并行slot，相较于第二种方案，在分组完成后串行执行，第三种方案，就变成了并行执行各个命令，所以他的耗时就非常短，但是实现呢，也更加复杂。\n\n第四种：hash_tag，redis计算key的slot的时候，其实是根据key的有效部分来计算的，通过这种方式就能一次处理所有的key，这种方式耗时最短，实现也简单，但是如果通过操作key的有效部分，那么就会导致所有的key都落在一个节点上，产生数据倾斜的问题，所以我们推荐使用第三种方式。\n\n# 串行化执行代码实践\n\npublic class jedisclustertest {\n\n    private jediscluster jediscluster;\n\n    @beforeeach\n    void setup() {\n        // 配置连接池\n        jedispoolconfig poolconfig = new jedispoolconfig();\n        poolconfig.setmaxtotal(8);\n        poolconfig.setmaxidle(8);\n        poolconfig.setminidle(0);\n        poolconfig.setmaxwaitmillis(1000);\n        hashset<hostandport> nodes = new hashset<>();\n        nodes.add(new hostandport("192.168.150.101", 7001));\n        nodes.add(new hostandport("192.168.150.101", 7002));\n        nodes.add(new hostandport("192.168.150.101", 7003));\n        nodes.add(new hostandport("192.168.150.101", 8001));\n        nodes.add(new hostandport("192.168.150.101", 8002));\n        nodes.add(new hostandport("192.168.150.101", 8003));\n        jediscluster = new jediscluster(nodes, poolconfig);\n    }\n\n    @test\n    void testmset() {\n        jediscluster.mset("name", "jack", "age", "21", "sex", "male");\n\n    }\n\n    @test\n    void testmset2() {\n        map<string, string> map = new hashmap<>(3);\n        map.put("name", "jack");\n        map.put("age", "21");\n        map.put("sex", "male");\n        //对map数据进行分组。根据相同的slot放在一个分组\n        //key就是slot，value就是一个组\n        map<integer, list<map.entry<string, string>>> result = map.entryset()\n                .stream()\n                .collect(collectors.groupingby(\n                        entry -> clusterslothashutil.calculateslot(entry.getkey()))\n                );\n        //串行的去执行mset的逻辑\n        for (list<map.entry<string, string>> list : result.values()) {\n            string[] arr = new string[list.size() * 2];\n            int j = 0;\n            for (int i = 0; i < list.size(); i++) {\n                j = i<<2;\n                map.entry<string, string> e = list.get(0);\n                arr[j] = e.getkey();\n                arr[j + 1] = e.getvalue();\n            }\n            jediscluster.mset(arr);\n        }\n    }\n\n    @aftereach\n    void teardown() {\n        if (jediscluster != null) {\n            jediscluster.close();\n        }\n    }\n}\n\n\n# spring集群环境下批处理代码\n\n   @test\n    void testmsetincluster() {\n        map<string, string> map = new hashmap<>(3);\n        map.put("name", "rose");\n        map.put("age", "21");\n        map.put("sex", "female");\n        stringredistemplate.opsforvalue().multiset(map);\n\n\n        list<string> strings = stringredistemplate.opsforvalue().multiget(arrays.aslist("name", "age", "sex"));\n        strings.foreach(system.out::println);\n\n    }\n\n\n原理分析\n\n在redisadvancedclusterasynccommandsimpl 类中\n\n首先根据slothash算出来一个partitioned的map，map中的key就是slot，而他的value就是对应的对应相同slot的key对应的数据\n\n通过 redisfuture<string> mset = super.mset(op);进行异步的消息发送\n\n@override\npublic redisfuture<string> mset(map<k, v> map) {\n\n    map<integer, list<k>> partitioned = slothash.partition(codec, map.keyset());\n\n    if (partitioned.size() < 2) {\n        return super.mset(map);\n    }\n\n    map<integer, redisfuture<string>> executions = new hashmap<>();\n\n    for (map.entry<integer, list<k>> entry : partitioned.entryset()) {\n\n        map<k, v> op = new hashmap<>();\n        entry.getvalue().foreach(k -> op.put(k, map.get(k)));\n\n        redisfuture<string> mset = super.mset(op);\n        executions.put(entry.getkey(), mset);\n    }\n\n    return multinodeexecution.firstofasync(executions);\n}\n\n\n\n# 服务器端优化-持久化配置\n\nredis的持久化虽然可以保证数据安全，但也会带来很多额外的开销，因此持久化请遵循下列建议：\n\n * 用来做缓存的redis实例尽量不要开启持久化功能\n * 建议关闭rdb持久化功能，使用aof持久化\n * 利用脚本定期在slave节点做rdb，实现数据备份\n * 设置合理的rewrite阈值，避免频繁的bgrewrite\n * 配置no-appendfsync-on-rewrite = yes，禁止在rewrite期间做aof，避免因aof引起的阻塞\n * 部署有关建议：\n   * redis实例的物理机要预留足够内存，应对fork和rewrite\n   * 单个redis实例内存上限不要太大，例如4g或8g。可以加快fork的速度、减少主从同步、数据迁移压力\n   * 不要与cpu密集型应用部署在一起\n   * 不要与高硬盘负载应用一起部署。例如：数据库、消息队列\n\n\n# 服务器端优化-慢查询优化\n\n\n# 什么是慢查询\n\n并不是很慢的查询才是慢查询，而是：在redis执行时耗时超过某个阈值的命令，称为慢查询。\n\n慢查询的危害：由于redis是单线程的，所以当客户端发出指令后，他们都会进入到redis底层的queue来执行，如果此时有一些慢查询的数据，就会导致大量请求阻塞，从而引起报错，所以我们需要解决慢查询问题。\n\n\n\n慢查询的阈值可以通过配置指定：\n\nslowlog-log-slower-than：慢查询阈值，单位是微秒。默认是10000，建议1000\n\n慢查询会被放入慢查询日志中，日志的长度有上限，可以通过配置指定：\n\nslowlog-max-len：慢查询日志（本质是一个队列）的长度。默认是128，建议1000\n\n\n\n修改这两个配置可以使用：config set命令：\n\n\n\n\n# 如何查看慢查询\n\n知道了以上内容之后，那么咱们如何去查看慢查询日志列表呢：\n\n * slowlog len：查询慢查询日志长度\n * slowlog get [n]：读取n条慢查询日志\n * slowlog reset：清空慢查询列表\n\n\n\n\n# 服务器端优化-命令及安全配置\n\n安全可以说是服务器端一个非常重要的话题，如果安全出现了问题，那么一旦这个漏洞被一些坏人知道了之后，并且进行攻击，那么这就会给咱们的系统带来很多的损失，所以我们这节课就来解决这个问题。\n\nredis会绑定在0.0.0.0:6379，这样将会将redis服务暴露到公网上，而redis如果没有做身份认证，会出现严重的安全漏洞. 漏洞重现方式：https://cloud.tencent.com/developer/article/1039000\n\n为什么会出现不需要密码也能够登录呢，主要是redis考虑到每次登录都比较麻烦，所以redis就有一种ssh免秘钥登录的方式，生成一对公钥和私钥，私钥放在本地，公钥放在redis端，当我们登录时服务器，再登录时候，他会去解析公钥和私钥，如果没有问题，则不需要利用redis的登录也能访问，这种做法本身也很常见，但是这里有一个前提，前提就是公钥必须保存在服务器上，才行，但是redis的漏洞在于在不登录的情况下，也能把秘钥送到linux服务器，从而产生漏洞\n\n漏洞出现的核心的原因有以下几点：\n\n * redis未设置密码\n * 利用了redis的config set命令动态修改redis配置\n * 使用了root账号权限启动redis\n\n所以：如何解决呢？我们可以采用如下几种方案\n\n为了避免这样的漏洞，这里给出一些建议：\n\n * redis一定要设置密码\n * 禁止线上使用下面命令：keys、flushall、flushdb、config set等命令。可以利用rename-command禁用。\n * bind：限制网卡，禁止外网网卡访问\n * 开启防火墙\n * 不要使用root账户启动redis\n * 尽量不是有默认的端口\n\n\n# 服务器端优化-redis内存划分和内存配置\n\n当redis内存不足时，可能导致key频繁被删除、响应时间变长、qps不稳定等问题。当内存使用率达到90%以上时就需要我们警惕，并快速定位到内存占用的原因。\n\n有关碎片问题分析\n\nredis底层分配并不是这个key有多大，他就会分配多大，而是有他自己的分配策略，比如8,16,20等等，假定当前key只需要10个字节，此时分配8肯定不够，那么他就会分配16个字节，多出来的6个字节就不能被使用，这就是我们常说的 碎片问题\n\n进程内存问题分析：\n\n这片内存，通常我们都可以忽略不计\n\n缓冲区内存问题分析：\n\n一般包括客户端缓冲区、aof缓冲区、复制缓冲区等。客户端缓冲区又包括输入缓冲区和输出缓冲区两种。这部分内存占用波动较大，所以这片内存也是我们需要重点分析的内存问题。\n\n内存占用    说明\n数据内存    是redis最主要的部分，存储redis的键值信息。主要问题是bigkey问题、内存碎片问题\n进程内存    redis主进程本身运⾏肯定需要占⽤内存，如代码、常量池等等；这部分内存⼤约⼏兆，在⼤多数⽣产环境中与redis数据占⽤的内存相⽐可以忽略。\n缓冲区内存   一般包括客户端缓冲区、aof缓冲区、复制缓冲区等。客户端缓冲区又包括输入缓冲区和输出缓冲区两种。这部分内存占用波动较大，不当使用bigkey，可能导致内存溢出。\n\n于是我们就需要通过一些命令，可以查看到redis目前的内存分配状态：\n\n * info memory：查看内存分配的情况\n\n\n\n * memory xxx：查看key的主要占用情况\n\n\n\n接下来我们看到了这些配置，最关键的缓存区内存如何定位和解决呢？\n\n内存缓冲区常见的有三种：\n\n * 复制缓冲区：主从复制的repl_backlog_buf，如果太小可能导致频繁的全量复制，影响性能。通过replbacklog-size来设置，默认1mb\n * aof缓冲区：aof刷盘之前的缓存区域，aof执行rewrite的缓冲区。无法设置容量上限\n * 客户端缓冲区：分为输入缓冲区和输出缓冲区，输入缓冲区最大1g且不能设置。输出缓冲区可以设置\n\n以上复制缓冲区和aof缓冲区 不会有问题，最关键就是客户端缓冲区的问题\n\n客户端缓冲区：指的就是我们发送命令时，客户端用来缓存命令的一个缓冲区，也就是我们向redis输入数据的输入端缓冲区和redis向客户端返回数据的响应缓存区，输入缓冲区最大1g且不能设置，所以这一块我们根本不用担心，如果超过了这个空间，redis会直接断开，因为本来此时此刻就代表着redis处理不过来了，我们需要担心的就是输出端缓冲区\n\n\n\n我们在使用redis过程中，处理大量的big value，那么会导致我们的输出结果过多，如果输出缓存区过大，会导致redis直接断开，而默认配置的情况下， 其实他是没有大小的，这就比较坑了，内存可能一下子被占满，会直接导致咱们的redis断开，所以解决方案有两个\n\n1、设置一个大小\n\n2、增加我们带宽的大小，避免我们出现大量数据从而直接超过了redis的承受能力\n\n\n# 服务器端集群优化-集群还是主从\n\n集群虽然具备高可用特性，能实现自动故障恢复，但是如果使用不当，也会存在一些问题：\n\n * 集群完整性问题\n * 集群带宽问题\n * 数据倾斜问题\n * 客户端性能问题\n * 命令的集群兼容性问题\n * lua和事务问题\n\n问题1、在redis的默认配置中，如果发现任意一个插槽不可用，则整个集群都会停止对外服务：\n\n大家可以设想一下，如果有几个slot不能使用，那么此时整个集群都不能用了，我们在开发中，其实最重要的是可用性，所以需要把如下配置修改成no，即有slot不能使用时，我们的redis集群还是可以对外提供服务\n\n\n\n问题2、集群带宽问题\n\n集群节点之间会不断的互相ping来确定集群中其它节点的状态。每次ping携带的信息至少包括：\n\n * 插槽信息\n * 集群状态信息\n\n集群中节点越多，集群状态信息数据量也越大，10个节点的相关信息可能达到1kb，此时每次集群互通需要的带宽会非常高，这样会导致集群中大量的带宽都会被ping信息所占用，这是一个非常可怕的问题，所以我们需要去解决这样的问题\n\n解决途径：\n\n * 避免大集群，集群节点数不要太多，最好少于1000，如果业务庞大，则建立多个集群。\n * 避免在单个物理机中运行太多redis实例\n * 配置合适的cluster-node-timeout值\n\n问题3、命令的集群兼容性问题\n\n有关这个问题咱们已经探讨过了，当我们使用批处理的命令时，redis要求我们的key必须落在相同的slot上，然后大量的key同时操作时，是无法完成的，所以客户端必须要对这样的数据进行处理，这些方案我们之前已经探讨过了，所以不再这个地方赘述了。\n\n问题4、lua和事务的问题\n\nlua和事务都是要保证原子性问题，如果你的key不在一个节点，那么是无法保证lua的执行和事务的特性的，所以在集群模式是没有办法执行lua和事务的\n\n那我们到底是集群还是主从\n\n单体redis（主从redis）已经能达到万级别的qps，并且也具备很强的高可用特性。如果主从能满足业务需求的情况下，所以如果不是在万不得已的情况下，尽量不搭建redis集群',charsets:{cjk:!0}},{title:"分布式缓存总结",frontmatter:{autoSort:92,title:"分布式缓存总结",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/981287/",categories:["后端","数据库","Redis"],tags:["知识","数据库","Redis"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/05.Redis/35.%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98%E6%80%BB%E7%BB%93.html",relativePath:"01.后端/30.数据库/05.Redis/35.分布式缓存总结.md",key:"v-3632f31f",path:"/pages/981287/",headers:[{level:2,title:"Redis缓存持久化",slug:"redis缓存持久化",normalizedTitle:"redis缓存持久化",charIndex:2},{level:2,title:"Redis集群模式",slug:"redis集群模式",normalizedTitle:"redis集群模式",charIndex:177}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"Redis缓存持久化 Redis集群模式",content:'# Redis缓存持久化\n\n * RDB持久化\n   * bgsave开始时会fork主进程得到子进程，子进程共享主进程的内存数据。完成fork后读取内存数据并写入 RDB 文件。\n * AOF持久化\n   * Redis处理的每一个写命令都会记录在AOF文件\n * 当redis重新启动后，会读取存储到磁盘的RDB文件或者AOF文件来更新数据\n\n\n# Redis集群模式\n\n * Redis 主从+哨兵模式\n   * 主从之间的数据同步有 全量同步，增量同步\n   * 哨兵可以监控Redis集群状态\n     * 当主节点宕机后，会自动选主，恢复故障\n * Redis 分片集群\n   * 类似于 ES的分片集群\n   * 采用散列插槽的模式\n     * 数据通过hash运算绑定一个插槽\n       * {a}num与a都在同一个插槽\n       * key中包含"{}"，且“{}”中至少包含1个字符，“{}”中的部分是有效部分。按照有效部分计算插槽。\n       * 一般同一个类型的 数据 加相同的{}，可以放到到同一个插槽，进而访问到同一片，即放到一台服务器上，不用进行重定向\n     * 数据与插槽绑定，每个片给分配插槽，分片上的插槽可以通过reshard进行转移\n     * 无论在哪一个结点访问数据都会自动路由到对应结点上\n   * 集群伸缩\n     * 加入集群，转移插槽，删除插槽\n       * 当一个结点上有插槽时，不能直接删除，需要将插槽转移才能删除\n   * 故障转移\n     * 分片集群不需要哨兵，可以互相之间进行心跳检测。当出现故障时，可进行自动故障转移。',normalizedContent:'# redis缓存持久化\n\n * rdb持久化\n   * bgsave开始时会fork主进程得到子进程，子进程共享主进程的内存数据。完成fork后读取内存数据并写入 rdb 文件。\n * aof持久化\n   * redis处理的每一个写命令都会记录在aof文件\n * 当redis重新启动后，会读取存储到磁盘的rdb文件或者aof文件来更新数据\n\n\n# redis集群模式\n\n * redis 主从+哨兵模式\n   * 主从之间的数据同步有 全量同步，增量同步\n   * 哨兵可以监控redis集群状态\n     * 当主节点宕机后，会自动选主，恢复故障\n * redis 分片集群\n   * 类似于 es的分片集群\n   * 采用散列插槽的模式\n     * 数据通过hash运算绑定一个插槽\n       * {a}num与a都在同一个插槽\n       * key中包含"{}"，且“{}”中至少包含1个字符，“{}”中的部分是有效部分。按照有效部分计算插槽。\n       * 一般同一个类型的 数据 加相同的{}，可以放到到同一个插槽，进而访问到同一片，即放到一台服务器上，不用进行重定向\n     * 数据与插槽绑定，每个片给分配插槽，分片上的插槽可以通过reshard进行转移\n     * 无论在哪一个结点访问数据都会自动路由到对应结点上\n   * 集群伸缩\n     * 加入集群，转移插槽，删除插槽\n       * 当一个结点上有插槽时，不能直接删除，需要将插槽转移才能删除\n   * 故障转移\n     * 分片集群不需要哨兵，可以互相之间进行心跳检测。当出现故障时，可进行自动故障转移。',charsets:{cjk:!0}},{title:"Redis哨兵",frontmatter:{autoSort:94,title:"Redis哨兵",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/74c463/",categories:["后端","数据库","Redis"],tags:["知识","数据库","Redis"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/05.Redis/27.Redis%E5%93%A8%E5%85%B5.html",relativePath:"01.后端/30.数据库/05.Redis/27.Redis哨兵.md",key:"v-c740fd8a",path:"/pages/74c463/",headers:[{level:2,title:"哨兵原理",slug:"哨兵原理",normalizedTitle:"哨兵原理",charIndex:94},{level:3,title:"集群结构和作用",slug:"集群结构和作用",normalizedTitle:"集群结构和作用",charIndex:103},{level:3,title:"集群监控原理",slug:"集群监控原理",normalizedTitle:"集群监控原理",charIndex:311},{level:3,title:"集群故障恢复原理",slug:"集群故障恢复原理",normalizedTitle:"集群故障恢复原理",charIndex:540},{level:3,title:"小结",slug:"小结",normalizedTitle:"小结",charIndex:1153},{level:2,title:"搭建哨兵集群",slug:"搭建哨兵集群",normalizedTitle:"搭建哨兵集群",charIndex:1416},{level:2,title:"RedisTemplate",slug:"redistemplate",normalizedTitle:"redistemplate",charIndex:1433},{level:3,title:"引入依赖",slug:"引入依赖",normalizedTitle:"引入依赖",charIndex:1604},{level:3,title:"配置Redis地址",slug:"配置redis地址",normalizedTitle:"配置redis地址",charIndex:1767},{level:3,title:"配置读写分离",slug:"配置读写分离",normalizedTitle:"配置读写分离",charIndex:1992}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"哨兵原理 集群结构和作用 集群监控原理 集群故障恢复原理 小结 搭建哨兵集群 RedisTemplate 引入依赖 配置Redis地址 配置读写分离",content:"Redis提供了哨兵（Sentinel）机制来实现主从集群的自动故障恢复。\n\n此哨兵(Sentinel) 与阿里巴巴的Sentinel的流量监控中间件是两种东西，只是恰巧重名了。\n\n\n# 哨兵原理\n\n\n# 集群结构和作用\n\n哨兵的结构如图：\n\n\n\n哨兵的作用如下：\n\n * 监控：Sentinel 会不断检查您的master和slave是否按预期工作\n * 自动故障恢复：如果master故障，Sentinel会将一个slave提升为master。当故障实例恢复后也以新的master为主\n * 通知：Sentinel充当Redis客户端的服务发现来源，当集群发生故障转移时，会将最新信息推送给Redis的客户端\n\n\n# 集群监控原理\n\nSentinel基于心跳机制监测服务状态，每隔1秒向集群的每个实例发送ping命令：\n\n * 就相当于投票，多于认为你挂了，你就挂了。\n\n•主观下线：如果某sentinel节点发现某实例未在规定时间响应，则认为该实例主观下线。\n\n•客观下线：若超过指定数量（quorum）的sentinel都认为该实例主观下线，则该实例客观下线。quorum值最好超过Sentinel实例数量的一半。\n\n * 客观下线，就表示哨兵认为你下线了。\n\n\n\n\n# 集群故障恢复原理\n\n一旦发现master故障，sentinel需要在salve中选择一个作为新的master，选择依据是这样的：\n\n * 首先会判断slave节点与master节点断开时间长短，如果超过指定值（down-after-milliseconds * 10）则会排除该slave节点\n * 然后判断slave节点的slave-priority值，越小优先级越高，如果是0则永不参与选举\n * 如果slave-prority一样，则判断slave节点的offset值，越大说明数据越新，优先级越高\n   * 这里就是看数据完整性，谁的数据更接近主节点的数据，谁当老大。\n * 最后是判断slave节点的运行id大小，越小优先级越高。\n\n当选出一个新的master后，该如何实现切换呢？\n\n流程如下：\n\n * sentinel给备选的slave1节点发送slaveof no one命令，让该节点成为master\n * sentinel给所有其它slave发送slaveof 192.168.150.101 7002 命令，让这些slave成为新master的从节点，开始从新的master上同步数据。\n * 最后，sentinel将故障节点标记为slave，当故障节点恢复后会自动成为新的master的slave节点\n   * 修改它的配置文件，给它打上奴隶印记，这样恢复启动后，会自动成为slave节点\n\n\n\n\n# 小结\n\nSentinel的三个作用是什么？\n\n * 监控\n * 故障转移\n * 通知\n\nSentinel如何判断一个redis实例是否健康？\n\n * 每隔1秒发送一次ping命令，如果超过一定时间没有相向则认为是主观下线\n * 如果大多数sentinel都认为实例主观下线，则判定服务下线\n\n故障转移步骤有哪些？\n\n * 首先选定一个slave作为新的master，执行slaveof no one\n * 然后让所有节点都执行slaveof 新master\n * 修改故障节点配置，添加slaveof 新master\n\n\n# 搭建哨兵集群\n\n具体流程\n\n\n# RedisTemplate\n\n在Sentinel集群监管下的Redis主从集群，其节点会因为自动故障转移而发生变化，Redis的客户端必须感知这种变化，及时更新连接信息。Spring的RedisTemplate底层利用lettuce实现了节点的感知和自动切换。\n\n下面，我们通过一个测试来实现RedisTemplate集成哨兵机制。\n\n\n# 引入依赖\n\n在项目的pom文件中引入依赖：\n\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-data-redis</artifactId>\n</dependency>\n\n\n\n# 配置Redis地址\n\n然后在配置文件application.yml中指定redis的sentinel相关信息：\n\nspring:\n  redis:\n    sentinel:\n      master: mymaster\n      nodes:\n        - 192.168.150.101:27001\n        - 192.168.150.101:27002\n        - 192.168.150.101:27003\n\n\n\n# 配置读写分离\n\n在项目的启动类中，添加一个新的bean：\n\n@Bean\npublic LettuceClientConfigurationBuilderCustomizer clientConfigurationBuilderCustomizer(){\n    return clientConfigurationBuilder -> clientConfigurationBuilder.readFrom(ReadFrom.REPLICA_PREFERRED);\n}\n\n\n这个bean中配置的就是读写策略，包括四种：\n\n * MASTER：从主节点读取\n * MASTER_PREFERRED：优先从master节点读取，master不可用才读取replica\n * REPLICA：从slave（replica）节点读取\n * REPLICA _PREFERRED：优先从slave（replica）节点读取，所有的slave都不可用才读取master",normalizedContent:"redis提供了哨兵（sentinel）机制来实现主从集群的自动故障恢复。\n\n此哨兵(sentinel) 与阿里巴巴的sentinel的流量监控中间件是两种东西，只是恰巧重名了。\n\n\n# 哨兵原理\n\n\n# 集群结构和作用\n\n哨兵的结构如图：\n\n\n\n哨兵的作用如下：\n\n * 监控：sentinel 会不断检查您的master和slave是否按预期工作\n * 自动故障恢复：如果master故障，sentinel会将一个slave提升为master。当故障实例恢复后也以新的master为主\n * 通知：sentinel充当redis客户端的服务发现来源，当集群发生故障转移时，会将最新信息推送给redis的客户端\n\n\n# 集群监控原理\n\nsentinel基于心跳机制监测服务状态，每隔1秒向集群的每个实例发送ping命令：\n\n * 就相当于投票，多于认为你挂了，你就挂了。\n\n•主观下线：如果某sentinel节点发现某实例未在规定时间响应，则认为该实例主观下线。\n\n•客观下线：若超过指定数量（quorum）的sentinel都认为该实例主观下线，则该实例客观下线。quorum值最好超过sentinel实例数量的一半。\n\n * 客观下线，就表示哨兵认为你下线了。\n\n\n\n\n# 集群故障恢复原理\n\n一旦发现master故障，sentinel需要在salve中选择一个作为新的master，选择依据是这样的：\n\n * 首先会判断slave节点与master节点断开时间长短，如果超过指定值（down-after-milliseconds * 10）则会排除该slave节点\n * 然后判断slave节点的slave-priority值，越小优先级越高，如果是0则永不参与选举\n * 如果slave-prority一样，则判断slave节点的offset值，越大说明数据越新，优先级越高\n   * 这里就是看数据完整性，谁的数据更接近主节点的数据，谁当老大。\n * 最后是判断slave节点的运行id大小，越小优先级越高。\n\n当选出一个新的master后，该如何实现切换呢？\n\n流程如下：\n\n * sentinel给备选的slave1节点发送slaveof no one命令，让该节点成为master\n * sentinel给所有其它slave发送slaveof 192.168.150.101 7002 命令，让这些slave成为新master的从节点，开始从新的master上同步数据。\n * 最后，sentinel将故障节点标记为slave，当故障节点恢复后会自动成为新的master的slave节点\n   * 修改它的配置文件，给它打上奴隶印记，这样恢复启动后，会自动成为slave节点\n\n\n\n\n# 小结\n\nsentinel的三个作用是什么？\n\n * 监控\n * 故障转移\n * 通知\n\nsentinel如何判断一个redis实例是否健康？\n\n * 每隔1秒发送一次ping命令，如果超过一定时间没有相向则认为是主观下线\n * 如果大多数sentinel都认为实例主观下线，则判定服务下线\n\n故障转移步骤有哪些？\n\n * 首先选定一个slave作为新的master，执行slaveof no one\n * 然后让所有节点都执行slaveof 新master\n * 修改故障节点配置，添加slaveof 新master\n\n\n# 搭建哨兵集群\n\n具体流程\n\n\n# redistemplate\n\n在sentinel集群监管下的redis主从集群，其节点会因为自动故障转移而发生变化，redis的客户端必须感知这种变化，及时更新连接信息。spring的redistemplate底层利用lettuce实现了节点的感知和自动切换。\n\n下面，我们通过一个测试来实现redistemplate集成哨兵机制。\n\n\n# 引入依赖\n\n在项目的pom文件中引入依赖：\n\n<dependency>\n    <groupid>org.springframework.boot</groupid>\n    <artifactid>spring-boot-starter-data-redis</artifactid>\n</dependency>\n\n\n\n# 配置redis地址\n\n然后在配置文件application.yml中指定redis的sentinel相关信息：\n\nspring:\n  redis:\n    sentinel:\n      master: mymaster\n      nodes:\n        - 192.168.150.101:27001\n        - 192.168.150.101:27002\n        - 192.168.150.101:27003\n\n\n\n# 配置读写分离\n\n在项目的启动类中，添加一个新的bean：\n\n@bean\npublic lettuceclientconfigurationbuildercustomizer clientconfigurationbuildercustomizer(){\n    return clientconfigurationbuilder -> clientconfigurationbuilder.readfrom(readfrom.replica_preferred);\n}\n\n\n这个bean中配置的就是读写策略，包括四种：\n\n * master：从主节点读取\n * master_preferred：优先从master节点读取，master不可用才读取replica\n * replica：从slave（replica）节点读取\n * replica _preferred：优先从slave（replica）节点读取，所有的slave都不可用才读取master",charsets:{cjk:!0}},{title:"Redis分片集群",frontmatter:{autoSort:93,title:"Redis分片集群",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/f86562/",categories:["后端","数据库","Redis"],tags:["知识","数据库","Redis"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/05.Redis/30.Redis%E5%88%86%E7%89%87%E9%9B%86%E7%BE%A4.html",relativePath:"01.后端/30.数据库/05.Redis/30.Redis分片集群.md",key:"v-b33a99d2",path:"/pages/f86562/",headers:[{level:2,title:"搭建分片集群",slug:"搭建分片集群",normalizedTitle:"搭建分片集群",charIndex:165},{level:2,title:"散列插槽",slug:"散列插槽",normalizedTitle:"散列插槽",charIndex:70},{level:3,title:"插槽原理",slug:"插槽原理",normalizedTitle:"插槽原理",charIndex:467},{level:3,title:"小结",slug:"小结",normalizedTitle:"小结",charIndex:899},{level:2,title:"集群伸缩",slug:"集群伸缩",normalizedTitle:"集群伸缩",charIndex:156},{level:3,title:"需求分析",slug:"需求分析",normalizedTitle:"需求分析",charIndex:1200},{level:3,title:"创建新的redis实例",slug:"创建新的redis实例",normalizedTitle:"创建新的redis实例",charIndex:1384},{level:3,title:"添加新节点到redis",slug:"添加新节点到redis",normalizedTitle:"添加新节点到redis",charIndex:1539},{level:3,title:"转移插槽",slug:"转移插槽",normalizedTitle:"转移插槽",charIndex:1914},{level:3,title:"删除结点",slug:"删除结点",normalizedTitle:"删除结点",charIndex:2333},{level:2,title:"故障转移",slug:"故障转移",normalizedTitle:"故障转移",charIndex:2576},{level:3,title:"自动故障转移",slug:"自动故障转移",normalizedTitle:"自动故障转移",charIndex:2639},{level:3,title:"手动故障转移",slug:"手动故障转移",normalizedTitle:"手动故障转移",charIndex:2896},{level:2,title:"RedisTemplate访问分片集群",slug:"redistemplate访问分片集群",normalizedTitle:"redistemplate访问分片集群",charIndex:3234}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"搭建分片集群 散列插槽 插槽原理 小结 集群伸缩 需求分析 创建新的redis实例 添加新节点到redis 转移插槽 删除结点 故障转移 自动故障转移 手动故障转移 RedisTemplate访问分片集群",content:'Redis分片集群-类似于分布式搜索引擎es的集群\n\n * 分片存储\n * 客户端请求可以访问集群任意节点，最终都会被转发到正确节点\n * 散列插槽\n   * 数据跟插槽绑定\n   * 客户端请求可以访问集群任意节点，最终都会被转发到正确节点——请求路由\n     * 先计算插槽，在根据插槽访问对应结点\n * 集群伸缩\n\n\n# 搭建分片集群\n\n**主从和哨兵可以解决高可用、高并发读的问题。**但是依然有两个问题没有解决：\n\n * 海量数据存储问题\n   \n   * 单点redis，内存小\n\n * 高并发写的问题\n   \n   * 读的slave虽然多，但是负责写的master只有一个。\n\n使用分片集群可以解决上述问题，如图:\n\n\n\n分片集群特征：\n\n * 集群中有多个master，每个master保存不同数据\n\n * 每个master都可以有多个slave节点\n\n * master之间通过ping监测彼此健康状态\n\n * 客户端请求可以访问集群任意节点，最终都会被转发到正确节点\n\n具体搭建流程\n\n\n# 散列插槽\n\n\n# 插槽原理\n\nRedis会把每一个master节点映射到0~16383共16384个插槽（hash slot）上，查看集群信息时就能看到：\n\n\n\n数据key不是与节点绑定，而是与插槽绑定。redis会根据key的有效部分计算插槽值，分两种情况：\n\n * key中包含"{}"，且“{}”中至少包含1个字符，“{}”中的部分是有效部分\n * key中不包含“{}”，整个key都是有效部分\n\n例如：key是num，那么就根据num计算，如果是{itcast}num，则根据itcast计算。计算方式是利用CRC16算法得到一个hash值，然后对16384取余，得到的结果就是slot值。\n\n\n\n如图，在7001这个节点执行set a 1时，对a做hash运算，对16384取余，得到的结果是15495，因此要存储到103节点。\n\n到了7003后，执行get num时，对num做hash运算，对16384取余，得到的结果是2765，因此需要切换到7001节点\n\n\n# 小结\n\nRedis如何判断某个key应该在哪个实例？\n\n * 将16384个插槽分配到不同的实例\n * 根据key的有效部分计算哈希值，对16384取余\n * 余数作为插槽，寻找插槽所在实例即可\n\n如何将同一类数据固定的保存在同一个Redis实例？\n\n * 这一类数据使用相同的有效部分，例如key都以{typeId}为前缀\n   \n   \n\n\n# 集群伸缩\n\nredis-cli --cluster提供了很多操作集群的命令，可以通过下面方式查看：\n\n\n\n比如，添加节点的命令：\n\n\n\n * 不加蓝色部分，默认是一个master\n * 加了就指定它为slave结点，并指定一个master结点\n\n\n# 需求分析\n\n需求：向集群中添加一个新的master节点，并向其中存储 num = 10\n\n * 启动一个新的redis实例，端口为7004\n * 添加7004到之前的集群，并作为一个master节点\n * 给7004节点分配插槽，使得num这个key可以存储到7004实例\n\n这里需要两个新的功能：\n\n * 添加一个节点到集群中\n * 将部分插槽分配到新插槽\n\n\n# 创建新的redis实例\n\n创建一个文件夹：\n\nmkdir 7004\n\n\n拷贝配置文件：\n\ncp redis.conf ./7004\n\n\n修改配置文件：\n\nsed -i /s/6379/7004/g 7004/redis.conf\n\n\n启动\n\nredis-server 7004/redis.conf\n\n\n\n# 添加新节点到redis\n\n添加节点的语法如下：\n\n\n\n执行命令：\n\n * redis-cli --cluster 新版本集群命令 大于5.0\n * ./redis-trib.rb 旧版本集群命令\n\n#旧版本\n./redis-trib.rb add-node 192.168.159.100:7004 192.168.159.100:7001\n\n#新版本\nredis-cli --cluster add-node  192.168.150.101:7004 192.168.150.101:7001\n\n\n通过命令查看集群状态：\n\nredis-cli -p 7001 cluster nodes\n\n\n如图，7004加入了集群，并且默认是一个master节点：\n\n\n\n但是，可以看到7004节点的插槽数量为0，因此没有任何数据可以存储到7004上\n\n\n# 转移插槽\n\n我们要将num存储到7004节点，因此需要先看看num的插槽是多少：\n\n\n\n如上图所示，num的插槽为2765.\n\n我们可以将0~3000的插槽从7001转移到7004，命令格式如下：\n\n\n\n具体命令如下：\n\n建立连接：\n\n\n\n得到下面的反馈：\n\n\n\n询问要移动多少个插槽，我们计划是3000个：\n\n新的问题来了：\n\n\n\n那个node来接收这些插槽？？\n\n显然是7004，那么7004节点的id是多少呢？\n\n\n\n复制这个id，然后拷贝到刚才的控制台后：\n\n\n\n这里询问，你的插槽是从哪里移动过来的？\n\n * all：代表全部，也就是三个节点各转移一部分\n * 具体的id：目标节点的id\n * done：没有了\n\n这里我们要从7001获取，因此填写7001的id：\n\n\n\n填完后，点击done，这样插槽转移就准备好了：\n\n\n\n确认要转移吗？输入yes：\n\n然后，通过命令查看结果：\n\n\n\n可以看到：\n\n\n\n目的达成。\n\n\n# 删除结点\n\n需求： 删除7004实例\n\n不能直接将带槽的结点删除，必须先转移到其他结点上\n\n#将插槽转移到其他结点-7001 (reshard)\nredis-cli --cluster  reshard 192.168.159.100:7004\n\n\n#删除结点-7004 (del-node)\nredis-cli --cluster del-node 192.168.159.100:7004 d4c35f052518c45d9a06a90e08680dade4043752\n\n\n\n# 故障转移\n\n集群初识状态是这样的：\n\n\n\n其中7001、7002、7003都是master，我们计划让7002宕机。\n\n\n# 自动故障转移\n\n当集群中有一个master宕机会发生什么呢？\n\n自动实现主从切换\n\n * 观测 结点状态\n   \n   watch redis-cli -p 7001 cluster nodes\n   \n\n直接停止一个redis实例，例如7002：\n\nredis-cli -p 7002 shutdown\n\n\n1）首先是该实例与其它实例失去连接\n\n2）然后是疑似宕机：\n\n\n\n3）最后是确定下线，自动提升一个slave为新的master：\n\n\n\n4）当7002再次启动，就会变为一个slave节点了：\n\n\n\n\n# 手动故障转移\n\n利用cluster failover命令可以手动让集群中的某个master宕机，切换到执行cluster failover命令的这个slave节点，实现无感知的数据迁移。其流程如下：\n\n\n\n这种failover命令可以指定三种模式：\n\n * 缺省：默认的流程，如图1~6歩\n * force：省略了对offset的一致性校验\n * takeover：直接执行第5歩，忽略数据一致性、忽略master状态和其它master的意见\n\n案例需求：在7002这个slave节点执行手动故障转移，重新夺回master地位\n\n步骤如下：\n\n1）利用redis-cli连接7002这个节点\n\n2）执行cluster failover命令\n\n如图：\n\n\n\n效果：\n\n\n\n\n# RedisTemplate访问分片集群\n\nRedisTemplate底层同样基于lettuce实现了分片集群的支持，而使用的步骤与哨兵模式基本一致：\n\n1）引入redis的starter依赖\n\n2）配置分片集群地址\n\n3）配置读写分离\n\n与哨兵模式相比，其中只有分片集群的配置方式略有差异，如下：\n\nspring:\n  redis:\n    cluster:\n      nodes:\n        - 192.168.150.101:7001\n        - 192.168.150.101:7002\n        - 192.168.150.101:7003\n        - 192.168.150.101:8001\n        - 192.168.150.101:8002\n        - 192.168.150.101:8003\n',normalizedContent:'redis分片集群-类似于分布式搜索引擎es的集群\n\n * 分片存储\n * 客户端请求可以访问集群任意节点，最终都会被转发到正确节点\n * 散列插槽\n   * 数据跟插槽绑定\n   * 客户端请求可以访问集群任意节点，最终都会被转发到正确节点——请求路由\n     * 先计算插槽，在根据插槽访问对应结点\n * 集群伸缩\n\n\n# 搭建分片集群\n\n**主从和哨兵可以解决高可用、高并发读的问题。**但是依然有两个问题没有解决：\n\n * 海量数据存储问题\n   \n   * 单点redis，内存小\n\n * 高并发写的问题\n   \n   * 读的slave虽然多，但是负责写的master只有一个。\n\n使用分片集群可以解决上述问题，如图:\n\n\n\n分片集群特征：\n\n * 集群中有多个master，每个master保存不同数据\n\n * 每个master都可以有多个slave节点\n\n * master之间通过ping监测彼此健康状态\n\n * 客户端请求可以访问集群任意节点，最终都会被转发到正确节点\n\n具体搭建流程\n\n\n# 散列插槽\n\n\n# 插槽原理\n\nredis会把每一个master节点映射到0~16383共16384个插槽（hash slot）上，查看集群信息时就能看到：\n\n\n\n数据key不是与节点绑定，而是与插槽绑定。redis会根据key的有效部分计算插槽值，分两种情况：\n\n * key中包含"{}"，且“{}”中至少包含1个字符，“{}”中的部分是有效部分\n * key中不包含“{}”，整个key都是有效部分\n\n例如：key是num，那么就根据num计算，如果是{itcast}num，则根据itcast计算。计算方式是利用crc16算法得到一个hash值，然后对16384取余，得到的结果就是slot值。\n\n\n\n如图，在7001这个节点执行set a 1时，对a做hash运算，对16384取余，得到的结果是15495，因此要存储到103节点。\n\n到了7003后，执行get num时，对num做hash运算，对16384取余，得到的结果是2765，因此需要切换到7001节点\n\n\n# 小结\n\nredis如何判断某个key应该在哪个实例？\n\n * 将16384个插槽分配到不同的实例\n * 根据key的有效部分计算哈希值，对16384取余\n * 余数作为插槽，寻找插槽所在实例即可\n\n如何将同一类数据固定的保存在同一个redis实例？\n\n * 这一类数据使用相同的有效部分，例如key都以{typeid}为前缀\n   \n   \n\n\n# 集群伸缩\n\nredis-cli --cluster提供了很多操作集群的命令，可以通过下面方式查看：\n\n\n\n比如，添加节点的命令：\n\n\n\n * 不加蓝色部分，默认是一个master\n * 加了就指定它为slave结点，并指定一个master结点\n\n\n# 需求分析\n\n需求：向集群中添加一个新的master节点，并向其中存储 num = 10\n\n * 启动一个新的redis实例，端口为7004\n * 添加7004到之前的集群，并作为一个master节点\n * 给7004节点分配插槽，使得num这个key可以存储到7004实例\n\n这里需要两个新的功能：\n\n * 添加一个节点到集群中\n * 将部分插槽分配到新插槽\n\n\n# 创建新的redis实例\n\n创建一个文件夹：\n\nmkdir 7004\n\n\n拷贝配置文件：\n\ncp redis.conf ./7004\n\n\n修改配置文件：\n\nsed -i /s/6379/7004/g 7004/redis.conf\n\n\n启动\n\nredis-server 7004/redis.conf\n\n\n\n# 添加新节点到redis\n\n添加节点的语法如下：\n\n\n\n执行命令：\n\n * redis-cli --cluster 新版本集群命令 大于5.0\n * ./redis-trib.rb 旧版本集群命令\n\n#旧版本\n./redis-trib.rb add-node 192.168.159.100:7004 192.168.159.100:7001\n\n#新版本\nredis-cli --cluster add-node  192.168.150.101:7004 192.168.150.101:7001\n\n\n通过命令查看集群状态：\n\nredis-cli -p 7001 cluster nodes\n\n\n如图，7004加入了集群，并且默认是一个master节点：\n\n\n\n但是，可以看到7004节点的插槽数量为0，因此没有任何数据可以存储到7004上\n\n\n# 转移插槽\n\n我们要将num存储到7004节点，因此需要先看看num的插槽是多少：\n\n\n\n如上图所示，num的插槽为2765.\n\n我们可以将0~3000的插槽从7001转移到7004，命令格式如下：\n\n\n\n具体命令如下：\n\n建立连接：\n\n\n\n得到下面的反馈：\n\n\n\n询问要移动多少个插槽，我们计划是3000个：\n\n新的问题来了：\n\n\n\n那个node来接收这些插槽？？\n\n显然是7004，那么7004节点的id是多少呢？\n\n\n\n复制这个id，然后拷贝到刚才的控制台后：\n\n\n\n这里询问，你的插槽是从哪里移动过来的？\n\n * all：代表全部，也就是三个节点各转移一部分\n * 具体的id：目标节点的id\n * done：没有了\n\n这里我们要从7001获取，因此填写7001的id：\n\n\n\n填完后，点击done，这样插槽转移就准备好了：\n\n\n\n确认要转移吗？输入yes：\n\n然后，通过命令查看结果：\n\n\n\n可以看到：\n\n\n\n目的达成。\n\n\n# 删除结点\n\n需求： 删除7004实例\n\n不能直接将带槽的结点删除，必须先转移到其他结点上\n\n#将插槽转移到其他结点-7001 (reshard)\nredis-cli --cluster  reshard 192.168.159.100:7004\n\n\n#删除结点-7004 (del-node)\nredis-cli --cluster del-node 192.168.159.100:7004 d4c35f052518c45d9a06a90e08680dade4043752\n\n\n\n# 故障转移\n\n集群初识状态是这样的：\n\n\n\n其中7001、7002、7003都是master，我们计划让7002宕机。\n\n\n# 自动故障转移\n\n当集群中有一个master宕机会发生什么呢？\n\n自动实现主从切换\n\n * 观测 结点状态\n   \n   watch redis-cli -p 7001 cluster nodes\n   \n\n直接停止一个redis实例，例如7002：\n\nredis-cli -p 7002 shutdown\n\n\n1）首先是该实例与其它实例失去连接\n\n2）然后是疑似宕机：\n\n\n\n3）最后是确定下线，自动提升一个slave为新的master：\n\n\n\n4）当7002再次启动，就会变为一个slave节点了：\n\n\n\n\n# 手动故障转移\n\n利用cluster failover命令可以手动让集群中的某个master宕机，切换到执行cluster failover命令的这个slave节点，实现无感知的数据迁移。其流程如下：\n\n\n\n这种failover命令可以指定三种模式：\n\n * 缺省：默认的流程，如图1~6歩\n * force：省略了对offset的一致性校验\n * takeover：直接执行第5歩，忽略数据一致性、忽略master状态和其它master的意见\n\n案例需求：在7002这个slave节点执行手动故障转移，重新夺回master地位\n\n步骤如下：\n\n1）利用redis-cli连接7002这个节点\n\n2）执行cluster failover命令\n\n如图：\n\n\n\n效果：\n\n\n\n\n# redistemplate访问分片集群\n\nredistemplate底层同样基于lettuce实现了分片集群的支持，而使用的步骤与哨兵模式基本一致：\n\n1）引入redis的starter依赖\n\n2）配置分片集群地址\n\n3）配置读写分离\n\n与哨兵模式相比，其中只有分片集群的配置方式略有差异，如下：\n\nspring:\n  redis:\n    cluster:\n      nodes:\n        - 192.168.150.101:7001\n        - 192.168.150.101:7002\n        - 192.168.150.101:7003\n        - 192.168.150.101:8001\n        - 192.168.150.101:8002\n        - 192.168.150.101:8003\n',charsets:{cjk:!0}},{title:"Redis通信协议",frontmatter:{autoSort:8,title:"Redis通信协议",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/11137c/",categories:["后端","数据库","Redis"],tags:["知识","数据库","Redis"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/05.Redis/45.Redis%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.html",relativePath:"01.后端/30.数据库/05.Redis/45.Redis网络模型.md",key:"v-351e657c",path:"/pages/11137c/",headers:[{level:2,title:"Redis通信协议",slug:"redis通信协议",normalizedTitle:"redis通信协议",charIndex:2},{level:2,title:"基于Socket自定义Redis的客户端",slug:"基于socket自定义redis的客户端",normalizedTitle:"基于socket自定义redis的客户端",charIndex:733}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"Redis通信协议 基于Socket自定义Redis的客户端",content:'# Redis通信协议\n\nRedis是一个CS架构的软件，通信一般分两步（不包括pipeline和PubSub）：\n\n客户端（client）向服务端（server）发送一条命令\n\n服务端解析并执行命令，返回响应结果给客户端\n\n因此客户端发送命令的格式、服务端响应结果的格式必须有一个规范，这个规范就是通信协议。\n\n而在Redis中采用的是RESP（Redis Serialization Protocol）协议：\n\nRedis 1.2版本引入了RESP协议\n\nRedis 2.0版本中成为与Redis服务端通信的标准，称为RESP2\n\nRedis 6.0版本中，从RESP2升级到了RESP3协议，增加了更多数据类型并且支持6.0的新特性--客户端缓存\n\n但目前，默认使用的依然是RESP2协议，也是我们要学习的协议版本（以下简称RESP）。\n\n在RESP中，通过首字节的字符来区分不同数据类型，常用的数据类型包括5种：\n\n单行字符串：首字节是 ‘+’ ，后面跟上单行字符串，以CRLF（ "\\r\\n" ）结尾。例如返回"OK"： "+OK\\r\\n"\n\n错误（Errors）：首字节是 ‘-’ ，与单行字符串格式一样，只是字符串是异常信息，例如："-Error message\\r\\n"\n\n数值：首字节是 ‘:’ ，后面跟上数字格式的字符串，以CRLF结尾。例如：":10\\r\\n"\n\n多行字符串：首字节是 ‘$’ ，表示二进制安全的字符串，最大支持512MB：\n\n如果大小为0，则代表空字符串："$0\\r\\n\\r\\n"\n\n如果大小为-1，则代表不存在："$-1\\r\\n"\n\n数组：首字节是 ‘*’，后面跟上数组元素个数，再跟上元素，元素数据类型不限:\n\n\n\n\n# 基于Socket自定义Redis的客户端\n\nRedis支持TCP通信，因此我们可以使用Socket来模拟客户端，与Redis服务端建立连接：\n\npublic class Main {\n\n    static Socket s;\n    static PrintWriter writer;\n    static BufferedReader reader;\n\n    public static void main(String[] args) {\n        try {\n            // 1.建立连接\n            String host = "192.168.150.101";\n            int port = 6379;\n            s = new Socket(host, port);\n            // 2.获取输出流、输入流\n            writer = new PrintWriter(new OutputStreamWriter(s.getOutputStream(), StandardCharsets.UTF_8));\n            reader = new BufferedReader(new InputStreamReader(s.getInputStream(), StandardCharsets.UTF_8));\n\n            // 3.发出请求\n            // 3.1.获取授权 auth 123321\n            sendRequest("auth", "123321");\n            Object obj = handleResponse();\n            System.out.println("obj = " + obj);\n\n            // 3.2.set name 虎哥\n            sendRequest("set", "name", "虎哥");\n            // 4.解析响应\n            obj = handleResponse();\n            System.out.println("obj = " + obj);\n\n            // 3.2.set name 虎哥\n            sendRequest("get", "name");\n            // 4.解析响应\n            obj = handleResponse();\n            System.out.println("obj = " + obj);\n\n            // 3.2.set name 虎哥\n            sendRequest("mget", "name", "num", "msg");\n            // 4.解析响应\n            obj = handleResponse();\n            System.out.println("obj = " + obj);\n        } catch (IOException e) {\n            e.printStackTrace();\n        } finally {\n            // 5.释放连接\n            try {\n                if (reader != null) reader.close();\n                if (writer != null) writer.close();\n                if (s != null) s.close();\n            } catch (IOException e) {\n                e.printStackTrace();\n            }\n        }\n    }\n\n    private static Object handleResponse() throws IOException {\n        // 读取首字节\n        int prefix = reader.read();\n        // 判断数据类型标示\n        switch (prefix) {\n            case \'+\': // 单行字符串，直接读一行\n                return reader.readLine();\n            case \'-\': // 异常，也读一行\n                throw new RuntimeException(reader.readLine());\n            case \':\': // 数字\n                return Long.parseLong(reader.readLine());\n            case \'$\': // 多行字符串\n                // 先读长度\n                int len = Integer.parseInt(reader.readLine());\n                if (len == -1) {\n                    return null;\n                }\n                if (len == 0) {\n                    return "";\n                }\n                // 再读数据,读len个字节。我们假设没有特殊字符，所以读一行（简化）\n                return reader.readLine();\n            case \'*\':\n                return readBulkString();\n            default:\n                throw new RuntimeException("错误的数据格式！");\n        }\n    }\n\n    private static Object readBulkString() throws IOException {\n        // 获取数组大小\n        int len = Integer.parseInt(reader.readLine());\n        if (len <= 0) {\n            return null;\n        }\n        // 定义集合，接收多个元素\n        List<Object> list = new ArrayList<>(len);\n        // 遍历，依次读取每个元素\n        for (int i = 0; i < len; i++) {\n            list.add(handleResponse());\n        }\n        return list;\n    }\n\n    // set name 虎哥\n    private static void sendRequest(String ... args) {\n        writer.println("*" + args.length);\n        for (String arg : args) {\n            writer.println("$" + arg.getBytes(StandardCharsets.UTF_8).length);\n            writer.println(arg);\n        }\n        writer.flush();\n    }\n}\n\n',normalizedContent:'# redis通信协议\n\nredis是一个cs架构的软件，通信一般分两步（不包括pipeline和pubsub）：\n\n客户端（client）向服务端（server）发送一条命令\n\n服务端解析并执行命令，返回响应结果给客户端\n\n因此客户端发送命令的格式、服务端响应结果的格式必须有一个规范，这个规范就是通信协议。\n\n而在redis中采用的是resp（redis serialization protocol）协议：\n\nredis 1.2版本引入了resp协议\n\nredis 2.0版本中成为与redis服务端通信的标准，称为resp2\n\nredis 6.0版本中，从resp2升级到了resp3协议，增加了更多数据类型并且支持6.0的新特性--客户端缓存\n\n但目前，默认使用的依然是resp2协议，也是我们要学习的协议版本（以下简称resp）。\n\n在resp中，通过首字节的字符来区分不同数据类型，常用的数据类型包括5种：\n\n单行字符串：首字节是 ‘+’ ，后面跟上单行字符串，以crlf（ "\\r\\n" ）结尾。例如返回"ok"： "+ok\\r\\n"\n\n错误（errors）：首字节是 ‘-’ ，与单行字符串格式一样，只是字符串是异常信息，例如："-error message\\r\\n"\n\n数值：首字节是 ‘:’ ，后面跟上数字格式的字符串，以crlf结尾。例如：":10\\r\\n"\n\n多行字符串：首字节是 ‘$’ ，表示二进制安全的字符串，最大支持512mb：\n\n如果大小为0，则代表空字符串："$0\\r\\n\\r\\n"\n\n如果大小为-1，则代表不存在："$-1\\r\\n"\n\n数组：首字节是 ‘*’，后面跟上数组元素个数，再跟上元素，元素数据类型不限:\n\n\n\n\n# 基于socket自定义redis的客户端\n\nredis支持tcp通信，因此我们可以使用socket来模拟客户端，与redis服务端建立连接：\n\npublic class main {\n\n    static socket s;\n    static printwriter writer;\n    static bufferedreader reader;\n\n    public static void main(string[] args) {\n        try {\n            // 1.建立连接\n            string host = "192.168.150.101";\n            int port = 6379;\n            s = new socket(host, port);\n            // 2.获取输出流、输入流\n            writer = new printwriter(new outputstreamwriter(s.getoutputstream(), standardcharsets.utf_8));\n            reader = new bufferedreader(new inputstreamreader(s.getinputstream(), standardcharsets.utf_8));\n\n            // 3.发出请求\n            // 3.1.获取授权 auth 123321\n            sendrequest("auth", "123321");\n            object obj = handleresponse();\n            system.out.println("obj = " + obj);\n\n            // 3.2.set name 虎哥\n            sendrequest("set", "name", "虎哥");\n            // 4.解析响应\n            obj = handleresponse();\n            system.out.println("obj = " + obj);\n\n            // 3.2.set name 虎哥\n            sendrequest("get", "name");\n            // 4.解析响应\n            obj = handleresponse();\n            system.out.println("obj = " + obj);\n\n            // 3.2.set name 虎哥\n            sendrequest("mget", "name", "num", "msg");\n            // 4.解析响应\n            obj = handleresponse();\n            system.out.println("obj = " + obj);\n        } catch (ioexception e) {\n            e.printstacktrace();\n        } finally {\n            // 5.释放连接\n            try {\n                if (reader != null) reader.close();\n                if (writer != null) writer.close();\n                if (s != null) s.close();\n            } catch (ioexception e) {\n                e.printstacktrace();\n            }\n        }\n    }\n\n    private static object handleresponse() throws ioexception {\n        // 读取首字节\n        int prefix = reader.read();\n        // 判断数据类型标示\n        switch (prefix) {\n            case \'+\': // 单行字符串，直接读一行\n                return reader.readline();\n            case \'-\': // 异常，也读一行\n                throw new runtimeexception(reader.readline());\n            case \':\': // 数字\n                return long.parselong(reader.readline());\n            case \'$\': // 多行字符串\n                // 先读长度\n                int len = integer.parseint(reader.readline());\n                if (len == -1) {\n                    return null;\n                }\n                if (len == 0) {\n                    return "";\n                }\n                // 再读数据,读len个字节。我们假设没有特殊字符，所以读一行（简化）\n                return reader.readline();\n            case \'*\':\n                return readbulkstring();\n            default:\n                throw new runtimeexception("错误的数据格式！");\n        }\n    }\n\n    private static object readbulkstring() throws ioexception {\n        // 获取数组大小\n        int len = integer.parseint(reader.readline());\n        if (len <= 0) {\n            return null;\n        }\n        // 定义集合，接收多个元素\n        list<object> list = new arraylist<>(len);\n        // 遍历，依次读取每个元素\n        for (int i = 0; i < len; i++) {\n            list.add(handleresponse());\n        }\n        return list;\n    }\n\n    // set name 虎哥\n    private static void sendrequest(string ... args) {\n        writer.println("*" + args.length);\n        for (string arg : args) {\n            writer.println("$" + arg.getbytes(standardcharsets.utf_8).length);\n            writer.println(arg);\n        }\n        writer.flush();\n    }\n}\n\n',charsets:{cjk:!0}},{title:"《数据库》",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"01.后端/30.数据库",imgUrl:"/assets/img/数据库.jpg",description:"数据库学习笔记--整理自黑马程序员，在原教程基础上添加学习笔记"}},title:"《数据库》",date:"2023-06-30T20:30:40.000Z",permalink:"/back/sql/",article:!1,comment:!1,editLink:!1,author:{name:"凉冰",link:"https://github.com/diana-devil"}},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/",relativePath:"01.后端/30.数据库/README.md",key:"v-c3bd5d9a",path:"/back/sql/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"Spring 基础",frontmatter:{autoSort:100,title:"Spring 基础",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/b2f859/",categories:["后端","SSM"],tags:["知识","SSM"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/40.SSM/05.Spring.html",relativePath:"01.后端/40.SSM/05.Spring.md",key:"v-701ec247",path:"/pages/b2f859/",headers:[{level:2,title:"Spring 优势",slug:"spring-优势",normalizedTitle:"spring 优势",charIndex:2},{level:2,title:"Spring 体系结构",slug:"spring-体系结构",normalizedTitle:"spring 体系结构",charIndex:529},{level:2,title:"Spring 开发步骤",slug:"spring-开发步骤",normalizedTitle:"spring 开发步骤",charIndex:547},{level:2,title:"Spring重点配置",slug:"spring重点配置",normalizedTitle:"spring重点配置",charIndex:1092},{level:2,title:"Spring注解开发",slug:"spring注解开发",normalizedTitle:"spring注解开发",charIndex:5048},{level:2,title:"Spring 集成Junit",slug:"spring-集成junit",normalizedTitle:"spring 集成junit",charIndex:5144},{level:2,title:"Spring集成Web",slug:"spring集成web",normalizedTitle:"spring集成web",charIndex:6120},{level:2,title:"Spring -AOP",slug:"spring-aop",normalizedTitle:"spring -aop",charIndex:7127},{level:2,title:"Spring 事务控制",slug:"spring-事务控制",normalizedTitle:"spring 事务控制",charIndex:15673}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"Spring 优势 Spring 体系结构 Spring 开发步骤 Spring重点配置 Spring注解开发 Spring 集成Junit Spring集成Web Spring -AOP Spring 事务控制",content:'# Spring 优势\n\n 1. 方便解耦，简化开发（bean）\n    \n    * 通过spring提供的IOC容器 可以将对象间的依赖关系交由spring进行控制。\n    * IOC :使用对象时，由主动new产生对象转换为由外部提供对象,此过程中对象创建控制权由程序转移到外部，此思想称为控制反转。\n\n 2. AOP编程支持\n    \n    * 通过Spring的AOP功能，方便的面向切面编程，许多不容易用传统OOP（面向对象编程）实现的功能都可以通过AOP实现。\n\n 3. 声明式事务的支持\n    \n    * 通过声明式灵活的进行事务管理，提高开发效率和质量。\n\n 4. 方便程序的测试 （Spring-junit）\n    \n    * 可以用非容器依赖的编程方式进行几乎所有的测试工作。\n\n 5. 方便集成各种优秀框架（Spring-mybatis）\n\n 6. 降低JavaEE API的使用难度（JDBC模板）\n    \n    * Spirng对JavaEE API 进行了薄薄的封装层，提供给用户一个模板来使用。\n\n 7. Java 源码是经典学习范例\n    \n    * Spring 的源代码设计精妙，值得学习。\n\n\n# Spring 体系结构\n\n\n\n\n# Spring 开发步骤\n\n 1. 导入坐标。\n    \n    <dependency>\n      <groupId>org.springframework</groupId>\n      <artifactId>spring-context</artifactId>\n      <version>5.0.5.RELEASE</version>\n    </dependency>\n    \n\n 2. 创建Bean，Dao(Mapper)目录下的接口加实现类。\n\n 3. 创建applicationContext.xml,并进行相关配置。\n    \n    <bean id="user" class="com.diana.Dao_Mapper.impl.UserImpl"></bean>\n    \n\n 4. 创建ApplicationContext对象，使用getBean方法得到对象。\n    \n    ApplicationContext app=new ClassPathXmlApplicationContext("applicationContext.xml");\n    User user = (User) app.getBean("user");\n    \n\n\n# Spring重点配置\n\n>  1. <bean> 标签\n>     \n>     * id属性： 在容器中Bean实例的唯一标识，不允许重复\n>     \n>     * class属性： 要实例化的Bean的全限定名\n>     \n>     * scope属性：Bean的作用范围，常用是Singleton(默认)和prototype\n>     \n>     * init-method属性: 初始化方法\n>     \n>     * destroy-method属性: 销毁方法\n>     \n> \n>  2. set方法注入 使用<property>标签 自己开发的模块 推荐使用set方法注入\n>     \n>     >  * name是属性名称\n>     >    \n>     >    name="user",  user是setUser 的set后面的部分的小写，即  setUser——User——user  一般来说即属性名 user\n>     >    \n>     > \n>     >  * 普通数据类型-----value\n>     >    \n>     >    <property name="username" value="张三"/>\n>     >    \n>     > \n>     >  * 集合数据类型-----list，map，props，set,array\n>     >    \n>     >    * list\n>     >      \n>     >      <property name="strList">\n>     >          <list>\n>     >              <value>aaa</value>\n>     >              <value>bb</value>\n>     >              <value>ccc</value>\n>     >          </list>\n>     >      </property>\n>     >      \n>     >    \n>     >    * map\n>     >      \n>     >      <property name="brandMap">\n>     >          <map>\n>     >              <entry key="key1" value-ref="brand1"/>\n>     >              <entry key="key2" value-ref="brand2"/>\n>     >          </map>\n>     >      </property>\n>     >      \n>     >    \n>     >    * props\n>     >      \n>     >      <property name="properties">\n>     >          <props>\n>     >              <prop key="p1">ppp1</prop>\n>     >              <prop key="p2">ppp2</prop>\n>     >              <prop key="p3">ppp3</prop>\n>     >          </props>\n>     >      </property>\n>     >      \n>     > \n>     >  * 引用数据类型-----ref\n>     >    \n>     >     <property name="user" ref="user">\n>     >    \n> \n>  3. <constructor-arg>标签-- 有参构造函数注入 不推荐\n>     \n>     <constructor-arg name="user2" ref="user"  />\n>     \n> \n>  4. <import> 标签：导入其他的Spring的分文件\n>     \n>     <import resource="applicationContext-insert.xml"/>  \x3c!--注入方法--\x3e\n>     \n> \n>  5. 加载外部配置property ,使用el表达式来获取值 ---引入数据源\n>     \n>     <context:property-placeholder location="classpath:jdbc.properties"/>\n>     \x3c!-- el表达式 来获取配置文件信息--\x3e\n>         <bean id="dataSource_c3p0" class="com.mchange.v2.c3p0.ComboPooledDataSource">\n>             <property name="driverClass" value="${jdbc.driver}"/>\n>             <property name="user" value="${jdbc.username}"/>\n>             <property name="password" value="${jdbc.password}"/>\n>             <property name="jdbcUrl" value="${jdbc.url}"/>\n>         </bean>\n>     \n> \n>  6. 组件扫描，配合注解开发使用\n>     \n>     <context:component-scan base-package="com.diana"/>\n>     \n> \n>  7. 实例化方法\n>     \n>     * 无参构造\n>       \n>       <bean id="user" class="com.diana.Dao_Mapper.impl.UserImpl"/>\n>       \n>     \n>     * 工厂静态方法实例化\n>       \n>       <bean id="user" class="com.diana.factory.StaticFactory" factory-method="getUser"/>\n>       \n>       //实例化bean  方式二  工厂静态方法实例化\n>       public class StaticFactory {\n>           public static User getUser(){\n>               return new UserImpl();\n>           }\n>       }\n>       \n>     \n>     * 工厂实例方法实例化\n>       \n>       <bean id="factory" class="com.diana.factory.DynamicFactory"/>\n>       <bean id="user" factory-bean="factory" factory-method="getUser"/>\n>            //实例化bean  方式三  工厂实例方法实例化\n>        public class DynamicFactory {\n>            public  User getUser(){\n>                return new UserImpl();\n>            }\n>        }\n>       \n>       \n>     \n>     * 使用FactoryBean实例化\n>       \n>       \x3c!--使用FactoryBean实例化--\x3e\n>       <bean id="user" class="com.diana.factory.UserFactoryBean"/>\n>        //实例化bean  方式四  使用FactoryBean实例化\n>       public class UserFactoryBean implements FactoryBean<User> {\n>       \n>        //代替原始实例工厂中创建对象的方法\n>        public User getObject() throws Exception {\n>            return new UserImpl();\n>        }\n>       \n>        //得到Bean的类型\n>        public Class<?> getObjectType() {\n>            return User.class;\n>        }\n>       \n>        //是否单例   工厂一般都是单例    所有这个方法一般不写  默认单例\n>        public boolean isSingleton() {\n>            return true;\n>        }\n>       }\n>       \n>       \n\n\n# Spring注解开发\n\n在Dao_Mapper--impl--UserImpl 中\n\n在demo-- UserController ，config 中\n\nXML配置与注解配置对比\n\n\n\n\n# Spring 集成Junit\n\n>  1. 导入spring集成Junit的坐标和Junit的坐标。\n>     \n>     * spring-context spring-test 两个版本要一致\n>     * junit 版本要在4.12 及以上\n> \n>  2. 使用@Runwith注解替换原来的运行期\n> \n>  3. 使用@ContextConfiguration指定配置文件或配置类\n> \n>  4. 使用@Autowired注入需要测试的对象\n> \n>  5. 创建测试方法进行测试\n>     \n>     @RunWith(SpringJUnit4ClassRunner.class)\n>     //@ContextConfiguration("classpath:applicationContext.xml")//配置文件方式\n>     @ContextConfiguration(classes = {SpringConfiguration.class})//注解类方式\n>     public class SpringJuintTest {\n>                                                     \n>         @Autowired\n>         private UserService userService;\n>                                                     \n>         @Autowired\n>         private DataSource dataSource;\n>                                                     \n>         @Test\n>         public void test1() throws SQLException {\n>             userService.save();\n>             System.out.println(dataSource.getConnection());\n>         }\n>     }\n>     \n\n\n# Spring集成Web\n\n放到了SpringMVC项目下\n\n>  1. ApplicationContext应用上下文获取方式\n>     \n>     * 在web.xml中配置ContextLoaderListener监听器 (导入spring-web坐标)\n>       \n>       \x3c!-- spring web监听器--\x3e\n>       <dependency>\n>         <groupId>org.springframework</groupId>\n>         <artifactId>spring-web</artifactId>\n>         <version>5.3.6</version>\n>       </dependency>\n>       \n>       \n>       \x3c!--配置监听器  来自动加载Spring应用上下文--\x3e\n>       <listener>\n>         <listener-class>org.springframework.web.context.ContextLoaderListener</listener-class>\n>       </listener>\n>       \n>     \n>     * 使用WebApplicationContextUtils获得应用上下文对象 ApplicationContext\n>       \n>       //调用Spring封装好的实现\n>       //得到servletContext  最大的域\n>       ServletContext servletContext = this.getServletContext();\n>       ApplicationContext app = WebApplicationContextUtils.getWebApplicationContext(servletContext);\n>       \n> \n>  2. 从容器中得到userService对象\n>     \n>     UserService userService = app.getBean(UserService.class); userService.save();\n>     \n>     \n>     \n\n\n# Spring -AOP\n\nAop 简化共性功能，方便开发\n\n# 一、AOP的基础知识\n\n 1. 什么是AOP\n    \n    >  * AOP 是面向切面编程，是通过 预编译方式和运行期动态代理实现程序功能的统一维护的一种技术。\n    >    * 动态代理： 在不修改源码的基础上，对目标方法进行增强。作用：完成代码间的松耦合\n    >  * 利用AOP可以对业务逻辑的各个部分进行隔离，从而使得业务逻辑各个部分之间的耦合度降低，提高程序的可重用性，提升开发效率。\n\n 2. AOP的作用及其优势\n    \n    >  * 作用： 在程序运行期间，在不修改源码的情况下，对方法进行功能增强。\n    > \n    >  * 优势：减少重复代码，提高开发效率，便于维护。\n\n 3. AOP 流程\n    \n    >  * 一个目标方法和一个功能增强方法就被称为一个切面。\n    >  * 将目标方法和功能增强方法 在运行时放到一起，达到功能增强的目的，同时达到松耦合的目的。\n    >  * 目标方法（save，update，delete），功能增强方法（日志控制），源码分离，在配置文件中声明 运行时进行结合。\n    \n    \n\n 4. AOP的动态代理技术\n    \n    > JDK动态代理 要求 目标对象必须有一个接口\n    > \n    > cglib动态代理 会生成一个目标对象的子类 从而实现动态代理\n    \n    \n\n 5. AOP相关概念\n    \n    >  * Target(目标对象)： 代理的目标对象。\n    >  * Proxy(代理)：一个类被AOP织入增强后，就产生一个结果代理类。\n    >  * Joinpoint（连接点）----\x3e（可以被增强的目标方法）： 所谓连接点是值那些被拦截到的点（方法）。在Spring中，这些点值得是方法，因为spring只支持方法类型的连接点。\n    >  * Pointcut（切入点/切点）----\x3e（你配置的，真正被增强的目标方法，是连接点的一部分）：所谓切入点是指我们要对哪些Joinpoint进行拦截的定义。\n    >  * Advice（通知/增强）----\x3e（用来增强目标方法的增强方法）： 所谓通知是指 拦截到Joinpoint之后 所要做的事情就是通知。\n    >  * Aspect（切面）：是切入点和通知的结合\n    >  * Weaving（织入）----\x3e（把切入点和通知结合的过程）：是指把增强应用到目标对象来创建新的代理对象的过程，采用配置文件实现，配置的过程就是织入的过程。spring采用动态代理织入，而AspecttJ 采用编译期织入和类装载期织入\n\n 6. AOP开发明确事项\n    \n    >  * 需要编写的内容\n    >    \n    >    * 编写核心业务代码（目标类的目标方法）\n    >    * 编写切面类，切面类中有通知(增强功能方法)\n    >    * 在配置文件中，配置织入关系，即将哪些通知与哪些切入点进行结合\n    > \n    >  * AOP技术实现的内容（Spring帮你实现动态代理的过程）\n    >    \n    >    Spring框架监控切入点方法的执行。一旦监控到切入点方法被运行，使用代理机制，动态创建目标对象的代理对象，根据通知类别，在代理对象的对应位置，将通知对应的功能织入，完成完整的代码逻辑运行。\n    > \n    >  * AOP底层代理方式\n    >    \n    >    在Spring中，框架会根据目标类是否实现了接口来决定采用哪种动态代理的方式。\n    >    \n    >    有接口-----\x3e jdk动态代理 无接口----\x3ecglib动态代理\n\n# 二、基于XML的AOP开发\n\n 1. 开发步骤\n    \n    >  * 导入坐标\n    >    \n    >    \x3c!-- aspectj  aop 配置--\x3e\n    >    <dependency>\n    >      <groupId>org.aspectj</groupId>\n    >      <artifactId>aspectjweaver</artifactId>\n    >      <version>1.9.4</version>\n    >    </dependency>\n    >    \n    > \n    >  * 创建目标接口和目标类(内部有切点)\n    > \n    >  * 创建切面类（内部有增强方法）\n    > \n    >  * 将目标类和切面类的对象创建权交给spring容器\n    >    \n    >    \x3c!-- 切面对象--\x3e\n    >    <bean id="aspect" class="com.diana.aop.MyAspect"/>\n    >    \n    >    \x3c!--目标对象--\x3e\n    >    <bean id="target" class="com.diana.aop.impl.TargetImpl"/>\n    >    \n    > \n    >  * 在配置文件中配置织入关系\n    >    \n    >    \x3c!--     配置织入:  告诉spring框架  哪些方法（切点）需要进行哪些增强（前置，后置，……）--\x3e\n    >    <aop:config>\n    >        \x3c!-- 声明切面--\x3e\n    >        <aop:aspect ref="aspect">\n    >            \x3c!--切面：切点+通知--\x3e\n    >            \x3c!--对save方法 使用before方法 进行前置增强--\x3e\n    >            <aop:before method="before1" pointcut="execution(public void com.diana.aop.impl.TargetImpl.save())"/>\n    >        </aop:aspect>\n    >    </aop:config>\n    >    \n    > \n    >  * 测试代码\n    >    \n    >    @RunWith(SpringJUnit4ClassRunner.class)\n    >    @ContextConfiguration("classpath:applicationContext-aop.xml")\n    >    //测试AOP\n    >    public class test_aop {\n    >        @Autowired\n    >        private Target target;\n    >        @Test\n    >        //测试前置增强方法\n    >        public void test_before(){\n    >            target.save();//调用切点\n    >        }\n    >    }\n    >    \n\n 2. 切点表达式的写法\n    \n    > 表达式语法 execution（[修饰符] 返回值类型 包名 . 类名 . 方法名(参数)）\n    > \n    >  * 访问修饰符可以省略\n    > \n    >  * 返回值类型、包名、类名、方法名可以使用※代表任意\n    > \n    >  * 包名与类名之间一个点代表当前包下的类，两个点 .. (不推荐，效率低)表示当前包及其子包下的类\n    > \n    >  * 参数列表可以使用两个点 表示任意个数，任意类型的参数列表\n    >    \n    >    1. 表示aop当前包下的Target类的method方法，无参数，返回值类型为空。\n    >    2. 表示aop当前包下的Target类的所有方法，任意参数，返回值类型为空。\n    >    3. 表示aop当前包下的所有类的所有方法，任意参数，返回值类型为任意。-------最常用\n    >    4. 表示aop当前包及其子包下的所有类的所有方法，任意参数，返回值类型为任意。\n    >    5. 表示任意包当前包及其子包下的所有类的所有方法，任意参数，返回值类型为任意。\n\n 3. 切点表达式的书写技巧\n    \n    \n    \n    示例\n    \n    //表示任意_service的所有的find_  方法\n    execution(* com.diana.*.*Service.find*(..))\n    \n    //表示任意_service的所有的save方法\n    execution(* com.diana.*.*Service.save(..))\n    \n\n 4. AOP通知类型 环绕通知是重点\n    \n    \n    \n    \x3c!--切面：切点+通知--\x3e\n    \x3c!--对save方法 使用before方法 进行前置增强--\x3e\n    <aop:before method="before1" pointcut="execution(public void com.diana.aop.impl.TargetImpl.save())"/>\n    \x3c!--对aop当前包及其子包下所有方法 使用before方法 进行前置增强--\x3e\n    <aop:before method="before1" pointcut="execution(* com.diana.aop..*.*(..))"/>\n    \x3c!-- 后置增强--\x3e\n    <aop:after-returning method="afterReturning" pointcut="execution(* com.diana.aop..*.*(..))"/>\n    \x3c!-- 环绕增强--\x3e\n    <aop:around method="around" pointcut="execution(* com.diana.aop..*.*(..))"/>\n    \x3c!-- 异常抛出增强--\x3e\n    <aop:after-throwing method="afterThrowing" pointcut="execution(* com.diana.aop..*.*(..))"/>\n    \x3c!-- 最终增强--\x3e\n    <aop:after method="after" pointcut="execution(* com.diana.aop..*.*(..))"/>\n    \n\n 5. 切点表达式的抽取\n    \n    \x3c!-- 抽取切点表达式--\x3e\n    <aop:pointcut id="mypointcut" expression="execution(* com.diana.aop..*.*(..))"/>\n    \n    \n    \x3c!-- 使用抽取后的切点表达式--\x3e\n    <aop:before method="before1" pointcut-ref="mypointcut"/>\n    <aop:around method="around" pointcut-ref="mypointcut"/>\n    \n\n# 三、基于注解的AOP开发\n\n 1. 开发步骤\n    \n    >  * 导入坐标\n    >    \n    >    \x3c!-- aspectj  aop 配置--\x3e\n    >    <dependency>\n    >      <groupId>org.aspectj</groupId>\n    >      <artifactId>aspectjweaver</artifactId>\n    >      <version>1.9.4</version>\n    >    </dependency>\n    >    \n    > \n    >  * 创建目标接口和目标类(内部有切点)\n    > \n    >  * 创建切面类（内部有增强方法）\n    > \n    >  * 将目标类和切面类的对象创建权交给spring容器（@Component）\n    > \n    >  * 在切面类中使用注解配置织入关系\n    >    \n    >    @Component("aspect")\n    >    @Aspect//标注当前类是一个切面类\n    >    public class MyAspect {\n    >    \n    >        //前置增强\n    >        @Before("execution(* com.diana.aop_ZJ..*.*(..))")\n    >        @Before("myponitcut()") //抽取切点表达式后使用\n    >        @Before("MyAspect.myponitcut()")//抽取切点表达式后使用\n    >        public void before1(){\n    >            System.out.println("前置增强……");\n    >        }\n    >    \n    >        //后置增强\n    >        @AfterReturning("execution(* com.diana.aop_ZJ..*.*(..))")\n    >        public void afterReturning(){\n    >            System.out.println("后置增强……");\n    >        }\n    >    \n    >        //环绕增强\n    >        @Around("execution(* com.diana.aop_ZJ..*.*(..))")\n    >        public Object around(ProceedingJoinPoint pjp) throws Throwable {\n    >            //Proceeding JoinPoint: 正在执行的连接点   切点\n    >            System.out.println("环绕前增强……");\n    >            //执行切点方法\n    >            Object proceed = pjp.proceed();\n    >            System.out.println("环绕后增强……");\n    >            return proceed;\n    >        }\n    >    \n    >        //异常抛出后增强\n    >        @AfterThrowing("execution(* com.diana.aop_ZJ..*.*(..))")\n    >        public void afterThrowing(){\n    >            System.out.println("抛出了异常……");\n    >        }\n    >    \n    >        //最终增强  不管有没有异常，都要执行\n    >        @After("execution(* com.diana.aop_ZJ..*.*(..))")\n    >        public void after(){\n    >            System.out.println("最终增强");\n    >        }\n    >        \n    >        //定义切点表达式\n    >        @Pointcut("execution(* com.diana.aop_ZJ..*.*(..))")\n    >        public void myponitcut(){}\n    >    }\n    >    \n    > \n    >  * 在配置文件中开启组件扫描和AOP的自动代理\n    >    \n    >    \x3c!--配置组件扫描--\x3e  \x3c!--配合注解开发使用--\x3e\n    >    <context:component-scan base-package="com.diana.aop_ZJ"/>\n    >    \n    >    \x3c!-- aop 自动代理--\x3e\n    >    <aop:aspectj-autoproxy/>\n    >    \n    > \n    >  * 使用纯注解开发 不留配置文件 --解决自动代理问题即可\n    >    \n    >    //启动AOP自动代理     代替 <aop:aspectj-autoproxy/>\n    >    @EnableAspectJAutoProxy\n    >    \n    >    //\x3c!--配置组件扫描--\x3e  \x3c!--配合注解开发使用--\x3e\n    >    //<context:component-scan base-package="com.diana"/>\n    >    @ComponentScan("com.diana")\n    >    \n    > \n    >  * 测试代码\n    >    \n    >    @RunWith(SpringJUnit4ClassRunner.class)\n    >    @ContextConfiguration("classpath:applicationContext-aop_ZJ.xml")\n    >    public class test_aop_ZJ {\n    >                    \n    >        @Autowired\n    >        Target target=new TargetImpl();\n    >                    \n    >        @Test\n    >        public void test(){\n    >            target.save();//调用切点\n    >        }\n    >    }\n    >    \n\n 2. 切点表达式的抽取\n    \n    > 在切面内定义方法，在该方法上使用@Pointcut注解定义切点表达式，然后在增强注解中引用。\n    > \n    > //定义切点表达式\n    > @Pointcut("execution(* com.diana.aop_ZJ..*.*(..))")\n    > public void myponitcut(){}\n    > \n    > \n    > //    @Before("execution(* com.diana.aop_ZJ..*.*(..))")\n    > //    @Before("myponitcut()") //抽取切点表达式后使用\n    > @Before("MyAspect.myponitcut()")//抽取切点表达式后使用\n    > public void before1(){\n    >     System.out.println("前置增强……");\n    > }\n\n# 四、AOP通知获得数据\n\n * 项目在 my_case_demo2中，\n   * 测试代码test-test_aop_param\n   * aop代码 aop-getParam\n * 获取切入点方法的参数\n   * JoinPoint ：适用于前置，后置，最终通知，抛出异常后 通知\n   * ProceedJointPoint：适用于环绕通知, 可以修改参数，以到达某些目的\n * 获取切入点方法返回值\n   * 后置\n   * 环绕通知\n * 获取切入点方法运行异常信息 （了解）\n   * 抛出异常后通知\n   * 环绕通知\n\n\n# Spring 事务控制\n\n测试代码在Controller层 一共三个，一个xml配置，一个半注解，一个纯注解\n\n# 一、编程式事务控制相关对象\n\n 1. PlatformTransactionManager （我们要告诉spring 用哪个实现类）\n    \n    >  * 该接口是Spring的事务管理器，它里面提供了常用的操作事务的方法。\n    >    \n    >    * TransactionStatus getTransaction(TransactionDefination defination) 获取事务的状态信息\n    >    * void commit(TransactionStatus status) 提交事务\n    >    * void rollback(TransactionStatus status) 回滚事务\n    > \n    >  * 该接口对应不同的Dao层有不同的实现类。 jdbc，mybatis 是org.springframework.jdbc.datasource.DataSourceTransactionManager\n\n 2. TransactionDefinition （我们要告诉spring 传入哪些参数，下面方法需要用到的）\n    \n    >  * TransactionDefinition它是事务的定义信息对象，里面如下方法\n    >    \n    >    * int getIsolationLevel() 获得事务的隔离级别\n    >    \n    >    * int getPropogationBehavior() 获得事务的传播行为\n    >    \n    >    * int getTimeout() 获得超时时间\n    >    \n    >    * boolean isReadOnly() 是否只读\n    > \n    >  * 事务隔离级别\n    >    \n    >    设置隔离级别，可以解决事务并发产生的问题，如脏读、不可重复读和虚读。 ISOLATION_DEFAULT\n    >    \n    >    ISOLATION_READ_UNCOMMITTED\n    >    \n    >    ISOLATION_READ_COMMITTED\n    >    \n    >    ISOLATION_REPEATABLE_READ\n    >    \n    >    ISOLATION_SERIALIZABLE\n    > \n    >  * 事务传播行为\n    >    \n    >    * REQUIRED： 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。一般的选择（默认值）\n    >    * SUPPORTS： 支持当前事务，如果当前没有事务，就以非事务方式执行（没有事务）\n    >    * MANDATORY：使用当前的事务，如果当前没有事务，就抛出异常\n    >    * REQUERS_NEW： 新建事务，如果当前在事务中，把当前事务挂起。\n    >    * NOT_SUPPORTED： 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起\n    >    * NEVER： 以非事务方式运行，如果当前存在事务，抛出异常\n    >    * NESTED： 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行 REQUIRED 类似的操作\n    > \n    >  * 超时和只读\n    >    \n    >    * 超时时间：默认值是-1，没有超时限制。如果有，以秒为单位进行设置\n    >    * 是否只读：建议查询时设置为只读\n    > \n    >  * 设置事务回滚异常\n    >    \n    >    * error系，运行时异常，Spring事务 默认才回滚，其他都不回滚，例如 IOException\n    >    \n    >    * 通过rollbackFor 设置除了默认回滚异常外的哪些异常时进行回滚\n\n 3. TransactionStatus （获取事务的运行状态）\n    \n    > TransactionStatus 接口提供的是事务具体的运行状态\n    > \n    >  * boolean hasSavepoint() 是否存储回滚点\n    >  * boolean isCompleted() 事务是否完成\n    >  * boolean isNewTransaction() 是否是新事务\n    >  * boolean isRollbackOnly() 事务是否回滚\n\n# 二、基于XML的声明式事务控制\n\n 1. 声明式事务控制\n    \n    >  * 在spring配置文件中声明要处理处理事务。\n    > \n    >  * 作用：事务管理不侵入开发的组件。业务逻辑和事务管理分开来。\n    >    \n    >    事务管理算是 增强方法，业务逻辑算是 目标方法 ，AOP思想\n    >    \n    >    Spring声明式事务控制底层就是AOP\n\n 2. 声明式事务控制的实现\n    \n    >  * 谁是切点？ ---service层 的transfer方法\n    > \n    >  * 谁是增强？ --- 开启事务 --- 提交事务 --回滚 等等\n    > \n    >  * xml配置\n    >    \n    >    \x3c!-- 配置一个平台事务管理器--\x3e\n    >    <bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager">\n    >        <property name="dataSource" ref="dataSource_c3p0"/>\n    >    </bean>\n    >                        \n    >    \x3c!-- 通知  事务的增强--\x3e\n    >    <tx:advice id="txAdvice" transaction-manager="transactionManager">\n    >        \x3c!-- 设置事务的属性信息--\x3e\n    >        <tx:attributes>\n    >            \x3c!-- 那些方法被增强--\x3e\n    >            \x3c!-- isolation:事务隔离级别  propagation：事务传播行为  read-only：是否只读 timeout: 超时时间2s--\x3e\n    >            <tx:method name="transfer" isolation="DEFAULT" propagation="REQUIRED" read-only="false" timeout="2"/>\n    >            <tx:method name="*"/> \x3c!-- 所有方法--\x3e\n    >            <tx:method name="select*"/>  \x3c!-- 所有以select开头的方法--\x3e\n    >        </tx:attributes>\n    >    </tx:advice>\n    >                        \n    >     \x3c!-- 配置事务的aop织入--\x3e\n    >    <aop:config>\n    >        <aop:pointcut id="pointcut" expression="execution(* com.diana.service..*.*(..))"/>\n    >        \x3c!-- 专门用来配置 事务的一个命令--\x3e\n    >        \x3c!--       <aop:advisor advice-ref="txAdvice" pointcut="execution(* com.diana.service..*.*(..))"/>--\x3e\n    >        <aop:advisor advice-ref="txAdvice" pointcut-ref="pointcut"/>\n    >    </aop:config>\n    >    \n\n# 三、基于注解的声明式事务控制\n\n开发步骤\n\n * service层注解\n   \n   * 在service方法上加@Transactional(isolation = Isolation.REPEATABLE_READ)，表示要对这个方法进行事务控制\n   \n   * 可在类上直接配置@Transactional，表示整个类所有方法的默认事务控制\n   \n   * 当方法和类上同时都有事务控制时，以方法上的为主，就近原则\n   \n   // 声明事务的  service层   注解方式配置\n   @Service\n   @Transactional(isolation = Isolation.REPEATABLE_READ) //整个类的所有方法的默认事务控制\n   public class AccountServiceImpl_ZJ implements AccountService {\n   \n       @Autowired\n       private AccountDao accountDao;\n   \n       // 这个方法需要进行事务控制,并在里面配置参数\n       @Transactional(isolation = Isolation.DEFAULT) //就近原则，用这个\n       public void transfer(String outMan, String inMan, double money) {\n           //开启事务 -aop 前置增强\n           accountDao.out(outMan,money);\n           int i = 1/0;\n           accountDao.in(inMan,money);\n           //提交事务  -aop 后置增强\n       }\n   }\n   \n\n * xml中剩余配置\n   \n   * 把原配置中的 增强方法 声明和 织入过程 用@Transactional（）来代替\n   * 添加了一个包扫描和事务的注解驱动配置\n   \n   \x3c!-- jdbc模板数据 别人的东西 一般在xml中配置--\x3e\n   \n   \x3c!--加载外部配置文件property--\x3e\n   <context:property-placeholder location="classpath:jdbc.properties"/>\n   \x3c!-- el表达式 来获取配置文件信息--\x3e\n   <bean id="dataSource_c3p0" class="com.mchange.v2.c3p0.ComboPooledDataSource">\n       <property name="driverClass" value="${jdbc.driver}"/>\n       <property name="user" value="${jdbc.username}"/>\n       <property name="password" value="${jdbc.password}"/>\n       <property name="jdbcUrl" value="${jdbc.url}"/>\n   </bean>\n   \x3c!-- 在spring 容器中产生 jdbc模板对象   同时注入数据源--\x3e\n   <bean id="jdbcTemplate" class="org.springframework.jdbc.core.JdbcTemplate">\n       <property name="dataSource" ref="dataSource_c3p0"/>\n   </bean>\n   \n   \n   \n   \x3c!-- 配置一个平台事务管理器--\x3e\n   <bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager">\n       <property name="dataSource" ref="dataSource_c3p0"/>\n   </bean>\n   \n   \n   \x3c!-- 事务的注解驱动--\x3e\n   <tx:annotation-driven transaction-manager="transactionManager"/>\n   \n   \n   \x3c!-- 包扫描--\x3e\n   <context:component-scan base-package="com.diana"/>\n   \n   \n   \n\n * 纯注解开发\n   \n   * 数据源，jdbc模板，平台事务管理器\n     \n     //存入数据源\n     @Bean("dataSource")//Spring 会将当前方法的返回值以指定名称存储到Spring容器中\n     public DataSource get_DateSource_c3p0(User user) throws Exception {\n         System.out.println(user);//保证容器内部有这个Bean引用类型 ， 只要传入形参  spring会自动找到\n     \n         ComboPooledDataSource comboPooledDataSource = new ComboPooledDataSource();\n         comboPooledDataSource.setDriverClass(driver);\n         comboPooledDataSource.setJdbcUrl(url);\n         comboPooledDataSource.setUser(username);\n         comboPooledDataSource.setPassword(password);\n         return comboPooledDataSource;\n     }\n     \n     \n     @Bean("jdbcTemplate") //存入jdbc模板\n     public JdbcTemplate jdbcTemplate(DataSource dataSource){\n         JdbcTemplate jdbcTemplate=new JdbcTemplate();\n         jdbcTemplate.setDataSource(dataSource);\n         return jdbcTemplate;\n     }\n     \n     @Bean//向容器中存入 事务管理器\n     public PlatformTransactionManager transactionManager(DataSource dataSource){\n         DataSourceTransactionManager transactionManager=new DataSourceTransactionManager();\n         transactionManager.setDataSource(dataSource);\n         return transactionManager;\n     \n     }\n     \n   \n   * 注解驱动，包扫描\n     \n     //开启事务\n     @EnableTransactionManagement\n     \n     //\x3c!--配置组件扫描--\x3e  \x3c!--配合注解开发使用--\x3e\n     @ComponentScan("com.diana")\n     ',normalizedContent:'# spring 优势\n\n 1. 方便解耦，简化开发（bean）\n    \n    * 通过spring提供的ioc容器 可以将对象间的依赖关系交由spring进行控制。\n    * ioc :使用对象时，由主动new产生对象转换为由外部提供对象,此过程中对象创建控制权由程序转移到外部，此思想称为控制反转。\n\n 2. aop编程支持\n    \n    * 通过spring的aop功能，方便的面向切面编程，许多不容易用传统oop（面向对象编程）实现的功能都可以通过aop实现。\n\n 3. 声明式事务的支持\n    \n    * 通过声明式灵活的进行事务管理，提高开发效率和质量。\n\n 4. 方便程序的测试 （spring-junit）\n    \n    * 可以用非容器依赖的编程方式进行几乎所有的测试工作。\n\n 5. 方便集成各种优秀框架（spring-mybatis）\n\n 6. 降低javaee api的使用难度（jdbc模板）\n    \n    * spirng对javaee api 进行了薄薄的封装层，提供给用户一个模板来使用。\n\n 7. java 源码是经典学习范例\n    \n    * spring 的源代码设计精妙，值得学习。\n\n\n# spring 体系结构\n\n\n\n\n# spring 开发步骤\n\n 1. 导入坐标。\n    \n    <dependency>\n      <groupid>org.springframework</groupid>\n      <artifactid>spring-context</artifactid>\n      <version>5.0.5.release</version>\n    </dependency>\n    \n\n 2. 创建bean，dao(mapper)目录下的接口加实现类。\n\n 3. 创建applicationcontext.xml,并进行相关配置。\n    \n    <bean id="user" class="com.diana.dao_mapper.impl.userimpl"></bean>\n    \n\n 4. 创建applicationcontext对象，使用getbean方法得到对象。\n    \n    applicationcontext app=new classpathxmlapplicationcontext("applicationcontext.xml");\n    user user = (user) app.getbean("user");\n    \n\n\n# spring重点配置\n\n>  1. <bean> 标签\n>     \n>     * id属性： 在容器中bean实例的唯一标识，不允许重复\n>     \n>     * class属性： 要实例化的bean的全限定名\n>     \n>     * scope属性：bean的作用范围，常用是singleton(默认)和prototype\n>     \n>     * init-method属性: 初始化方法\n>     \n>     * destroy-method属性: 销毁方法\n>     \n> \n>  2. set方法注入 使用<property>标签 自己开发的模块 推荐使用set方法注入\n>     \n>     >  * name是属性名称\n>     >    \n>     >    name="user",  user是setuser 的set后面的部分的小写，即  setuser——user——user  一般来说即属性名 user\n>     >    \n>     > \n>     >  * 普通数据类型-----value\n>     >    \n>     >    <property name="username" value="张三"/>\n>     >    \n>     > \n>     >  * 集合数据类型-----list，map，props，set,array\n>     >    \n>     >    * list\n>     >      \n>     >      <property name="strlist">\n>     >          <list>\n>     >              <value>aaa</value>\n>     >              <value>bb</value>\n>     >              <value>ccc</value>\n>     >          </list>\n>     >      </property>\n>     >      \n>     >    \n>     >    * map\n>     >      \n>     >      <property name="brandmap">\n>     >          <map>\n>     >              <entry key="key1" value-ref="brand1"/>\n>     >              <entry key="key2" value-ref="brand2"/>\n>     >          </map>\n>     >      </property>\n>     >      \n>     >    \n>     >    * props\n>     >      \n>     >      <property name="properties">\n>     >          <props>\n>     >              <prop key="p1">ppp1</prop>\n>     >              <prop key="p2">ppp2</prop>\n>     >              <prop key="p3">ppp3</prop>\n>     >          </props>\n>     >      </property>\n>     >      \n>     > \n>     >  * 引用数据类型-----ref\n>     >    \n>     >     <property name="user" ref="user">\n>     >    \n> \n>  3. <constructor-arg>标签-- 有参构造函数注入 不推荐\n>     \n>     <constructor-arg name="user2" ref="user"  />\n>     \n> \n>  4. <import> 标签：导入其他的spring的分文件\n>     \n>     <import resource="applicationcontext-insert.xml"/>  \x3c!--注入方法--\x3e\n>     \n> \n>  5. 加载外部配置property ,使用el表达式来获取值 ---引入数据源\n>     \n>     <context:property-placeholder location="classpath:jdbc.properties"/>\n>     \x3c!-- el表达式 来获取配置文件信息--\x3e\n>         <bean id="datasource_c3p0" class="com.mchange.v2.c3p0.combopooleddatasource">\n>             <property name="driverclass" value="${jdbc.driver}"/>\n>             <property name="user" value="${jdbc.username}"/>\n>             <property name="password" value="${jdbc.password}"/>\n>             <property name="jdbcurl" value="${jdbc.url}"/>\n>         </bean>\n>     \n> \n>  6. 组件扫描，配合注解开发使用\n>     \n>     <context:component-scan base-package="com.diana"/>\n>     \n> \n>  7. 实例化方法\n>     \n>     * 无参构造\n>       \n>       <bean id="user" class="com.diana.dao_mapper.impl.userimpl"/>\n>       \n>     \n>     * 工厂静态方法实例化\n>       \n>       <bean id="user" class="com.diana.factory.staticfactory" factory-method="getuser"/>\n>       \n>       //实例化bean  方式二  工厂静态方法实例化\n>       public class staticfactory {\n>           public static user getuser(){\n>               return new userimpl();\n>           }\n>       }\n>       \n>     \n>     * 工厂实例方法实例化\n>       \n>       <bean id="factory" class="com.diana.factory.dynamicfactory"/>\n>       <bean id="user" factory-bean="factory" factory-method="getuser"/>\n>            //实例化bean  方式三  工厂实例方法实例化\n>        public class dynamicfactory {\n>            public  user getuser(){\n>                return new userimpl();\n>            }\n>        }\n>       \n>       \n>     \n>     * 使用factorybean实例化\n>       \n>       \x3c!--使用factorybean实例化--\x3e\n>       <bean id="user" class="com.diana.factory.userfactorybean"/>\n>        //实例化bean  方式四  使用factorybean实例化\n>       public class userfactorybean implements factorybean<user> {\n>       \n>        //代替原始实例工厂中创建对象的方法\n>        public user getobject() throws exception {\n>            return new userimpl();\n>        }\n>       \n>        //得到bean的类型\n>        public class<?> getobjecttype() {\n>            return user.class;\n>        }\n>       \n>        //是否单例   工厂一般都是单例    所有这个方法一般不写  默认单例\n>        public boolean issingleton() {\n>            return true;\n>        }\n>       }\n>       \n>       \n\n\n# spring注解开发\n\n在dao_mapper--impl--userimpl 中\n\n在demo-- usercontroller ，config 中\n\nxml配置与注解配置对比\n\n\n\n\n# spring 集成junit\n\n>  1. 导入spring集成junit的坐标和junit的坐标。\n>     \n>     * spring-context spring-test 两个版本要一致\n>     * junit 版本要在4.12 及以上\n> \n>  2. 使用@runwith注解替换原来的运行期\n> \n>  3. 使用@contextconfiguration指定配置文件或配置类\n> \n>  4. 使用@autowired注入需要测试的对象\n> \n>  5. 创建测试方法进行测试\n>     \n>     @runwith(springjunit4classrunner.class)\n>     //@contextconfiguration("classpath:applicationcontext.xml")//配置文件方式\n>     @contextconfiguration(classes = {springconfiguration.class})//注解类方式\n>     public class springjuinttest {\n>                                                     \n>         @autowired\n>         private userservice userservice;\n>                                                     \n>         @autowired\n>         private datasource datasource;\n>                                                     \n>         @test\n>         public void test1() throws sqlexception {\n>             userservice.save();\n>             system.out.println(datasource.getconnection());\n>         }\n>     }\n>     \n\n\n# spring集成web\n\n放到了springmvc项目下\n\n>  1. applicationcontext应用上下文获取方式\n>     \n>     * 在web.xml中配置contextloaderlistener监听器 (导入spring-web坐标)\n>       \n>       \x3c!-- spring web监听器--\x3e\n>       <dependency>\n>         <groupid>org.springframework</groupid>\n>         <artifactid>spring-web</artifactid>\n>         <version>5.3.6</version>\n>       </dependency>\n>       \n>       \n>       \x3c!--配置监听器  来自动加载spring应用上下文--\x3e\n>       <listener>\n>         <listener-class>org.springframework.web.context.contextloaderlistener</listener-class>\n>       </listener>\n>       \n>     \n>     * 使用webapplicationcontextutils获得应用上下文对象 applicationcontext\n>       \n>       //调用spring封装好的实现\n>       //得到servletcontext  最大的域\n>       servletcontext servletcontext = this.getservletcontext();\n>       applicationcontext app = webapplicationcontextutils.getwebapplicationcontext(servletcontext);\n>       \n> \n>  2. 从容器中得到userservice对象\n>     \n>     userservice userservice = app.getbean(userservice.class); userservice.save();\n>     \n>     \n>     \n\n\n# spring -aop\n\naop 简化共性功能，方便开发\n\n# 一、aop的基础知识\n\n 1. 什么是aop\n    \n    >  * aop 是面向切面编程，是通过 预编译方式和运行期动态代理实现程序功能的统一维护的一种技术。\n    >    * 动态代理： 在不修改源码的基础上，对目标方法进行增强。作用：完成代码间的松耦合\n    >  * 利用aop可以对业务逻辑的各个部分进行隔离，从而使得业务逻辑各个部分之间的耦合度降低，提高程序的可重用性，提升开发效率。\n\n 2. aop的作用及其优势\n    \n    >  * 作用： 在程序运行期间，在不修改源码的情况下，对方法进行功能增强。\n    > \n    >  * 优势：减少重复代码，提高开发效率，便于维护。\n\n 3. aop 流程\n    \n    >  * 一个目标方法和一个功能增强方法就被称为一个切面。\n    >  * 将目标方法和功能增强方法 在运行时放到一起，达到功能增强的目的，同时达到松耦合的目的。\n    >  * 目标方法（save，update，delete），功能增强方法（日志控制），源码分离，在配置文件中声明 运行时进行结合。\n    \n    \n\n 4. aop的动态代理技术\n    \n    > jdk动态代理 要求 目标对象必须有一个接口\n    > \n    > cglib动态代理 会生成一个目标对象的子类 从而实现动态代理\n    \n    \n\n 5. aop相关概念\n    \n    >  * target(目标对象)： 代理的目标对象。\n    >  * proxy(代理)：一个类被aop织入增强后，就产生一个结果代理类。\n    >  * joinpoint（连接点）----\x3e（可以被增强的目标方法）： 所谓连接点是值那些被拦截到的点（方法）。在spring中，这些点值得是方法，因为spring只支持方法类型的连接点。\n    >  * pointcut（切入点/切点）----\x3e（你配置的，真正被增强的目标方法，是连接点的一部分）：所谓切入点是指我们要对哪些joinpoint进行拦截的定义。\n    >  * advice（通知/增强）----\x3e（用来增强目标方法的增强方法）： 所谓通知是指 拦截到joinpoint之后 所要做的事情就是通知。\n    >  * aspect（切面）：是切入点和通知的结合\n    >  * weaving（织入）----\x3e（把切入点和通知结合的过程）：是指把增强应用到目标对象来创建新的代理对象的过程，采用配置文件实现，配置的过程就是织入的过程。spring采用动态代理织入，而aspecttj 采用编译期织入和类装载期织入\n\n 6. aop开发明确事项\n    \n    >  * 需要编写的内容\n    >    \n    >    * 编写核心业务代码（目标类的目标方法）\n    >    * 编写切面类，切面类中有通知(增强功能方法)\n    >    * 在配置文件中，配置织入关系，即将哪些通知与哪些切入点进行结合\n    > \n    >  * aop技术实现的内容（spring帮你实现动态代理的过程）\n    >    \n    >    spring框架监控切入点方法的执行。一旦监控到切入点方法被运行，使用代理机制，动态创建目标对象的代理对象，根据通知类别，在代理对象的对应位置，将通知对应的功能织入，完成完整的代码逻辑运行。\n    > \n    >  * aop底层代理方式\n    >    \n    >    在spring中，框架会根据目标类是否实现了接口来决定采用哪种动态代理的方式。\n    >    \n    >    有接口-----\x3e jdk动态代理 无接口----\x3ecglib动态代理\n\n# 二、基于xml的aop开发\n\n 1. 开发步骤\n    \n    >  * 导入坐标\n    >    \n    >    \x3c!-- aspectj  aop 配置--\x3e\n    >    <dependency>\n    >      <groupid>org.aspectj</groupid>\n    >      <artifactid>aspectjweaver</artifactid>\n    >      <version>1.9.4</version>\n    >    </dependency>\n    >    \n    > \n    >  * 创建目标接口和目标类(内部有切点)\n    > \n    >  * 创建切面类（内部有增强方法）\n    > \n    >  * 将目标类和切面类的对象创建权交给spring容器\n    >    \n    >    \x3c!-- 切面对象--\x3e\n    >    <bean id="aspect" class="com.diana.aop.myaspect"/>\n    >    \n    >    \x3c!--目标对象--\x3e\n    >    <bean id="target" class="com.diana.aop.impl.targetimpl"/>\n    >    \n    > \n    >  * 在配置文件中配置织入关系\n    >    \n    >    \x3c!--     配置织入:  告诉spring框架  哪些方法（切点）需要进行哪些增强（前置，后置，……）--\x3e\n    >    <aop:config>\n    >        \x3c!-- 声明切面--\x3e\n    >        <aop:aspect ref="aspect">\n    >            \x3c!--切面：切点+通知--\x3e\n    >            \x3c!--对save方法 使用before方法 进行前置增强--\x3e\n    >            <aop:before method="before1" pointcut="execution(public void com.diana.aop.impl.targetimpl.save())"/>\n    >        </aop:aspect>\n    >    </aop:config>\n    >    \n    > \n    >  * 测试代码\n    >    \n    >    @runwith(springjunit4classrunner.class)\n    >    @contextconfiguration("classpath:applicationcontext-aop.xml")\n    >    //测试aop\n    >    public class test_aop {\n    >        @autowired\n    >        private target target;\n    >        @test\n    >        //测试前置增强方法\n    >        public void test_before(){\n    >            target.save();//调用切点\n    >        }\n    >    }\n    >    \n\n 2. 切点表达式的写法\n    \n    > 表达式语法 execution（[修饰符] 返回值类型 包名 . 类名 . 方法名(参数)）\n    > \n    >  * 访问修饰符可以省略\n    > \n    >  * 返回值类型、包名、类名、方法名可以使用※代表任意\n    > \n    >  * 包名与类名之间一个点代表当前包下的类，两个点 .. (不推荐，效率低)表示当前包及其子包下的类\n    > \n    >  * 参数列表可以使用两个点 表示任意个数，任意类型的参数列表\n    >    \n    >    1. 表示aop当前包下的target类的method方法，无参数，返回值类型为空。\n    >    2. 表示aop当前包下的target类的所有方法，任意参数，返回值类型为空。\n    >    3. 表示aop当前包下的所有类的所有方法，任意参数，返回值类型为任意。-------最常用\n    >    4. 表示aop当前包及其子包下的所有类的所有方法，任意参数，返回值类型为任意。\n    >    5. 表示任意包当前包及其子包下的所有类的所有方法，任意参数，返回值类型为任意。\n\n 3. 切点表达式的书写技巧\n    \n    \n    \n    示例\n    \n    //表示任意_service的所有的find_  方法\n    execution(* com.diana.*.*service.find*(..))\n    \n    //表示任意_service的所有的save方法\n    execution(* com.diana.*.*service.save(..))\n    \n\n 4. aop通知类型 环绕通知是重点\n    \n    \n    \n    \x3c!--切面：切点+通知--\x3e\n    \x3c!--对save方法 使用before方法 进行前置增强--\x3e\n    <aop:before method="before1" pointcut="execution(public void com.diana.aop.impl.targetimpl.save())"/>\n    \x3c!--对aop当前包及其子包下所有方法 使用before方法 进行前置增强--\x3e\n    <aop:before method="before1" pointcut="execution(* com.diana.aop..*.*(..))"/>\n    \x3c!-- 后置增强--\x3e\n    <aop:after-returning method="afterreturning" pointcut="execution(* com.diana.aop..*.*(..))"/>\n    \x3c!-- 环绕增强--\x3e\n    <aop:around method="around" pointcut="execution(* com.diana.aop..*.*(..))"/>\n    \x3c!-- 异常抛出增强--\x3e\n    <aop:after-throwing method="afterthrowing" pointcut="execution(* com.diana.aop..*.*(..))"/>\n    \x3c!-- 最终增强--\x3e\n    <aop:after method="after" pointcut="execution(* com.diana.aop..*.*(..))"/>\n    \n\n 5. 切点表达式的抽取\n    \n    \x3c!-- 抽取切点表达式--\x3e\n    <aop:pointcut id="mypointcut" expression="execution(* com.diana.aop..*.*(..))"/>\n    \n    \n    \x3c!-- 使用抽取后的切点表达式--\x3e\n    <aop:before method="before1" pointcut-ref="mypointcut"/>\n    <aop:around method="around" pointcut-ref="mypointcut"/>\n    \n\n# 三、基于注解的aop开发\n\n 1. 开发步骤\n    \n    >  * 导入坐标\n    >    \n    >    \x3c!-- aspectj  aop 配置--\x3e\n    >    <dependency>\n    >      <groupid>org.aspectj</groupid>\n    >      <artifactid>aspectjweaver</artifactid>\n    >      <version>1.9.4</version>\n    >    </dependency>\n    >    \n    > \n    >  * 创建目标接口和目标类(内部有切点)\n    > \n    >  * 创建切面类（内部有增强方法）\n    > \n    >  * 将目标类和切面类的对象创建权交给spring容器（@component）\n    > \n    >  * 在切面类中使用注解配置织入关系\n    >    \n    >    @component("aspect")\n    >    @aspect//标注当前类是一个切面类\n    >    public class myaspect {\n    >    \n    >        //前置增强\n    >        @before("execution(* com.diana.aop_zj..*.*(..))")\n    >        @before("myponitcut()") //抽取切点表达式后使用\n    >        @before("myaspect.myponitcut()")//抽取切点表达式后使用\n    >        public void before1(){\n    >            system.out.println("前置增强……");\n    >        }\n    >    \n    >        //后置增强\n    >        @afterreturning("execution(* com.diana.aop_zj..*.*(..))")\n    >        public void afterreturning(){\n    >            system.out.println("后置增强……");\n    >        }\n    >    \n    >        //环绕增强\n    >        @around("execution(* com.diana.aop_zj..*.*(..))")\n    >        public object around(proceedingjoinpoint pjp) throws throwable {\n    >            //proceeding joinpoint: 正在执行的连接点   切点\n    >            system.out.println("环绕前增强……");\n    >            //执行切点方法\n    >            object proceed = pjp.proceed();\n    >            system.out.println("环绕后增强……");\n    >            return proceed;\n    >        }\n    >    \n    >        //异常抛出后增强\n    >        @afterthrowing("execution(* com.diana.aop_zj..*.*(..))")\n    >        public void afterthrowing(){\n    >            system.out.println("抛出了异常……");\n    >        }\n    >    \n    >        //最终增强  不管有没有异常，都要执行\n    >        @after("execution(* com.diana.aop_zj..*.*(..))")\n    >        public void after(){\n    >            system.out.println("最终增强");\n    >        }\n    >        \n    >        //定义切点表达式\n    >        @pointcut("execution(* com.diana.aop_zj..*.*(..))")\n    >        public void myponitcut(){}\n    >    }\n    >    \n    > \n    >  * 在配置文件中开启组件扫描和aop的自动代理\n    >    \n    >    \x3c!--配置组件扫描--\x3e  \x3c!--配合注解开发使用--\x3e\n    >    <context:component-scan base-package="com.diana.aop_zj"/>\n    >    \n    >    \x3c!-- aop 自动代理--\x3e\n    >    <aop:aspectj-autoproxy/>\n    >    \n    > \n    >  * 使用纯注解开发 不留配置文件 --解决自动代理问题即可\n    >    \n    >    //启动aop自动代理     代替 <aop:aspectj-autoproxy/>\n    >    @enableaspectjautoproxy\n    >    \n    >    //\x3c!--配置组件扫描--\x3e  \x3c!--配合注解开发使用--\x3e\n    >    //<context:component-scan base-package="com.diana"/>\n    >    @componentscan("com.diana")\n    >    \n    > \n    >  * 测试代码\n    >    \n    >    @runwith(springjunit4classrunner.class)\n    >    @contextconfiguration("classpath:applicationcontext-aop_zj.xml")\n    >    public class test_aop_zj {\n    >                    \n    >        @autowired\n    >        target target=new targetimpl();\n    >                    \n    >        @test\n    >        public void test(){\n    >            target.save();//调用切点\n    >        }\n    >    }\n    >    \n\n 2. 切点表达式的抽取\n    \n    > 在切面内定义方法，在该方法上使用@pointcut注解定义切点表达式，然后在增强注解中引用。\n    > \n    > //定义切点表达式\n    > @pointcut("execution(* com.diana.aop_zj..*.*(..))")\n    > public void myponitcut(){}\n    > \n    > \n    > //    @before("execution(* com.diana.aop_zj..*.*(..))")\n    > //    @before("myponitcut()") //抽取切点表达式后使用\n    > @before("myaspect.myponitcut()")//抽取切点表达式后使用\n    > public void before1(){\n    >     system.out.println("前置增强……");\n    > }\n\n# 四、aop通知获得数据\n\n * 项目在 my_case_demo2中，\n   * 测试代码test-test_aop_param\n   * aop代码 aop-getparam\n * 获取切入点方法的参数\n   * joinpoint ：适用于前置，后置，最终通知，抛出异常后 通知\n   * proceedjointpoint：适用于环绕通知, 可以修改参数，以到达某些目的\n * 获取切入点方法返回值\n   * 后置\n   * 环绕通知\n * 获取切入点方法运行异常信息 （了解）\n   * 抛出异常后通知\n   * 环绕通知\n\n\n# spring 事务控制\n\n测试代码在controller层 一共三个，一个xml配置，一个半注解，一个纯注解\n\n# 一、编程式事务控制相关对象\n\n 1. platformtransactionmanager （我们要告诉spring 用哪个实现类）\n    \n    >  * 该接口是spring的事务管理器，它里面提供了常用的操作事务的方法。\n    >    \n    >    * transactionstatus gettransaction(transactiondefination defination) 获取事务的状态信息\n    >    * void commit(transactionstatus status) 提交事务\n    >    * void rollback(transactionstatus status) 回滚事务\n    > \n    >  * 该接口对应不同的dao层有不同的实现类。 jdbc，mybatis 是org.springframework.jdbc.datasource.datasourcetransactionmanager\n\n 2. transactiondefinition （我们要告诉spring 传入哪些参数，下面方法需要用到的）\n    \n    >  * transactiondefinition它是事务的定义信息对象，里面如下方法\n    >    \n    >    * int getisolationlevel() 获得事务的隔离级别\n    >    \n    >    * int getpropogationbehavior() 获得事务的传播行为\n    >    \n    >    * int gettimeout() 获得超时时间\n    >    \n    >    * boolean isreadonly() 是否只读\n    > \n    >  * 事务隔离级别\n    >    \n    >    设置隔离级别，可以解决事务并发产生的问题，如脏读、不可重复读和虚读。 isolation_default\n    >    \n    >    isolation_read_uncommitted\n    >    \n    >    isolation_read_committed\n    >    \n    >    isolation_repeatable_read\n    >    \n    >    isolation_serializable\n    > \n    >  * 事务传播行为\n    >    \n    >    * required： 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。一般的选择（默认值）\n    >    * supports： 支持当前事务，如果当前没有事务，就以非事务方式执行（没有事务）\n    >    * mandatory：使用当前的事务，如果当前没有事务，就抛出异常\n    >    * requers_new： 新建事务，如果当前在事务中，把当前事务挂起。\n    >    * not_supported： 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起\n    >    * never： 以非事务方式运行，如果当前存在事务，抛出异常\n    >    * nested： 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行 required 类似的操作\n    > \n    >  * 超时和只读\n    >    \n    >    * 超时时间：默认值是-1，没有超时限制。如果有，以秒为单位进行设置\n    >    * 是否只读：建议查询时设置为只读\n    > \n    >  * 设置事务回滚异常\n    >    \n    >    * error系，运行时异常，spring事务 默认才回滚，其他都不回滚，例如 ioexception\n    >    \n    >    * 通过rollbackfor 设置除了默认回滚异常外的哪些异常时进行回滚\n\n 3. transactionstatus （获取事务的运行状态）\n    \n    > transactionstatus 接口提供的是事务具体的运行状态\n    > \n    >  * boolean hassavepoint() 是否存储回滚点\n    >  * boolean iscompleted() 事务是否完成\n    >  * boolean isnewtransaction() 是否是新事务\n    >  * boolean isrollbackonly() 事务是否回滚\n\n# 二、基于xml的声明式事务控制\n\n 1. 声明式事务控制\n    \n    >  * 在spring配置文件中声明要处理处理事务。\n    > \n    >  * 作用：事务管理不侵入开发的组件。业务逻辑和事务管理分开来。\n    >    \n    >    事务管理算是 增强方法，业务逻辑算是 目标方法 ，aop思想\n    >    \n    >    spring声明式事务控制底层就是aop\n\n 2. 声明式事务控制的实现\n    \n    >  * 谁是切点？ ---service层 的transfer方法\n    > \n    >  * 谁是增强？ --- 开启事务 --- 提交事务 --回滚 等等\n    > \n    >  * xml配置\n    >    \n    >    \x3c!-- 配置一个平台事务管理器--\x3e\n    >    <bean id="transactionmanager" class="org.springframework.jdbc.datasource.datasourcetransactionmanager">\n    >        <property name="datasource" ref="datasource_c3p0"/>\n    >    </bean>\n    >                        \n    >    \x3c!-- 通知  事务的增强--\x3e\n    >    <tx:advice id="txadvice" transaction-manager="transactionmanager">\n    >        \x3c!-- 设置事务的属性信息--\x3e\n    >        <tx:attributes>\n    >            \x3c!-- 那些方法被增强--\x3e\n    >            \x3c!-- isolation:事务隔离级别  propagation：事务传播行为  read-only：是否只读 timeout: 超时时间2s--\x3e\n    >            <tx:method name="transfer" isolation="default" propagation="required" read-only="false" timeout="2"/>\n    >            <tx:method name="*"/> \x3c!-- 所有方法--\x3e\n    >            <tx:method name="select*"/>  \x3c!-- 所有以select开头的方法--\x3e\n    >        </tx:attributes>\n    >    </tx:advice>\n    >                        \n    >     \x3c!-- 配置事务的aop织入--\x3e\n    >    <aop:config>\n    >        <aop:pointcut id="pointcut" expression="execution(* com.diana.service..*.*(..))"/>\n    >        \x3c!-- 专门用来配置 事务的一个命令--\x3e\n    >        \x3c!--       <aop:advisor advice-ref="txadvice" pointcut="execution(* com.diana.service..*.*(..))"/>--\x3e\n    >        <aop:advisor advice-ref="txadvice" pointcut-ref="pointcut"/>\n    >    </aop:config>\n    >    \n\n# 三、基于注解的声明式事务控制\n\n开发步骤\n\n * service层注解\n   \n   * 在service方法上加@transactional(isolation = isolation.repeatable_read)，表示要对这个方法进行事务控制\n   \n   * 可在类上直接配置@transactional，表示整个类所有方法的默认事务控制\n   \n   * 当方法和类上同时都有事务控制时，以方法上的为主，就近原则\n   \n   // 声明事务的  service层   注解方式配置\n   @service\n   @transactional(isolation = isolation.repeatable_read) //整个类的所有方法的默认事务控制\n   public class accountserviceimpl_zj implements accountservice {\n   \n       @autowired\n       private accountdao accountdao;\n   \n       // 这个方法需要进行事务控制,并在里面配置参数\n       @transactional(isolation = isolation.default) //就近原则，用这个\n       public void transfer(string outman, string inman, double money) {\n           //开启事务 -aop 前置增强\n           accountdao.out(outman,money);\n           int i = 1/0;\n           accountdao.in(inman,money);\n           //提交事务  -aop 后置增强\n       }\n   }\n   \n\n * xml中剩余配置\n   \n   * 把原配置中的 增强方法 声明和 织入过程 用@transactional（）来代替\n   * 添加了一个包扫描和事务的注解驱动配置\n   \n   \x3c!-- jdbc模板数据 别人的东西 一般在xml中配置--\x3e\n   \n   \x3c!--加载外部配置文件property--\x3e\n   <context:property-placeholder location="classpath:jdbc.properties"/>\n   \x3c!-- el表达式 来获取配置文件信息--\x3e\n   <bean id="datasource_c3p0" class="com.mchange.v2.c3p0.combopooleddatasource">\n       <property name="driverclass" value="${jdbc.driver}"/>\n       <property name="user" value="${jdbc.username}"/>\n       <property name="password" value="${jdbc.password}"/>\n       <property name="jdbcurl" value="${jdbc.url}"/>\n   </bean>\n   \x3c!-- 在spring 容器中产生 jdbc模板对象   同时注入数据源--\x3e\n   <bean id="jdbctemplate" class="org.springframework.jdbc.core.jdbctemplate">\n       <property name="datasource" ref="datasource_c3p0"/>\n   </bean>\n   \n   \n   \n   \x3c!-- 配置一个平台事务管理器--\x3e\n   <bean id="transactionmanager" class="org.springframework.jdbc.datasource.datasourcetransactionmanager">\n       <property name="datasource" ref="datasource_c3p0"/>\n   </bean>\n   \n   \n   \x3c!-- 事务的注解驱动--\x3e\n   <tx:annotation-driven transaction-manager="transactionmanager"/>\n   \n   \n   \x3c!-- 包扫描--\x3e\n   <context:component-scan base-package="com.diana"/>\n   \n   \n   \n\n * 纯注解开发\n   \n   * 数据源，jdbc模板，平台事务管理器\n     \n     //存入数据源\n     @bean("datasource")//spring 会将当前方法的返回值以指定名称存储到spring容器中\n     public datasource get_datesource_c3p0(user user) throws exception {\n         system.out.println(user);//保证容器内部有这个bean引用类型 ， 只要传入形参  spring会自动找到\n     \n         combopooleddatasource combopooleddatasource = new combopooleddatasource();\n         combopooleddatasource.setdriverclass(driver);\n         combopooleddatasource.setjdbcurl(url);\n         combopooleddatasource.setuser(username);\n         combopooleddatasource.setpassword(password);\n         return combopooleddatasource;\n     }\n     \n     \n     @bean("jdbctemplate") //存入jdbc模板\n     public jdbctemplate jdbctemplate(datasource datasource){\n         jdbctemplate jdbctemplate=new jdbctemplate();\n         jdbctemplate.setdatasource(datasource);\n         return jdbctemplate;\n     }\n     \n     @bean//向容器中存入 事务管理器\n     public platformtransactionmanager transactionmanager(datasource datasource){\n         datasourcetransactionmanager transactionmanager=new datasourcetransactionmanager();\n         transactionmanager.setdatasource(datasource);\n         return transactionmanager;\n     \n     }\n     \n   \n   * 注解驱动，包扫描\n     \n     //开启事务\n     @enabletransactionmanagement\n     \n     //\x3c!--配置组件扫描--\x3e  \x3c!--配合注解开发使用--\x3e\n     @componentscan("com.diana")\n     ',charsets:{cjk:!0}},{title:"Redis内存回收",frontmatter:{autoSort:6,title:"Redis内存回收",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/11b084/",categories:["后端","数据库","Redis"],tags:["知识","数据库","Redis"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/05.Redis/55.Redis%E5%86%85%E5%AD%98%E5%9B%9E%E6%94%B6.html",relativePath:"01.后端/30.数据库/05.Redis/55.Redis内存回收.md",key:"v-4ae15768",path:"/pages/11b084/",headers:[{level:2,title:"过期key处理",slug:"过期key处理",normalizedTitle:"过期key处理",charIndex:2},{level:2,title:"内存淘汰策略",slug:"内存淘汰策略",normalizedTitle:"内存淘汰策略",charIndex:1489}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"过期key处理 内存淘汰策略",content:"# 过期key处理\n\nRedis之所以性能强，最主要的原因就是基于内存存储。然而单节点的Redis其内存大小不宜过大，会影响持久化或主从同步性能。 我们可以通过修改配置文件来设置Redis的最大内存：\n\n\n\n当内存使用达到上限时，就无法存储更多数据了。为了解决这个问题，Redis提供了一些策略实现内存回收：\n\n内存过期策略\n\n在学习Redis缓存的时候我们说过，可以通过expire命令给Redis的key设置TTL（存活时间）：\n\n\n\n可以发现，当key的TTL到期以后，再次访问name返回的是nil，说明这个key已经不存在了，对应的内存也得到释放。从而起到内存回收的目的。\n\nRedis本身是一个典型的key-value内存存储数据库，因此所有的key、value都保存在之前学习过的Dict结构中。不过在其database结构体中，有两个Dict：一个用来记录key-value；另一个用来记录key-TTL。\n\n\n\n\n\n这里有两个问题需要我们思考： Redis是如何知道一个key是否过期呢？\n\n利用两个Dict分别记录key-value对及key-ttl对\n\n是不是TTL到期就立即删除了呢？\n\n惰性删除\n\n惰性删除：顾明思议并不是在TTL到期后就立刻删除，而是在访问一个key的时候，检查该key的存活时间，如果已经过期才执行删除。\n\n\n\n周期删除\n\n周期删除：顾明思议是通过一个定时任务，周期性的抽样部分过期的key，然后执行删除。执行周期有两种： Redis服务初始化函数initServer()中设置定时任务，按照server.hz的频率来执行过期key清理，模式为SLOW Redis的每个事件循环前会调用beforeSleep()函数，执行过期key清理，模式为FAST\n\n周期删除：顾明思议是通过一个定时任务，周期性的抽样部分过期的key，然后执行删除。执行周期有两种： Redis服务初始化函数initServer()中设置定时任务，按照server.hz的频率来执行过期key清理，模式为SLOW Redis的每个事件循环前会调用beforeSleep()函数，执行过期key清理，模式为FAST\n\nSLOW模式规则：\n\n * 执行频率受server.hz影响，默认为10，即每秒执行10次，每个执行周期100ms。\n * 执行清理耗时不超过一次执行周期的25%.默认slow模式耗时不超过25ms\n * 逐个遍历db，逐个遍历db中的bucket，抽取20个key判断是否过期\n * 如果没达到时间上限（25ms）并且过期key比例大于10%，再进行一次抽样，否则结束\n * FAST模式规则（过期key比例小于10%不执行 ）：\n * 执行频率受beforeSleep()调用频率影响，但两次FAST模式间隔不低于2ms\n * 执行清理耗时不超过1ms\n * 逐个遍历db，逐个遍历db中的bucket，抽取20个key判断是否过期 如果没达到时间上限（1ms）并且过期key比例大于10%，再进行一次抽样，否则结束\n\n小总结：\n\nRedisKey的TTL记录方式：\n\n在RedisDB中通过一个Dict记录每个Key的TTL时间\n\n过期key的删除策略：\n\n惰性清理：每次查找key时判断是否过期，如果过期则删除\n\n定期清理：定期抽样部分key，判断是否过期，如果过期则删除。 定期清理的两种模式：\n\nSLOW模式执行频率默认为10，每次不超过25ms\n\nFAST模式执行频率不固定，但两次间隔不低于2ms，每次耗时不超过1ms\n\n\n# 内存淘汰策略\n\n内存淘汰：就是当Redis内存使用达到设置的上限时，主动挑选部分key删除以释放更多内存的流程。Redis会在处理客户端命令的方法processCommand()中尝试做内存淘汰：\n\n\n\n淘汰策略\n\nRedis支持8种不同策略来选择要删除的key：\n\n * noeviction： 不淘汰任何key，但是内存满时不允许写入新数据，默认就是这种策略。\n * volatile-ttl： 对设置了TTL的key，比较key的剩余TTL值，TTL越小越先被淘汰\n * allkeys-random：对全体key ，随机进行淘汰。也就是直接从db->dict中随机挑选\n * volatile-random：对设置了TTL的key ，随机进行淘汰。也就是从db->expires中随机挑选。\n * allkeys-lru： 对全体key，基于LRU算法进行淘汰\n * volatile-lru： 对设置了TTL的key，基于LRU算法进行淘汰\n * allkeys-lfu： 对全体key，基于LFU算法进行淘汰\n * volatile-lfu： 对设置了TTL的key，基于LFI算法进行淘汰 比较容易混淆的有两个：\n   * LRU（Least Recently Used），最少最近使用。用当前时间减去最后一次访问时间，这个值越大则淘汰优先级越高。\n   * LFU（Least Frequently Used），最少频率使用。会统计每个key的访问频率，值越小淘汰优先级越高。\n\nRedis的数据都会被封装为RedisObject结构：\n\n\n\nLFU的访问次数之所以叫做逻辑访问次数，是因为并不是每次key被访问都计数，而是通过运算：\n\n * 生成0~1之间的随机数R\n * 计算 (旧次数 * lfu_log_factor + 1)，记录为P\n * 如果 R < P ，则计数器 + 1，且最大不超过255\n * 访问次数会随时间衰减，距离上一次访问时间每隔 lfu_decay_time 分钟，计数器 -1\n\n最后用一副图来描述当前的这个流程吧\n\n",normalizedContent:"# 过期key处理\n\nredis之所以性能强，最主要的原因就是基于内存存储。然而单节点的redis其内存大小不宜过大，会影响持久化或主从同步性能。 我们可以通过修改配置文件来设置redis的最大内存：\n\n\n\n当内存使用达到上限时，就无法存储更多数据了。为了解决这个问题，redis提供了一些策略实现内存回收：\n\n内存过期策略\n\n在学习redis缓存的时候我们说过，可以通过expire命令给redis的key设置ttl（存活时间）：\n\n\n\n可以发现，当key的ttl到期以后，再次访问name返回的是nil，说明这个key已经不存在了，对应的内存也得到释放。从而起到内存回收的目的。\n\nredis本身是一个典型的key-value内存存储数据库，因此所有的key、value都保存在之前学习过的dict结构中。不过在其database结构体中，有两个dict：一个用来记录key-value；另一个用来记录key-ttl。\n\n\n\n\n\n这里有两个问题需要我们思考： redis是如何知道一个key是否过期呢？\n\n利用两个dict分别记录key-value对及key-ttl对\n\n是不是ttl到期就立即删除了呢？\n\n惰性删除\n\n惰性删除：顾明思议并不是在ttl到期后就立刻删除，而是在访问一个key的时候，检查该key的存活时间，如果已经过期才执行删除。\n\n\n\n周期删除\n\n周期删除：顾明思议是通过一个定时任务，周期性的抽样部分过期的key，然后执行删除。执行周期有两种： redis服务初始化函数initserver()中设置定时任务，按照server.hz的频率来执行过期key清理，模式为slow redis的每个事件循环前会调用beforesleep()函数，执行过期key清理，模式为fast\n\n周期删除：顾明思议是通过一个定时任务，周期性的抽样部分过期的key，然后执行删除。执行周期有两种： redis服务初始化函数initserver()中设置定时任务，按照server.hz的频率来执行过期key清理，模式为slow redis的每个事件循环前会调用beforesleep()函数，执行过期key清理，模式为fast\n\nslow模式规则：\n\n * 执行频率受server.hz影响，默认为10，即每秒执行10次，每个执行周期100ms。\n * 执行清理耗时不超过一次执行周期的25%.默认slow模式耗时不超过25ms\n * 逐个遍历db，逐个遍历db中的bucket，抽取20个key判断是否过期\n * 如果没达到时间上限（25ms）并且过期key比例大于10%，再进行一次抽样，否则结束\n * fast模式规则（过期key比例小于10%不执行 ）：\n * 执行频率受beforesleep()调用频率影响，但两次fast模式间隔不低于2ms\n * 执行清理耗时不超过1ms\n * 逐个遍历db，逐个遍历db中的bucket，抽取20个key判断是否过期 如果没达到时间上限（1ms）并且过期key比例大于10%，再进行一次抽样，否则结束\n\n小总结：\n\nrediskey的ttl记录方式：\n\n在redisdb中通过一个dict记录每个key的ttl时间\n\n过期key的删除策略：\n\n惰性清理：每次查找key时判断是否过期，如果过期则删除\n\n定期清理：定期抽样部分key，判断是否过期，如果过期则删除。 定期清理的两种模式：\n\nslow模式执行频率默认为10，每次不超过25ms\n\nfast模式执行频率不固定，但两次间隔不低于2ms，每次耗时不超过1ms\n\n\n# 内存淘汰策略\n\n内存淘汰：就是当redis内存使用达到设置的上限时，主动挑选部分key删除以释放更多内存的流程。redis会在处理客户端命令的方法processcommand()中尝试做内存淘汰：\n\n\n\n淘汰策略\n\nredis支持8种不同策略来选择要删除的key：\n\n * noeviction： 不淘汰任何key，但是内存满时不允许写入新数据，默认就是这种策略。\n * volatile-ttl： 对设置了ttl的key，比较key的剩余ttl值，ttl越小越先被淘汰\n * allkeys-random：对全体key ，随机进行淘汰。也就是直接从db->dict中随机挑选\n * volatile-random：对设置了ttl的key ，随机进行淘汰。也就是从db->expires中随机挑选。\n * allkeys-lru： 对全体key，基于lru算法进行淘汰\n * volatile-lru： 对设置了ttl的key，基于lru算法进行淘汰\n * allkeys-lfu： 对全体key，基于lfu算法进行淘汰\n * volatile-lfu： 对设置了ttl的key，基于lfi算法进行淘汰 比较容易混淆的有两个：\n   * lru（least recently used），最少最近使用。用当前时间减去最后一次访问时间，这个值越大则淘汰优先级越高。\n   * lfu（least frequently used），最少频率使用。会统计每个key的访问频率，值越小淘汰优先级越高。\n\nredis的数据都会被封装为redisobject结构：\n\n\n\nlfu的访问次数之所以叫做逻辑访问次数，是因为并不是每次key被访问都计数，而是通过运算：\n\n * 生成0~1之间的随机数r\n * 计算 (旧次数 * lfu_log_factor + 1)，记录为p\n * 如果 r < p ，则计数器 + 1，且最大不超过255\n * 访问次数会随时间衰减，距离上一次访问时间每隔 lfu_decay_time 分钟，计数器 -1\n\n最后用一副图来描述当前的这个流程吧\n\n",charsets:{cjk:!0}},{title:"SpringMVC-XML开发",frontmatter:{autoSort:99,title:"SpringMVC-XML开发",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/1d17ee/",categories:["后端","SSM"],tags:["知识","SSM"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/40.SSM/10.SpringMVC-XML.html",relativePath:"01.后端/40.SSM/10.SpringMVC-XML.md",key:"v-39774543",path:"/pages/1d17ee/",headers:[{level:2,title:"SpringMVC开发步骤",slug:"springmvc开发步骤",normalizedTitle:"springmvc开发步骤",charIndex:2},{level:2,title:"SpringMVC 组件解析",slug:"springmvc-组件解析",normalizedTitle:"springmvc 组件解析",charIndex:2e3},{level:2,title:"SpringMVC 数据响应",slug:"springmvc-数据响应",normalizedTitle:"springmvc 数据响应",charIndex:2023},{level:2,title:"SpringMVC 获取请求数据",slug:"springmvc-获取请求数据",normalizedTitle:"springmvc 获取请求数据",charIndex:4790},{level:2,title:"Spring  JdbcTemplate 使用",slug:"spring-jdbctemplate-使用",normalizedTitle:"spring  jdbctemplate 使用",charIndex:null},{level:2,title:"SpinrgMVC拦截器",slug:"spinrgmvc拦截器",normalizedTitle:"spinrgmvc拦截器",charIndex:17213},{level:2,title:"SpringMVC异常处理",slug:"springmvc异常处理",normalizedTitle:"springmvc异常处理",charIndex:18971}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"SpringMVC开发步骤 SpringMVC 组件解析 SpringMVC 数据响应 SpringMVC 获取请求数据 Spring  JdbcTemplate 使用 SpinrgMVC拦截器 SpringMVC异常处理",content:'# SpringMVC开发步骤\n\n>  1. 导入SpringMVC相关坐标\n> \n> \x3c!-- spring mvc--\x3e\n> <dependency>\n>   <groupId>org.springframework</groupId>\n>   <artifactId>spring-webmvc</artifactId>\n>   <version>5.3.6</version>\n> </dependency>\n> \n> \n>  2. 在web.xml中配置SpringMVC核心控制器DispathcerServlet\n> \n> \x3c!--配置SpringMVC的前端控制器--\x3e\n> <servlet>\n>   <servlet-name>DispatcherServlet</servlet-name>\n>   <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>\n>   <init-param> \x3c!-- 加载spring-mvc 配置文件--\x3e\n>     <param-name>contextConfigLocation</param-name>\n>     <param-value>classpath:spring-mvc.xml</param-value>\n>   </init-param>\n>   <load-on-startup>1</load-on-startup>\n> </servlet>\n> <servlet-mapping>\n>   <servlet-name>DispatcherServlet</servlet-name>\n>   <url-pattern>/</url-pattern> \x3c!-- 所有请求都要走 这个--\x3e\n> </servlet-mapping>\n> \n> \n>  3. 创建Controller类(代替以前的servlet类)，并使用注解配置Controller类中业务方法的映射地址\n> \n> @Controller("usercontroller1")\n> @RequestMapping("/user")\n> public class UserController {\n> \t//页面跳转1 -- 返回字符串\n>     //请求方法和网站之间的映射关系\n>     @RequestMapping(value = "/quick1",method = RequestMethod.GET,params = {"username"})\n>     public String save(){\n>         System.out.println("Controller save running……");\n>         //return "/jsp/success.jsp"; //每个网页的前缀都是    /jsp/      后缀都是   .jsp\n>          return "success"; //指定内部资源解析器后  省略前缀和后缀\n> \t\t//return "redirect:/jsp/success.jsp"; //不能省略前缀和后缀\n>     }\n>     \n>     \n>     //回写数据4   --- 先返回User对象 ，然后SpringMVC帮忙转换成JSON字符串（需要导入依赖坐标）\n>     @RequestMapping(value = "/response4")\n>     @ResponseBody\n>     public User res4() throws IOException { \n>         //这里虽然返回的是User对象，但是经过处理器适配器后 会转换成JSON字符串\n>         //模拟数据获取\n>         User user=new User();\n>         user.setUsername("凉冰");\n>         user.setPassword("123");\n>         return user;\n>     }\n> }\n> \n> \n>  4. 编写视图页面(.jsp .html)\n> \n>  5. 配置SpringMVC核心配置文件spring-mvc.xml\n> \n> …………………………\n> \x3c!-- Controller 组件扫描--\x3e\n> <context:component-scan base-package="com.diana.controller"/>\n> …………………………\n> \n> \n>  6. 客户端发起请求测试\n\n\n# SpringMVC 组件解析\n\n\n\n\n\n\n# SpringMVC 数据响应\n\n 1. @RequestMapping的参数 ---\x3e请求方法和网站之间的映射关系\n    \n    > @RequestMapping(value = "/quick1",method = RequestMethod.GET,params = {"username"})\n    > 1.value  用于指定请求的url\n    > 2.method  用于指定请求的方式\n    > 3.params  用于指定限制请求参数的条件\n    >    params={"username"}  表示请求参数中必须要用username\n    >    params={"money！=100"}  表示请求参数中money不能是100\n    > \n    > \n    > 放到类名头上，表示进入该类的方法都需要加前缀 /user\n    > @RequestMapping("/user")\n    > public class UserController{}\n    > \n    > 放到方法名头上，表示执行该方法的最后路径   /quick1\n    > @RequestMapping(value = "/quick1",method = RequestMethod.GET,params = {"username"})\n    > public String save(){}\n    > \n    > 即要想执行save方法，则   ../user/quick1\n\n 2. 页面跳转\n    \n    > 1.返回String字符串(success)形式\n    >     与前(/jsp/)后(.jsp)缀拼好后，默认进行转发(forward)  forward:/jsp/success.jsp\n    >     重定向的话  redirect:/success.jsp\n    > 2.返回ModelAndView类型\n    >     model: 模型   用来封装数据\n    >     view: 视图    用来展示数据   ---与上面返回方式一样，拼字符串访问\n    > \n    > \n    > \x3c!--配置内部资源视图解析器--\x3e\n    >  <bean id="viewResolver" class="org.springframework.web.servlet.view.InternalResourceViewResolver">\n    >      \x3c!-- 设置默认前缀和后缀 --\x3e\n    >      <property name="prefix" value="/jsp/"/>\n    >      <property name="suffix" value=".jsp"/>\n    >  </bean>\n\n 3. 回写数据\n    \n    >  * @ResponseBody 作用\n    >    \n    >    设置当前控制器返回值作为响应体\n    > \n    >  * 回写json数据\n    > \n    > //回写数据4   --- 直接返回User对象 ，SpringMVC自动将对象转换成JSON字符串（需要导入依赖坐标）\n    > @RequestMapping(value = "/response4")\n    > @ResponseBody\n    > public User res4() throws IOException { //这里虽然返回的是User对象，但是经过处理器适配器后 会转换成JSON字符串\n    >     //模拟数据获取\n    >     User user=new User();\n    >     user.setUsername("凉冰");\n    >     user.setPassword("123");\n    >     return user;\n    > }\n    \n    > 1.直接返回字符串\n    >     * 使用response对象，response.getWriter().write("hello world");  --不推荐\n    >     * 使用@ResponseBody 注解  告诉SpringMVC 不进行页面跳转，要响应数据\n    > 2.返回对象或者集合\n    >     *先在spring-mvc中进行处理器适配器配置，然后导入json依赖坐标，\n    >      然后函数直接返回对象或者集合，让mvc自动将其转换为json字符串，也要带着@ResponseBody 注解\n    >     *在spring-mvc中进行mvc的注解驱动，然后导入json依赖坐标， ---\x3e配置更简单\n    >     然后函数直接返回对象或者集合，让mvc自动将其转换为json字符串，也要带着@ResponseBody 注解\n    > \n    > \n    > \x3c!--spring mvc 用 json转换器   1个--\x3e\n    > <dependency>\n    >   <groupId>com.fasterxml.jackson.core</groupId>\n    >   <artifactId>jackson-databind</artifactId>\n    >   <version>2.11.4</version>\n    > </dependency>\n    > \n    > \n    > \x3c!-- mvc的注解驱动\n    >     可以自动加载  处理映射器(requestMappingHandlerMapping)，处理适配器(requestMappingHandlerAdapter)，\n    >     所有这这个配置可以代替 处理映射器和处理适配器的配置\n    >     同时， 默认底层会集成 jackson进行对象或者集合的json字符串转换\n    > \n    >     补：SpringMVC的三大组件\n    >         处理映射器(requestMappingHandlerMapping)\n    >         处理适配器(requestMappingHandlerAdapter)\n    >        视图解析器(InternalResourceViewResolver)\n    > --\x3e\n    > <mvc:annotation-driven/>\n\n\n# SpringMVC 获取请求数据\n\n# MVC实现数据请求\n\n 1. 基本类型参数\n    \n    >  * Controller中业务方法的参数名称（形参） 与 请求参数的名称一致，参数值会自动映射匹配。\n    >    \n    >    * 传入的形参 public void param1(String username,String password)\n    >    * 访问url /param1?username=凉冰&password=123\n    > \n    >  * Controller中业务方法的参数名称（形参） 与 请求参数的名称不一致，使用@RequestParam注解显示的绑定。\n    >    \n    >    * (@RequestParam(value = "name",required = false,defaultValue = "diana") String username)\n    >      *value: 请求参数名称的别名\n    >      *required: 该参数  name  是否必须提供， true为必须提供， 如果没有提供 会报错\n    >      *defaultValue: 当没有指定参数时，则使用指定的默认值为参数赋值\n    >      \n\n 2. POJO类型参数\n    \n    > Controller中业务方法的POJO参数的属性名 与 请求参数的名称一致，参数值会自动映射匹配.\n    > \n    > 传入的形参 public void param1(User user) User的属性 String username; String password; Address address;--String city; String province;\n    > \n    >  * 单个pojo\n    >    \n    >    * 访问url /param2?username=凉冰&password=123\n    >    * 输出 //User{username=\'凉冰\', password=\'123\', address=null}\n    > \n    >  * 嵌套pojo\n    >    \n    >    * 访问url /param2?username=凉冰&password=123&address.city=青岛&address.province=山东\n    >    \n    >    * 输出 //User{username=\'凉冰\', password=\'123\', address=Address{city=\'青岛\', province=\'山东\'}}\n\n 3. 数组类型参数（用的不多）\n    \n    >  * Controller中业务方法的数组名称 与 请求参数的名称一致，参数值会自动映射匹配\n    >    * 传入的形参 public void param3(String[] hobbys)\n    >    * 访问url /param3?hobbys=凉冰&hobbys=英雄联盟\n\n 4. 集合类型参数\n    \n    总结\n    \n    * 集合中存普通参数 --- 使用@RequestParam 去接，也可以用json 数据传，不过没太大意义\n    * 集合中存json数据 --- 使用@RequestBody 去接 ----springMVC可以直接将json数据转换成对象(需要开启mvc注解驱动)\n    \n    >  * 集合中存储普通参数\n    >    \n    >    参数前加上@RequestParam public void param51(@RequestParam List<String> params)\n    >    \n    >    * 访问url /param51?params=diana&params=凉冰\n    >    * 输出结果 [diana, 凉冰]\n    > \n    >  * 获得集合参数时，要将集合参数包装到一个pojo中。 ----不常用\n    > \n    >  * 当使用ajax提交时，可以指定contentType为json形式，这样可以直接用集合接收集合数据（形参前加@RequestBody）,无需进行pojo的封装。\n    >    \n    >    * (1).axios 提交请求参数 可以将js的参数转换为json类型，这样在请求时不用指定contentType，但是参数users必须是集合类型，具体参照ajax.html\n    >          js: 键值可以不带引号     json： 键值必须带引号\n    >          axios({\n    >              method:"POST",\n    >              url:"http://localhost/SpringMVC_war/param5",\n    >              data:users\n    >          });\n    >                                                  \n    >      (2).传入的形参  public void param5(@RequestBody List<User> users)\n    >                                                  \n    >      (3).因为要引入js文件，所以必须要开启（静态）资源访问权限\n    >          <mvc:resources mapping="/js/**" location="/js/"/>\n    >      (4).如果要使用html，也要开启（静态）资源访问权限\n    >         <mvc:resources mapping="/html/**" location="/html/"/>\n    >      \x3c!-- 开放资源的访问 （一般是静态资源）--\x3e\n    >      \x3c!--    <mvc:resources mapping="/js/**" location="/js/"/>--\x3e\n    >      \x3c!--    <mvc:resources mapping="/img/**" location="/img/"/>--\x3e\n    >      \x3c!--    <mvc:resources mapping="/html/**" location="/html/"/>--\x3e\n    >                                                  \n    >      \x3c!-- 使用原始tomcat  找静态资源  与上面虽然过程不一样，但是功能一样--\x3e\n    >      <mvc:default-servlet-handler/>\n    >                                                  \n    >      (5).在html中导入js文件时，因为不在有两层文件夹，html->ajax.html;js->axios-0.18.0.js\n    >        <script src="../js/axios-0.18.0.js"><\/script>\n    >      \n\n 5. Date类型的参数\n    \n    默认是 2021/02/02 这种，如果想传入其他类型的时间，需要使用@DateTimeFormat\n    \n    //接收Date类型的参数  -- 不使用转换器 使用@DateTimeFormat --都可以\n    @RequestMapping("/date1")\n    @ResponseBody\n    public void test_Date1(Date date1,\n                           @DateTimeFormat(pattern = "yyyy-MM-dd") Date date2,\n                           @DateTimeFormat(pattern = "yyyy-MM-dd HH:mm:ss") Date date3){\n        // 请求url  date1?date1=2021/02/02&date2=2021-04-11&date3=2021-04-11 8:08:08\n        System.out.println(date1);//Tue Feb 02 00:00:00 CST 2021\n        System.out.println(date2);//Sun Apr 11 00:00:00 CST 2021\n        System.out.println(date3);//Sun Apr 11 08:08:08 CST 2021\n    }\n    \n\n# MVC 获取json数据\n\n使用@RequestBody 传入json数据 由mvc转换成对象——————json数据————重点掌握\n\n> User的属性 String username; String password; Address address;--String city; String province;\n> \n>  * 嵌套pojo\n> \n>  * 形参列表 public void param22(@RequestBody User user)\n> \n>  * json\n>    \n>    {\n>        "username":"diana",\n>        "password":"123",\n>        "address":{\n>            "province":"山东",\n>            "city":"青岛"\n>        }\n>    }\n>    \n> \n>  * 输出 User{username=\'diana\', password=\'123\', address=Address{city=\'青岛\', province=\'山东\'}}\n> \n>  * 集合pojo\n> \n>  * 形参列表 public void param5(@RequestBody List<User> users)\n> \n>  * json数据\n>    \n>    * 1. [{"username":"diana","password":"123"}]\n>    \n>    * 2. [{\n>             "username":"diana",\n>             "password":"123",\n>             "address":{\n>                 "province":"山东",\n>                 "city":"青岛"\n>             }},\n>          {\n>              "username":"凉冰",\n>              "password":"456",\n>              "address":{\n>                  "province":"浙江",\n>                  "city":"杭州"\n>              }}\n>         ]\n>         \n> \n>  * 输出\n>    \n>    * 1. [User{username=\'diana\', password=\'123\', address=null}]-- 不分address数据就是null\n>    \n>    * 2. [User{username=\'diana\', password=\'123\', address=Address{city=\'青岛\', province=\'山东\'}},\n>         \n>         User{username=\'凉冰\', password=\'456\', address=Address{city=\'杭州\', province=\'浙江\'}}]\n> \n>  * 集合普通参数\n> \n>  * 形参列表 public void param52(@RequestBody List<String> params)\n> \n>  * json数据 ["diana","凉冰"]\n> \n>  * 输出 //[diana, 凉冰]\n\n# MVC获取数据细节\n\n 1. 中文乱码问题\n    \n    > 在web.xml中配置全局过滤的filter\n    > \n    > \x3c!-- 配置全局过滤的filter--\x3e\n    > <filter>\n    >   <filter-name>CharacterEncodingFilter</filter-name>\n    >   <filter-class>org.springframework.web.filter.CharacterEncodingFilter</filter-class>\n    >   <init-param>   \x3c!-- 解决乱码问题  post --\x3e\n    >     <param-name>encoding</param-name>\n    >     <param-value>UTF-8</param-value>\n    >   </init-param>\n    > </filter>\n    > <filter-mapping>\n    >   <filter-name>CharacterEncodingFilter</filter-name>\n    >   <url-pattern>/*</url-pattern>\n    > </filter-mapping>\n\n 2. Restful风格参数获取\n    \n    >  * @RequestMapping("/restful/{name}") {}里面是占位符\n    >  * 请求url /restful/diana\n    >  * @PathVariable(value="name",required = true)String username 使用注解进行占位符的匹配获取\n    >    * value的值必须要与{}占位符的值一样\n    >    * required设置true 表示必须要这个参数\n\n 3. 自定义类型转换器 步骤\n    \n    >  * 定义转换器实现Converter接口\n    >    \n    >    * public class DateConverter implements Converter<String, Date> {\n    >          //将字符串转换成Date\n    >          // 可以将输入为   /?date=2021-02-02   的字符串 转换为Date类型\n    >          public Date convert(String s) {\n    >              SimpleDateFormat simpleDateFormat = new SimpleDateFormat("yyyy-MM-dd");\n    >              Date date=null;\n    >              try {\n    >                  date=simpleDateFormat.parse(s);\n    >              } catch (ParseException e) {\n    >                  e.printStackTrace();\n    >              }\n    >              return date;\n    >          }\n    >      }\n    >      \n    > \n    >  * 在配置文件中声明转换器\n    >    \n    >    * \x3c!-- 声明转换器--\x3e  \x3c!-- 然后在mvc的注解驱动中声明--\x3e\n    >      <bean id="conversionService" class="org.springframework.context.support.ConversionServiceFactoryBean">\n    >          <property name="converters">\n    >              <list>\n    >                  <bean class="com.diana.converter.DateConverter"/>\n    >              </list>\n    >          </property>\n    >      </bean>\n    >      \n    > \n    >  * 在<annotation-driven>中引用转换器\n    >    \n    >    * <mvc:annotation-driven conversion-service="conversionService"/>\n    >      \n\n 4. 获取Servlet相关API-- 作为形参直接注入即可\n    \n    >  * *HttpServletRequest\n    >    *HttpServletResponse\n    >    *HttpSession\n    >    \n    > \n    >  * public void test_servlet(HttpServletRequest request, HttpServletResponse response, HttpSession session){\n    >            System.out.println(request);\n    >            System.out.println(response);\n    >            System.out.println(session);\n    >    }\n    >    \n\n 5. 获取请求头数据\n    \n    > *使用@RequestHeader   相当于  request.getHeader(name)\n    >     *  value  请求头的名称\n    >     *  required  是否必须携带请求头\n    > *@CookieValue  专门用来获取Cookie的注解\n    >      *  value  指定cookie的名称\n    >      *  required  是否必须携带cookie\n\n 6. 文件上传\n    \n    >  * 文件上传客户端三要素\n    >    \n    >    *type="file"\n    >    *表单提交方式为 post\n    >    *表单的enctype属性是多部分表单形式  即 enctype="multipart/form-data"\n    >    \n    >    \n    >    \x3c!-- 单文件上传--\x3e\n    >    <form action="${pageContext.request.contextPath}/file" method="post" enctype="multipart/form-data">\n    >        名称<input type="text" name="username"><br>\n    >        文件<input type="file" name="upload"><br>\n    >        <input type="submit" value="提交"><br>\n    >    </form>\n    >    \n    > \n    >  * 文件上传步骤\n    >    \n    >    * 导入fileupload和io坐标\n    >      \n    >      \x3c!-- io--\x3e\n    >      <dependency>\n    >        <groupId>commons-io</groupId>\n    >        <artifactId>commons-io</artifactId>\n    >        <version>2.6</version>\n    >      </dependency>\n    >      \x3c!--fileupload--\x3e\n    >      <dependency>\n    >        <groupId>commons-fileupload</groupId>\n    >        <artifactId>commons-fileupload</artifactId>\n    >        <version>1.3.3</version>\n    >      </dependency>\n    >      \n    >    \n    >    * 配置文件上传解析器 spring-mvc\n    >      \n    >      \x3c!-- 配置文件上传解析器--\x3e\n    >      <bean id="multipartResolver" class="org.springframework.web.multipart.commons.CommonsMultipartResolver">\n    >          <property name="defaultEncoding" value="UTF-8"/>\n    >          <property name="maxUploadSize" value="50000"/>  \x3c!-- 上传文件总大小--\x3e\n    >          <property name="maxUploadSizePerFile" value="5000"/>  \x3c!-- 上传文件总大小--\x3e\n    >      </bean>\n    >      \n    >    \n    >    * 编写文件上传代码\n    >      \n    >      * 单文件上传\n    >      \n    >      名称<input type="text" name="username"><br>\n    >      文件<input type="file" name="upload"><br>\n    >      String username, MultipartFile upload\n    >      第一个参数（String）对应第一个name值，第二个参数(MultipartFile 对象)对应第二个name值\n    >      \n    >      \n    >      * 多文件上传\n    >        \n    >        * 直接用两个参数进行接收\n    >          \n    >          String username, MultipartFile upload1,MultipartFile upload2\n    >          \n    >        \n    >        * 使用数组接收\n    >          \n    >          MultipartFile[] uploads   -- 同hobbys\n    >          \n\n\n# Spring JdbcTemplate 使用\n\n不常用了解即可，后期有mybatis\n\n>  1. 导入spring-jdbc，spring-tx坐标\n>     \n>     <dependency>\n>       <groupId>org.springframework</groupId>\n>       <artifactId>spring-jdbc</artifactId>\n>       <version>5.3.6</version>\n>     </dependency>\n>     <dependency>\n>       <groupId>org.springframework</groupId>\n>       <artifactId>spring-tx</artifactId>\n>       <version>5.3.6</version>\n>     </dependency>\n>     \n> \n>  2. 创建数据库和实体\n> \n>  3. 创建jdbcTemplate对象，或者让spring帮我们创建对象\n>     \n>     \x3c!--加载外部配置文件property--\x3e\n>     <context:property-placeholder location="classpath:jdbc.properties"/>\n>     \x3c!-- el表达式 来获取配置文件信息--\x3e\n>     <bean id="dataSource_c3p0" class="com.mchange.v2.c3p0.ComboPooledDataSource">\n>         <property name="driverClass" value="${jdbc.driver}"/>\n>         <property name="user" value="${jdbc.username}"/>\n>         <property name="password" value="${jdbc.password}"/>\n>         <property name="jdbcUrl" value="${jdbc.url}"/>\n>     </bean>\n>     \n>     \x3c!-- 在spring 容器中产生 jdbc模板对象   同时注入数据源--\x3e\n>     <bean id="jdbcTemplate" class="org.springframework.jdbc.core.JdbcTemplate">\n>         <property name="dataSource" ref="dataSource_c3p0"/>\n>     </bean>\n>     \n> \n>  4. 执行数据库操作\n>     \n>     @Test//删除\n>     public void testDelete(){\n>         //增删改 都是 update方法   里面语句不一样而已\n>         int row = jdbcTemplate.update("delete from account where name=?",  "diana");\n>         System.out.println(row);\n>     }\n>                                                     \n>     @Test//查询全部\n>     public void testQueryAll(){\n>         List<User> users = jdbcTemplate.query("select * from user", new BeanPropertyRowMapper<User>(User.class));\n>         System.out.println(users);\n>     }\n>                                                     \n>     @Test//查询单个\n>     public void testQueryOne(){//Bean 类型封装\n>         User user = jdbcTemplate.queryForObject("select * from user where username=?", new BeanPropertyRowMapper<User>(User.class),"凉冰");\n>         System.out.println(user);\n>     }\n>                                                     \n>     @Test//聚合查询\n>     public void testQueryCount(){//简单类型\n>         int count = jdbcTemplate.queryForObject("select count(*) from user",int.class);\n>         System.out.println(count);\n>     }\n>     \n\n\n# SpinrgMVC拦截器\n\n这里的异常处理是xml完成的\n\n注解开发\n\n>  1. 生成拦截类，实现HandlerInterceptor接口（有三个方法）。\n>     \n>     public class MyInterceptor2 implements HandlerInterceptor {\n>     \n>         //在目标方法执行之前  执行\n>         public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {\n>             System.out.println("要执行方法了~~~~222");\n>             return true;\n>         }\n>     \n>         //在目标方法执行之后 视图对象返回之前执行\n>         public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception {\n>             System.out.println("执行完方法了，要跳转到视图对象了！！222");\n>             modelAndView.addObject("user1","凉冰");//将user1的参数由diana改成了凉冰\n>         }\n>     \n>         //在所有流程执行完毕后\n>         public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception {\n>             System.out.println("全部执行完了！！！222");\n>         }\n>     }\n>     \n> \n>  2. 配置拦截器。\n>     \n>     \x3c!-- 配置拦截器--\x3e\n>     <mvc:interceptors>\n>         \x3c!--拦截器1--\x3e\n>         <mvc:interceptor>\n>             \x3c!--对哪些资源进行拦截 /** 表示拦截所有--\x3e\n>         <mvc:mapping path="/**"/>\n>         <bean class="com.diana.interceptor.MyInterceptor1"/>\n>     </mvc:interceptor>\n>         \x3c!--拦截器2--\x3e\n>         <mvc:interceptor>\n>             \x3c!--对哪些资源进行拦截--\x3e\n>             <mvc:mapping path="/**"/>\n>             <bean class="com.diana.interceptor.MyInterceptor2"/>\n>         </mvc:interceptor>\n>     </mvc:interceptors>\n>     \n>     \n>     ---拦截器链的执行顺序----\n>     \n>     要执行方法了~~~~\n>     要执行方法了~~~~222\n>     目标方法正在执行！！\n>     执行完方法了，要跳转到视图对象了！！222\n>     执行完方法了，要跳转到视图对象了！！\n>     全部执行完了！！！222\n>     全部执行完了！！！\n>     \n>     \n>     \n>     \n> \n>  3. 编写方法并进行测试。\n\n\n# SpringMVC异常处理\n\n这里的异常处理是xml完成的\n\n注解开发\n\n>  1. instanceof的用法\n> \n> instanceof 是 Java 的一个二元操作符，类似于 ==，>，< 等操作符。\n> instanceof 是 Java 的保留关键字。它的作用是测试它左边的对象是否是它右边的类的实例，返回 boolean 的数据类型。\n> \n> \n>  1. 异常处理的两种方法\n>     \n>     * 使用SpringMVC提供的简单异常处理器SimpleMappingExceptionResolver\n>       \n>       \x3c!-- 配置简单映射异常处理器--\x3e\n>       <bean class="org.springframework.web.servlet.handler.SimpleMappingExceptionResolver">\n>           \x3c!-- 默认错误视图    value 是视图名称（自己随便起名）  省略了前后缀--\x3e\n>           <property name="defaultErrorView" value="/exception/error"/>\n>           \x3c!-- 异常错误映射  优先判断里面的异常类型，里面没有时 才会进入默认错误视图--\x3e\n>           <property name="exceptionMappings">\n>               <map>\n>                   <entry key="com.diana.exception.MyException" value="/exception/Myerror"/>\x3c!-- 自定义异常--\x3e\n>                   <entry key="java.lang.ClassCastException" value="/exception/error_class"/>\x3c!-- 类型转换异常--\x3e\n>               </map>\n>           </property>\n>       </bean>\n>       \n>     \n>     * 实现Spring的异常处理接口HandlerExceptionResolver自定义自己的异常处理器\n>       \n>       * 创建异常处理类实现 接口 HandlerExceptionResolver\n>         \n>         public class MyExceptionResolver implements HandlerExceptionResolver {\n>             /*\n>                 参数  Exception e :异常对象\n>                返回值 ModelAndView : 跳转到错误视图信息\n>              */\n>             public ModelAndView resolveException(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) {\n>                 ModelAndView modelAndView=new ModelAndView();\n>                 //instanceof 是 Java 的一个二元操作符，类似于 ==，>，< 等操作符。\n>                 //instanceof 是 Java 的保留关键字。它的作用是测试它左边的对象是否是它右边的类的实例，返回 boolean 的数据类型。\n>                 if(e instanceof MyException){\n>                     //执行异常相应的操作\n>                     modelAndView.addObject("info","自定义异常");\n>                     //modelAndView.setViewName("/exception/Myerror"); //跳到自己的页面\n>                 }else if(e instanceof ClassCastException){\n>                     //执行异常相应的操作\n>                     modelAndView.addObject("info","类转换异常");\n>                     //modelAndView.setViewName("/exception/error_class");//跳到自己的页面\n>                 }else{\n>                     //执行异常相应的操作\n>                     modelAndView.addObject("info","默认类型异常");\n>                     //modelAndView.setViewName("/exception/error");//跳到自己的页面\n>                 }\n>                 modelAndView.setViewName("/exception/error"); //都跳转error页面\n>                 return modelAndView;\n>             }\n>         }\n>         \n>       \n>       * 配置异常处理器\n>         \n>         \x3c!-- 配置自定义异常处理器--\x3e\n>         <bean class="com.diana.resolver.MyExceptionResolver"/>\n>         \n>       \n>       * 编写异常页面\n>       \n>       * 测试异常跳转\n>       \n>       3. 在编写代码时，遇到异常就往外抛，只在最后同一对异常进行处理。',normalizedContent:'# springmvc开发步骤\n\n>  1. 导入springmvc相关坐标\n> \n> \x3c!-- spring mvc--\x3e\n> <dependency>\n>   <groupid>org.springframework</groupid>\n>   <artifactid>spring-webmvc</artifactid>\n>   <version>5.3.6</version>\n> </dependency>\n> \n> \n>  2. 在web.xml中配置springmvc核心控制器dispathcerservlet\n> \n> \x3c!--配置springmvc的前端控制器--\x3e\n> <servlet>\n>   <servlet-name>dispatcherservlet</servlet-name>\n>   <servlet-class>org.springframework.web.servlet.dispatcherservlet</servlet-class>\n>   <init-param> \x3c!-- 加载spring-mvc 配置文件--\x3e\n>     <param-name>contextconfiglocation</param-name>\n>     <param-value>classpath:spring-mvc.xml</param-value>\n>   </init-param>\n>   <load-on-startup>1</load-on-startup>\n> </servlet>\n> <servlet-mapping>\n>   <servlet-name>dispatcherservlet</servlet-name>\n>   <url-pattern>/</url-pattern> \x3c!-- 所有请求都要走 这个--\x3e\n> </servlet-mapping>\n> \n> \n>  3. 创建controller类(代替以前的servlet类)，并使用注解配置controller类中业务方法的映射地址\n> \n> @controller("usercontroller1")\n> @requestmapping("/user")\n> public class usercontroller {\n> \t//页面跳转1 -- 返回字符串\n>     //请求方法和网站之间的映射关系\n>     @requestmapping(value = "/quick1",method = requestmethod.get,params = {"username"})\n>     public string save(){\n>         system.out.println("controller save running……");\n>         //return "/jsp/success.jsp"; //每个网页的前缀都是    /jsp/      后缀都是   .jsp\n>          return "success"; //指定内部资源解析器后  省略前缀和后缀\n> \t\t//return "redirect:/jsp/success.jsp"; //不能省略前缀和后缀\n>     }\n>     \n>     \n>     //回写数据4   --- 先返回user对象 ，然后springmvc帮忙转换成json字符串（需要导入依赖坐标）\n>     @requestmapping(value = "/response4")\n>     @responsebody\n>     public user res4() throws ioexception { \n>         //这里虽然返回的是user对象，但是经过处理器适配器后 会转换成json字符串\n>         //模拟数据获取\n>         user user=new user();\n>         user.setusername("凉冰");\n>         user.setpassword("123");\n>         return user;\n>     }\n> }\n> \n> \n>  4. 编写视图页面(.jsp .html)\n> \n>  5. 配置springmvc核心配置文件spring-mvc.xml\n> \n> …………………………\n> \x3c!-- controller 组件扫描--\x3e\n> <context:component-scan base-package="com.diana.controller"/>\n> …………………………\n> \n> \n>  6. 客户端发起请求测试\n\n\n# springmvc 组件解析\n\n\n\n\n\n\n# springmvc 数据响应\n\n 1. @requestmapping的参数 ---\x3e请求方法和网站之间的映射关系\n    \n    > @requestmapping(value = "/quick1",method = requestmethod.get,params = {"username"})\n    > 1.value  用于指定请求的url\n    > 2.method  用于指定请求的方式\n    > 3.params  用于指定限制请求参数的条件\n    >    params={"username"}  表示请求参数中必须要用username\n    >    params={"money！=100"}  表示请求参数中money不能是100\n    > \n    > \n    > 放到类名头上，表示进入该类的方法都需要加前缀 /user\n    > @requestmapping("/user")\n    > public class usercontroller{}\n    > \n    > 放到方法名头上，表示执行该方法的最后路径   /quick1\n    > @requestmapping(value = "/quick1",method = requestmethod.get,params = {"username"})\n    > public string save(){}\n    > \n    > 即要想执行save方法，则   ../user/quick1\n\n 2. 页面跳转\n    \n    > 1.返回string字符串(success)形式\n    >     与前(/jsp/)后(.jsp)缀拼好后，默认进行转发(forward)  forward:/jsp/success.jsp\n    >     重定向的话  redirect:/success.jsp\n    > 2.返回modelandview类型\n    >     model: 模型   用来封装数据\n    >     view: 视图    用来展示数据   ---与上面返回方式一样，拼字符串访问\n    > \n    > \n    > \x3c!--配置内部资源视图解析器--\x3e\n    >  <bean id="viewresolver" class="org.springframework.web.servlet.view.internalresourceviewresolver">\n    >      \x3c!-- 设置默认前缀和后缀 --\x3e\n    >      <property name="prefix" value="/jsp/"/>\n    >      <property name="suffix" value=".jsp"/>\n    >  </bean>\n\n 3. 回写数据\n    \n    >  * @responsebody 作用\n    >    \n    >    设置当前控制器返回值作为响应体\n    > \n    >  * 回写json数据\n    > \n    > //回写数据4   --- 直接返回user对象 ，springmvc自动将对象转换成json字符串（需要导入依赖坐标）\n    > @requestmapping(value = "/response4")\n    > @responsebody\n    > public user res4() throws ioexception { //这里虽然返回的是user对象，但是经过处理器适配器后 会转换成json字符串\n    >     //模拟数据获取\n    >     user user=new user();\n    >     user.setusername("凉冰");\n    >     user.setpassword("123");\n    >     return user;\n    > }\n    \n    > 1.直接返回字符串\n    >     * 使用response对象，response.getwriter().write("hello world");  --不推荐\n    >     * 使用@responsebody 注解  告诉springmvc 不进行页面跳转，要响应数据\n    > 2.返回对象或者集合\n    >     *先在spring-mvc中进行处理器适配器配置，然后导入json依赖坐标，\n    >      然后函数直接返回对象或者集合，让mvc自动将其转换为json字符串，也要带着@responsebody 注解\n    >     *在spring-mvc中进行mvc的注解驱动，然后导入json依赖坐标， ---\x3e配置更简单\n    >     然后函数直接返回对象或者集合，让mvc自动将其转换为json字符串，也要带着@responsebody 注解\n    > \n    > \n    > \x3c!--spring mvc 用 json转换器   1个--\x3e\n    > <dependency>\n    >   <groupid>com.fasterxml.jackson.core</groupid>\n    >   <artifactid>jackson-databind</artifactid>\n    >   <version>2.11.4</version>\n    > </dependency>\n    > \n    > \n    > \x3c!-- mvc的注解驱动\n    >     可以自动加载  处理映射器(requestmappinghandlermapping)，处理适配器(requestmappinghandleradapter)，\n    >     所有这这个配置可以代替 处理映射器和处理适配器的配置\n    >     同时， 默认底层会集成 jackson进行对象或者集合的json字符串转换\n    > \n    >     补：springmvc的三大组件\n    >         处理映射器(requestmappinghandlermapping)\n    >         处理适配器(requestmappinghandleradapter)\n    >        视图解析器(internalresourceviewresolver)\n    > --\x3e\n    > <mvc:annotation-driven/>\n\n\n# springmvc 获取请求数据\n\n# mvc实现数据请求\n\n 1. 基本类型参数\n    \n    >  * controller中业务方法的参数名称（形参） 与 请求参数的名称一致，参数值会自动映射匹配。\n    >    \n    >    * 传入的形参 public void param1(string username,string password)\n    >    * 访问url /param1?username=凉冰&password=123\n    > \n    >  * controller中业务方法的参数名称（形参） 与 请求参数的名称不一致，使用@requestparam注解显示的绑定。\n    >    \n    >    * (@requestparam(value = "name",required = false,defaultvalue = "diana") string username)\n    >      *value: 请求参数名称的别名\n    >      *required: 该参数  name  是否必须提供， true为必须提供， 如果没有提供 会报错\n    >      *defaultvalue: 当没有指定参数时，则使用指定的默认值为参数赋值\n    >      \n\n 2. pojo类型参数\n    \n    > controller中业务方法的pojo参数的属性名 与 请求参数的名称一致，参数值会自动映射匹配.\n    > \n    > 传入的形参 public void param1(user user) user的属性 string username; string password; address address;--string city; string province;\n    > \n    >  * 单个pojo\n    >    \n    >    * 访问url /param2?username=凉冰&password=123\n    >    * 输出 //user{username=\'凉冰\', password=\'123\', address=null}\n    > \n    >  * 嵌套pojo\n    >    \n    >    * 访问url /param2?username=凉冰&password=123&address.city=青岛&address.province=山东\n    >    \n    >    * 输出 //user{username=\'凉冰\', password=\'123\', address=address{city=\'青岛\', province=\'山东\'}}\n\n 3. 数组类型参数（用的不多）\n    \n    >  * controller中业务方法的数组名称 与 请求参数的名称一致，参数值会自动映射匹配\n    >    * 传入的形参 public void param3(string[] hobbys)\n    >    * 访问url /param3?hobbys=凉冰&hobbys=英雄联盟\n\n 4. 集合类型参数\n    \n    总结\n    \n    * 集合中存普通参数 --- 使用@requestparam 去接，也可以用json 数据传，不过没太大意义\n    * 集合中存json数据 --- 使用@requestbody 去接 ----springmvc可以直接将json数据转换成对象(需要开启mvc注解驱动)\n    \n    >  * 集合中存储普通参数\n    >    \n    >    参数前加上@requestparam public void param51(@requestparam list<string> params)\n    >    \n    >    * 访问url /param51?params=diana&params=凉冰\n    >    * 输出结果 [diana, 凉冰]\n    > \n    >  * 获得集合参数时，要将集合参数包装到一个pojo中。 ----不常用\n    > \n    >  * 当使用ajax提交时，可以指定contenttype为json形式，这样可以直接用集合接收集合数据（形参前加@requestbody）,无需进行pojo的封装。\n    >    \n    >    * (1).axios 提交请求参数 可以将js的参数转换为json类型，这样在请求时不用指定contenttype，但是参数users必须是集合类型，具体参照ajax.html\n    >          js: 键值可以不带引号     json： 键值必须带引号\n    >          axios({\n    >              method:"post",\n    >              url:"http://localhost/springmvc_war/param5",\n    >              data:users\n    >          });\n    >                                                  \n    >      (2).传入的形参  public void param5(@requestbody list<user> users)\n    >                                                  \n    >      (3).因为要引入js文件，所以必须要开启（静态）资源访问权限\n    >          <mvc:resources mapping="/js/**" location="/js/"/>\n    >      (4).如果要使用html，也要开启（静态）资源访问权限\n    >         <mvc:resources mapping="/html/**" location="/html/"/>\n    >      \x3c!-- 开放资源的访问 （一般是静态资源）--\x3e\n    >      \x3c!--    <mvc:resources mapping="/js/**" location="/js/"/>--\x3e\n    >      \x3c!--    <mvc:resources mapping="/img/**" location="/img/"/>--\x3e\n    >      \x3c!--    <mvc:resources mapping="/html/**" location="/html/"/>--\x3e\n    >                                                  \n    >      \x3c!-- 使用原始tomcat  找静态资源  与上面虽然过程不一样，但是功能一样--\x3e\n    >      <mvc:default-servlet-handler/>\n    >                                                  \n    >      (5).在html中导入js文件时，因为不在有两层文件夹，html->ajax.html;js->axios-0.18.0.js\n    >        <script src="../js/axios-0.18.0.js"><\/script>\n    >      \n\n 5. date类型的参数\n    \n    默认是 2021/02/02 这种，如果想传入其他类型的时间，需要使用@datetimeformat\n    \n    //接收date类型的参数  -- 不使用转换器 使用@datetimeformat --都可以\n    @requestmapping("/date1")\n    @responsebody\n    public void test_date1(date date1,\n                           @datetimeformat(pattern = "yyyy-mm-dd") date date2,\n                           @datetimeformat(pattern = "yyyy-mm-dd hh:mm:ss") date date3){\n        // 请求url  date1?date1=2021/02/02&date2=2021-04-11&date3=2021-04-11 8:08:08\n        system.out.println(date1);//tue feb 02 00:00:00 cst 2021\n        system.out.println(date2);//sun apr 11 00:00:00 cst 2021\n        system.out.println(date3);//sun apr 11 08:08:08 cst 2021\n    }\n    \n\n# mvc 获取json数据\n\n使用@requestbody 传入json数据 由mvc转换成对象——————json数据————重点掌握\n\n> user的属性 string username; string password; address address;--string city; string province;\n> \n>  * 嵌套pojo\n> \n>  * 形参列表 public void param22(@requestbody user user)\n> \n>  * json\n>    \n>    {\n>        "username":"diana",\n>        "password":"123",\n>        "address":{\n>            "province":"山东",\n>            "city":"青岛"\n>        }\n>    }\n>    \n> \n>  * 输出 user{username=\'diana\', password=\'123\', address=address{city=\'青岛\', province=\'山东\'}}\n> \n>  * 集合pojo\n> \n>  * 形参列表 public void param5(@requestbody list<user> users)\n> \n>  * json数据\n>    \n>    * 1. [{"username":"diana","password":"123"}]\n>    \n>    * 2. [{\n>             "username":"diana",\n>             "password":"123",\n>             "address":{\n>                 "province":"山东",\n>                 "city":"青岛"\n>             }},\n>          {\n>              "username":"凉冰",\n>              "password":"456",\n>              "address":{\n>                  "province":"浙江",\n>                  "city":"杭州"\n>              }}\n>         ]\n>         \n> \n>  * 输出\n>    \n>    * 1. [user{username=\'diana\', password=\'123\', address=null}]-- 不分address数据就是null\n>    \n>    * 2. [user{username=\'diana\', password=\'123\', address=address{city=\'青岛\', province=\'山东\'}},\n>         \n>         user{username=\'凉冰\', password=\'456\', address=address{city=\'杭州\', province=\'浙江\'}}]\n> \n>  * 集合普通参数\n> \n>  * 形参列表 public void param52(@requestbody list<string> params)\n> \n>  * json数据 ["diana","凉冰"]\n> \n>  * 输出 //[diana, 凉冰]\n\n# mvc获取数据细节\n\n 1. 中文乱码问题\n    \n    > 在web.xml中配置全局过滤的filter\n    > \n    > \x3c!-- 配置全局过滤的filter--\x3e\n    > <filter>\n    >   <filter-name>characterencodingfilter</filter-name>\n    >   <filter-class>org.springframework.web.filter.characterencodingfilter</filter-class>\n    >   <init-param>   \x3c!-- 解决乱码问题  post --\x3e\n    >     <param-name>encoding</param-name>\n    >     <param-value>utf-8</param-value>\n    >   </init-param>\n    > </filter>\n    > <filter-mapping>\n    >   <filter-name>characterencodingfilter</filter-name>\n    >   <url-pattern>/*</url-pattern>\n    > </filter-mapping>\n\n 2. restful风格参数获取\n    \n    >  * @requestmapping("/restful/{name}") {}里面是占位符\n    >  * 请求url /restful/diana\n    >  * @pathvariable(value="name",required = true)string username 使用注解进行占位符的匹配获取\n    >    * value的值必须要与{}占位符的值一样\n    >    * required设置true 表示必须要这个参数\n\n 3. 自定义类型转换器 步骤\n    \n    >  * 定义转换器实现converter接口\n    >    \n    >    * public class dateconverter implements converter<string, date> {\n    >          //将字符串转换成date\n    >          // 可以将输入为   /?date=2021-02-02   的字符串 转换为date类型\n    >          public date convert(string s) {\n    >              simpledateformat simpledateformat = new simpledateformat("yyyy-mm-dd");\n    >              date date=null;\n    >              try {\n    >                  date=simpledateformat.parse(s);\n    >              } catch (parseexception e) {\n    >                  e.printstacktrace();\n    >              }\n    >              return date;\n    >          }\n    >      }\n    >      \n    > \n    >  * 在配置文件中声明转换器\n    >    \n    >    * \x3c!-- 声明转换器--\x3e  \x3c!-- 然后在mvc的注解驱动中声明--\x3e\n    >      <bean id="conversionservice" class="org.springframework.context.support.conversionservicefactorybean">\n    >          <property name="converters">\n    >              <list>\n    >                  <bean class="com.diana.converter.dateconverter"/>\n    >              </list>\n    >          </property>\n    >      </bean>\n    >      \n    > \n    >  * 在<annotation-driven>中引用转换器\n    >    \n    >    * <mvc:annotation-driven conversion-service="conversionservice"/>\n    >      \n\n 4. 获取servlet相关api-- 作为形参直接注入即可\n    \n    >  * *httpservletrequest\n    >    *httpservletresponse\n    >    *httpsession\n    >    \n    > \n    >  * public void test_servlet(httpservletrequest request, httpservletresponse response, httpsession session){\n    >            system.out.println(request);\n    >            system.out.println(response);\n    >            system.out.println(session);\n    >    }\n    >    \n\n 5. 获取请求头数据\n    \n    > *使用@requestheader   相当于  request.getheader(name)\n    >     *  value  请求头的名称\n    >     *  required  是否必须携带请求头\n    > *@cookievalue  专门用来获取cookie的注解\n    >      *  value  指定cookie的名称\n    >      *  required  是否必须携带cookie\n\n 6. 文件上传\n    \n    >  * 文件上传客户端三要素\n    >    \n    >    *type="file"\n    >    *表单提交方式为 post\n    >    *表单的enctype属性是多部分表单形式  即 enctype="multipart/form-data"\n    >    \n    >    \n    >    \x3c!-- 单文件上传--\x3e\n    >    <form action="${pagecontext.request.contextpath}/file" method="post" enctype="multipart/form-data">\n    >        名称<input type="text" name="username"><br>\n    >        文件<input type="file" name="upload"><br>\n    >        <input type="submit" value="提交"><br>\n    >    </form>\n    >    \n    > \n    >  * 文件上传步骤\n    >    \n    >    * 导入fileupload和io坐标\n    >      \n    >      \x3c!-- io--\x3e\n    >      <dependency>\n    >        <groupid>commons-io</groupid>\n    >        <artifactid>commons-io</artifactid>\n    >        <version>2.6</version>\n    >      </dependency>\n    >      \x3c!--fileupload--\x3e\n    >      <dependency>\n    >        <groupid>commons-fileupload</groupid>\n    >        <artifactid>commons-fileupload</artifactid>\n    >        <version>1.3.3</version>\n    >      </dependency>\n    >      \n    >    \n    >    * 配置文件上传解析器 spring-mvc\n    >      \n    >      \x3c!-- 配置文件上传解析器--\x3e\n    >      <bean id="multipartresolver" class="org.springframework.web.multipart.commons.commonsmultipartresolver">\n    >          <property name="defaultencoding" value="utf-8"/>\n    >          <property name="maxuploadsize" value="50000"/>  \x3c!-- 上传文件总大小--\x3e\n    >          <property name="maxuploadsizeperfile" value="5000"/>  \x3c!-- 上传文件总大小--\x3e\n    >      </bean>\n    >      \n    >    \n    >    * 编写文件上传代码\n    >      \n    >      * 单文件上传\n    >      \n    >      名称<input type="text" name="username"><br>\n    >      文件<input type="file" name="upload"><br>\n    >      string username, multipartfile upload\n    >      第一个参数（string）对应第一个name值，第二个参数(multipartfile 对象)对应第二个name值\n    >      \n    >      \n    >      * 多文件上传\n    >        \n    >        * 直接用两个参数进行接收\n    >          \n    >          string username, multipartfile upload1,multipartfile upload2\n    >          \n    >        \n    >        * 使用数组接收\n    >          \n    >          multipartfile[] uploads   -- 同hobbys\n    >          \n\n\n# spring jdbctemplate 使用\n\n不常用了解即可，后期有mybatis\n\n>  1. 导入spring-jdbc，spring-tx坐标\n>     \n>     <dependency>\n>       <groupid>org.springframework</groupid>\n>       <artifactid>spring-jdbc</artifactid>\n>       <version>5.3.6</version>\n>     </dependency>\n>     <dependency>\n>       <groupid>org.springframework</groupid>\n>       <artifactid>spring-tx</artifactid>\n>       <version>5.3.6</version>\n>     </dependency>\n>     \n> \n>  2. 创建数据库和实体\n> \n>  3. 创建jdbctemplate对象，或者让spring帮我们创建对象\n>     \n>     \x3c!--加载外部配置文件property--\x3e\n>     <context:property-placeholder location="classpath:jdbc.properties"/>\n>     \x3c!-- el表达式 来获取配置文件信息--\x3e\n>     <bean id="datasource_c3p0" class="com.mchange.v2.c3p0.combopooleddatasource">\n>         <property name="driverclass" value="${jdbc.driver}"/>\n>         <property name="user" value="${jdbc.username}"/>\n>         <property name="password" value="${jdbc.password}"/>\n>         <property name="jdbcurl" value="${jdbc.url}"/>\n>     </bean>\n>     \n>     \x3c!-- 在spring 容器中产生 jdbc模板对象   同时注入数据源--\x3e\n>     <bean id="jdbctemplate" class="org.springframework.jdbc.core.jdbctemplate">\n>         <property name="datasource" ref="datasource_c3p0"/>\n>     </bean>\n>     \n> \n>  4. 执行数据库操作\n>     \n>     @test//删除\n>     public void testdelete(){\n>         //增删改 都是 update方法   里面语句不一样而已\n>         int row = jdbctemplate.update("delete from account where name=?",  "diana");\n>         system.out.println(row);\n>     }\n>                                                     \n>     @test//查询全部\n>     public void testqueryall(){\n>         list<user> users = jdbctemplate.query("select * from user", new beanpropertyrowmapper<user>(user.class));\n>         system.out.println(users);\n>     }\n>                                                     \n>     @test//查询单个\n>     public void testqueryone(){//bean 类型封装\n>         user user = jdbctemplate.queryforobject("select * from user where username=?", new beanpropertyrowmapper<user>(user.class),"凉冰");\n>         system.out.println(user);\n>     }\n>                                                     \n>     @test//聚合查询\n>     public void testquerycount(){//简单类型\n>         int count = jdbctemplate.queryforobject("select count(*) from user",int.class);\n>         system.out.println(count);\n>     }\n>     \n\n\n# spinrgmvc拦截器\n\n这里的异常处理是xml完成的\n\n注解开发\n\n>  1. 生成拦截类，实现handlerinterceptor接口（有三个方法）。\n>     \n>     public class myinterceptor2 implements handlerinterceptor {\n>     \n>         //在目标方法执行之前  执行\n>         public boolean prehandle(httpservletrequest request, httpservletresponse response, object handler) throws exception {\n>             system.out.println("要执行方法了~~~~222");\n>             return true;\n>         }\n>     \n>         //在目标方法执行之后 视图对象返回之前执行\n>         public void posthandle(httpservletrequest request, httpservletresponse response, object handler, modelandview modelandview) throws exception {\n>             system.out.println("执行完方法了，要跳转到视图对象了！！222");\n>             modelandview.addobject("user1","凉冰");//将user1的参数由diana改成了凉冰\n>         }\n>     \n>         //在所有流程执行完毕后\n>         public void aftercompletion(httpservletrequest request, httpservletresponse response, object handler, exception ex) throws exception {\n>             system.out.println("全部执行完了！！！222");\n>         }\n>     }\n>     \n> \n>  2. 配置拦截器。\n>     \n>     \x3c!-- 配置拦截器--\x3e\n>     <mvc:interceptors>\n>         \x3c!--拦截器1--\x3e\n>         <mvc:interceptor>\n>             \x3c!--对哪些资源进行拦截 /** 表示拦截所有--\x3e\n>         <mvc:mapping path="/**"/>\n>         <bean class="com.diana.interceptor.myinterceptor1"/>\n>     </mvc:interceptor>\n>         \x3c!--拦截器2--\x3e\n>         <mvc:interceptor>\n>             \x3c!--对哪些资源进行拦截--\x3e\n>             <mvc:mapping path="/**"/>\n>             <bean class="com.diana.interceptor.myinterceptor2"/>\n>         </mvc:interceptor>\n>     </mvc:interceptors>\n>     \n>     \n>     ---拦截器链的执行顺序----\n>     \n>     要执行方法了~~~~\n>     要执行方法了~~~~222\n>     目标方法正在执行！！\n>     执行完方法了，要跳转到视图对象了！！222\n>     执行完方法了，要跳转到视图对象了！！\n>     全部执行完了！！！222\n>     全部执行完了！！！\n>     \n>     \n>     \n>     \n> \n>  3. 编写方法并进行测试。\n\n\n# springmvc异常处理\n\n这里的异常处理是xml完成的\n\n注解开发\n\n>  1. instanceof的用法\n> \n> instanceof 是 java 的一个二元操作符，类似于 ==，>，< 等操作符。\n> instanceof 是 java 的保留关键字。它的作用是测试它左边的对象是否是它右边的类的实例，返回 boolean 的数据类型。\n> \n> \n>  1. 异常处理的两种方法\n>     \n>     * 使用springmvc提供的简单异常处理器simplemappingexceptionresolver\n>       \n>       \x3c!-- 配置简单映射异常处理器--\x3e\n>       <bean class="org.springframework.web.servlet.handler.simplemappingexceptionresolver">\n>           \x3c!-- 默认错误视图    value 是视图名称（自己随便起名）  省略了前后缀--\x3e\n>           <property name="defaulterrorview" value="/exception/error"/>\n>           \x3c!-- 异常错误映射  优先判断里面的异常类型，里面没有时 才会进入默认错误视图--\x3e\n>           <property name="exceptionmappings">\n>               <map>\n>                   <entry key="com.diana.exception.myexception" value="/exception/myerror"/>\x3c!-- 自定义异常--\x3e\n>                   <entry key="java.lang.classcastexception" value="/exception/error_class"/>\x3c!-- 类型转换异常--\x3e\n>               </map>\n>           </property>\n>       </bean>\n>       \n>     \n>     * 实现spring的异常处理接口handlerexceptionresolver自定义自己的异常处理器\n>       \n>       * 创建异常处理类实现 接口 handlerexceptionresolver\n>         \n>         public class myexceptionresolver implements handlerexceptionresolver {\n>             /*\n>                 参数  exception e :异常对象\n>                返回值 modelandview : 跳转到错误视图信息\n>              */\n>             public modelandview resolveexception(httpservletrequest httpservletrequest, httpservletresponse httpservletresponse, object o, exception e) {\n>                 modelandview modelandview=new modelandview();\n>                 //instanceof 是 java 的一个二元操作符，类似于 ==，>，< 等操作符。\n>                 //instanceof 是 java 的保留关键字。它的作用是测试它左边的对象是否是它右边的类的实例，返回 boolean 的数据类型。\n>                 if(e instanceof myexception){\n>                     //执行异常相应的操作\n>                     modelandview.addobject("info","自定义异常");\n>                     //modelandview.setviewname("/exception/myerror"); //跳到自己的页面\n>                 }else if(e instanceof classcastexception){\n>                     //执行异常相应的操作\n>                     modelandview.addobject("info","类转换异常");\n>                     //modelandview.setviewname("/exception/error_class");//跳到自己的页面\n>                 }else{\n>                     //执行异常相应的操作\n>                     modelandview.addobject("info","默认类型异常");\n>                     //modelandview.setviewname("/exception/error");//跳到自己的页面\n>                 }\n>                 modelandview.setviewname("/exception/error"); //都跳转error页面\n>                 return modelandview;\n>             }\n>         }\n>         \n>       \n>       * 配置异常处理器\n>         \n>         \x3c!-- 配置自定义异常处理器--\x3e\n>         <bean class="com.diana.resolver.myexceptionresolver"/>\n>         \n>       \n>       * 编写异常页面\n>       \n>       * 测试异常跳转\n>       \n>       3. 在编写代码时，遇到异常就往外抛，只在最后同一对异常进行处理。',charsets:{cjk:!0}},{title:"Redis数据结构",frontmatter:{autoSort:10,title:"Redis数据结构",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/a0085d/",categories:["后端","数据库","Redis"],tags:["知识","数据库","Redis"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/05.Redis/40.Redis%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.html",relativePath:"01.后端/30.数据库/05.Redis/40.Redis数据结构.md",key:"v-13f7fe09",path:"/pages/a0085d/",headers:[{level:2,title:"动态字符串",slug:"动态字符串",normalizedTitle:"动态字符串",charIndex:2},{level:2,title:"intset",slug:"intset",normalizedTitle:"intset",charIndex:495},{level:2,title:"Dict",slug:"dict",normalizedTitle:"dict",charIndex:1111},{level:2,title:"ZipList",slug:"ziplist",normalizedTitle:"ziplist",charIndex:2625},{level:2,title:"ZipList的连锁更新问题",slug:"ziplist的连锁更新问题",normalizedTitle:"ziplist的连锁更新问题",charIndex:4299},{level:2,title:"QuickList",slug:"quicklist",normalizedTitle:"quicklist",charIndex:4772},{level:2,title:"SkipList",slug:"skiplist",normalizedTitle:"skiplist",charIndex:5535},{level:2,title:"RedisObject",slug:"redisobject",normalizedTitle:"redisobject",charIndex:5920},{level:2,title:"String",slug:"string",normalizedTitle:"string",charIndex:184},{level:2,title:"List",slug:"list",normalizedTitle:"list",charIndex:2628},{level:2,title:"Set结构",slug:"set结构",normalizedTitle:"set结构",charIndex:8590},{level:2,title:"ZSET",slug:"zset",normalizedTitle:"zset",charIndex:7107},{level:2,title:"Hash",slug:"hash",normalizedTitle:"hash",charIndex:1216}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"动态字符串 intset Dict ZipList ZipList的连锁更新问题 QuickList SkipList RedisObject String List Set结构 ZSET Hash",content:'# 动态字符串\n\n我们都知道Redis中保存的Key是字符串，value往往是字符串或者字符串的集合。可见字符串是Redis中最常用的一种数据结构。\n\n不过Redis没有直接使用C语言中的字符串，因为C语言字符串存在很多问题： 获取字符串长度的需要通过运算 非二进制安全 不可修改 Redis构建了一种新的字符串结构，称为简单动态字符串（Simple Dynamic String），简称SDS。 例如，我们执行命令：\n\n\n\n那么Redis将在底层创建两个SDS，其中一个是包含“name”的SDS，另一个是包含“虎哥”的SDS。\n\nRedis是C语言实现的，其中SDS是一个结构体，源码如下：\n\n\n\n例如，一个包含字符串“name”的sds结构如下：\n\n\n\nSDS之所以叫做动态字符串，是因为它具备动态扩容的能力，例如一个内容为“hi”的SDS：\n\n\n\n假如我们要给SDS追加一段字符串“,Amy”，这里首先会申请新内存空间：\n\n如果新字符串小于1M，则新空间为扩展后字符串长度的两倍+1；\n\n如果新字符串大于1M，则新空间为扩展后字符串长度+1M+1。称为内存预分配。\n\n\n\n\n\n\n# intset\n\nIntSet是Redis中set集合的一种实现方式，基于整数数组来实现，并且具备长度可变、有序等特征。 结构如下：\n\n\n\n其中的encoding包含三种模式，表示存储的整数大小不同：\n\n\n\n为了方便查找，Redis会将intset中所有的整数按照升序依次保存在contents数组中，结构如图：\n\n\n\n现在，数组中每个数字都在int16_t的范围内，因此采用的编码方式是INTSET_ENC_INT16，每部分占用的字节大小为： encoding：4字节 length：4字节 contents：2字节 * 3 = 6字节\n\n\n\n我们向该其中添加一个数字：50000，这个数字超出了int16_t的范围，intset会自动升级编码方式到合适的大小。 以当前案例来说流程如下：\n\n * 升级编码为INTSET_ENC_INT32, 每个整数占4字节，并按照新的编码方式及元素个数扩容数组\n * 倒序依次将数组中的元素拷贝到扩容后的正确位置\n * 将待添加的元素放入数组末尾\n * 最后，将inset的encoding属性改为INTSET_ENC_INT32，将length属性改为4\n\n\n\n源码如下：\n\n\n\n\n\n小总结：\n\nIntset可以看做是特殊的整数数组，具备一些特点：\n\n * Redis会确保Intset中的元素唯一、有序\n * 具备类型升级机制，可以节省内存空间\n * 底层采用二分查找方式来查询\n\n\n# Dict\n\n我们知道Redis是一个键值型（Key-Value Pair）的数据库，我们可以根据键实现快速的增删改查。而键与值的映射关系正是通过Dict来实现的。 Dict由三部分组成，分别是：哈希表（DictHashTable）、哈希节点（DictEntry）、字典（Dict）\n\n\n\n当我们向Dict添加键值对时，Redis首先根据key计算出hash值（h），然后利用 h & sizemask来计算元素应该存储到数组中的哪个索引位置。我们存储k1=v1，假设k1的哈希值h =1，则1&3 =1，因此k1=v1要存储到数组角标1位置。\n\n\n\nDict由三部分组成，分别是：哈希表（DictHashTable）、哈希节点（DictEntry）、字典（Dict）\n\n\n\n\n\n\n\nDict的扩容\n\nDict中的HashTable就是数组结合单向链表的实现，当集合中元素较多时，必然导致哈希冲突增多，链表过长，则查询效率会大大降低。 Dict在每次新增键值对时都会检查负载因子（LoadFactor = used/size） ，满足以下两种情况时会触发哈希表扩容： 哈希表的 LoadFactor >= 1，并且服务器没有执行 BGSAVE 或者 BGREWRITEAOF 等后台进程； 哈希表的 LoadFactor > 5 ；\n\n\n\n\n\nDict的rehash\n\n不管是扩容还是收缩，必定会创建新的哈希表，导致哈希表的size和sizemask变化，而key的查询与sizemask有关。因此必须对哈希表中的每一个key重新计算索引，插入新的哈希表，这个过程称为rehash。过程是这样的：\n\n * 计算新hash表的realeSize，值取决于当前要做的是扩容还是收缩：\n   \n   * 如果是扩容，则新size为第一个大于等于dict.ht[0].used + 1的2^n\n   * 如果是收缩，则新size为第一个大于等于dict.ht[0].used的2^n （不得小于4）\n\n * 按照新的realeSize申请内存空间，创建dictht，并赋值给dict.ht[1]\n\n * 设置dict.rehashidx = 0，标示开始rehash\n\n * 将dict.ht[0]中的每一个dictEntry都rehash到dict.ht[1]\n\n * 将dict.ht[1]赋值给dict.ht[0]，给dict.ht[1]初始化为空哈希表，释放原来的dict.ht[0]的内存\n\n * 将rehashidx赋值为-1，代表rehash结束\n\n * 在rehash过程中，新增操作，则直接写入ht[1]，查询、修改和删除则会在dict.ht[0]和dict.ht[1]依次查找并执行。这样可以确保ht[0]的数据只减不增，随着rehash最终为空\n\n整个过程可以描述成：\n\n\n\n小总结：\n\nDict的结构：\n\n * 类似java的HashTable，底层是数组加链表来解决哈希冲突\n * Dict包含两个哈希表，ht[0]平常用，ht[1]用来rehash\n\nDict的伸缩：\n\n * 当LoadFactor大于5或者LoadFactor大于1并且没有子进程任务时，Dict扩容\n * 当LoadFactor小于0.1时，Dict收缩\n * 扩容大小为第一个大于等于used + 1的2^n\n * 收缩大小为第一个大于等于used 的2^n\n * Dict采用渐进式rehash，每次访问Dict时执行一次rehash\n * rehash时ht[0]只减不增，新增操作只在ht[1]执行，其它操作在两个哈希表\n\n\n# ZipList\n\nZipList 是一种特殊的“双端链表” ，由一系列特殊编码的连续内存块组成。可以在任意一端进行压入/弹出操作, 并且该操作的时间复杂度为 O(1)。\n\n\n\n\n\n属性        类型         长度     用途\nzlbytes   uint32_t   4 字节   记录整个压缩列表占用的内存字节数\nzltail    uint32_t   4 字节   记录压缩列表表尾节点距离压缩列表的起始地址有多少字节，通过这个偏移量，可以确定表尾节点的地址。\nzllen     uint16_t   2 字节   记录了压缩列表包含的节点数量。 最大值为UINT16_MAX\n                            （65534），如果超过这个值，此处会记录为65535，但节点的真实数量需要遍历整个压缩列表才能计算得出。\nentry     列表节点       不定     压缩列表包含的各个节点，节点的长度由节点保存的内容决定。\nzlend     uint8_t    1 字节   特殊值 0xFF （十进制 255 ），用于标记压缩列表的末端。\n\nZipListEntry\n\nZipList 中的Entry并不像普通链表那样记录前后节点的指针，因为记录两个指针要占用16个字节，浪费内存。而是采用了下面的结构：\n\n\n\n * previous_entry_length：前一节点的长度，占1个或5个字节。\n   \n   * 如果前一节点的长度小于254字节，则采用1个字节来保存这个长度值\n   * 如果前一节点的长度大于254字节，则采用5个字节来保存这个长度值，第一个字节为0xfe，后四个字节才是真实长度数据\n\n * encoding：编码属性，记录content的数据类型（字符串还是整数）以及长度，占用1个、2个或5个字节\n\n * contents：负责保存节点的数据，可以是字符串或整数\n\nZipList中所有存储长度的数值均采用小端字节序，即低位字节在前，高位字节在后。例如：数值0x1234，采用小端字节序后实际存储值为：0x3412\n\nEncoding编码\n\nZipListEntry中的encoding编码分为字符串和整数两种： 字符串：如果encoding是以“00”、“01”或者“10”开头，则证明content是字符串\n\n编码                                               编码长度      字符串大小\n|00pppppp|                                       1 bytes   <= 63 bytes\n|01pppppp|qqqqqqqq|                              2 bytes   <= 16383 bytes\n|10000000|qqqqqqqq|rrrrrrrr|ssssssss|tttttttt|   5 bytes   <= 4294967295 bytes\n\n例如，我们要保存字符串：“ab”和 “bc”\n\n\n\nZipListEntry中的encoding编码分为字符串和整数两种：\n\n * 整数：如果encoding是以“11”开始，则证明content是整数，且encoding固定只占用1个字节\n\n编码         编码长度   整数类型\n11000000   1      int16_t（2 bytes）\n11010000   1      int32_t（4 bytes）\n11100000   1      int64_t（8 bytes）\n11110000   1      24位有符整数(3 bytes)\n11111110   1      8位有符整数(1 bytes)\n1111xxxx   1      直接在xxxx位置保存数值，范围从0001~1101，减1后结果为实际值\n\n\n\n\n\n\n# ZipList的连锁更新问题\n\nZipList的每个Entry都包含previous_entry_length来记录上一个节点的大小，长度是1个或5个字节： 如果前一节点的长度小于254字节，则采用1个字节来保存这个长度值 如果前一节点的长度大于等于254字节，则采用5个字节来保存这个长度值，第一个字节为0xfe，后四个字节才是真实长度数据 现在，假设我们有N个连续的、长度为250~253字节之间的entry，因此entry的previous_entry_length属性用1个字节即可表示，如图所示：\n\n\n\nZipList这种特殊情况下产生的连续多次空间扩展操作称之为连锁更新（Cascade Update）。新增、删除都可能导致连锁更新的发生。\n\n小总结：\n\nZipList特性：\n\n * 压缩列表的可以看做一种连续内存空间的"双向链表"\n * 列表的节点之间不是通过指针连接，而是记录上一节点和本节点长度来寻址，内存占用较低\n * 如果列表数据过多，导致链表过长，可能影响查询性能\n * 增或删较大数据时有可能发生连续更新问题\n\n\n# QuickList\n\n问题1：ZipList虽然节省内存，但申请内存必须是连续空间，如果内存占用较多，申请内存效率很低。怎么办？\n\n答：为了缓解这个问题，我们必须限制ZipList的长度和entry大小。\n\n问题2：但是我们要存储大量数据，超出了ZipList最佳的上限该怎么办？\n\n答：我们可以创建多个ZipList来分片存储数据。\n\n问题3：数据拆分后比较分散，不方便管理和查找，这多个ZipList如何建立联系？\n\n答：Redis在3.2版本引入了新的数据结构QuickList，它是一个双端链表，只不过链表中的每个节点都是一个ZipList。\n\n\n\n为了避免QuickList中的每个ZipList中entry过多，Redis提供了一个配置项：list-max-ziplist-size来限制。 如果值为正，则代表ZipList的允许的entry个数的最大值 如果值为负，则代表ZipList的最大内存大小，分5种情况：\n\n * -1：每个ZipList的内存占用不能超过4kb\n * -2：每个ZipList的内存占用不能超过8kb\n * -3：每个ZipList的内存占用不能超过16kb\n * -4：每个ZipList的内存占用不能超过32kb\n * -5：每个ZipList的内存占用不能超过64kb\n\n其默认值为 -2：\n\n\n\n以下是QuickList的和QuickListNode的结构源码：\n\n\n\n我们接下来用一段流程图来描述当前的这个结构\n\n\n\n总结：\n\nQuickList的特点：\n\n * 是一个节点为ZipList的双端链表\n * 节点采用ZipList，解决了传统链表的内存占用问题\n * 控制了ZipList大小，解决连续内存空间申请效率问题\n * 中间节点可以压缩，进一步节省了内存\n\n\n# SkipList\n\nSkipList（跳表）首先是链表，但与传统链表相比有几点差异： 元素按照升序排列存储 节点可能包含多个指针，指针跨度不同。\n\n\n\nSkipList（跳表）首先是链表，但与传统链表相比有几点差异： 元素按照升序排列存储 节点可能包含多个指针，指针跨度不同。\n\n\n\nSkipList（跳表）首先是链表，但与传统链表相比有几点差异： 元素按照升序排列存储 节点可能包含多个指针，指针跨度不同。\n\n\n\n小总结：\n\nSkipList的特点：\n\n * 跳跃表是一个双向链表，每个节点都包含score和ele值\n * 节点按照score值排序，score值一样则按照ele字典排序\n * 每个节点都可以包含多层指针，层数是1到32之间的随机数\n * 不同层指针到下一个节点的跨度不同，层级越高，跨度越大\n * 增删改查效率与红黑树基本一致，实现却更简单\n\n\n# RedisObject\n\nRedis中的任意数据类型的键和值都会被封装为一个RedisObject，也叫做Redis对象，源码如下：\n\n1、什么是redisObject： 从Redis的使用者的角度来看，⼀个Redis节点包含多个database（非cluster模式下默认是16个，cluster模式下只能是1个），而一个database维护了从key space到object space的映射关系。这个映射关系的key是string类型，⽽value可以是多种数据类型，比如： string, list, hash、set、sorted set等。我们可以看到，key的类型固定是string，而value可能的类型是多个。 ⽽从Redis内部实现的⾓度来看，database内的这个映射关系是用⼀个dict来维护的。dict的key固定用⼀种数据结构来表达就够了，这就是动态字符串sds。而value则比较复杂，为了在同⼀个dict内能够存储不同类型的value，这就需要⼀个通⽤的数据结构，这个通用的数据结构就是robj，全名是redisObject。\n\n\n\nRedis的编码方式\n\nRedis中会根据存储的数据类型不同，选择不同的编码方式，共包含11种不同类型：\n\n编号   编码方式                      说明\n0    OBJ_ENCODING_RAW          raw编码动态字符串\n1    OBJ_ENCODING_INT          long类型的整数的字符串\n2    OBJ_ENCODING_HT           hash表（字典dict）\n3    OBJ_ENCODING_ZIPMAP       已废弃\n4    OBJ_ENCODING_LINKEDLIST   双端链表\n5    OBJ_ENCODING_ZIPLIST      压缩列表\n6    OBJ_ENCODING_INTSET       整数集合\n7    OBJ_ENCODING_SKIPLIST     跳表\n8    OBJ_ENCODING_EMBSTR       embstr的动态字符串\n9    OBJ_ENCODING_QUICKLIST    快速列表\n10   OBJ_ENCODING_STREAM       Stream流\n\n五种数据结构\n\nRedis中会根据存储的数据类型不同，选择不同的编码方式。每种数据类型的使用的编码方式如下：\n\n数据类型         编码方式\nOBJ_STRING   int、embstr、raw\nOBJ_LIST     LinkedList和ZipList(3.2以前)、QuickList（3.2以后）\nOBJ_SET      intset、HT\nOBJ_ZSET     ZipList、HT、SkipList\nOBJ_HASH     ZipList、HT\n\n\n# String\n\nString是Redis中最常见的数据存储类型：\n\n其基本编码方式是RAW，基于简单动态字符串（SDS）实现，存储上限为512mb。\n\n如果存储的SDS长度小于44字节，则会采用EMBSTR编码，此时object head与SDS是一段连续空间。申请内存时\n\n只需要调用一次内存分配函数，效率更高。\n\n（1）底层实现⽅式：动态字符串sds 或者 long String的内部存储结构⼀般是sds（Simple Dynamic String，可以动态扩展内存），但是如果⼀个String类型的value的值是数字，那么Redis内部会把它转成long类型来存储，从⽽减少内存的使用。\n\n\n\n如果存储的字符串是整数值，并且大小在LONG_MAX范围内，则会采用INT编码：直接将数据保存在RedisObject的ptr指针位置（刚好8字节），不再需要SDS了。\n\n\n\n\n\n\n\n确切地说，String在Redis中是⽤⼀个robj来表示的。\n\n用来表示String的robj可能编码成3种内部表⽰：OBJ_ENCODING_RAW，OBJ_ENCODING_EMBSTR，OBJ_ENCODING_INT。 其中前两种编码使⽤的是sds来存储，最后⼀种OBJ_ENCODING_INT编码直接把string存成了long型。 在对string进行incr, decr等操作的时候，如果它内部是OBJ_ENCODING_INT编码，那么可以直接行加减操作；如果它内部是OBJ_ENCODING_RAW或OBJ_ENCODING_EMBSTR编码，那么Redis会先试图把sds存储的字符串转成long型，如果能转成功，再进行加减操作。对⼀个内部表示成long型的string执行append, setbit, getrange这些命令，针对的仍然是string的值（即⼗进制表示的字符串），而不是针对内部表⽰的long型进⾏操作。比如字符串”32”，如果按照字符数组来解释，它包含两个字符，它们的ASCII码分别是0x33和0x32。当我们执行命令setbit key 7 0的时候，相当于把字符0x33变成了0x32，这样字符串的值就变成了”22”。⽽如果将字符串”32”按照内部的64位long型来解释，那么它是0x0000000000000020，在这个基础上执⾏setbit位操作，结果就完全不对了。因此，在这些命令的实现中，会把long型先转成字符串再进行相应的操作。\n\n\n# List\n\nRedis的List类型可以从首、尾操作列表中的元素：\n\n\n\n哪一个数据结构能满足上述特征？\n\n * LinkedList ：普通链表，可以从双端访问，内存占用较高，内存碎片较多\n * ZipList ：压缩列表，可以从双端访问，内存占用低，存储上限低\n * QuickList：LinkedList + ZipList，可以从双端访问，内存占用较低，包含多个ZipList，存储上限高\n\nRedis的List结构类似一个双端链表，可以从首、尾操作列表中的元素：\n\n在3.2版本之前，Redis采用ZipList和LinkedList来实现List，当元素数量小于512并且元素大小小于64字节时采用ZipList编码，超过则采用LinkedList编码。\n\n在3.2版本之后，Redis统一采用QuickList来实现List：\n\n\n\n\n# Set结构\n\nSet是Redis中的单列集合，满足下列特点：\n\n * 不保证有序性\n * 保证元素唯一\n * 求交集、并集、差集\n\n\n\n可以看出，Set对查询元素的效率要求非常高，思考一下，什么样的数据结构可以满足？ HashTable，也就是Redis中的Dict，不过Dict是双列集合（可以存键、值对）\n\nSet是Redis中的集合，不一定确保元素有序，可以满足元素唯一、查询效率要求极高。 为了查询效率和唯一性，set采用HT编码（Dict）。Dict中的key用来存储元素，value统一为null。 当存储的所有数据都是整数，并且元素数量不超过set-max-intset-entries时，Set会采用IntSet编码，以节省内存\n\n\n\n结构如下：\n\n\n\n\n# ZSET\n\nZSet也就是SortedSet，其中每一个元素都需要指定一个score值和member值：\n\n * 可以根据score值排序后\n * member必须唯一\n * 可以根据member查询分数\n\n\n\n因此，zset底层数据结构必须满足键值存储、键必须唯一、可排序这几个需求。之前学习的哪种编码结构可以满足？\n\n * SkipList：可以排序，并且可以同时存储score和ele值（member）\n * HT（Dict）：可以键值存储，并且可以根据key找value\n\n\n\n\n\n当元素数量不多时，HT和SkipList的优势不明显，而且更耗内存。因此zset还会采用ZipList结构来节省内存，不过需要同时满足两个条件：\n\n * 元素数量小于zset_max_ziplist_entries，默认值128\n * 每个元素都小于zset_max_ziplist_value字节，默认值64\n\nziplist本身没有排序功能，而且没有键值对的概念，因此需要有zset通过编码实现：\n\n * ZipList是连续内存，因此score和element是紧挨在一起的两个entry， element在前，score在后\n * score越小越接近队首，score越大越接近队尾，按照score值升序排列\n\n\n\n\n\n\n# Hash\n\nHash结构与Redis中的Zset非常类似：\n\n * 都是键值存储\n * 都需求根据键获取值\n * 键必须唯一\n\n区别如下：\n\n * zset的键是member，值是score；hash的键和值都是任意值\n * zset要根据score排序；hash则无需排序\n\n（1）底层实现方式：压缩列表ziplist 或者 字典dict 当Hash中数据项比较少的情况下，Hash底层才⽤压缩列表ziplist进⾏存储数据，随着数据的增加，底层的ziplist就可能会转成dict，具体配置如下：\n\nhash-max-ziplist-entries 512\n\nhash-max-ziplist-value 64\n\n当满足上面两个条件其中之⼀的时候，Redis就使⽤dict字典来实现hash。 Redis的hash之所以这样设计，是因为当ziplist变得很⼤的时候，它有如下几个缺点：\n\n * 每次插⼊或修改引发的realloc操作会有更⼤的概率造成内存拷贝，从而降低性能。\n * ⼀旦发生内存拷贝，内存拷贝的成本也相应增加，因为要拷贝更⼤的⼀块数据。\n * 当ziplist数据项过多的时候，在它上⾯查找指定的数据项就会性能变得很低，因为ziplist上的查找需要进行遍历。\n\n总之，ziplist本来就设计为各个数据项挨在⼀起组成连续的内存空间，这种结构并不擅长做修改操作。⼀旦数据发⽣改动，就会引发内存realloc，可能导致内存拷贝。\n\nhash结构如下：\n\n\n\nzset集合如下：\n\n\n\n因此，Hash底层采用的编码与Zset也基本一致，只需要把排序有关的SkipList去掉即可：\n\nHash结构默认采用ZipList编码，用以节省内存。 ZipList中相邻的两个entry 分别保存field和value\n\n当数据量较大时，Hash结构会转为HT编码，也就是Dict，触发条件有两个：\n\n * ZipList中的元素数量超过了hash-max-ziplist-entries（默认512）\n * ZipList中的任意entry大小超过了hash-max-ziplist-value（默认64字节）\n\n',normalizedContent:'# 动态字符串\n\n我们都知道redis中保存的key是字符串，value往往是字符串或者字符串的集合。可见字符串是redis中最常用的一种数据结构。\n\n不过redis没有直接使用c语言中的字符串，因为c语言字符串存在很多问题： 获取字符串长度的需要通过运算 非二进制安全 不可修改 redis构建了一种新的字符串结构，称为简单动态字符串（simple dynamic string），简称sds。 例如，我们执行命令：\n\n\n\n那么redis将在底层创建两个sds，其中一个是包含“name”的sds，另一个是包含“虎哥”的sds。\n\nredis是c语言实现的，其中sds是一个结构体，源码如下：\n\n\n\n例如，一个包含字符串“name”的sds结构如下：\n\n\n\nsds之所以叫做动态字符串，是因为它具备动态扩容的能力，例如一个内容为“hi”的sds：\n\n\n\n假如我们要给sds追加一段字符串“,amy”，这里首先会申请新内存空间：\n\n如果新字符串小于1m，则新空间为扩展后字符串长度的两倍+1；\n\n如果新字符串大于1m，则新空间为扩展后字符串长度+1m+1。称为内存预分配。\n\n\n\n\n\n\n# intset\n\nintset是redis中set集合的一种实现方式，基于整数数组来实现，并且具备长度可变、有序等特征。 结构如下：\n\n\n\n其中的encoding包含三种模式，表示存储的整数大小不同：\n\n\n\n为了方便查找，redis会将intset中所有的整数按照升序依次保存在contents数组中，结构如图：\n\n\n\n现在，数组中每个数字都在int16_t的范围内，因此采用的编码方式是intset_enc_int16，每部分占用的字节大小为： encoding：4字节 length：4字节 contents：2字节 * 3 = 6字节\n\n\n\n我们向该其中添加一个数字：50000，这个数字超出了int16_t的范围，intset会自动升级编码方式到合适的大小。 以当前案例来说流程如下：\n\n * 升级编码为intset_enc_int32, 每个整数占4字节，并按照新的编码方式及元素个数扩容数组\n * 倒序依次将数组中的元素拷贝到扩容后的正确位置\n * 将待添加的元素放入数组末尾\n * 最后，将inset的encoding属性改为intset_enc_int32，将length属性改为4\n\n\n\n源码如下：\n\n\n\n\n\n小总结：\n\nintset可以看做是特殊的整数数组，具备一些特点：\n\n * redis会确保intset中的元素唯一、有序\n * 具备类型升级机制，可以节省内存空间\n * 底层采用二分查找方式来查询\n\n\n# dict\n\n我们知道redis是一个键值型（key-value pair）的数据库，我们可以根据键实现快速的增删改查。而键与值的映射关系正是通过dict来实现的。 dict由三部分组成，分别是：哈希表（dicthashtable）、哈希节点（dictentry）、字典（dict）\n\n\n\n当我们向dict添加键值对时，redis首先根据key计算出hash值（h），然后利用 h & sizemask来计算元素应该存储到数组中的哪个索引位置。我们存储k1=v1，假设k1的哈希值h =1，则1&3 =1，因此k1=v1要存储到数组角标1位置。\n\n\n\ndict由三部分组成，分别是：哈希表（dicthashtable）、哈希节点（dictentry）、字典（dict）\n\n\n\n\n\n\n\ndict的扩容\n\ndict中的hashtable就是数组结合单向链表的实现，当集合中元素较多时，必然导致哈希冲突增多，链表过长，则查询效率会大大降低。 dict在每次新增键值对时都会检查负载因子（loadfactor = used/size） ，满足以下两种情况时会触发哈希表扩容： 哈希表的 loadfactor >= 1，并且服务器没有执行 bgsave 或者 bgrewriteaof 等后台进程； 哈希表的 loadfactor > 5 ；\n\n\n\n\n\ndict的rehash\n\n不管是扩容还是收缩，必定会创建新的哈希表，导致哈希表的size和sizemask变化，而key的查询与sizemask有关。因此必须对哈希表中的每一个key重新计算索引，插入新的哈希表，这个过程称为rehash。过程是这样的：\n\n * 计算新hash表的realesize，值取决于当前要做的是扩容还是收缩：\n   \n   * 如果是扩容，则新size为第一个大于等于dict.ht[0].used + 1的2^n\n   * 如果是收缩，则新size为第一个大于等于dict.ht[0].used的2^n （不得小于4）\n\n * 按照新的realesize申请内存空间，创建dictht，并赋值给dict.ht[1]\n\n * 设置dict.rehashidx = 0，标示开始rehash\n\n * 将dict.ht[0]中的每一个dictentry都rehash到dict.ht[1]\n\n * 将dict.ht[1]赋值给dict.ht[0]，给dict.ht[1]初始化为空哈希表，释放原来的dict.ht[0]的内存\n\n * 将rehashidx赋值为-1，代表rehash结束\n\n * 在rehash过程中，新增操作，则直接写入ht[1]，查询、修改和删除则会在dict.ht[0]和dict.ht[1]依次查找并执行。这样可以确保ht[0]的数据只减不增，随着rehash最终为空\n\n整个过程可以描述成：\n\n\n\n小总结：\n\ndict的结构：\n\n * 类似java的hashtable，底层是数组加链表来解决哈希冲突\n * dict包含两个哈希表，ht[0]平常用，ht[1]用来rehash\n\ndict的伸缩：\n\n * 当loadfactor大于5或者loadfactor大于1并且没有子进程任务时，dict扩容\n * 当loadfactor小于0.1时，dict收缩\n * 扩容大小为第一个大于等于used + 1的2^n\n * 收缩大小为第一个大于等于used 的2^n\n * dict采用渐进式rehash，每次访问dict时执行一次rehash\n * rehash时ht[0]只减不增，新增操作只在ht[1]执行，其它操作在两个哈希表\n\n\n# ziplist\n\nziplist 是一种特殊的“双端链表” ，由一系列特殊编码的连续内存块组成。可以在任意一端进行压入/弹出操作, 并且该操作的时间复杂度为 o(1)。\n\n\n\n\n\n属性        类型         长度     用途\nzlbytes   uint32_t   4 字节   记录整个压缩列表占用的内存字节数\nzltail    uint32_t   4 字节   记录压缩列表表尾节点距离压缩列表的起始地址有多少字节，通过这个偏移量，可以确定表尾节点的地址。\nzllen     uint16_t   2 字节   记录了压缩列表包含的节点数量。 最大值为uint16_max\n                            （65534），如果超过这个值，此处会记录为65535，但节点的真实数量需要遍历整个压缩列表才能计算得出。\nentry     列表节点       不定     压缩列表包含的各个节点，节点的长度由节点保存的内容决定。\nzlend     uint8_t    1 字节   特殊值 0xff （十进制 255 ），用于标记压缩列表的末端。\n\nziplistentry\n\nziplist 中的entry并不像普通链表那样记录前后节点的指针，因为记录两个指针要占用16个字节，浪费内存。而是采用了下面的结构：\n\n\n\n * previous_entry_length：前一节点的长度，占1个或5个字节。\n   \n   * 如果前一节点的长度小于254字节，则采用1个字节来保存这个长度值\n   * 如果前一节点的长度大于254字节，则采用5个字节来保存这个长度值，第一个字节为0xfe，后四个字节才是真实长度数据\n\n * encoding：编码属性，记录content的数据类型（字符串还是整数）以及长度，占用1个、2个或5个字节\n\n * contents：负责保存节点的数据，可以是字符串或整数\n\nziplist中所有存储长度的数值均采用小端字节序，即低位字节在前，高位字节在后。例如：数值0x1234，采用小端字节序后实际存储值为：0x3412\n\nencoding编码\n\nziplistentry中的encoding编码分为字符串和整数两种： 字符串：如果encoding是以“00”、“01”或者“10”开头，则证明content是字符串\n\n编码                                               编码长度      字符串大小\n|00pppppp|                                       1 bytes   <= 63 bytes\n|01pppppp|qqqqqqqq|                              2 bytes   <= 16383 bytes\n|10000000|qqqqqqqq|rrrrrrrr|ssssssss|tttttttt|   5 bytes   <= 4294967295 bytes\n\n例如，我们要保存字符串：“ab”和 “bc”\n\n\n\nziplistentry中的encoding编码分为字符串和整数两种：\n\n * 整数：如果encoding是以“11”开始，则证明content是整数，且encoding固定只占用1个字节\n\n编码         编码长度   整数类型\n11000000   1      int16_t（2 bytes）\n11010000   1      int32_t（4 bytes）\n11100000   1      int64_t（8 bytes）\n11110000   1      24位有符整数(3 bytes)\n11111110   1      8位有符整数(1 bytes)\n1111xxxx   1      直接在xxxx位置保存数值，范围从0001~1101，减1后结果为实际值\n\n\n\n\n\n\n# ziplist的连锁更新问题\n\nziplist的每个entry都包含previous_entry_length来记录上一个节点的大小，长度是1个或5个字节： 如果前一节点的长度小于254字节，则采用1个字节来保存这个长度值 如果前一节点的长度大于等于254字节，则采用5个字节来保存这个长度值，第一个字节为0xfe，后四个字节才是真实长度数据 现在，假设我们有n个连续的、长度为250~253字节之间的entry，因此entry的previous_entry_length属性用1个字节即可表示，如图所示：\n\n\n\nziplist这种特殊情况下产生的连续多次空间扩展操作称之为连锁更新（cascade update）。新增、删除都可能导致连锁更新的发生。\n\n小总结：\n\nziplist特性：\n\n * 压缩列表的可以看做一种连续内存空间的"双向链表"\n * 列表的节点之间不是通过指针连接，而是记录上一节点和本节点长度来寻址，内存占用较低\n * 如果列表数据过多，导致链表过长，可能影响查询性能\n * 增或删较大数据时有可能发生连续更新问题\n\n\n# quicklist\n\n问题1：ziplist虽然节省内存，但申请内存必须是连续空间，如果内存占用较多，申请内存效率很低。怎么办？\n\n答：为了缓解这个问题，我们必须限制ziplist的长度和entry大小。\n\n问题2：但是我们要存储大量数据，超出了ziplist最佳的上限该怎么办？\n\n答：我们可以创建多个ziplist来分片存储数据。\n\n问题3：数据拆分后比较分散，不方便管理和查找，这多个ziplist如何建立联系？\n\n答：redis在3.2版本引入了新的数据结构quicklist，它是一个双端链表，只不过链表中的每个节点都是一个ziplist。\n\n\n\n为了避免quicklist中的每个ziplist中entry过多，redis提供了一个配置项：list-max-ziplist-size来限制。 如果值为正，则代表ziplist的允许的entry个数的最大值 如果值为负，则代表ziplist的最大内存大小，分5种情况：\n\n * -1：每个ziplist的内存占用不能超过4kb\n * -2：每个ziplist的内存占用不能超过8kb\n * -3：每个ziplist的内存占用不能超过16kb\n * -4：每个ziplist的内存占用不能超过32kb\n * -5：每个ziplist的内存占用不能超过64kb\n\n其默认值为 -2：\n\n\n\n以下是quicklist的和quicklistnode的结构源码：\n\n\n\n我们接下来用一段流程图来描述当前的这个结构\n\n\n\n总结：\n\nquicklist的特点：\n\n * 是一个节点为ziplist的双端链表\n * 节点采用ziplist，解决了传统链表的内存占用问题\n * 控制了ziplist大小，解决连续内存空间申请效率问题\n * 中间节点可以压缩，进一步节省了内存\n\n\n# skiplist\n\nskiplist（跳表）首先是链表，但与传统链表相比有几点差异： 元素按照升序排列存储 节点可能包含多个指针，指针跨度不同。\n\n\n\nskiplist（跳表）首先是链表，但与传统链表相比有几点差异： 元素按照升序排列存储 节点可能包含多个指针，指针跨度不同。\n\n\n\nskiplist（跳表）首先是链表，但与传统链表相比有几点差异： 元素按照升序排列存储 节点可能包含多个指针，指针跨度不同。\n\n\n\n小总结：\n\nskiplist的特点：\n\n * 跳跃表是一个双向链表，每个节点都包含score和ele值\n * 节点按照score值排序，score值一样则按照ele字典排序\n * 每个节点都可以包含多层指针，层数是1到32之间的随机数\n * 不同层指针到下一个节点的跨度不同，层级越高，跨度越大\n * 增删改查效率与红黑树基本一致，实现却更简单\n\n\n# redisobject\n\nredis中的任意数据类型的键和值都会被封装为一个redisobject，也叫做redis对象，源码如下：\n\n1、什么是redisobject： 从redis的使用者的角度来看，⼀个redis节点包含多个database（非cluster模式下默认是16个，cluster模式下只能是1个），而一个database维护了从key space到object space的映射关系。这个映射关系的key是string类型，⽽value可以是多种数据类型，比如： string, list, hash、set、sorted set等。我们可以看到，key的类型固定是string，而value可能的类型是多个。 ⽽从redis内部实现的⾓度来看，database内的这个映射关系是用⼀个dict来维护的。dict的key固定用⼀种数据结构来表达就够了，这就是动态字符串sds。而value则比较复杂，为了在同⼀个dict内能够存储不同类型的value，这就需要⼀个通⽤的数据结构，这个通用的数据结构就是robj，全名是redisobject。\n\n\n\nredis的编码方式\n\nredis中会根据存储的数据类型不同，选择不同的编码方式，共包含11种不同类型：\n\n编号   编码方式                      说明\n0    obj_encoding_raw          raw编码动态字符串\n1    obj_encoding_int          long类型的整数的字符串\n2    obj_encoding_ht           hash表（字典dict）\n3    obj_encoding_zipmap       已废弃\n4    obj_encoding_linkedlist   双端链表\n5    obj_encoding_ziplist      压缩列表\n6    obj_encoding_intset       整数集合\n7    obj_encoding_skiplist     跳表\n8    obj_encoding_embstr       embstr的动态字符串\n9    obj_encoding_quicklist    快速列表\n10   obj_encoding_stream       stream流\n\n五种数据结构\n\nredis中会根据存储的数据类型不同，选择不同的编码方式。每种数据类型的使用的编码方式如下：\n\n数据类型         编码方式\nobj_string   int、embstr、raw\nobj_list     linkedlist和ziplist(3.2以前)、quicklist（3.2以后）\nobj_set      intset、ht\nobj_zset     ziplist、ht、skiplist\nobj_hash     ziplist、ht\n\n\n# string\n\nstring是redis中最常见的数据存储类型：\n\n其基本编码方式是raw，基于简单动态字符串（sds）实现，存储上限为512mb。\n\n如果存储的sds长度小于44字节，则会采用embstr编码，此时object head与sds是一段连续空间。申请内存时\n\n只需要调用一次内存分配函数，效率更高。\n\n（1）底层实现⽅式：动态字符串sds 或者 long string的内部存储结构⼀般是sds（simple dynamic string，可以动态扩展内存），但是如果⼀个string类型的value的值是数字，那么redis内部会把它转成long类型来存储，从⽽减少内存的使用。\n\n\n\n如果存储的字符串是整数值，并且大小在long_max范围内，则会采用int编码：直接将数据保存在redisobject的ptr指针位置（刚好8字节），不再需要sds了。\n\n\n\n\n\n\n\n确切地说，string在redis中是⽤⼀个robj来表示的。\n\n用来表示string的robj可能编码成3种内部表⽰：obj_encoding_raw，obj_encoding_embstr，obj_encoding_int。 其中前两种编码使⽤的是sds来存储，最后⼀种obj_encoding_int编码直接把string存成了long型。 在对string进行incr, decr等操作的时候，如果它内部是obj_encoding_int编码，那么可以直接行加减操作；如果它内部是obj_encoding_raw或obj_encoding_embstr编码，那么redis会先试图把sds存储的字符串转成long型，如果能转成功，再进行加减操作。对⼀个内部表示成long型的string执行append, setbit, getrange这些命令，针对的仍然是string的值（即⼗进制表示的字符串），而不是针对内部表⽰的long型进⾏操作。比如字符串”32”，如果按照字符数组来解释，它包含两个字符，它们的ascii码分别是0x33和0x32。当我们执行命令setbit key 7 0的时候，相当于把字符0x33变成了0x32，这样字符串的值就变成了”22”。⽽如果将字符串”32”按照内部的64位long型来解释，那么它是0x0000000000000020，在这个基础上执⾏setbit位操作，结果就完全不对了。因此，在这些命令的实现中，会把long型先转成字符串再进行相应的操作。\n\n\n# list\n\nredis的list类型可以从首、尾操作列表中的元素：\n\n\n\n哪一个数据结构能满足上述特征？\n\n * linkedlist ：普通链表，可以从双端访问，内存占用较高，内存碎片较多\n * ziplist ：压缩列表，可以从双端访问，内存占用低，存储上限低\n * quicklist：linkedlist + ziplist，可以从双端访问，内存占用较低，包含多个ziplist，存储上限高\n\nredis的list结构类似一个双端链表，可以从首、尾操作列表中的元素：\n\n在3.2版本之前，redis采用ziplist和linkedlist来实现list，当元素数量小于512并且元素大小小于64字节时采用ziplist编码，超过则采用linkedlist编码。\n\n在3.2版本之后，redis统一采用quicklist来实现list：\n\n\n\n\n# set结构\n\nset是redis中的单列集合，满足下列特点：\n\n * 不保证有序性\n * 保证元素唯一\n * 求交集、并集、差集\n\n\n\n可以看出，set对查询元素的效率要求非常高，思考一下，什么样的数据结构可以满足？ hashtable，也就是redis中的dict，不过dict是双列集合（可以存键、值对）\n\nset是redis中的集合，不一定确保元素有序，可以满足元素唯一、查询效率要求极高。 为了查询效率和唯一性，set采用ht编码（dict）。dict中的key用来存储元素，value统一为null。 当存储的所有数据都是整数，并且元素数量不超过set-max-intset-entries时，set会采用intset编码，以节省内存\n\n\n\n结构如下：\n\n\n\n\n# zset\n\nzset也就是sortedset，其中每一个元素都需要指定一个score值和member值：\n\n * 可以根据score值排序后\n * member必须唯一\n * 可以根据member查询分数\n\n\n\n因此，zset底层数据结构必须满足键值存储、键必须唯一、可排序这几个需求。之前学习的哪种编码结构可以满足？\n\n * skiplist：可以排序，并且可以同时存储score和ele值（member）\n * ht（dict）：可以键值存储，并且可以根据key找value\n\n\n\n\n\n当元素数量不多时，ht和skiplist的优势不明显，而且更耗内存。因此zset还会采用ziplist结构来节省内存，不过需要同时满足两个条件：\n\n * 元素数量小于zset_max_ziplist_entries，默认值128\n * 每个元素都小于zset_max_ziplist_value字节，默认值64\n\nziplist本身没有排序功能，而且没有键值对的概念，因此需要有zset通过编码实现：\n\n * ziplist是连续内存，因此score和element是紧挨在一起的两个entry， element在前，score在后\n * score越小越接近队首，score越大越接近队尾，按照score值升序排列\n\n\n\n\n\n\n# hash\n\nhash结构与redis中的zset非常类似：\n\n * 都是键值存储\n * 都需求根据键获取值\n * 键必须唯一\n\n区别如下：\n\n * zset的键是member，值是score；hash的键和值都是任意值\n * zset要根据score排序；hash则无需排序\n\n（1）底层实现方式：压缩列表ziplist 或者 字典dict 当hash中数据项比较少的情况下，hash底层才⽤压缩列表ziplist进⾏存储数据，随着数据的增加，底层的ziplist就可能会转成dict，具体配置如下：\n\nhash-max-ziplist-entries 512\n\nhash-max-ziplist-value 64\n\n当满足上面两个条件其中之⼀的时候，redis就使⽤dict字典来实现hash。 redis的hash之所以这样设计，是因为当ziplist变得很⼤的时候，它有如下几个缺点：\n\n * 每次插⼊或修改引发的realloc操作会有更⼤的概率造成内存拷贝，从而降低性能。\n * ⼀旦发生内存拷贝，内存拷贝的成本也相应增加，因为要拷贝更⼤的⼀块数据。\n * 当ziplist数据项过多的时候，在它上⾯查找指定的数据项就会性能变得很低，因为ziplist上的查找需要进行遍历。\n\n总之，ziplist本来就设计为各个数据项挨在⼀起组成连续的内存空间，这种结构并不擅长做修改操作。⼀旦数据发⽣改动，就会引发内存realloc，可能导致内存拷贝。\n\nhash结构如下：\n\n\n\nzset集合如下：\n\n\n\n因此，hash底层采用的编码与zset也基本一致，只需要把排序有关的skiplist去掉即可：\n\nhash结构默认采用ziplist编码，用以节省内存。 ziplist中相邻的两个entry 分别保存field和value\n\n当数据量较大时，hash结构会转为ht编码，也就是dict，触发条件有两个：\n\n * ziplist中的元素数量超过了hash-max-ziplist-entries（默认512）\n * ziplist中的任意entry大小超过了hash-max-ziplist-value（默认64字节）\n\n',charsets:{cjk:!0}},{title:"SSM整合—XML开发",frontmatter:{autoSort:95,title:"SSM整合—XML开发",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/f036b6/",categories:["后端","SSM"],tags:["知识","SSM"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/40.SSM/25.SSM%20%E6%95%B4%E5%90%88-XML%E6%96%B9%E5%BC%8F.html",relativePath:"01.后端/40.SSM/25.SSM 整合-XML方式.md",key:"v-a024d1dc",path:"/pages/f036b6/",headers:[{level:2,title:"注入mapper",slug:"注入mapper",normalizedTitle:"注入mapper",charIndex:2},{level:2,title:"事务控制   （同spring原始的事务控制）",slug:"事务控制-同spring原始的事务控制",normalizedTitle:"事务控制   （同spring原始的事务控制）",charIndex:null},{level:2,title:"注意问题",slug:"注意问题",normalizedTitle:"注意问题",charIndex:4092},{level:3,title:"1.模型设置",slug:"_1-模型设置",normalizedTitle:"1.模型设置",charIndex:4101},{level:3,title:"2.数据添加",slug:"_2-数据添加",normalizedTitle:"2.数据添加",charIndex:4235},{level:3,title:"3.查询带有角色信息的所有用户",slug:"_3-查询带有角色信息的所有用户",normalizedTitle:"3.查询带有角色信息的所有用户",charIndex:4347},{level:3,title:"4.新增用户",slug:"_4-新增用户",normalizedTitle:"4.新增用户",charIndex:5301},{level:3,title:"5.删除用户信息",slug:"_5-删除用户信息",normalizedTitle:"5.删除用户信息",charIndex:6918},{level:3,title:"6.用户登录",slug:"_6-用户登录",normalizedTitle:"6.用户登录",charIndex:7490},{level:3,title:"7.service注入",slug:"_7-service注入",normalizedTitle:"7.service注入",charIndex:9358}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"注入mapper 事务控制   （同spring原始的事务控制） 注意问题 1.模型设置 2.数据添加 3.查询带有角色信息的所有用户 4.新增用户 5.删除用户信息 6.用户登录 7.service注入",content:'# 注入mapper\n\n>  1. 导入依赖坐标（注意，c3p0的0.9.1.2版本必须对应mybatis3.4.5版本或者以下）\n>     \n>     \x3c!--spring _ mybatis--\x3e\n>     <dependency>\n>       <groupId>org.mybatis</groupId>\n>       <artifactId>mybatis</artifactId>\n>       <version>3.4.5</version>\n>     </dependency>\n>     <dependency>\n>       <groupId>org.mybatis</groupId>\n>       <artifactId>mybatis-spring</artifactId>\n>       <version>2.0.5</version>\n>     </dependency>\n>     \n>     \x3c!-- 数据源--\x3e\n>     <dependency>\n>         <groupId>c3p0</groupId>\n>         <artifactId>c3p0</artifactId>\n>         <version>0.9.1.2</version>\n>     </dependency>\n>     \n>     \x3c!-- spring -tx  这个包一定要有  即使不用jdbc模板也要有--\x3e\n>     <dependency>\n>         <groupId>org.springframework</groupId>\n>         <artifactId>spring-tx</artifactId>\n>         <version>5.3.6</version>\n>     </dependency>\n>     \n> \n>  2. 在spring配置文件中 配置sqlSessionFactory和mapper\n> \n> \x3c!--配置 sqlSessionFactory--\x3e\n> <bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean">\n>     <property name="dataSource" ref="dataSource_c3p0"/>\n>     \x3c!-- 加载核心配置文件--\x3e\n>     <property name="configLocation" value="classpath:mybatis-config-spring.xml"/>\n> </bean>\n> \n> \x3c!-- 扫描mapper所在的包 为mapper包创建实现类--\x3e\n> <bean class="org.mybatis.spring.mapper.MapperScannerConfigurer">\n>     <property name="basePackage" value="com.diana.mapper"/>\n>     <property name="sqlSessionFactoryBeanName" value="sqlSessionFactory"/>\x3c!-- 这句可没有--\x3e\n> </bean>\n> \n> \n>  3. 代码编写\n>     \n>     @Service("userService")\n>     public class UserService {\n>         @Autowired\n>         private userMapper mapper;//注入mapper\n>                             \n>         public User selectById(int id){\n>             User user = mapper.selectById(id);//调用方法\n>             System.out.println(user);\n>             return user;\n>         }\n>     }\n>     \n>     \n>     @Controller("userController")\n>     public class UserController {\n>         @Autowired\n>         private UserService userService;//注入service\n>                             \n>         @RequestMapping("/user")\n>         public String selectById(){\n>             int id=1;\n>             User user = userService.selectById(id);\n>             System.out.println(user);\n>             return "success";\n>         }\n>     }\n>     \n\n\n# 事务控制 （同spring原始的事务控制）\n\n 1. 导入依赖 （aop配置的依赖）因为spring实现事务控制底层就是aop\n\n\x3c!-- aspectj  aop 配置--\x3e\n<dependency>\n  <groupId>org.aspectj</groupId>\n  <artifactId>aspectjweaver</artifactId>\n  <version>1.9.4</version>\n</dependency>\n\n\n 2. 开发步骤\n\n * service层注解\n   \n   * 在service方法上加@Transactional(isolation = Isolation.REPEATABLE_READ)，表示要对这个方法进行事务控制\n   \n   * 可在类上直接配置@Transactional，表示整个类所有方法的默认事务控制\n   \n   * 当方法和类上同时都有事务控制时，以方法上的为主，就近原则\n   \n   //先删除外键对，再删除用户  两个id是一样的   //要开启事务控制\n   @Transactional\n   public void delete(int id){\n       System.out.println("删除id "+id);\n       //删除外键对\n       mapper_role.deleteByUid(id);\n       //int i=1/0; //模拟异常\n       //删除用户\n       mapper.deleteById(id);\n   }\n   \n\n * xml中剩余配置\n   \n   * 把原配置中的 增强方法 声明和 织入过程 用@Transactional（）来代替\n   * 添加了一个包扫描和事务的注解驱动配置\n   \n   \x3c!-- jdbc模板数据 别人的东西 一般在xml中配置--\x3e\n   \n   \x3c!--加载外部配置文件property--\x3e\n   <context:property-placeholder location="classpath:jdbc.properties"/>\n   \x3c!-- el表达式 来获取配置文件信息--\x3e\n   <bean id="dataSource_c3p0" class="com.mchange.v2.c3p0.ComboPooledDataSource">\n       <property name="driverClass" value="${jdbc.driver}"/>\n       <property name="user" value="${jdbc.username}"/>\n       <property name="password" value="${jdbc.password}"/>\n       <property name="jdbcUrl" value="${jdbc.url}"/>\n   </bean>\n   \x3c!-- 在spring 容器中产生 jdbc模板对象   同时注入数据源--\x3e\n   <bean id="jdbcTemplate" class="org.springframework.jdbc.core.JdbcTemplate">\n       <property name="dataSource" ref="dataSource_c3p0"/>\n   </bean>\n   \n   \n   \n   \x3c!-- 配置一个平台事务管理器--\x3e\n   <bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager">\n       <property name="dataSource" ref="dataSource_c3p0"/>\n   </bean>\n   \n   \n   \x3c!-- 事务的注解驱动--\x3e\n   <tx:annotation-driven transaction-manager="transactionManager"/>\n   \n   \n   \x3c!-- 包扫描--\x3e\n   <context:component-scan base-package="com.diana"/>\n   \n\n\n# 注意问题\n\n\n# 1.模型设置\n\n> 记得起名字，然后在网页中调用模型的名字。\n> \n>  modelAndView.addObject("roleList",roles);\n> \n> \n> <c:forEach items="${roleList}" var="role">\n\n\n# 2.数据添加\n\n> 数据添加完成后，不要直接跳转到展示页面，因为单单一个视图是没有数据的，要重定向(redirect)到查询所有的url。\n> \n>    return "redirect:/role/list";\n\n\n# 3.查询带有角色信息的所有用户\n\n> 因为要显示用户的角色信息，所有涉及到多表查询。在User对象中封装一个Role的集合，作为外键连接，然后在输出时进行映射。\n> \n> <resultMap id="user_order" type="user_role">\n>     <id column="userId" property="id"/>\n>     <result column="username" property="username"/>\n>     <result column="password" property="password"/>\n>     <result column="email" property="email"/>\n>     <result column="phoneNum" property="phoneNum"/>\n>     <collection property="roles" ofType="role">\n>         <result column="roleName" property="roleName"/>\n>     </collection>\n> </resultMap>\n> \n> <select id="selectAll_Order" resultMap="user_order">\n>     select *,u.id,r.id\n>     from sys_user u,sys_role r,sys_user_role ur\n>     where\n>         u.id=userId and r.id=roleId\n> </select>\n> \n> \n> public class User_role {\n>     private int id;\n>     private String username;\n>     private String email;\n>     private String password;\n>     private String phoneNum;\n> \n>     //封装一个角色对象\n>     private List<Role> roles;}\n\n\n# 4.新增用户\n\n>  * 因为涉及到多对多，所以有第三张外键对应表。即，在新增用户表的时候，还有添加对应的外键表。\n> \n>  * 因为在新增页面，需要显示当前角色有哪些，所以要进行数据回显，先查询当前角色信息，然后在跳转到添加页面。一般该页面被称为addUI。\n>    \n>    \n> \n> <c:forEach items="${roleList}" var="role" >   ${role.roleName}   </c:forEach>\n> \n> \n> ```java\n> //进入添加页面  回显数据   显示角色信息\n> @RequestMapping("/addUI")\n> public ModelAndView addUI(){\n>     ModelAndView modelAndView=new ModelAndView();\n>     //查询数据\n>     List<Role> roles = roleService.selectAll();\n>     modelAndView.addObject("roleList",roles);\n>     modelAndView.setViewName("user-add");\n>     return modelAndView;\n> }\n> \n> \n>  * 外键表需要Uid和Rid，Uid在从网页获取新增数据的时候是获取不到，因为Uid对于User表来说是主键，自增的，不能让用户输入。所以要在插入User表数据后，获取对应的Uid。而Rid则以数组的形式直接从网页读取即可，这里数组的名字要和表单提交信息的name值相同。\n>    \n>     \x3c!-- 这里非常重要 ，一般的插入是得不到id的--\x3e\n>    <insert id="add" useGeneratedKeys="true" keyProperty="id" >\n>        insert into sys_user(username, email, password, phoneNum)\n>        VALUES (#{username},#{email},#{password},#{phoneNum})\n>    </insert>\n>    \n>    \n>     //service层\n>    \n>    //新增用户 +新增对应外键  //要开启事务控制\n>    @Transactional   //开始事务控制\n>    public void add(User_role user_role,int[] roleIds){\n>        // 新增用户表\n>        mapper.add(user_role);\n>        //要先插入数据，通过插入数据来获取id\n>        int userId=user_role.getId();\n>        //新增外键\n>        for(int roleId:roleIds){\n>            mapper_role.add(userId,roleId);\n>        }\n>    }\n>    \n>    \n>     //两个roleId 名称必须一致  Rid通过value值获取\n>    @RequestMapping("/add")\n>    public String add(User_role user,int[] roleId){}\n>    \n>    <input class="" type="checkbox" name="roleId" value="${role.id}">${role.roleName}\n>    \n> \n>  * 要进行事务操作 @Transactional\n\n\n# 5.删除用户信息\n\n>    * 因为有外键约束，所以要先删除对应外键表，然后在删除用户信息表。\n> \n>    ```java\n>    //service层\n>    //先删除外键对，再删除用户  两个id是一样的   //要开启事务控制\n>    @Transactional  //开启事务控制\n>    public void delete(int id){\n>      System.out.println("删除id "+id);\n>      //删除外键对\n>       mapper_role.deleteByUid(id);\n>       //删除用户\n>       mapper.deleteById(id);\n>    }\n>    ```\n> \n>    * 通过get方式得到要删除的id。User表的id和外键表的Uid是一样的，所以只需要一个参数即可。\n> \n>    ```jsp\n>    <a href="${pageContext.request.contextPath}/user/delete?id=${user.id}" class="btn bg-olive btn-xs">删除</a>\n>    ```\n> \n>    * 要进行事务操作  @Transactional\n\n\n# 6.用户登录\n\n>  * 使用拦截器拦截对url的访问，只有当完成登录操作后，才可以进入后台。\n>    \n>    <mvc:interceptors>\n>        <mvc:interceptor>\n>            \x3c!-- 配置对哪些资源进行拦截操作--\x3e\n>            <mvc:mapping path="/**"/>\n>            \x3c!-- 配置对哪些资源不进行拦截操作--\x3e\n>            <mvc:exclude-mapping path="/login"/> \x3c!-- 不拦截登陆页面--\x3e\n>            <bean class="com.diana.interceptor.Myinterceptor"/>\n>        </mvc:interceptor>\n>    </mvc:interceptors>\n>    \n> \n>  * 重定向到另一个页面时，路径名要写全 spring-mvc中配置的内部资源解析器（“/pages/”+str+".jsp"）不适应用它\n>    \n>    @RequestMapping("/login")\n>        public String  login(String username, String password, HttpSession session){\n>            //获取数据库查询的用户\n>            User_role user = userService.login(username, password);\n>            System.out.println(user);\n>            //如果用户存在\n>            if(user!=null){\n>                //如果正确，将用户信息存入session域中，重定向到主页面\n>                session.setAttribute("user",user);\n>    //            return "redirect:/index.jsp";\n>                return "redirect:/pages/main.jsp"; //重定向 的话 要写全路径名   spring-mvc中配置的内部资源解析器不适应用它\n>            }else{\n>                //如果不正确，跳转到登陆页面\n>                return "redirect:/login.jsp";\n>            }\n>        }\n>    \n> \n>  * 获取url访问的外部名称--request.getContextPath()\n>    \n>    //执行方法之前\n>    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {\n>        //得到session域中存放的用户对象\n>        HttpSession session = request.getSession();\n>        User_role user = (User_role)session.getAttribute("user");\n>        System.out.println(user);\n>        //如果session域中没有用户对象\n>        if(user==null){\n>            //request.getContextPath()  获取外部名称\n>            response.sendRedirect(request.getContextPath()+"/login.jsp");//如果没有登陆，则重定向到登陆页面\n>            return false;\n>        }\n>        //如果session域中有用户对象\n>        return true;\n>                        \n>    }\n>    \n\n\n# 7.service注入\n\n> 因为要使用事务，所以使用了动态代理技术，所以在注入的时候要使用接口进行注入\n> \n> @Autowired\n> private UserService userService;//注入userService   用接口来接  因为用到了动态代理技术 事务\n> @Autowired\n> private RoleService roleService;//注入roleService',normalizedContent:'# 注入mapper\n\n>  1. 导入依赖坐标（注意，c3p0的0.9.1.2版本必须对应mybatis3.4.5版本或者以下）\n>     \n>     \x3c!--spring _ mybatis--\x3e\n>     <dependency>\n>       <groupid>org.mybatis</groupid>\n>       <artifactid>mybatis</artifactid>\n>       <version>3.4.5</version>\n>     </dependency>\n>     <dependency>\n>       <groupid>org.mybatis</groupid>\n>       <artifactid>mybatis-spring</artifactid>\n>       <version>2.0.5</version>\n>     </dependency>\n>     \n>     \x3c!-- 数据源--\x3e\n>     <dependency>\n>         <groupid>c3p0</groupid>\n>         <artifactid>c3p0</artifactid>\n>         <version>0.9.1.2</version>\n>     </dependency>\n>     \n>     \x3c!-- spring -tx  这个包一定要有  即使不用jdbc模板也要有--\x3e\n>     <dependency>\n>         <groupid>org.springframework</groupid>\n>         <artifactid>spring-tx</artifactid>\n>         <version>5.3.6</version>\n>     </dependency>\n>     \n> \n>  2. 在spring配置文件中 配置sqlsessionfactory和mapper\n> \n> \x3c!--配置 sqlsessionfactory--\x3e\n> <bean id="sqlsessionfactory" class="org.mybatis.spring.sqlsessionfactorybean">\n>     <property name="datasource" ref="datasource_c3p0"/>\n>     \x3c!-- 加载核心配置文件--\x3e\n>     <property name="configlocation" value="classpath:mybatis-config-spring.xml"/>\n> </bean>\n> \n> \x3c!-- 扫描mapper所在的包 为mapper包创建实现类--\x3e\n> <bean class="org.mybatis.spring.mapper.mapperscannerconfigurer">\n>     <property name="basepackage" value="com.diana.mapper"/>\n>     <property name="sqlsessionfactorybeanname" value="sqlsessionfactory"/>\x3c!-- 这句可没有--\x3e\n> </bean>\n> \n> \n>  3. 代码编写\n>     \n>     @service("userservice")\n>     public class userservice {\n>         @autowired\n>         private usermapper mapper;//注入mapper\n>                             \n>         public user selectbyid(int id){\n>             user user = mapper.selectbyid(id);//调用方法\n>             system.out.println(user);\n>             return user;\n>         }\n>     }\n>     \n>     \n>     @controller("usercontroller")\n>     public class usercontroller {\n>         @autowired\n>         private userservice userservice;//注入service\n>                             \n>         @requestmapping("/user")\n>         public string selectbyid(){\n>             int id=1;\n>             user user = userservice.selectbyid(id);\n>             system.out.println(user);\n>             return "success";\n>         }\n>     }\n>     \n\n\n# 事务控制 （同spring原始的事务控制）\n\n 1. 导入依赖 （aop配置的依赖）因为spring实现事务控制底层就是aop\n\n\x3c!-- aspectj  aop 配置--\x3e\n<dependency>\n  <groupid>org.aspectj</groupid>\n  <artifactid>aspectjweaver</artifactid>\n  <version>1.9.4</version>\n</dependency>\n\n\n 2. 开发步骤\n\n * service层注解\n   \n   * 在service方法上加@transactional(isolation = isolation.repeatable_read)，表示要对这个方法进行事务控制\n   \n   * 可在类上直接配置@transactional，表示整个类所有方法的默认事务控制\n   \n   * 当方法和类上同时都有事务控制时，以方法上的为主，就近原则\n   \n   //先删除外键对，再删除用户  两个id是一样的   //要开启事务控制\n   @transactional\n   public void delete(int id){\n       system.out.println("删除id "+id);\n       //删除外键对\n       mapper_role.deletebyuid(id);\n       //int i=1/0; //模拟异常\n       //删除用户\n       mapper.deletebyid(id);\n   }\n   \n\n * xml中剩余配置\n   \n   * 把原配置中的 增强方法 声明和 织入过程 用@transactional（）来代替\n   * 添加了一个包扫描和事务的注解驱动配置\n   \n   \x3c!-- jdbc模板数据 别人的东西 一般在xml中配置--\x3e\n   \n   \x3c!--加载外部配置文件property--\x3e\n   <context:property-placeholder location="classpath:jdbc.properties"/>\n   \x3c!-- el表达式 来获取配置文件信息--\x3e\n   <bean id="datasource_c3p0" class="com.mchange.v2.c3p0.combopooleddatasource">\n       <property name="driverclass" value="${jdbc.driver}"/>\n       <property name="user" value="${jdbc.username}"/>\n       <property name="password" value="${jdbc.password}"/>\n       <property name="jdbcurl" value="${jdbc.url}"/>\n   </bean>\n   \x3c!-- 在spring 容器中产生 jdbc模板对象   同时注入数据源--\x3e\n   <bean id="jdbctemplate" class="org.springframework.jdbc.core.jdbctemplate">\n       <property name="datasource" ref="datasource_c3p0"/>\n   </bean>\n   \n   \n   \n   \x3c!-- 配置一个平台事务管理器--\x3e\n   <bean id="transactionmanager" class="org.springframework.jdbc.datasource.datasourcetransactionmanager">\n       <property name="datasource" ref="datasource_c3p0"/>\n   </bean>\n   \n   \n   \x3c!-- 事务的注解驱动--\x3e\n   <tx:annotation-driven transaction-manager="transactionmanager"/>\n   \n   \n   \x3c!-- 包扫描--\x3e\n   <context:component-scan base-package="com.diana"/>\n   \n\n\n# 注意问题\n\n\n# 1.模型设置\n\n> 记得起名字，然后在网页中调用模型的名字。\n> \n>  modelandview.addobject("rolelist",roles);\n> \n> \n> <c:foreach items="${rolelist}" var="role">\n\n\n# 2.数据添加\n\n> 数据添加完成后，不要直接跳转到展示页面，因为单单一个视图是没有数据的，要重定向(redirect)到查询所有的url。\n> \n>    return "redirect:/role/list";\n\n\n# 3.查询带有角色信息的所有用户\n\n> 因为要显示用户的角色信息，所有涉及到多表查询。在user对象中封装一个role的集合，作为外键连接，然后在输出时进行映射。\n> \n> <resultmap id="user_order" type="user_role">\n>     <id column="userid" property="id"/>\n>     <result column="username" property="username"/>\n>     <result column="password" property="password"/>\n>     <result column="email" property="email"/>\n>     <result column="phonenum" property="phonenum"/>\n>     <collection property="roles" oftype="role">\n>         <result column="rolename" property="rolename"/>\n>     </collection>\n> </resultmap>\n> \n> <select id="selectall_order" resultmap="user_order">\n>     select *,u.id,r.id\n>     from sys_user u,sys_role r,sys_user_role ur\n>     where\n>         u.id=userid and r.id=roleid\n> </select>\n> \n> \n> public class user_role {\n>     private int id;\n>     private string username;\n>     private string email;\n>     private string password;\n>     private string phonenum;\n> \n>     //封装一个角色对象\n>     private list<role> roles;}\n\n\n# 4.新增用户\n\n>  * 因为涉及到多对多，所以有第三张外键对应表。即，在新增用户表的时候，还有添加对应的外键表。\n> \n>  * 因为在新增页面，需要显示当前角色有哪些，所以要进行数据回显，先查询当前角色信息，然后在跳转到添加页面。一般该页面被称为addui。\n>    \n>    \n> \n> <c:foreach items="${rolelist}" var="role" >   ${role.rolename}   </c:foreach>\n> \n> \n> ```java\n> //进入添加页面  回显数据   显示角色信息\n> @requestmapping("/addui")\n> public modelandview addui(){\n>     modelandview modelandview=new modelandview();\n>     //查询数据\n>     list<role> roles = roleservice.selectall();\n>     modelandview.addobject("rolelist",roles);\n>     modelandview.setviewname("user-add");\n>     return modelandview;\n> }\n> \n> \n>  * 外键表需要uid和rid，uid在从网页获取新增数据的时候是获取不到，因为uid对于user表来说是主键，自增的，不能让用户输入。所以要在插入user表数据后，获取对应的uid。而rid则以数组的形式直接从网页读取即可，这里数组的名字要和表单提交信息的name值相同。\n>    \n>     \x3c!-- 这里非常重要 ，一般的插入是得不到id的--\x3e\n>    <insert id="add" usegeneratedkeys="true" keyproperty="id" >\n>        insert into sys_user(username, email, password, phonenum)\n>        values (#{username},#{email},#{password},#{phonenum})\n>    </insert>\n>    \n>    \n>     //service层\n>    \n>    //新增用户 +新增对应外键  //要开启事务控制\n>    @transactional   //开始事务控制\n>    public void add(user_role user_role,int[] roleids){\n>        // 新增用户表\n>        mapper.add(user_role);\n>        //要先插入数据，通过插入数据来获取id\n>        int userid=user_role.getid();\n>        //新增外键\n>        for(int roleid:roleids){\n>            mapper_role.add(userid,roleid);\n>        }\n>    }\n>    \n>    \n>     //两个roleid 名称必须一致  rid通过value值获取\n>    @requestmapping("/add")\n>    public string add(user_role user,int[] roleid){}\n>    \n>    <input class="" type="checkbox" name="roleid" value="${role.id}">${role.rolename}\n>    \n> \n>  * 要进行事务操作 @transactional\n\n\n# 5.删除用户信息\n\n>    * 因为有外键约束，所以要先删除对应外键表，然后在删除用户信息表。\n> \n>    ```java\n>    //service层\n>    //先删除外键对，再删除用户  两个id是一样的   //要开启事务控制\n>    @transactional  //开启事务控制\n>    public void delete(int id){\n>      system.out.println("删除id "+id);\n>      //删除外键对\n>       mapper_role.deletebyuid(id);\n>       //删除用户\n>       mapper.deletebyid(id);\n>    }\n>    ```\n> \n>    * 通过get方式得到要删除的id。user表的id和外键表的uid是一样的，所以只需要一个参数即可。\n> \n>    ```jsp\n>    <a href="${pagecontext.request.contextpath}/user/delete?id=${user.id}" class="btn bg-olive btn-xs">删除</a>\n>    ```\n> \n>    * 要进行事务操作  @transactional\n\n\n# 6.用户登录\n\n>  * 使用拦截器拦截对url的访问，只有当完成登录操作后，才可以进入后台。\n>    \n>    <mvc:interceptors>\n>        <mvc:interceptor>\n>            \x3c!-- 配置对哪些资源进行拦截操作--\x3e\n>            <mvc:mapping path="/**"/>\n>            \x3c!-- 配置对哪些资源不进行拦截操作--\x3e\n>            <mvc:exclude-mapping path="/login"/> \x3c!-- 不拦截登陆页面--\x3e\n>            <bean class="com.diana.interceptor.myinterceptor"/>\n>        </mvc:interceptor>\n>    </mvc:interceptors>\n>    \n> \n>  * 重定向到另一个页面时，路径名要写全 spring-mvc中配置的内部资源解析器（“/pages/”+str+".jsp"）不适应用它\n>    \n>    @requestmapping("/login")\n>        public string  login(string username, string password, httpsession session){\n>            //获取数据库查询的用户\n>            user_role user = userservice.login(username, password);\n>            system.out.println(user);\n>            //如果用户存在\n>            if(user!=null){\n>                //如果正确，将用户信息存入session域中，重定向到主页面\n>                session.setattribute("user",user);\n>    //            return "redirect:/index.jsp";\n>                return "redirect:/pages/main.jsp"; //重定向 的话 要写全路径名   spring-mvc中配置的内部资源解析器不适应用它\n>            }else{\n>                //如果不正确，跳转到登陆页面\n>                return "redirect:/login.jsp";\n>            }\n>        }\n>    \n> \n>  * 获取url访问的外部名称--request.getcontextpath()\n>    \n>    //执行方法之前\n>    public boolean prehandle(httpservletrequest request, httpservletresponse response, object handler) throws exception {\n>        //得到session域中存放的用户对象\n>        httpsession session = request.getsession();\n>        user_role user = (user_role)session.getattribute("user");\n>        system.out.println(user);\n>        //如果session域中没有用户对象\n>        if(user==null){\n>            //request.getcontextpath()  获取外部名称\n>            response.sendredirect(request.getcontextpath()+"/login.jsp");//如果没有登陆，则重定向到登陆页面\n>            return false;\n>        }\n>        //如果session域中有用户对象\n>        return true;\n>                        \n>    }\n>    \n\n\n# 7.service注入\n\n> 因为要使用事务，所以使用了动态代理技术，所以在注入的时候要使用接口进行注入\n> \n> @autowired\n> private userservice userservice;//注入userservice   用接口来接  因为用到了动态代理技术 事务\n> @autowired\n> private roleservice roleservice;//注入roleservice',charsets:{cjk:!0}},{title:"Redis网络模型",frontmatter:{autoSort:9,title:"Redis网络模型",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/638026/",categories:["后端","数据库","Redis"],tags:["知识","数据库","Redis"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/30.%E6%95%B0%E6%8D%AE%E5%BA%93/05.Redis/50.Redis%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE.html",relativePath:"01.后端/30.数据库/05.Redis/50.Redis通信协议.md",key:"v-3934d917",path:"/pages/638026/",headers:[{level:2,title:"用户空间和内核态空间",slug:"用户空间和内核态空间",normalizedTitle:"用户空间和内核态空间",charIndex:2},{level:2,title:"网络模型-阻塞IO",slug:"网络模型-阻塞io",normalizedTitle:"网络模型-阻塞io",charIndex:1257},{level:2,title:"网络模型-非阻塞IO",slug:"网络模型-非阻塞io",normalizedTitle:"网络模型-非阻塞io",charIndex:1933},{level:2,title:"网络模型-IO多路复用",slug:"网络模型-io多路复用",normalizedTitle:"网络模型-io多路复用",charIndex:2243},{level:2,title:"网络模型-IO多路复用-select方式",slug:"网络模型-io多路复用-select方式",normalizedTitle:"网络模型-io多路复用-select方式",charIndex:3567},{level:2,title:"网络模型-IO多路复用模型-poll模式",slug:"网络模型-io多路复用模型-poll模式",normalizedTitle:"网络模型-io多路复用模型-poll模式",charIndex:4037},{level:2,title:"网络模型-IO多路复用模型-epoll函数",slug:"网络模型-io多路复用模型-epoll函数",normalizedTitle:"网络模型-io多路复用模型-epoll函数",charIndex:4377},{level:2,title:"网络模型-epoll中的ET和LT",slug:"网络模型-epoll中的et和lt",normalizedTitle:"网络模型-epoll中的et和lt",charIndex:5220},{level:2,title:"网络模型-基于epoll的服务器端流程",slug:"网络模型-基于epoll的服务器端流程",normalizedTitle:"网络模型-基于epoll的服务器端流程",charIndex:5693},{level:2,title:"网络模型-信号驱动",slug:"网络模型-信号驱动",normalizedTitle:"网络模型-信号驱动",charIndex:6195},{level:3,title:"异步IO",slug:"异步io",normalizedTitle:"异步io",charIndex:1401},{level:3,title:"对比",slug:"对比",normalizedTitle:"对比",charIndex:4284},{level:2,title:"网络模型-Redis是单线程的吗？为什么使用单线程",slug:"网络模型-redis是单线程的吗-为什么使用单线程",normalizedTitle:"网络模型-redis是单线程的吗？为什么使用单线程",charIndex:6703},{level:2,title:"Redis的单线程模型-Redis单线程和多线程网络模型变更",slug:"redis的单线程模型-redis单线程和多线程网络模型变更",normalizedTitle:"redis的单线程模型-redis单线程和多线程网络模型变更",charIndex:7196}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"用户空间和内核态空间 网络模型-阻塞IO 网络模型-非阻塞IO 网络模型-IO多路复用 网络模型-IO多路复用-select方式 网络模型-IO多路复用模型-poll模式 网络模型-IO多路复用模型-epoll函数 网络模型-epoll中的ET和LT 网络模型-基于epoll的服务器端流程 网络模型-信号驱动 异步IO 对比 网络模型-Redis是单线程的吗？为什么使用单线程 Redis的单线程模型-Redis单线程和多线程网络模型变更",content:"# 用户空间和内核态空间\n\n服务器大多都采用Linux系统，这里我们以Linux为例来讲解:\n\nubuntu和Centos 都是Linux的发行版，发行版可以看成对linux包了一层壳，任何Linux发行版，其系统内核都是Linux。我们的应用都需要通过Linux内核与硬件交互\n\n\n\n用户的应用，比如redis，mysql等其实是没有办法去执行访问我们操作系统的硬件的，所以我们可以通过发行版的这个壳子去访问内核，再通过内核去访问计算机硬件\n\n\n\n计算机硬件包括，如cpu，内存，网卡等等，内核（通过寻址空间）可以操作硬件的，但是内核需要不同设备的驱动，有了这些驱动之后，内核就可以去对计算机硬件去进行 内存管理，文件系统的管理，进程的管理等等\n\n\n\n我们想要用户的应用来访问，计算机就必须要通过对外暴露的一些接口，才能访问到，从而简介的实现对内核的操控，但是内核本身上来说也是一个应用，所以他本身也需要一些内存，cpu等设备资源，用户应用本身也在消耗这些资源，如果不加任何限制，用户去操作随意的去操作我们的资源，就有可能导致一些冲突，甚至有可能导致我们的系统出现无法运行的问题，因此我们需要把用户和内核隔离开\n\n进程的寻址空间划分成两部分：内核空间、用户空间\n\n什么是寻址空间呢？我们的应用程序也好，还是内核空间也好，都是没有办法直接去物理内存的，而是通过分配一些虚拟内存映射到物理内存中，我们的内核和应用程序去访问虚拟内存的时候，就需要一个虚拟地址，这个地址是一个无符号的整数，比如一个32位的操作系统，他的带宽就是32，他的虚拟地址就是2的32次方，也就是说他寻址的范围就是0~2的32次方， 这片寻址空间对应的就是2的32个字节，就是4GB，这个4GB，会有3个GB分给用户空间，会有1GB给内核系统\n\n\n\n在linux中，他们权限分成两个等级，0和3，用户空间只能执行受限的命令（Ring3），而且不能直接调用系统资源，必须通过内核提供的接口来访问内核空间可以执行特权命令（Ring0），调用一切系统资源，所以一般情况下，用户的操作是运行在用户空间，而内核运行的数据是在内核空间的，而有的情况下，一个应用程序需要去调用一些特权资源，去调用一些内核空间的操作，所以此时他俩需要在用户态和内核态之间进行切换。\n\n比如：\n\nLinux系统为了提高IO效率，会在用户空间和内核空间都加入缓冲区：\n\n写数据时，要把用户缓冲数据拷贝到内核缓冲区，然后写入设备\n\n读数据时，要从设备读取数据到内核缓冲区，然后拷贝到用户缓冲区\n\n针对这个操作：我们的用户在写读数据时，会去向内核态申请，想要读取内核的数据，而内核数据要去等待驱动程序从硬件上读取数据，当从磁盘上加载到数据之后，内核会将数据写入到内核的缓冲区中，然后再将数据拷贝到用户态的buffer中，然后再返回给应用程序，整体而言，速度慢，就是这个原因，为了加速，我们希望read也好，还是wait for data也最好都不要等待，或者时间尽量的短。\n\n\n\n\n# 网络模型-阻塞IO\n\n在《UNIX网络编程》一书中，总结归纳了5种IO模型：\n\n * 阻塞IO（Blocking IO）\n * 非阻塞IO（Nonblocking IO）\n * IO多路复用（IO Multiplexing）\n * 信号驱动IO（Signal Driven IO）\n * 异步IO（Asynchronous IO）\n\n应用程序想要去读取数据，他是无法直接去读取磁盘数据的，他需要先到内核里边去等待内核操作硬件拿到数据，这个过程就是1，是需要等待的，等到内核从磁盘上把数据加载出来之后，再把这个数据写给用户的缓存区，这个过程是2，如果是阻塞IO，那么整个过程中，用户从发起读请求开始，一直到读取到数据，都是一个阻塞状态。\n\n\n\n具体流程如下图：\n\n用户去读取数据时，会去先发起recvform一个命令，去尝试从内核上加载数据，如果内核没有数据，那么用户就会等待，此时内核会去从硬件上读取数据，内核读取数据之后，会把数据拷贝到用户态，并且返回ok，整个过程，都是阻塞等待的，这就是阻塞IO\n\n总结如下：\n\n顾名思义，阻塞IO就是两个阶段都必须阻塞等待：\n\n阶段一：\n\n * 用户进程尝试读取数据（比如网卡数据）\n * 此时数据尚未到达，内核需要等待数据\n * 此时用户进程也处于阻塞状态\n\n阶段二：\n\n * 数据到达并拷贝到内核缓冲区，代表已就绪\n * 将内核数据拷贝到用户缓冲区\n * 拷贝过程中，用户进程依然阻塞等待\n * 拷贝完成，用户进程解除阻塞，处理数据\n\n可以看到，阻塞IO模型中，用户进程在两个阶段都是阻塞状态。\n\n\n\n\n# 网络模型-非阻塞IO\n\n顾名思义，非阻塞IO的recvfrom操作会立即返回结果而不是阻塞用户进程。\n\n阶段一：\n\n * 用户进程尝试读取数据（比如网卡数据）\n * 此时数据尚未到达，内核需要等待数据\n * 返回异常给用户进程\n * 用户进程拿到error后，再次尝试读取\n * 循环往复，直到数据就绪\n\n阶段二：\n\n * 将内核数据拷贝到用户缓冲区\n * 拷贝过程中，用户进程依然阻塞等待\n * 拷贝完成，用户进程解除阻塞，处理数据\n * 可以看到，非阻塞IO模型中，用户进程在第一个阶段是非阻塞，第二个阶段是阻塞状态。虽然是非阻塞，但性能并没有得到提高。而且忙等机制会导致CPU空转，CPU使用率暴增。\n\n\n\n\n# 网络模型-IO多路复用\n\n无论是阻塞IO还是非阻塞IO，用户应用在一阶段都需要调用recvfrom来获取数据，差别在于无数据时的处理方案：\n\n如果调用recvfrom时，恰好没有数据，阻塞IO会使CPU阻塞，非阻塞IO使CPU空转，都不能充分发挥CPU的作用。 如果调用recvfrom时，恰好有数据，则用户进程可以直接进入第二阶段，读取并处理数据\n\n所以怎么看起来以上两种方式性能都不好\n\n而在单线程情况下，只能依次处理IO事件，如果正在处理的IO事件恰好未就绪（数据不可读或不可写），线程就会被阻塞，所有IO事件都必须等待，性能自然会很差。\n\n就比如服务员给顾客点餐，分两步：\n\n * 顾客思考要吃什么（等待数据就绪）\n * 顾客想好了，开始点餐（读取数据）\n\n要提高效率有几种办法？\n\n方案一：增加更多服务员（多线程） 方案二：不排队，谁想好了吃什么（数据就绪了），服务员就给谁点餐（用户应用就去读取数据）\n\n那么问题来了：用户进程如何知道内核中数据是否就绪呢？\n\n所以接下来就需要详细的来解决多路复用模型是如何知道到底怎么知道内核数据是否就绪的问题了\n\n这个问题的解决依赖于提出的\n\n文件描述符（File Descriptor）：简称FD，是一个从0 开始的无符号整数，用来关联Linux中的一个文件。在Linux中，一切皆文件，例如常规文件、视频、硬件设备等，当然也包括网络套接字（Socket）。\n\n通过FD，我们的网络模型可以利用一个线程监听多个FD，并在某个FD可读、可写时得到通知，从而避免无效的等待，充分利用CPU资源。\n\n阶段一：\n\n * 用户进程调用select，指定要监听的FD集合\n * 核监听FD对应的多个socket\n * 任意一个或多个socket数据就绪则返回readable\n * 此过程中用户进程阻塞\n\n阶段二：\n\n * 用户进程找到就绪的socket\n * 依次调用recvfrom读取数据\n * 内核将数据拷贝到用户空间\n * 用户进程处理数据\n\n当用户去读取数据的时候，不再去直接调用recvfrom了，而是调用select的函数，select函数会将需要监听的数据交给内核，由内核去检查这些数据是否就绪了，如果说这个数据就绪了，就会通知应用程序数据就绪，然后来读取数据，再从内核中把数据拷贝给用户态，完成数据处理，如果N多个FD一个都没处理完，此时就进行等待。\n\n用IO复用模式，可以确保去读数据的时候，数据是一定存在的，他的效率比原来的阻塞IO和非阻塞IO性能都要高\n\n\n\nIO多路复用是利用单个线程来同时监听多个FD，并在某个FD可读、可写时得到通知，从而避免无效的等待，充分利用CPU资源。不过监听FD的方式、通知的方式又有多种实现，常见的有：\n\n * select\n * poll\n * epoll\n\n其中select和pool相当于是当被监听的数据准备好之后，他会把你监听的FD整个数据都发给你，你需要到整个FD中去找，哪些是处理好了的，需要通过遍历的方式，所以性能也并不是那么好\n\n而epoll，则相当于内核准备好了之后，他会把准备好的数据，直接发给你，咱们就省去了遍历的动作。\n\n\n# 网络模型-IO多路复用-select方式\n\nselect是Linux最早是由的I/O多路复用技术：\n\n简单说，就是我们把需要处理的数据封装成FD，然后在用户态时创建一个fd的集合（这个集合的大小是要监听的那个FD的最大值+1，但是大小整体是有限制的 ），这个集合的长度大小是有限制的，同时在这个集合中，标明出来我们要控制哪些数据，\n\n比如要监听的数据，是1,2,5三个数据，此时会执行select函数，然后将整个fd发给内核态，内核态会去遍历用户态传递过来的数据，如果发现这里边都数据都没有就绪，就休眠，直到有数据准备好时，就会被唤醒，唤醒之后，再次遍历一遍，看看谁准备好了，然后再将处理掉没有准备好的数据，最后再将这个FD集合写回到用户态中去，此时用户态就知道了，奥，有人准备好了，但是对于用户态而言，并不知道谁处理好了，所以用户态也需要去进行遍历，然后找到对应准备好数据的节点，再去发起读请求，我们会发现，这种模式下他虽然比阻塞IO和非阻塞IO好，但是依然有些麻烦的事情， 比如说频繁的传递fd集合，频繁的去遍历FD等问题\n\n\n\n\n# 网络模型-IO多路复用模型-poll模式\n\npoll模式对select模式做了简单改进，但性能提升不明显，部分关键代码如下：\n\nIO流程：\n\n * 创建pollfd数组，向其中添加关注的fd信息，数组大小自定义\n * 调用poll函数，将pollfd数组拷贝到内核空间，转链表存储，无上限\n * 内核遍历fd，判断是否就绪\n * 数据就绪或超时后，拷贝pollfd数组到用户空间，返回就绪fd数量n\n * 用户进程判断n是否大于0,大于0则遍历pollfd数组，找到就绪的fd\n\n与select对比：\n\n * select模式中的fd_set大小固定为1024，而pollfd在内核中采用链表，理论上无上限\n * 监听FD越多，每次遍历消耗时间也越久，性能反而会下降\n\n\n\n\n# 网络模型-IO多路复用模型-epoll函数\n\nepoll模式是对select和poll的改进，它提供了三个函数：\n\n第一个是：eventpoll的函数，他内部包含两个东西\n\n一个是：\n\n1、红黑树-> 记录的事要监听的FD\n\n2、一个是链表->一个链表，记录的是就绪的FD\n\n紧接着调用epoll_ctl操作，将要监听的数据添加到红黑树上去，并且给每个fd设置一个监听函数，这个函数会在fd数据就绪时触发，就是准备好了，现在就把fd把数据添加到list_head中去\n\n3、调用epoll_wait函数\n\n就去等待，在用户态创建一个空的events数组，当就绪之后，我们的回调函数会把数据添加到list_head中去，当调用这个函数的时候，会去检查list_head，当然这个过程需要参考配置的等待时间，可以等一定时间，也可以一直等， 如果在此过程中，检查到了list_head中有数据会将数据添加到链表中，此时将数据放入到events数组中，并且返回对应的操作的数量，用户态的此时收到响应后，从events中拿到对应准备好的数据的节点，再去调用方法去拿数据。\n\n小总结：\n\nselect模式存在的三个问题：\n\n * 能监听的FD最大不超过1024\n * 每次select都需要把所有要监听的FD都拷贝到内核空间\n * 每次都要遍历所有FD来判断就绪状态\n\npoll模式的问题：\n\n * poll利用链表解决了select中监听FD上限的问题，但依然要遍历所有FD，如果监听较多，性能会下降\n\nepoll模式中如何解决这些问题的？\n\n * 基于epoll实例中的红黑树保存要监听的FD，理论上无上限，而且增删改查效率都非常高\n * 每个FD只需要执行一次epoll_ctl添加到红黑树，以后每次epol_wait无需传递任何参数，无需重复拷贝FD到内核空间\n * 利用ep_poll_callback机制来监听FD状态，无需遍历所有FD，因此性能不会随监听的FD数量增多而下降\n\n\n# 网络模型-epoll中的ET和LT\n\n当FD有数据可读时，我们调用epoll_wait（或者select、poll）可以得到通知。但是事件通知的模式有两种：\n\n * LevelTriggered：简称LT，也叫做水平触发。只要某个FD中有数据可读，每次调用epoll_wait都会得到通知。\n * EdgeTriggered：简称ET，也叫做边沿触发。只有在某个FD有状态变化时，调用epoll_wait才会被通知。\n\n举个栗子：\n\n * 假设一个客户端socket对应的FD已经注册到了epoll实例中\n * 客户端socket发送了2kb的数据\n * 服务端调用epoll_wait，得到通知说FD就绪\n * 服务端从FD读取了1kb数据回到步骤3（再次调用epoll_wait，形成循环）\n\n结论\n\n如果我们采用LT模式，因为FD中仍有1kb数据，则第⑤步依然会返回结果，并且得到通知 如果我们采用ET模式，因为第③步已经消费了FD可读事件，第⑤步FD状态没有变化，因此epoll_wait不会返回，数据无法读取，客户端响应超时。\n\n\n# 网络模型-基于epoll的服务器端流程\n\n我们来梳理一下这张图\n\n服务器启动以后，服务端会去调用epoll_create，创建一个epoll实例，epoll实例中包含两个数据\n\n1、红黑树（为空）：rb_root 用来去记录需要被监听的FD\n\n2、链表（为空）：list_head，用来存放已经就绪的FD\n\n创建好了之后，会去调用epoll_ctl函数，此函数会会将需要监听的数据添加到rb_root中去，并且对当前这些存在于红黑树的节点设置回调函数，当这些被监听的数据一旦准备完成，就会被调用，而调用的结果就是将红黑树的fd添加到list_head中去(但是此时并没有完成)\n\n3、当第二步完成后，就会调用epoll_wait函数，这个函数会去校验是否有数据准备完毕（因为数据一旦准备就绪，就会被回调函数添加到list_head中），在等待了一段时间后(可以进行配置)，如果等够了超时时间，则返回没有数据，如果有，则进一步判断当前是什么事件，如果是建立连接时间，则调用accept() 接受客户端socket，拿到建立连接的socket，然后建立起来连接，如果是其他事件，则把数据进行写出\n\n\n\n\n# 网络模型-信号驱动\n\n信号驱动IO是与内核建立SIGIO的信号关联并设置回调，当内核有FD就绪时，会发出SIGIO信号通知用户，期间用户应用可以执行其它业务，无需阻塞等待。\n\n阶段一：\n\n * 用户进程调用sigaction，注册信号处理函数\n * 内核返回成功，开始监听FD\n * 用户进程不阻塞等待，可以执行其它业务\n * 当内核数据就绪后，回调用户进程的SIGIO处理函数\n\n阶段二：\n\n * 收到SIGIO回调信号\n * 调用recvfrom，读取\n * 内核将数据拷贝到用户空间\n * 用户进程处理数据\n\n\n\n当有大量IO操作时，信号较多，SIGIO处理函数不能及时处理可能导致信号队列溢出，而且内核空间与用户空间的频繁信号交互性能也较低。\n\n\n# 异步IO\n\n这种方式，不仅仅是用户态在试图读取数据后，不阻塞，而且当内核的数据准备完成后，也不会阻塞\n\n他会由内核将所有数据处理完成后，由内核将数据写入到用户态中，然后才算完成，所以性能极高，不会有任何阻塞，全部都由内核完成，可以看到，异步IO模型中，用户进程在两个阶段都是非阻塞状态。\n\n\n\n\n# 对比\n\n最后用一幅图，来说明他们之间的区别\n\n\n\n\n# 网络模型-Redis是单线程的吗？为什么使用单线程\n\nRedis到底是单线程还是多线程？\n\n * 如果仅仅聊Redis的核心业务部分（命令处理），答案是单线程\n * 如果是聊整个Redis，那么答案就是多线程\n\n在Redis版本迭代过程中，在两个重要的时间节点上引入了多线程的支持：\n\n * Redis v4.0：引入多线程异步处理一些耗时较旧的任务，例如异步删除命令unlink\n * Redis v6.0：在核心网络模型中引入 多线程，进一步提高对于多核CPU的利用率\n\n因此，对于Redis的核心网络模型，在Redis 6.0之前确实都是单线程。是利用epoll（Linux系统）这样的IO多路复用技术在事件循环中不断处理客户端情况。\n\n为什么Redis要选择单线程？\n\n * 抛开持久化不谈，Redis是纯 内存操作，执行速度非常快，它的性能瓶颈是网络延迟而不是执行速度，因此多线程并不会带来巨大的性能提升。\n * 多线程会导致过多的上下文切换，带来不必要的开销\n * 引入多线程会面临线程安全问题，必然要引入线程锁这样的安全手段，实现复杂度增高，而且性能也会大打折扣\n\n\n# Redis的单线程模型-Redis单线程和多线程网络模型变更\n\n\n\n当我们的客户端想要去连接我们服务器，会去先到IO多路复用模型去进行排队，会有一个连接应答处理器，他会去接受读请求，然后又把读请求注册到具体模型中去，此时这些建立起来的连接，如果是客户端请求处理器去进行执行命令时，他会去把数据读取出来，然后把数据放入到client中， clinet去解析当前的命令转化为redis认识的命令，接下来就开始处理这些命令，从redis中的command中找到这些命令，然后就真正的去操作对应的数据了，当数据操作完成后，会去找到命令回复处理器，再由他将数据写出。",normalizedContent:"# 用户空间和内核态空间\n\n服务器大多都采用linux系统，这里我们以linux为例来讲解:\n\nubuntu和centos 都是linux的发行版，发行版可以看成对linux包了一层壳，任何linux发行版，其系统内核都是linux。我们的应用都需要通过linux内核与硬件交互\n\n\n\n用户的应用，比如redis，mysql等其实是没有办法去执行访问我们操作系统的硬件的，所以我们可以通过发行版的这个壳子去访问内核，再通过内核去访问计算机硬件\n\n\n\n计算机硬件包括，如cpu，内存，网卡等等，内核（通过寻址空间）可以操作硬件的，但是内核需要不同设备的驱动，有了这些驱动之后，内核就可以去对计算机硬件去进行 内存管理，文件系统的管理，进程的管理等等\n\n\n\n我们想要用户的应用来访问，计算机就必须要通过对外暴露的一些接口，才能访问到，从而简介的实现对内核的操控，但是内核本身上来说也是一个应用，所以他本身也需要一些内存，cpu等设备资源，用户应用本身也在消耗这些资源，如果不加任何限制，用户去操作随意的去操作我们的资源，就有可能导致一些冲突，甚至有可能导致我们的系统出现无法运行的问题，因此我们需要把用户和内核隔离开\n\n进程的寻址空间划分成两部分：内核空间、用户空间\n\n什么是寻址空间呢？我们的应用程序也好，还是内核空间也好，都是没有办法直接去物理内存的，而是通过分配一些虚拟内存映射到物理内存中，我们的内核和应用程序去访问虚拟内存的时候，就需要一个虚拟地址，这个地址是一个无符号的整数，比如一个32位的操作系统，他的带宽就是32，他的虚拟地址就是2的32次方，也就是说他寻址的范围就是0~2的32次方， 这片寻址空间对应的就是2的32个字节，就是4gb，这个4gb，会有3个gb分给用户空间，会有1gb给内核系统\n\n\n\n在linux中，他们权限分成两个等级，0和3，用户空间只能执行受限的命令（ring3），而且不能直接调用系统资源，必须通过内核提供的接口来访问内核空间可以执行特权命令（ring0），调用一切系统资源，所以一般情况下，用户的操作是运行在用户空间，而内核运行的数据是在内核空间的，而有的情况下，一个应用程序需要去调用一些特权资源，去调用一些内核空间的操作，所以此时他俩需要在用户态和内核态之间进行切换。\n\n比如：\n\nlinux系统为了提高io效率，会在用户空间和内核空间都加入缓冲区：\n\n写数据时，要把用户缓冲数据拷贝到内核缓冲区，然后写入设备\n\n读数据时，要从设备读取数据到内核缓冲区，然后拷贝到用户缓冲区\n\n针对这个操作：我们的用户在写读数据时，会去向内核态申请，想要读取内核的数据，而内核数据要去等待驱动程序从硬件上读取数据，当从磁盘上加载到数据之后，内核会将数据写入到内核的缓冲区中，然后再将数据拷贝到用户态的buffer中，然后再返回给应用程序，整体而言，速度慢，就是这个原因，为了加速，我们希望read也好，还是wait for data也最好都不要等待，或者时间尽量的短。\n\n\n\n\n# 网络模型-阻塞io\n\n在《unix网络编程》一书中，总结归纳了5种io模型：\n\n * 阻塞io（blocking io）\n * 非阻塞io（nonblocking io）\n * io多路复用（io multiplexing）\n * 信号驱动io（signal driven io）\n * 异步io（asynchronous io）\n\n应用程序想要去读取数据，他是无法直接去读取磁盘数据的，他需要先到内核里边去等待内核操作硬件拿到数据，这个过程就是1，是需要等待的，等到内核从磁盘上把数据加载出来之后，再把这个数据写给用户的缓存区，这个过程是2，如果是阻塞io，那么整个过程中，用户从发起读请求开始，一直到读取到数据，都是一个阻塞状态。\n\n\n\n具体流程如下图：\n\n用户去读取数据时，会去先发起recvform一个命令，去尝试从内核上加载数据，如果内核没有数据，那么用户就会等待，此时内核会去从硬件上读取数据，内核读取数据之后，会把数据拷贝到用户态，并且返回ok，整个过程，都是阻塞等待的，这就是阻塞io\n\n总结如下：\n\n顾名思义，阻塞io就是两个阶段都必须阻塞等待：\n\n阶段一：\n\n * 用户进程尝试读取数据（比如网卡数据）\n * 此时数据尚未到达，内核需要等待数据\n * 此时用户进程也处于阻塞状态\n\n阶段二：\n\n * 数据到达并拷贝到内核缓冲区，代表已就绪\n * 将内核数据拷贝到用户缓冲区\n * 拷贝过程中，用户进程依然阻塞等待\n * 拷贝完成，用户进程解除阻塞，处理数据\n\n可以看到，阻塞io模型中，用户进程在两个阶段都是阻塞状态。\n\n\n\n\n# 网络模型-非阻塞io\n\n顾名思义，非阻塞io的recvfrom操作会立即返回结果而不是阻塞用户进程。\n\n阶段一：\n\n * 用户进程尝试读取数据（比如网卡数据）\n * 此时数据尚未到达，内核需要等待数据\n * 返回异常给用户进程\n * 用户进程拿到error后，再次尝试读取\n * 循环往复，直到数据就绪\n\n阶段二：\n\n * 将内核数据拷贝到用户缓冲区\n * 拷贝过程中，用户进程依然阻塞等待\n * 拷贝完成，用户进程解除阻塞，处理数据\n * 可以看到，非阻塞io模型中，用户进程在第一个阶段是非阻塞，第二个阶段是阻塞状态。虽然是非阻塞，但性能并没有得到提高。而且忙等机制会导致cpu空转，cpu使用率暴增。\n\n\n\n\n# 网络模型-io多路复用\n\n无论是阻塞io还是非阻塞io，用户应用在一阶段都需要调用recvfrom来获取数据，差别在于无数据时的处理方案：\n\n如果调用recvfrom时，恰好没有数据，阻塞io会使cpu阻塞，非阻塞io使cpu空转，都不能充分发挥cpu的作用。 如果调用recvfrom时，恰好有数据，则用户进程可以直接进入第二阶段，读取并处理数据\n\n所以怎么看起来以上两种方式性能都不好\n\n而在单线程情况下，只能依次处理io事件，如果正在处理的io事件恰好未就绪（数据不可读或不可写），线程就会被阻塞，所有io事件都必须等待，性能自然会很差。\n\n就比如服务员给顾客点餐，分两步：\n\n * 顾客思考要吃什么（等待数据就绪）\n * 顾客想好了，开始点餐（读取数据）\n\n要提高效率有几种办法？\n\n方案一：增加更多服务员（多线程） 方案二：不排队，谁想好了吃什么（数据就绪了），服务员就给谁点餐（用户应用就去读取数据）\n\n那么问题来了：用户进程如何知道内核中数据是否就绪呢？\n\n所以接下来就需要详细的来解决多路复用模型是如何知道到底怎么知道内核数据是否就绪的问题了\n\n这个问题的解决依赖于提出的\n\n文件描述符（file descriptor）：简称fd，是一个从0 开始的无符号整数，用来关联linux中的一个文件。在linux中，一切皆文件，例如常规文件、视频、硬件设备等，当然也包括网络套接字（socket）。\n\n通过fd，我们的网络模型可以利用一个线程监听多个fd，并在某个fd可读、可写时得到通知，从而避免无效的等待，充分利用cpu资源。\n\n阶段一：\n\n * 用户进程调用select，指定要监听的fd集合\n * 核监听fd对应的多个socket\n * 任意一个或多个socket数据就绪则返回readable\n * 此过程中用户进程阻塞\n\n阶段二：\n\n * 用户进程找到就绪的socket\n * 依次调用recvfrom读取数据\n * 内核将数据拷贝到用户空间\n * 用户进程处理数据\n\n当用户去读取数据的时候，不再去直接调用recvfrom了，而是调用select的函数，select函数会将需要监听的数据交给内核，由内核去检查这些数据是否就绪了，如果说这个数据就绪了，就会通知应用程序数据就绪，然后来读取数据，再从内核中把数据拷贝给用户态，完成数据处理，如果n多个fd一个都没处理完，此时就进行等待。\n\n用io复用模式，可以确保去读数据的时候，数据是一定存在的，他的效率比原来的阻塞io和非阻塞io性能都要高\n\n\n\nio多路复用是利用单个线程来同时监听多个fd，并在某个fd可读、可写时得到通知，从而避免无效的等待，充分利用cpu资源。不过监听fd的方式、通知的方式又有多种实现，常见的有：\n\n * select\n * poll\n * epoll\n\n其中select和pool相当于是当被监听的数据准备好之后，他会把你监听的fd整个数据都发给你，你需要到整个fd中去找，哪些是处理好了的，需要通过遍历的方式，所以性能也并不是那么好\n\n而epoll，则相当于内核准备好了之后，他会把准备好的数据，直接发给你，咱们就省去了遍历的动作。\n\n\n# 网络模型-io多路复用-select方式\n\nselect是linux最早是由的i/o多路复用技术：\n\n简单说，就是我们把需要处理的数据封装成fd，然后在用户态时创建一个fd的集合（这个集合的大小是要监听的那个fd的最大值+1，但是大小整体是有限制的 ），这个集合的长度大小是有限制的，同时在这个集合中，标明出来我们要控制哪些数据，\n\n比如要监听的数据，是1,2,5三个数据，此时会执行select函数，然后将整个fd发给内核态，内核态会去遍历用户态传递过来的数据，如果发现这里边都数据都没有就绪，就休眠，直到有数据准备好时，就会被唤醒，唤醒之后，再次遍历一遍，看看谁准备好了，然后再将处理掉没有准备好的数据，最后再将这个fd集合写回到用户态中去，此时用户态就知道了，奥，有人准备好了，但是对于用户态而言，并不知道谁处理好了，所以用户态也需要去进行遍历，然后找到对应准备好数据的节点，再去发起读请求，我们会发现，这种模式下他虽然比阻塞io和非阻塞io好，但是依然有些麻烦的事情， 比如说频繁的传递fd集合，频繁的去遍历fd等问题\n\n\n\n\n# 网络模型-io多路复用模型-poll模式\n\npoll模式对select模式做了简单改进，但性能提升不明显，部分关键代码如下：\n\nio流程：\n\n * 创建pollfd数组，向其中添加关注的fd信息，数组大小自定义\n * 调用poll函数，将pollfd数组拷贝到内核空间，转链表存储，无上限\n * 内核遍历fd，判断是否就绪\n * 数据就绪或超时后，拷贝pollfd数组到用户空间，返回就绪fd数量n\n * 用户进程判断n是否大于0,大于0则遍历pollfd数组，找到就绪的fd\n\n与select对比：\n\n * select模式中的fd_set大小固定为1024，而pollfd在内核中采用链表，理论上无上限\n * 监听fd越多，每次遍历消耗时间也越久，性能反而会下降\n\n\n\n\n# 网络模型-io多路复用模型-epoll函数\n\nepoll模式是对select和poll的改进，它提供了三个函数：\n\n第一个是：eventpoll的函数，他内部包含两个东西\n\n一个是：\n\n1、红黑树-> 记录的事要监听的fd\n\n2、一个是链表->一个链表，记录的是就绪的fd\n\n紧接着调用epoll_ctl操作，将要监听的数据添加到红黑树上去，并且给每个fd设置一个监听函数，这个函数会在fd数据就绪时触发，就是准备好了，现在就把fd把数据添加到list_head中去\n\n3、调用epoll_wait函数\n\n就去等待，在用户态创建一个空的events数组，当就绪之后，我们的回调函数会把数据添加到list_head中去，当调用这个函数的时候，会去检查list_head，当然这个过程需要参考配置的等待时间，可以等一定时间，也可以一直等， 如果在此过程中，检查到了list_head中有数据会将数据添加到链表中，此时将数据放入到events数组中，并且返回对应的操作的数量，用户态的此时收到响应后，从events中拿到对应准备好的数据的节点，再去调用方法去拿数据。\n\n小总结：\n\nselect模式存在的三个问题：\n\n * 能监听的fd最大不超过1024\n * 每次select都需要把所有要监听的fd都拷贝到内核空间\n * 每次都要遍历所有fd来判断就绪状态\n\npoll模式的问题：\n\n * poll利用链表解决了select中监听fd上限的问题，但依然要遍历所有fd，如果监听较多，性能会下降\n\nepoll模式中如何解决这些问题的？\n\n * 基于epoll实例中的红黑树保存要监听的fd，理论上无上限，而且增删改查效率都非常高\n * 每个fd只需要执行一次epoll_ctl添加到红黑树，以后每次epol_wait无需传递任何参数，无需重复拷贝fd到内核空间\n * 利用ep_poll_callback机制来监听fd状态，无需遍历所有fd，因此性能不会随监听的fd数量增多而下降\n\n\n# 网络模型-epoll中的et和lt\n\n当fd有数据可读时，我们调用epoll_wait（或者select、poll）可以得到通知。但是事件通知的模式有两种：\n\n * leveltriggered：简称lt，也叫做水平触发。只要某个fd中有数据可读，每次调用epoll_wait都会得到通知。\n * edgetriggered：简称et，也叫做边沿触发。只有在某个fd有状态变化时，调用epoll_wait才会被通知。\n\n举个栗子：\n\n * 假设一个客户端socket对应的fd已经注册到了epoll实例中\n * 客户端socket发送了2kb的数据\n * 服务端调用epoll_wait，得到通知说fd就绪\n * 服务端从fd读取了1kb数据回到步骤3（再次调用epoll_wait，形成循环）\n\n结论\n\n如果我们采用lt模式，因为fd中仍有1kb数据，则第⑤步依然会返回结果，并且得到通知 如果我们采用et模式，因为第③步已经消费了fd可读事件，第⑤步fd状态没有变化，因此epoll_wait不会返回，数据无法读取，客户端响应超时。\n\n\n# 网络模型-基于epoll的服务器端流程\n\n我们来梳理一下这张图\n\n服务器启动以后，服务端会去调用epoll_create，创建一个epoll实例，epoll实例中包含两个数据\n\n1、红黑树（为空）：rb_root 用来去记录需要被监听的fd\n\n2、链表（为空）：list_head，用来存放已经就绪的fd\n\n创建好了之后，会去调用epoll_ctl函数，此函数会会将需要监听的数据添加到rb_root中去，并且对当前这些存在于红黑树的节点设置回调函数，当这些被监听的数据一旦准备完成，就会被调用，而调用的结果就是将红黑树的fd添加到list_head中去(但是此时并没有完成)\n\n3、当第二步完成后，就会调用epoll_wait函数，这个函数会去校验是否有数据准备完毕（因为数据一旦准备就绪，就会被回调函数添加到list_head中），在等待了一段时间后(可以进行配置)，如果等够了超时时间，则返回没有数据，如果有，则进一步判断当前是什么事件，如果是建立连接时间，则调用accept() 接受客户端socket，拿到建立连接的socket，然后建立起来连接，如果是其他事件，则把数据进行写出\n\n\n\n\n# 网络模型-信号驱动\n\n信号驱动io是与内核建立sigio的信号关联并设置回调，当内核有fd就绪时，会发出sigio信号通知用户，期间用户应用可以执行其它业务，无需阻塞等待。\n\n阶段一：\n\n * 用户进程调用sigaction，注册信号处理函数\n * 内核返回成功，开始监听fd\n * 用户进程不阻塞等待，可以执行其它业务\n * 当内核数据就绪后，回调用户进程的sigio处理函数\n\n阶段二：\n\n * 收到sigio回调信号\n * 调用recvfrom，读取\n * 内核将数据拷贝到用户空间\n * 用户进程处理数据\n\n\n\n当有大量io操作时，信号较多，sigio处理函数不能及时处理可能导致信号队列溢出，而且内核空间与用户空间的频繁信号交互性能也较低。\n\n\n# 异步io\n\n这种方式，不仅仅是用户态在试图读取数据后，不阻塞，而且当内核的数据准备完成后，也不会阻塞\n\n他会由内核将所有数据处理完成后，由内核将数据写入到用户态中，然后才算完成，所以性能极高，不会有任何阻塞，全部都由内核完成，可以看到，异步io模型中，用户进程在两个阶段都是非阻塞状态。\n\n\n\n\n# 对比\n\n最后用一幅图，来说明他们之间的区别\n\n\n\n\n# 网络模型-redis是单线程的吗？为什么使用单线程\n\nredis到底是单线程还是多线程？\n\n * 如果仅仅聊redis的核心业务部分（命令处理），答案是单线程\n * 如果是聊整个redis，那么答案就是多线程\n\n在redis版本迭代过程中，在两个重要的时间节点上引入了多线程的支持：\n\n * redis v4.0：引入多线程异步处理一些耗时较旧的任务，例如异步删除命令unlink\n * redis v6.0：在核心网络模型中引入 多线程，进一步提高对于多核cpu的利用率\n\n因此，对于redis的核心网络模型，在redis 6.0之前确实都是单线程。是利用epoll（linux系统）这样的io多路复用技术在事件循环中不断处理客户端情况。\n\n为什么redis要选择单线程？\n\n * 抛开持久化不谈，redis是纯 内存操作，执行速度非常快，它的性能瓶颈是网络延迟而不是执行速度，因此多线程并不会带来巨大的性能提升。\n * 多线程会导致过多的上下文切换，带来不必要的开销\n * 引入多线程会面临线程安全问题，必然要引入线程锁这样的安全手段，实现复杂度增高，而且性能也会大打折扣\n\n\n# redis的单线程模型-redis单线程和多线程网络模型变更\n\n\n\n当我们的客户端想要去连接我们服务器，会去先到io多路复用模型去进行排队，会有一个连接应答处理器，他会去接受读请求，然后又把读请求注册到具体模型中去，此时这些建立起来的连接，如果是客户端请求处理器去进行执行命令时，他会去把数据读取出来，然后把数据放入到client中， clinet去解析当前的命令转化为redis认识的命令，接下来就开始处理这些命令，从redis中的command中找到这些命令，然后就真正的去操作对应的数据了，当数据操作完成后，会去找到命令回复处理器，再由他将数据写出。",charsets:{cjk:!0}},{title:"MybatisPlus",frontmatter:{autoSort:96,title:"MybatisPlus",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/3c9c2a/",categories:["后端","SSM"],tags:["知识","SSM"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/40.SSM/20.MyBatisPlus.html",relativePath:"01.后端/40.SSM/20.MyBatisPlus.md",key:"v-0b5ac23a",path:"/pages/3c9c2a/",headers:[{level:2,title:"基本使用",slug:"基本使用",normalizedTitle:"基本使用",charIndex:2},{level:2,title:"标准开发",slug:"标准开发",normalizedTitle:"标准开发",charIndex:823},{level:3,title:"插入",slug:"插入",normalizedTitle:"插入",charIndex:832},{level:3,title:"删除",slug:"删除",normalizedTitle:"删除",charIndex:796},{level:3,title:"逻辑删除",slug:"逻辑删除",normalizedTitle:"逻辑删除",charIndex:1492},{level:3,title:"更新",slug:"更新",normalizedTitle:"更新",charIndex:785},{level:3,title:"乐观锁",slug:"乐观锁",normalizedTitle:"乐观锁",charIndex:2309},{level:3,title:"查询",slug:"查询",normalizedTitle:"查询",charIndex:772},{level:3,title:"分页查询",slug:"分页查询",normalizedTitle:"分页查询",charIndex:3591},{level:3,title:"条件查询",slug:"条件查询",normalizedTitle:"条件查询",charIndex:4531},{level:3,title:"查询投影",slug:"查询投影",normalizedTitle:"查询投影",charIndex:6794},{level:3,title:"查询条件",slug:"查询条件",normalizedTitle:"查询条件",charIndex:7752},{level:3,title:"设置映射",slug:"设置映射",normalizedTitle:"设置映射",charIndex:324},{level:3,title:"ID 自增策略",slug:"id-自增策略",normalizedTitle:"id 自增策略",charIndex:9343},{level:3,title:"全局配置",slug:"全局配置",normalizedTitle:"全局配置",charIndex:9548},{level:2,title:"实体类工具",slug:"实体类工具",normalizedTitle:"实体类工具",charIndex:10290},{level:2,title:"代码生成器",slug:"代码生成器",normalizedTitle:"代码生成器",charIndex:10776}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"基本使用 标准开发 插入 删除 逻辑删除 更新 乐观锁 查询 分页查询 条件查询 查询投影 查询条件 设置映射 ID 自增策略 全局配置 实体类工具 代码生成器",content:'# 基本使用\n\n * 导入依赖坐标\n   \n   \x3c!-- 引入 mybatis -plus --\x3e\n   <dependency>\n      <groupId>com.baomidou</groupId>\n      <artifactId>mybatis-plus-boot-starter</artifactId>\n      <version>3.5.1</version>\n   </dependency>\n   \n\n * 继承父类\n   \n   @Mapper\n   public interface UserMapper extends BaseMapper<User> {\n   }\n   \n   \n\n * 测试（修改方式见下方 设置映射）\n   \n   * 默认表名是 user——>实体类的小写\n   * 默认是下划线命名\n     * pojo——>private String phoneNum\n     * 数据库的列名——> 对应列名为phone_num\n   \n   @SpringBootTest\n   class MybatisplusApplicationTests {\n   \n      @Autowired\n      private UserMapper userMapper;\n      \n      @Test\n      void testAll() {\n         List<User> users = userMapper.selectList(null);\n         System.out.println(users);\n      }\n   }\n   \n\n * MyBatisPlus特性\n   \n   \n\n * CRUD\n   \n   * 增加(Create)、读取查询(Retrieve)、更新(Update)和删除(Delete)\n     \n     \n\n\n# 标准开发\n\n\n# 插入\n\n@Test  //插入\nvoid testSave() {\n   User user=new User();\n   user.setUsername("皎月女神");\n   user.setPassword("169384");\n   user.setEmail("123@123.com");\n   user.setPhoneNum("177");\n   //调用插入操作\n   userMapper.insert(user);\n}\n\n\n\n# 删除\n\n * 按id删除一个\n   \n   @Test  //删除\n   void testDelete() {\n       //调用删除操作\n       //    userMapper.deleteById("1519198458484707330");//字符串\n       userMapper.deleteById(1519198458484707330L); //long类型\n   }\n   \n\n * 删除一组\n   \n   @Test  // 多条删除\n   void testDeleteSome() {\n       List<Integer> list=new ArrayList<Integer>();\n       list.add(17);\n       list.add(18);\n       list.add(20);\n       userMapper.deleteBatchIds(list);\n   }\n   \n\n\n# 逻辑删除\n\n * 通过一个特殊字段的值来判断是否删除，并不真正删除数据 —— 设置为deleted\n   \n   * 千万不要设置成 delete,会影响sql语句\n\n * 设置为逻辑删除值 —— 一个是使用注解，一个是配置了全局设置\n   \n   //逻辑删除字段，标记当前记录是否被删除    \n   @TableLogic(value = "0",delval = "1") //value-0表示 没有被删除   delval-1 表示被删除\n   private int deleted;\n   \n   \n   db-config:\n     id-type: assign_id   #设置主键id 自增策略\n     table-prefix: tb_   #设置表的前缀\n     logic-delete-field: deleted   #设置代表逻辑删除的列名\n     logic-delete-value: 0     #不删除的数据  deleted为0\n     logic-not-delete-value: 1   # 删除的数据  deleted为1\n   \n\n * 删除语句的MP执行sql\n   \n   * UPDATE tb_user SET deleted=1 WHERE id=? AND deleted=0\n\n * 查询语句的MP执行sql\n   \n   * SELECT id,age,username,email,phone_num,deleted FROM tb_user WHERE id=? AND deleted=0\n\n\n# 更新\n\n * 方式1\n   \n   //更新方法1   带上要修改的id,带上版本号, 带上要修改的数据字段\n   User user=new User();\n   user.setId(2L);\n   user.setVersion(1); //乐观锁必带这个属性\n   \n   user.setUsername("阿波尼亚"); //要修改的字段\n   //调用更新操作  根据id修改值\n   userMapper.updateById(user);\n   \n\n * 方式2 —— 带乐观锁 更推荐第二种\n   \n   //更新方法2   根据要修改的id 查询出对象来,这样查询出来的数据一定带着乐观锁的版本号，就不用自己手动设置了\n   User user=userMapper.selectById(2L);\n   user.setUsername("阿波尼亚妈妈");\n   userMapper.updateById(user);\n   \n\n\n# 乐观锁\n\n * 通过一个特殊字段的值来实现小规模的并发访问控制—— version\n   \n   * UPDATE tb_user SET username=?, version=? WHERE id=? AND version=? AND deleted=1\n     \n     阿波尼亚(String), 2(Integer), 2(Long), 1(Integer)\n\n * 乐观锁拦截器\n   \n   //2.2添加乐观锁拦截器\n   interceptor.addInnerInterceptor(new OptimisticLockerInnerInterceptor());\n   \n\n * 多用户并发访问处理\n   \n   * 当用户1 修改这个对象时，version=4，成功修改，数据库乐观锁 version=5\n     \n     \n   \n   * 当用户2同时修改这个对象时，自身携带的version=4，但是数据库的乐观锁version=5，所以修改失败\n     \n     \n\n\n# 查询\n\n * 按id查询单个\n   \n   @Test //查询单个\n   void testOne() {\n      User user =userMapper.selectById(2);\n      System.out.println(user);\n   }\n   \n\n * 按ids查询一组\n   \n   @Test  // 多条查询\n   void testSelectSome() {\n       List<Integer> list=new ArrayList<Integer>();\n       list.add(11);\n       list.add(15);\n       list.add(16);\n       userMapper.selectBatchIds(list);\n   }\n   \n\n * 查询全部\n   \n   @Test //查询全部\n   void testAll() {\n      List<User> users = userMapper.selectList(null);\n      System.out.println(users);\n   }\n   \n\n\n# 分页查询\n\n * 方法使用\n   \n   @Test //分页查询  需要开启拦截器\n   void testGetPage() {\n      IPage page=new Page(2,2);//当前页，每页条数\n      userMapper.selectPage(page,null);\n      System.out.println("当前页码值:"+page.getCurrent());\n      System.out.println("每页显示数:"+page.getSize());\n      System.out.println("一共多少页:"+page.getPages());\n      System.out.println("一共多少条数据:"+page.getTotal());\n      System.out.println("查询数据:"+page.getRecords());\n   \n   }\n   \n\n * 拦截器配置\n   \n   @Configuration\n   public class MpConfig {\n   \n       @Bean\n       public MybatisPlusInterceptor pageInterceptor(){\n           //1.定义Mp拦截器\n           MybatisPlusInterceptor interceptor=new MybatisPlusInterceptor();\n           //2.添加分页拦截器\n           interceptor.addInnerInterceptor(new PaginationInnerInterceptor());\n           return interceptor;\n       }\n   }\n   \n\n * Mp 日志开启\n   \n   #开启mp日志\n   mybatis-plus:\n     configuration:\n       log-impl: org.apache.ibatis.logging.stdout.StdOutImpl\n   \n\n\n# 条件查询\n\n * 方式1\n   \n   @Test//方式1  按条件查询\n   void testGetAll() {\n       QueryWrapper qw =new QueryWrapper();\n       qw.lt("id","12"); //小于\n       qw.gt("id","5"); //大于\n       List<User> users = userMapper.selectList(qw); //大于5，小于12\n       System.out.println(users);\n   }\n   \n\n * 方式2\n   \n   @Test//方式2  lambda 格式按条件查询\n   void testGetAll2() {\n       QueryWrapper<User> qw =new QueryWrapper<User>();\n       qw.lambda().lt(User::getId,12); //小于\n       qw.lambda().gt(User::getId,5); //大于\n       List<User> users = userMapper.selectList(qw); //大于5，小于12\n       System.out.println(users);\n   }\n   \n\n * 方式3 ——>推荐\n   \n   @Test//方式3  lambda 格式按条件查询   推荐使用\n       void testGetAll3() {\n           LambdaQueryWrapper<User> lqw=new LambdaQueryWrapper<User>();\n           //一般多条件查询\n   //        lqw.lt(User::getId,12); //小于\n   //        lqw.gt(User::getId,5); //大于\n   \n           //链式编程 多条件查询  并且   and   //大于5且小于12\n   //        lqw.lt(User::getId,12).gt(User::getId,5);\n   \n           //链式编程 多条件查询  或者  or  //大于12或者小于5\n           lqw.lt(User::getId,5).or().gt(User::getId,12);\n   \n           List<User> users = userMapper.selectList(lqw);\n           System.out.println(users);\n       }\n   \n\n * 链式编程法则\n   \n   * 书写 ——>and\n     \n     //链式编程 多条件查询  并且   and   //大于5且小于12\n     lqw.lt(User::getId,12).gt(User::getId,5);\n     \n   \n   * 书写 ——>or\n     \n     //链式编程 多条件查询  或者  or  //大于12或者小于5\n     lqw.lt(User::getId,5).or().gt(User::getId,12);\n     \n\n * 条件查询null值判断\n   \n   * int与Integer的区别\n     \n     * 详细区别\n     * 简单说明\n       * int,是基本数据类型，默认值为0，可以直接使用；不赋值，默认就是0\n       * Integer ,是int的封装类，是一个对象，默认值是null，需要实例化才能使用，实例化就是赋值，不赋值就是null。\n   \n   * 使用if判断条件\n     \n     //NULL 判定\n     LambdaQueryWrapper<User> lqw=new LambdaQueryWrapper<User>();\n     lqw.lt(User::getAge,uq.getAge2());\n     if(uq.getAge() !=null){\n         lqw.gt(User::getAge,uq.getAge());\n     }\n     \n   \n   * 使用方法内置条件\n     \n     链式编程，长度不要超过一行，打回车即可，点好尽量对齐\n     \n     //NULL 判定\n     LambdaQueryWrapper<User> lqw=new LambdaQueryWrapper<User>();\n     \n     //多个条件\n     //lqw.lt(uq.getAge2()!=null,User::getAge,uq.getAge2());\n     //lqw.gt(uq.getAge() !=null,User::getAge,uq.getAge());\n     \n     //链式\n     lqw.lt(uq.getAge2()!=null,User::getAge,uq.getAge2())\n         .gt(uq.getAge() !=null,User::getAge,uq.getAge());\n     \n\n\n# 查询投影\n\n * 设置查询字段 QueryWrapper与LambdaQueryWrapper都可以使用\n   \n   @Test//查询投影----设置查询字段\n   void testSelect1() {\n       QueryWrapper<User> qw =new QueryWrapper<User>();\n       qw.select("id","username","age");  //只查询这三个列，其他设为空\n   \n       LambdaQueryWrapper<User> lqw=new LambdaQueryWrapper<User>();\n       lqw.select(User::getId,User::getAge,User::getUsername);\n   \n       //查询全部数据，限制列数\n       List<User> users = userMapper.selectList(lqw);\n       System.out.println(users);\n   }\n   \n\n * 查询个数 只能用QueryWrapper\n   \n   @Test//查询投影----查询个数  只能用 QueryWrapper\n       void testSelect2() {\n           QueryWrapper<User> qw =new QueryWrapper<User>();\n           //查询全部\n   //        qw.select("count(*) as count");\n   \n           //分组查询\n           qw.select("count(*) as count,password");\n           qw.groupBy("password");\n   \n   \n           //查询个数\n           List<Map<String, Object>> maps = userMapper.selectMaps(qw);\n           System.out.println(maps);\n       }\n   \n\n\n# 查询条件\n\n * 等匹配\n   \n   @Test//查询条件———— 等匹配\n   void testSelectCon1() {\n       LambdaQueryWrapper<User> lqw=new LambdaQueryWrapper<User>();\n       //等同于\n       lqw.eq(User::getUsername,"diana").eq(User::getPassword,"456"); //md5加密\n       User user = userMapper.selectOne(lqw);\n       System.out.println(user);\n   }\n   \n\n * 范围查询\n   \n   @Test//查询条件———— 范围查询\n   void testSelectCon2() {\n       LambdaQueryWrapper<User> lqw=new LambdaQueryWrapper<User>();\n       //范围查询  lt,gt;  le,ge; between\n       lqw.between(User::getAge,10,20); //between  两边都包括\n       List<User> users = userMapper.selectList(lqw);\n       System.out.println(users);\n   }\n   \n\n * 模糊匹配\n   \n   @Test//查询条件———— 模糊匹配\n   void testSelectCon3() {\n       LambdaQueryWrapper<User> lqw=new LambdaQueryWrapper<User>();\n       //模糊匹配  like  likeLeft    likeRight\n       lqw.likeRight(User::getUsername,"d");\n       List<User> users = userMapper.selectList(lqw);\n       System.out.println(users);\n   }\n   \n\n * API 查询\n\n\n# 设置映射\n\n * 设置表名映射\n   \n   * 默认表名是 实体类的首字母小写，即user,设置后为sys_user\n   \n   @TableName("sys_user")\n   public class User {}\n   \n\n * 设置列名映射\n   \n   * 数据库列名为 psd，实体类属性名为password, 使用value属性设置映射关系\n   \n   @TableField(value = "psd",select = false) //设置当前属性对应的数据库表中的字段关系  //select 属性 设置是否参与查询\n   private  String password;\n   \n\n * 设置是否参与查询\n   \n   * 使用select 属性 设置是否参与查询\n   \n   @TableField(value = "psd",select = false) //设置当前属性对应的数据库表中的字段关系  //select 属性 设置是否参与查询\n   private  String password;\n   \n\n * 设置存在属性\n   \n   * 一些属性，数据库中没有，但是实体类中有，要用 exist属性去声明下\n   \n   @TableField(exist = false)//数据库中不存在，但是我实体类中定义了\n   private Integer online;\n   \n\n\n# ID 自增策略\n\n//    @TableId(type= IdType.AUTO) //按照数据库规则自动递增， +1\n//    @TableId(type= IdType.INPUT) //要求用户自己输入id\n        @TableId(type= IdType.ASSIGN_ID) //使用雪花算法，生成Long类型id  超长id\n    private  Long id;\n\n\n\n\n\n# 全局配置\n\n * 设置了全局配置以后，就不用在对应的java代码上使用注解了\n\n * 表的前缀，数据库表去掉前缀后，若剩余表名与数据库实体类名能对应上，就不需要使用@TableName\n\ndb-config:\n    id-type: assign_id   #设置主键id 自增策略\n    table-prefix: tb_   #设置表的前缀\n    logic-delete-field: deleted   #设置代表逻辑删除的列名\n    logic-delete-value: 0     #不删除的数据  deleted为0\n    logic-not-delete-value: 1   # 删除的数据  deleted为1\n\n\n#开启mp日志\nmybatis-plus:\n  configuration:\n    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl  # sql查询语句及数据\n  global-config:\n    banner: false    #禁止输出mybatisPlus的logo\n    db-config:\n      id-type: assign_id   #设置主键id 自增策略\n      table-prefix: tb_   #设置表的前缀\n      logic-delete-field: deleted   #设置代表逻辑删除的列名\n      logic-delete-value: 0     #不删除的数据  deleted为0\n      logic-not-delete-value: 1   # 删除的数据  deleted为1\n\n\n\n# 实体类工具\n\n * 导入依赖\n   \n   \x3c!-- 快速开发实体类--\x3e\n   <dependency>\n      <groupId>org.projectlombok</groupId>\n      <artifactId>lombok</artifactId>\n      <scope>provided</scope>\n   </dependency>\n   \n\n * 使用方法\n   \n   通过注解方式，来生成对应的方法，简化代码的书写\n   \n   * @Data\n     \n     //@Setter\n     //@Getter\n     //@ToString\n     //@EqualsAndHashCode //equal方法和hashcode方法\n     \n     \n     @Data  //等于上面一堆   不包含构造方法\n     \n   \n   * 构造方法\n     \n     @NoArgsConstructor   //无参构造\n     @AllArgsConstructor  //全参构造\n     \n\n\n# 代码生成器\n\n * 导入依赖\n   \n   \x3c!-- 引入 代码生成器--\x3e\n   <dependency>\n       <groupId>com.baomidou</groupId>\n       <artifactId>mybatis-plus-generator</artifactId>\n       <version>3.4.0</version>\n   </dependency>\n   \n   \x3c!-- 引入模板引擎--\x3e\n   <dependency>\n       <groupId>org.apache.velocity</groupId>\n       <artifactId>velocity-engine-core</artifactId>\n       <version>2.2</version>\n   </dependency>\n   \n\n * 代码生成器 步骤\n   \n   //1.生成代码生成器\n   AutoGenerator autoGenerator=new AutoGenerator();\n   //2.配置代码生成器\n   ………………………………………………\n   //3.执行代码生成器\n   autoGenerator.execute();\n   \n   \n   * 设置数据源\n     \n     //2.1配置数据源\n     DataSourceConfig dataSource=new DataSourceConfig();\n     dataSource.setDriverName("com.mysql.cj.jdbc.Driver");\n     dataSource.setUsername("root");\n     dataSource.setPassword("1234");\n     dataSource.setUrl("jdbc:mysql:///test?useSSL=false&useServerPrepStmts=true");\n     autoGenerator.setDataSource(dataSource);\n     \n   \n   * 设置全局配置\n     \n     //2.2设置全局配置\n     GlobalConfig globalConfig = new GlobalConfig();\n     globalConfig.setOutputDir(System.getProperty("user.dir")+"/codeAuto/src/main/java");    //设置代码生成位置\n     globalConfig.setOpen(false);    //设置生成完毕后是否打开生成代码所在的目录\n     globalConfig.setAuthor("diana");    //设置作者\n     globalConfig.setFileOverride(true);     //设置是否覆盖原始生成的文件\n     globalConfig.setMapperName("%sMapper");    //设置数据层接口名，%s为占位符，指代模块名称   不加修改指表名\n     globalConfig.setIdType(IdType.ASSIGN_ID);   //设置Id生成策略\n     autoGenerator.setGlobalConfig(globalConfig);\n     \n   \n   * 设置包名相关配置\n     \n     //2.3设置包名相关配置\n     PackageConfig packageInfo = new PackageConfig();\n     packageInfo.setParent("com.liangbing");   //设置生成的包名，与代码所在位置不冲突，二者叠加组成完整路径\n     packageInfo.setEntity("pojo");    //设置实体类包名\n     packageInfo.setMapper("mapper");   //设置数据层包名\n     autoGenerator.setPackageInfo(packageInfo);\n     \n   \n   * 策略设置\n     \n     //2.4策略设置\n     StrategyConfig strategyConfig = new StrategyConfig();\n     strategyConfig.setInclude("tb_user");  //设置当前参与生成的表名，参数为可变参数\n     strategyConfig.setTablePrefix("tb_");  //设置数据库表的前缀名称，模块名 = 数据库表名 - 前缀名  例如： User = tbl_user - tbl_\n     strategyConfig.setRestControllerStyle(true);    //设置是否启用Rest风格\n     strategyConfig.setVersionFieldName("version");  //设置乐观锁字段名\n     strategyConfig.setLogicDeleteFieldName("deleted");  //设置逻辑删除字段名\n     strategyConfig.setEntityLombokModel(true);  //设置是否启用lombok\n     autoGenerator.setStrategy(strategyConfig);\n     \n\n * 生成的service 接口名称\n   \n   * IUserService 使用I开头',normalizedContent:'# 基本使用\n\n * 导入依赖坐标\n   \n   \x3c!-- 引入 mybatis -plus --\x3e\n   <dependency>\n      <groupid>com.baomidou</groupid>\n      <artifactid>mybatis-plus-boot-starter</artifactid>\n      <version>3.5.1</version>\n   </dependency>\n   \n\n * 继承父类\n   \n   @mapper\n   public interface usermapper extends basemapper<user> {\n   }\n   \n   \n\n * 测试（修改方式见下方 设置映射）\n   \n   * 默认表名是 user——>实体类的小写\n   * 默认是下划线命名\n     * pojo——>private string phonenum\n     * 数据库的列名——> 对应列名为phone_num\n   \n   @springboottest\n   class mybatisplusapplicationtests {\n   \n      @autowired\n      private usermapper usermapper;\n      \n      @test\n      void testall() {\n         list<user> users = usermapper.selectlist(null);\n         system.out.println(users);\n      }\n   }\n   \n\n * mybatisplus特性\n   \n   \n\n * crud\n   \n   * 增加(create)、读取查询(retrieve)、更新(update)和删除(delete)\n     \n     \n\n\n# 标准开发\n\n\n# 插入\n\n@test  //插入\nvoid testsave() {\n   user user=new user();\n   user.setusername("皎月女神");\n   user.setpassword("169384");\n   user.setemail("123@123.com");\n   user.setphonenum("177");\n   //调用插入操作\n   usermapper.insert(user);\n}\n\n\n\n# 删除\n\n * 按id删除一个\n   \n   @test  //删除\n   void testdelete() {\n       //调用删除操作\n       //    usermapper.deletebyid("1519198458484707330");//字符串\n       usermapper.deletebyid(1519198458484707330l); //long类型\n   }\n   \n\n * 删除一组\n   \n   @test  // 多条删除\n   void testdeletesome() {\n       list<integer> list=new arraylist<integer>();\n       list.add(17);\n       list.add(18);\n       list.add(20);\n       usermapper.deletebatchids(list);\n   }\n   \n\n\n# 逻辑删除\n\n * 通过一个特殊字段的值来判断是否删除，并不真正删除数据 —— 设置为deleted\n   \n   * 千万不要设置成 delete,会影响sql语句\n\n * 设置为逻辑删除值 —— 一个是使用注解，一个是配置了全局设置\n   \n   //逻辑删除字段，标记当前记录是否被删除    \n   @tablelogic(value = "0",delval = "1") //value-0表示 没有被删除   delval-1 表示被删除\n   private int deleted;\n   \n   \n   db-config:\n     id-type: assign_id   #设置主键id 自增策略\n     table-prefix: tb_   #设置表的前缀\n     logic-delete-field: deleted   #设置代表逻辑删除的列名\n     logic-delete-value: 0     #不删除的数据  deleted为0\n     logic-not-delete-value: 1   # 删除的数据  deleted为1\n   \n\n * 删除语句的mp执行sql\n   \n   * update tb_user set deleted=1 where id=? and deleted=0\n\n * 查询语句的mp执行sql\n   \n   * select id,age,username,email,phone_num,deleted from tb_user where id=? and deleted=0\n\n\n# 更新\n\n * 方式1\n   \n   //更新方法1   带上要修改的id,带上版本号, 带上要修改的数据字段\n   user user=new user();\n   user.setid(2l);\n   user.setversion(1); //乐观锁必带这个属性\n   \n   user.setusername("阿波尼亚"); //要修改的字段\n   //调用更新操作  根据id修改值\n   usermapper.updatebyid(user);\n   \n\n * 方式2 —— 带乐观锁 更推荐第二种\n   \n   //更新方法2   根据要修改的id 查询出对象来,这样查询出来的数据一定带着乐观锁的版本号，就不用自己手动设置了\n   user user=usermapper.selectbyid(2l);\n   user.setusername("阿波尼亚妈妈");\n   usermapper.updatebyid(user);\n   \n\n\n# 乐观锁\n\n * 通过一个特殊字段的值来实现小规模的并发访问控制—— version\n   \n   * update tb_user set username=?, version=? where id=? and version=? and deleted=1\n     \n     阿波尼亚(string), 2(integer), 2(long), 1(integer)\n\n * 乐观锁拦截器\n   \n   //2.2添加乐观锁拦截器\n   interceptor.addinnerinterceptor(new optimisticlockerinnerinterceptor());\n   \n\n * 多用户并发访问处理\n   \n   * 当用户1 修改这个对象时，version=4，成功修改，数据库乐观锁 version=5\n     \n     \n   \n   * 当用户2同时修改这个对象时，自身携带的version=4，但是数据库的乐观锁version=5，所以修改失败\n     \n     \n\n\n# 查询\n\n * 按id查询单个\n   \n   @test //查询单个\n   void testone() {\n      user user =usermapper.selectbyid(2);\n      system.out.println(user);\n   }\n   \n\n * 按ids查询一组\n   \n   @test  // 多条查询\n   void testselectsome() {\n       list<integer> list=new arraylist<integer>();\n       list.add(11);\n       list.add(15);\n       list.add(16);\n       usermapper.selectbatchids(list);\n   }\n   \n\n * 查询全部\n   \n   @test //查询全部\n   void testall() {\n      list<user> users = usermapper.selectlist(null);\n      system.out.println(users);\n   }\n   \n\n\n# 分页查询\n\n * 方法使用\n   \n   @test //分页查询  需要开启拦截器\n   void testgetpage() {\n      ipage page=new page(2,2);//当前页，每页条数\n      usermapper.selectpage(page,null);\n      system.out.println("当前页码值:"+page.getcurrent());\n      system.out.println("每页显示数:"+page.getsize());\n      system.out.println("一共多少页:"+page.getpages());\n      system.out.println("一共多少条数据:"+page.gettotal());\n      system.out.println("查询数据:"+page.getrecords());\n   \n   }\n   \n\n * 拦截器配置\n   \n   @configuration\n   public class mpconfig {\n   \n       @bean\n       public mybatisplusinterceptor pageinterceptor(){\n           //1.定义mp拦截器\n           mybatisplusinterceptor interceptor=new mybatisplusinterceptor();\n           //2.添加分页拦截器\n           interceptor.addinnerinterceptor(new paginationinnerinterceptor());\n           return interceptor;\n       }\n   }\n   \n\n * mp 日志开启\n   \n   #开启mp日志\n   mybatis-plus:\n     configuration:\n       log-impl: org.apache.ibatis.logging.stdout.stdoutimpl\n   \n\n\n# 条件查询\n\n * 方式1\n   \n   @test//方式1  按条件查询\n   void testgetall() {\n       querywrapper qw =new querywrapper();\n       qw.lt("id","12"); //小于\n       qw.gt("id","5"); //大于\n       list<user> users = usermapper.selectlist(qw); //大于5，小于12\n       system.out.println(users);\n   }\n   \n\n * 方式2\n   \n   @test//方式2  lambda 格式按条件查询\n   void testgetall2() {\n       querywrapper<user> qw =new querywrapper<user>();\n       qw.lambda().lt(user::getid,12); //小于\n       qw.lambda().gt(user::getid,5); //大于\n       list<user> users = usermapper.selectlist(qw); //大于5，小于12\n       system.out.println(users);\n   }\n   \n\n * 方式3 ——>推荐\n   \n   @test//方式3  lambda 格式按条件查询   推荐使用\n       void testgetall3() {\n           lambdaquerywrapper<user> lqw=new lambdaquerywrapper<user>();\n           //一般多条件查询\n   //        lqw.lt(user::getid,12); //小于\n   //        lqw.gt(user::getid,5); //大于\n   \n           //链式编程 多条件查询  并且   and   //大于5且小于12\n   //        lqw.lt(user::getid,12).gt(user::getid,5);\n   \n           //链式编程 多条件查询  或者  or  //大于12或者小于5\n           lqw.lt(user::getid,5).or().gt(user::getid,12);\n   \n           list<user> users = usermapper.selectlist(lqw);\n           system.out.println(users);\n       }\n   \n\n * 链式编程法则\n   \n   * 书写 ——>and\n     \n     //链式编程 多条件查询  并且   and   //大于5且小于12\n     lqw.lt(user::getid,12).gt(user::getid,5);\n     \n   \n   * 书写 ——>or\n     \n     //链式编程 多条件查询  或者  or  //大于12或者小于5\n     lqw.lt(user::getid,5).or().gt(user::getid,12);\n     \n\n * 条件查询null值判断\n   \n   * int与integer的区别\n     \n     * 详细区别\n     * 简单说明\n       * int,是基本数据类型，默认值为0，可以直接使用；不赋值，默认就是0\n       * integer ,是int的封装类，是一个对象，默认值是null，需要实例化才能使用，实例化就是赋值，不赋值就是null。\n   \n   * 使用if判断条件\n     \n     //null 判定\n     lambdaquerywrapper<user> lqw=new lambdaquerywrapper<user>();\n     lqw.lt(user::getage,uq.getage2());\n     if(uq.getage() !=null){\n         lqw.gt(user::getage,uq.getage());\n     }\n     \n   \n   * 使用方法内置条件\n     \n     链式编程，长度不要超过一行，打回车即可，点好尽量对齐\n     \n     //null 判定\n     lambdaquerywrapper<user> lqw=new lambdaquerywrapper<user>();\n     \n     //多个条件\n     //lqw.lt(uq.getage2()!=null,user::getage,uq.getage2());\n     //lqw.gt(uq.getage() !=null,user::getage,uq.getage());\n     \n     //链式\n     lqw.lt(uq.getage2()!=null,user::getage,uq.getage2())\n         .gt(uq.getage() !=null,user::getage,uq.getage());\n     \n\n\n# 查询投影\n\n * 设置查询字段 querywrapper与lambdaquerywrapper都可以使用\n   \n   @test//查询投影----设置查询字段\n   void testselect1() {\n       querywrapper<user> qw =new querywrapper<user>();\n       qw.select("id","username","age");  //只查询这三个列，其他设为空\n   \n       lambdaquerywrapper<user> lqw=new lambdaquerywrapper<user>();\n       lqw.select(user::getid,user::getage,user::getusername);\n   \n       //查询全部数据，限制列数\n       list<user> users = usermapper.selectlist(lqw);\n       system.out.println(users);\n   }\n   \n\n * 查询个数 只能用querywrapper\n   \n   @test//查询投影----查询个数  只能用 querywrapper\n       void testselect2() {\n           querywrapper<user> qw =new querywrapper<user>();\n           //查询全部\n   //        qw.select("count(*) as count");\n   \n           //分组查询\n           qw.select("count(*) as count,password");\n           qw.groupby("password");\n   \n   \n           //查询个数\n           list<map<string, object>> maps = usermapper.selectmaps(qw);\n           system.out.println(maps);\n       }\n   \n\n\n# 查询条件\n\n * 等匹配\n   \n   @test//查询条件———— 等匹配\n   void testselectcon1() {\n       lambdaquerywrapper<user> lqw=new lambdaquerywrapper<user>();\n       //等同于\n       lqw.eq(user::getusername,"diana").eq(user::getpassword,"456"); //md5加密\n       user user = usermapper.selectone(lqw);\n       system.out.println(user);\n   }\n   \n\n * 范围查询\n   \n   @test//查询条件———— 范围查询\n   void testselectcon2() {\n       lambdaquerywrapper<user> lqw=new lambdaquerywrapper<user>();\n       //范围查询  lt,gt;  le,ge; between\n       lqw.between(user::getage,10,20); //between  两边都包括\n       list<user> users = usermapper.selectlist(lqw);\n       system.out.println(users);\n   }\n   \n\n * 模糊匹配\n   \n   @test//查询条件———— 模糊匹配\n   void testselectcon3() {\n       lambdaquerywrapper<user> lqw=new lambdaquerywrapper<user>();\n       //模糊匹配  like  likeleft    likeright\n       lqw.likeright(user::getusername,"d");\n       list<user> users = usermapper.selectlist(lqw);\n       system.out.println(users);\n   }\n   \n\n * api 查询\n\n\n# 设置映射\n\n * 设置表名映射\n   \n   * 默认表名是 实体类的首字母小写，即user,设置后为sys_user\n   \n   @tablename("sys_user")\n   public class user {}\n   \n\n * 设置列名映射\n   \n   * 数据库列名为 psd，实体类属性名为password, 使用value属性设置映射关系\n   \n   @tablefield(value = "psd",select = false) //设置当前属性对应的数据库表中的字段关系  //select 属性 设置是否参与查询\n   private  string password;\n   \n\n * 设置是否参与查询\n   \n   * 使用select 属性 设置是否参与查询\n   \n   @tablefield(value = "psd",select = false) //设置当前属性对应的数据库表中的字段关系  //select 属性 设置是否参与查询\n   private  string password;\n   \n\n * 设置存在属性\n   \n   * 一些属性，数据库中没有，但是实体类中有，要用 exist属性去声明下\n   \n   @tablefield(exist = false)//数据库中不存在，但是我实体类中定义了\n   private integer online;\n   \n\n\n# id 自增策略\n\n//    @tableid(type= idtype.auto) //按照数据库规则自动递增， +1\n//    @tableid(type= idtype.input) //要求用户自己输入id\n        @tableid(type= idtype.assign_id) //使用雪花算法，生成long类型id  超长id\n    private  long id;\n\n\n\n\n\n# 全局配置\n\n * 设置了全局配置以后，就不用在对应的java代码上使用注解了\n\n * 表的前缀，数据库表去掉前缀后，若剩余表名与数据库实体类名能对应上，就不需要使用@tablename\n\ndb-config:\n    id-type: assign_id   #设置主键id 自增策略\n    table-prefix: tb_   #设置表的前缀\n    logic-delete-field: deleted   #设置代表逻辑删除的列名\n    logic-delete-value: 0     #不删除的数据  deleted为0\n    logic-not-delete-value: 1   # 删除的数据  deleted为1\n\n\n#开启mp日志\nmybatis-plus:\n  configuration:\n    log-impl: org.apache.ibatis.logging.stdout.stdoutimpl  # sql查询语句及数据\n  global-config:\n    banner: false    #禁止输出mybatisplus的logo\n    db-config:\n      id-type: assign_id   #设置主键id 自增策略\n      table-prefix: tb_   #设置表的前缀\n      logic-delete-field: deleted   #设置代表逻辑删除的列名\n      logic-delete-value: 0     #不删除的数据  deleted为0\n      logic-not-delete-value: 1   # 删除的数据  deleted为1\n\n\n\n# 实体类工具\n\n * 导入依赖\n   \n   \x3c!-- 快速开发实体类--\x3e\n   <dependency>\n      <groupid>org.projectlombok</groupid>\n      <artifactid>lombok</artifactid>\n      <scope>provided</scope>\n   </dependency>\n   \n\n * 使用方法\n   \n   通过注解方式，来生成对应的方法，简化代码的书写\n   \n   * @data\n     \n     //@setter\n     //@getter\n     //@tostring\n     //@equalsandhashcode //equal方法和hashcode方法\n     \n     \n     @data  //等于上面一堆   不包含构造方法\n     \n   \n   * 构造方法\n     \n     @noargsconstructor   //无参构造\n     @allargsconstructor  //全参构造\n     \n\n\n# 代码生成器\n\n * 导入依赖\n   \n   \x3c!-- 引入 代码生成器--\x3e\n   <dependency>\n       <groupid>com.baomidou</groupid>\n       <artifactid>mybatis-plus-generator</artifactid>\n       <version>3.4.0</version>\n   </dependency>\n   \n   \x3c!-- 引入模板引擎--\x3e\n   <dependency>\n       <groupid>org.apache.velocity</groupid>\n       <artifactid>velocity-engine-core</artifactid>\n       <version>2.2</version>\n   </dependency>\n   \n\n * 代码生成器 步骤\n   \n   //1.生成代码生成器\n   autogenerator autogenerator=new autogenerator();\n   //2.配置代码生成器\n   ………………………………………………\n   //3.执行代码生成器\n   autogenerator.execute();\n   \n   \n   * 设置数据源\n     \n     //2.1配置数据源\n     datasourceconfig datasource=new datasourceconfig();\n     datasource.setdrivername("com.mysql.cj.jdbc.driver");\n     datasource.setusername("root");\n     datasource.setpassword("1234");\n     datasource.seturl("jdbc:mysql:///test?usessl=false&useserverprepstmts=true");\n     autogenerator.setdatasource(datasource);\n     \n   \n   * 设置全局配置\n     \n     //2.2设置全局配置\n     globalconfig globalconfig = new globalconfig();\n     globalconfig.setoutputdir(system.getproperty("user.dir")+"/codeauto/src/main/java");    //设置代码生成位置\n     globalconfig.setopen(false);    //设置生成完毕后是否打开生成代码所在的目录\n     globalconfig.setauthor("diana");    //设置作者\n     globalconfig.setfileoverride(true);     //设置是否覆盖原始生成的文件\n     globalconfig.setmappername("%smapper");    //设置数据层接口名，%s为占位符，指代模块名称   不加修改指表名\n     globalconfig.setidtype(idtype.assign_id);   //设置id生成策略\n     autogenerator.setglobalconfig(globalconfig);\n     \n   \n   * 设置包名相关配置\n     \n     //2.3设置包名相关配置\n     packageconfig packageinfo = new packageconfig();\n     packageinfo.setparent("com.liangbing");   //设置生成的包名，与代码所在位置不冲突，二者叠加组成完整路径\n     packageinfo.setentity("pojo");    //设置实体类包名\n     packageinfo.setmapper("mapper");   //设置数据层包名\n     autogenerator.setpackageinfo(packageinfo);\n     \n   \n   * 策略设置\n     \n     //2.4策略设置\n     strategyconfig strategyconfig = new strategyconfig();\n     strategyconfig.setinclude("tb_user");  //设置当前参与生成的表名，参数为可变参数\n     strategyconfig.settableprefix("tb_");  //设置数据库表的前缀名称，模块名 = 数据库表名 - 前缀名  例如： user = tbl_user - tbl_\n     strategyconfig.setrestcontrollerstyle(true);    //设置是否启用rest风格\n     strategyconfig.setversionfieldname("version");  //设置乐观锁字段名\n     strategyconfig.setlogicdeletefieldname("deleted");  //设置逻辑删除字段名\n     strategyconfig.setentitylombokmodel(true);  //设置是否启用lombok\n     autogenerator.setstrategy(strategyconfig);\n     \n\n * 生成的service 接口名称\n   \n   * iuserservice 使用i开头',charsets:{cjk:!0}},{title:"SpringMVC-注解开发",frontmatter:{autoSort:98,title:"SpringMVC-注解开发",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/ee4b7e/",categories:["后端","SSM"],tags:["知识","SSM"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/40.SSM/15.SpringMVC-%E6%B3%A8%E8%A7%A3.html",relativePath:"01.后端/40.SSM/15.SpringMVC-注解.md",key:"v-3a6aa74c",path:"/pages/ee4b7e/",headers:[{level:2,title:"学习目标",slug:"学习目标",normalizedTitle:"学习目标",charIndex:29},{level:2,title:"配置转换",slug:"配置转换",normalizedTitle:"配置转换",charIndex:228},{level:3,title:"原始xml文件",slug:"原始xml文件",normalizedTitle:"原始xml文件",charIndex:237},{level:3,title:"配置类",slug:"配置类",normalizedTitle:"配置类",charIndex:444},{level:2,title:"执行流程",slug:"执行流程",normalizedTitle:"执行流程",charIndex:6512},{level:2,title:"REST风格",slug:"rest风格",normalizedTitle:"rest风格",charIndex:81},{level:3,title:"1.REST风格简介",slug:"_1-rest风格简介",normalizedTitle:"1.rest风格简介",charIndex:11243},{level:3,title:"2. 参数获取",slug:"_2-参数获取",normalizedTitle:"2. 参数获取",charIndex:11619},{level:3,title:"3.简化开发",slug:"_3-简化开发",normalizedTitle:"3.简化开发",charIndex:13168},{level:3,title:"4.案例",slug:"_4-案例",normalizedTitle:"4.案例",charIndex:14546},{level:2,title:"POSTMAN",slug:"postman",normalizedTitle:"postman",charIndex:15083}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"学习目标 配置转换 原始xml文件 配置类 执行流程 REST风格 1.REST风格简介 2. 参数获取 3.简化开发 4.案例 POSTMAN",content:'这里只提供一些原SpringMVC没有学到的功能\n\n\n# 学习目标\n\n 1. 掌握基于SpringMVC获取请求参数与响应json数据操作\n\n 2. 熟练应用基于REST风格的请求路径设置与参数传递\n\n 3. 能够根据实际业务建立前后端开发通信协议并进行实现\n\n 4. 基于SSM整合技术开发任意业务模板功能\n\n 5. 现阶段开发流程\n    \n    * Spring MVC负责实现controller 和json\n      \n      \n\n\n# 配置转换\n\n\n# 原始xml文件\n\n# spring-mvc.xml\n\n> \x3c!-- s1.Controller 组件扫描--\x3e\n> <context:component-scan base-package="com.diana.controller"/>\n> \n>    \n> \x3c!-- s2.配置内部资源视图解析器--\x3e                                            \x3c!-- 没有对应的配置类--\x3e\n> <bean id="viewResolver" class="org.springframework.web.servlet.view.InternalResourceViewResolver">\n>     \x3c!-- 设置默认前缀和后缀 --\x3e\n>     \x3c!--        <property name="prefix" value="/jsp/"/>--\x3e\n>     <property name="prefix" value=""/>\n>     <property name="suffix" value=".jsp"/>\n> </bean>\n> \n> \n> \n> \n> \x3c!-- s4.mvc的注解驱动--\x3e\n> <mvc:annotation-driven conversion-service="conversionService"/>\n> <mvc:annotation-driven/>\n> \n> \x3c!-- s5.1开放资源的访问 （一般是静态资源）--\x3e\n> \x3c!--    <mvc:resources mapping="/js/**" location="/js/"/>--\x3e\n> \x3c!--    <mvc:resources mapping="/img/**" location="/img/"/>--\x3e\n> \x3c!--    <mvc:resources mapping="/html/**" location="/html/"/>--\x3e\n> \x3c!-- s5.2使用原始tomcat  找静态资源  与上面虽然过程不一样，但是功能一样--\x3e\n> <mvc:default-servlet-handler/>\n> \n> \n> \x3c!-- s6.声明转换器--\x3e  \x3c!-- 然后在mvc的注解驱动中声明--\x3e                       \x3c!-- 没有对应的配置类--\x3e\n> <bean id="conversionService" class="org.springframework.context.support.ConversionServiceFactoryBean">\n>  <property name="converters">\n>      <list>\n>          <bean class="com.diana.converter.DateConverter"/>\n>      </list>\n>  </property>\n> </bean>\n> \n>  \n>    \x3c!-- s7.配置文件上传解析器--\x3e                                                 \x3c!-- 没有对应的配置类--\x3e\n>    <bean id="multipartResolver" class="org.springframework.web.multipart.commons.CommonsMultipartResolver">\n>     <property name="defaultEncoding" value="UTF-8"/>\n>     <property name="maxUploadSize" value="50000"/>  \x3c!-- 上传文件总大小--\x3e\n>     <property name="maxUploadSizePerFile" value="5000"/>  \x3c!-- 上传文件总大小--\x3e\n> </bean>\n> \n> \n> \x3c!-- s8.配置拦截器--\x3e\n> <mvc:interceptors>\n>     \x3c!--拦截器1--\x3e\n>     <mvc:interceptor>\n>         \x3c!--对哪些资源进行拦截--\x3e\n>      <mvc:mapping path="/**"/>\n>      <bean class="com.diana.interceptor.MyInterceptor1"/>\n>  </mvc:interceptor>\n>  \x3c!--拦截器2--\x3e\n>  <mvc:interceptor>\n>         \x3c!--对哪些资源进行拦截--\x3e\n>         <mvc:mapping path="/**"/>\n>         <bean class="com.diana.interceptor.MyInterceptor2"/>\n>     </mvc:interceptor>\n>    </mvc:interceptors>\n>    \n>    \n>    \x3c!-- s9.1配置简单映射异常处理器--\x3e\n>    <bean class="org.springframework.web.servlet.handler.SimpleMappingExceptionResolver">\n>     \x3c!-- 默认错误视图    value 是视图名称（自己随便起名）  省略了前后缀--\x3e\n>     <property name="defaultErrorView" value="/exception/error"/>\n>     \x3c!-- 异常错误映射  优先判断里面的异常类型，里面没有时 才会进入默认错误视图--\x3e\n>  <property name="exceptionMappings">\n>      <map>\n>          <entry key="com.diana.exception.MyException" value="/exception/Myerror"/>\x3c!-- 自定义异常--\x3e\n>          <entry key="java.lang.ClassCastException" value="/exception/error_class"/>\x3c!-- 类型转换异常--\x3e\n>      </map>\n>     </property>\n>    </bean>\n>    \x3c!-- s9.2配置自定义异常处理器--\x3e\n>    <bean class="com.diana.resolver.MyExceptionResolver"/>\n\n# web.xml\n\n> \x3c!-- w1.配置全局过滤的filter--\x3e\n> <filter>\n>   <filter-name>CharacterEncodingFilter</filter-name>\n>   <filter-class>org.springframework.web.filter.CharacterEncodingFilter</filter-class>\n>   <init-param>   \x3c!-- 解决乱码问题  post --\x3e\n>     <param-name>encoding</param-name>\n>     <param-value>UTF-8</param-value>\n>   </init-param>\n> </filter>\n> <filter-mapping>\n>   <filter-name>CharacterEncodingFilter</filter-name>\n>   <url-pattern>/*</url-pattern>\n> </filter-mapping>\n> \n> \n> \x3c!-- w2.配置SpringMVC的前端控制器--\x3e\n> <servlet>\n>   <servlet-name>DispatcherServlet</servlet-name>\n>   <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>\n>   <init-param> \x3c!-- 加载spring-mvc 配置文件--\x3e\n>     <param-name>contextConfigLocation</param-name>\n>     <param-value>classpath:spring-mvc.xml</param-value>\n>   </init-param>\n>   <load-on-startup>1</load-on-startup>\n> </servlet>\n> <servlet-mapping>\n>   <servlet-name>DispatcherServlet</servlet-name>\n>   <url-pattern>/</url-pattern> \x3c!-- 所有请求都要走 这个--\x3e\n> </servlet-mapping>\n> \n> \n> \x3c!-- w3.全局初始化参数   加载Spring配置文件 解耦--\x3e\n> <context-param>\n>   <param-name>contextConfigLocation</param-name>\n>   <param-value>classpath:applicationContext.xml</param-value>\n> </context-param>\n> \n> \n> \x3c!--w4.配置监听器  来自动加载Spring应用上下文--\x3e\n> <listener>\n>   <listener-class>org.springframework.web.context.ContextLoaderListener</listener-class>\n> </listener>\n\n\n# 配置类\n\n# Spring配置类\n\n> 在该类中要排除对Controller包的扫描，推荐第一种方法\n> \n> @Configuration\n> \n> // 精准加载个别包\n> //@ComponentScan({"com.diana.service","com.diana.mapper"}) //推荐第一种\n> \n> //扫描所有包，排除controller包\n> @ComponentScan(value="com.diana",\n>         excludeFilters = @ComponentScan.Filter(\n>                 type= FilterType.ANNOTATION,  //按注解过滤\n>                 classes = Controller.class    //过滤controller\n> ))\n> \n> public class SpringConfig {\n> }\n\n# SpringMVC配置类\n\n> //SpringMVC 配置文件\n> \n> @Configuration\n> //专门扫描controller包   \x3c!-- s1.Controller 组件扫描--\x3e\n> @ComponentScan({"com.diana.controller","com.diana.config"})  //要通过扫描的方式加入springMVC支持类\n> \n> //springmvc 开启注解驱动   \x3c!-- s4.mvc的注解驱动--\x3e\n> //可以自动加载  处理映射器(requestMappingHandlerMapping)，处理适配器(requestMappingHandlerAdapter)\n> //由json数据转换成对象的功能 在处理适配器中\n> @EnableWebMvc\n> \n> public class SpringMvcConfig {\n> }\n\n# SpringMVC支持类\n\n> //springmvc支持类\n> @Configuration\n> \n> public class SpringMvcSupport extends WebMvcConfigurationSupport{\n> \n>     @Autowired\n>     private ProjectInterceptor projectInterceptor;\n> \n>     @Autowired\n>     private ProjectInterceptor2 projectInterceptor2;\n> \n> \n>     @Override //放行静态资源   \x3c!-- s5.1开放资源的访问 （一般是静态资源）--\x3e\n>     protected void addResourceHandlers(ResourceHandlerRegistry registry) {\n>         //当访问/pages/??? 时候， 走/pages目录下的内容\n>         registry.addResourceHandler("/pages/**").addResourceLocations("/pages/");\n>         registry.addResourceHandler("/js/**").addResourceLocations("/js/");\n>         registry.addResourceHandler("/css/**").addResourceLocations("/css/");\n>         registry.addResourceHandler("/plugins/**").addResourceLocations("/plugins/");\n>     }\n> \n> \n>     @Override//添加拦截器链   在前面的先执行    \x3c!-- s8.配置拦截器--\x3e\n>     protected void addInterceptors(InterceptorRegistry registry) {\n>         //拦截  /books和/books/*\n>         //  /books只能拦截 /books  不能拦截/books/1\n>         registry.addInterceptor(projectInterceptor).addPathPatterns("/books","/books/*");\n>         registry.addInterceptor(projectInterceptor2).addPathPatterns("/books","/books/*");\n>     }\n> }\n\n# web容器启动类\n\n>  * 原始方法，根源 代码见下方执行流程，及配图\n> \n> //简化开发\n> public class ServletContainersInitConfig extends AbstractAnnotationConfigDispatcherServletInitializer{\n>    //加载spring配置类  \x3c!-- w3.全局初始化参数   加载Spring配置文件 解耦--\x3e   \x3c!--w4.配置监听器  来自动加载Spring应用上下文--\x3e\n>    protected Class<?>[] getRootConfigClasses() {\n>        return new Class[]{SpringConfig.class};\n>    }\n>    //加载springMVC配置类 \x3c!-- w2.配置SpringMVC的前端控制器——加载springmvc配置文件--\x3e\n>    protected Class<?>[] getServletConfigClasses() {\n>        return new Class[]{SpringMvcConfig.class};\n>    }\n>    //哪些请求归SpringMVC处理  \x3c!-- w2.配置SpringMVC的前端控制器——设置哪些请求归SpringMVC处理--\x3e\n>    protected String[] getServletMappings() {\n>        return new String[]{"/"};\n>    }\n>    \n>    //全局过滤器  \x3c!-- w1.配置全局过滤的filter--\x3e\n>    @Override\n>    protected Filter[] getServletFilters() {\n>        //乱码处理-- 只能处理post\n>        CharacterEncodingFilter filter = new CharacterEncodingFilter();\n>        filter.setEncoding("UTF-8");\n>        return new Filter[]{filter};\n>    }\n>    \n> }\n\n# 异常处理类\n\n>  * 统一异常处理\n> \n> @RestControllerAdvice //RESTful 异常处理    \x3c!-- s9.2配置自定义异常处理器--\x3e\n> public class ProjectExceptionAdvice {\n> \n>     //处理系统异常\n>     @ExceptionHandler(SystemException.class)\n>     public Result doSystemException(SystemException ex){\n>         //记录日志\n>         //发送特定消息给运维人员，提醒维护\n>         //发送邮件给开发人员，ex对象发送给开发人员\n>         //发送固定消息传递给用户，进行安抚\n>         return new Result(ex.getCode(),null,ex.getMessage()); //这里也返回Result集，保证统一\n>     }\n> \n>     //处理业务异常\n>     @ExceptionHandler(BusinessException.class)\n>     public Result doBusinessException(BusinessException ex){\n>         //发送对应消息传递给用户，提醒操作规范\n>         return new Result(ex.getCode(),null,ex.getMessage()); //这里也返回Result集，保证统一\n>     }\n> \n>     //集中的、统一的处理项目中出现的异常\n>     @ExceptionHandler(Exception.class) //捕获所有异常\n>     public Result doException(Exception ex){\n>         //记录日志\n>         //发送特定消息给运维人员，提醒维护\n>         //发送邮件给开发人员，ex对象发送给开发人员\n>         //发送固定消息传递给用户，进行安抚\n>         return new Result(Code.SYSTEM_UNKNOW_ERR,null,"系统繁忙，请稍后重试"); //这里也返回Result集，保证统一\n>     }\n> }\n> \n> \n>  * 自定义异常类\n> \n> //自定义业务异常\n> \n> public class BusinessException extends RuntimeException{ //继承运行时异常，这样不用每个方法后面抛异常\n> \n>     private Integer code;\n> \n>     public Integer getCode() {\n>         return code;\n>     }\n> \n>     public void setCode(Integer code) {\n>         this.code = code;\n>     }\n> \n>     public BusinessException(Integer code, String message) {\n>         super(message);\n>         this.code = code;\n>     }\n> \n>     public BusinessException(Integer code, String message, Throwable cause) {\n>         super(message, cause);\n>         this.code = code;\n>     }\n> \n>     public BusinessException(Integer code) {\n>         this.code = code;\n>     }\n> \n> \n>     public BusinessException(Throwable cause, Integer code) {\n>         super(cause);\n>         this.code = code;\n>     }\n> \n>     public BusinessException(String message, Throwable cause, boolean enableSuppression, boolean writableStackTrace, Integer code) {\n>         this.code = code;\n>     }\n> }\n> \n> \n> //自定义系统异常\n> \n> public class SystemException extends RuntimeException{ //继承运行时异常，这样不用每个方法后面抛异常\n> \n>     private Integer code;\n> \n>     public Integer getCode() {\n>         return code;\n>     }\n> \n>     public void setCode(Integer code) {\n>         this.code = code;\n>     }\n> \n> \n>     public SystemException(Integer code, String message) {\n>         super(message);\n>         this.code = code;\n>     }\n> \n>     public SystemException(Integer code,String message, Throwable cause) {\n>         super(message, cause);\n>         this.code = code;\n>     }\n> \n>     public SystemException(Integer code) {\n>         this.code = code;\n>     }\n> \n> \n>     public SystemException(Throwable cause, Integer code) {\n>         super(cause);\n>         this.code = code;\n>     }\n> \n>     public SystemException(String message, Throwable cause, boolean enableSuppression, boolean writableStackTrace, Integer code) {\n>         this.code = code;\n>     }\n> }\n\n# 拦截器类\n\n\n# 执行流程\n\n//web容器启动类\n\n//原始方法，根源\npublic class ServletContainersInitConfig extends AbstractDispatcherServletInitializer {\n\n    //加载SpringMVC容器对象\n    protected WebApplicationContext createServletApplicationContext() {\n        AnnotationConfigWebApplicationContext ctx = new AnnotationConfigWebApplicationContext();\n        ctx.register(SpringMvcConfig.class);\n        return  ctx;\n    }\n\n    //哪些请求归SpringMVC处理\n    protected String[] getServletMappings() {\n        return new String[]{"/"}; //所有请求都归springMVC处理\n    }\n\n    //记载Spring容器对象\n    protected WebApplicationContext createRootApplicationContext() {\n        AnnotationConfigWebApplicationContext ctx = new AnnotationConfigWebApplicationContext();\n        ctx.register(SpringConfig.class);\n        return  ctx;\n    }\n}\n\n\n\n\n\n# REST风格\n\n\n# 1.REST风格简介\n\n>  * http://localhost/users 查询全部用户信息 GET(查询)\n> \n>  * http://localhost/users /1 查询指定用户信息 GET(查询)\n> \n>  * http://localhost/users 添加用户信息 POST(新增/保存)\n> \n>  * http://localhost/users 修改用户信息 PUT(修改/更新)\n> \n>  * http://localhost/users /1 删除用户信息 DELETE(删除)\n> \n>  * 上述行为是约定方式，约定不是规范，可以打破，所以称为REST风格\n> \n>  * 描述模块的名称通常使用复数，就是加s的格式描述，表示此类资源\n> \n>  * 根据REST风格对资源进行访问称为RESTful\n\n\n# 2. 参数获取\n\n * json数据——>@RequestBody\n   \n   //POST提交 --新增数据\n   @RequestMapping(value = "/users",method = RequestMethod.POST)\n   @ResponseBody //不进行跳转，return的就是响应体      return 默认的是页面跳转\n   public String save(@RequestBody User user){ //接收json数据\n       System.out.println("user save "+user);\n       return "{\'info\':\'save\'}";\n   }\n   \n   \n   * 访问url http://localhost/users\n   \n   * 请求数据\n     \n     { "username":"diana",\n     \n       "password":"123"}\n     \n   \n   * 输出 user save User{username=\'diana\', password=\'123\', address=null}\n\n * 请求路径参数——>@PathVariable\n   \n   * 访问路径后面 用{}包裹参数----参数名要和形参一致\n   \n   //delete提交 --删除数据\n   @RequestMapping(value = "/users/{id}",method = RequestMethod.DELETE)\n   @ResponseBody\n   public String delete(@PathVariable Integer id){//接收请求路径数据\n       System.out.println("user delete id="+id);\n       return "{\'info\':\'delete\'}";\n   }\n   \n   \n   * 访问url http://localhost/users/1\n   \n   * 请求参数 1 对应id=1\n   \n   * 输出 user delete id=1\n\n * url地址传参或表单传参 ——>@RequestParam\n   \n   //集合类型参数1 --参数前加上@RequestParam   --集合中存储普通参数\n   @RequestMapping(value = "/param51")\n   @ResponseBody\n   public void param51(@RequestParam List<String> params) throws IOException {// void 表示响应体为空\n       // 访问url  /param51?params=diana&params=凉冰\n       System.out.println(params);//[diana, 凉冰]\n   }\n   \n   \n   * 访问url http://localhost//param51?params=diana&params=凉冰\n   * 请求参数 params=diana&params=凉冰\n   * 输出 [diana, 凉冰]\n\n * 传参方式选取\n   \n   * 如果请求参数超过1个,以json格式为主，使用@RequestBody\n   * 采用RESTful进行开发时，当参数较少时，使用@PathVariable\n   * 如果发送非json数据，选用@RequestParam接收请求数据\n\n\n# 3.简化开发\n\n----UserController_REST_Simple.java\n\n//REST风格开发\n// 简化开发   修改\n/*\n    1. 提出公共访问路径   在类上面加   @RequestMapping("/users")\n    2. 在类上面加 @ResponseBody   表示整个类的所有方法都 不进行页面跳转  将当前返回值作为响应体\n    3. 将@Controller  @ResponseBody   合二为一  @RestController\n\t4. 将@RequestMapping(method = RequestMethod.GET)  替换为对应的    @GetMapping\n\n */\n\n\n@RestController  //合二为一版\n@RequestMapping("/userss")\npublic class UserController_REST_Simple {\n\n    @PostMapping\n    public String save(@RequestBody User user){ //接收json数据\n        System.out.println("user save "+user);\n        return "{\'info\':\'save\'}";\n    }\n\n    @DeleteMapping("/{id}")\n    public String delete(@PathVariable Integer id){//接收请求路径数据\n        System.out.println("user delete id="+id);\n        return "{\'info\':\'delete\'}";\n    }\n\n    @PutMapping\n    public String update(@RequestBody User user){ //接收json数据\n        System.out.println("user update "+user);\n        return "{\'info\':\'springmvc\'}";\n    }\n\n\n    @GetMapping("/{id}")\n    public String getById(@PathVariable Integer id){//接收请求路径数据\n        // 1.@PathVariable 来自路径\n        //2. /users/{id} =====int id    两个名字对应起来\n\n        System.out.println("user getById");\n        System.out.println(id);\n        return "{\'info\':\'getById\'}";\n    }\n\n\n    @GetMapping\n    public String getAll(){ //接收请求路径数据\n        System.out.println("user getAll");\n        return "{\'info\':\'getAll\'}";\n    }\n\n}\n\n\n\n# 4.案例\n\n基于RESTful页面数据交互\n\n * 配置SpringMvcSupport支持类，放行静态资源\n\n * 回顾ajax请求写法\n   \n   //古老写法\n   var _this=this;\n   axios({\n       "method":"GET",\n       "url":"/books"\n   }).then(function (resp){\n       _this.dataList=resp.data;\n   });\n   \n   \n   //中间写法    ------暂时推荐这种   使用箭头的方式，可以自动识别this，不用var _this=this\n   axios({\n       "method":"GET",\n       "url":"/books"\n   }).then((res)=>{\n       this.dataList=res.data;\n   });\n   \n   \n   //最简单写法   --------等写熟了，推荐这种\n   axios.get("/books").then((res)=>{\n       this.dataList = res.data;\n   });\n   \n\n\n# POSTMAN\n\n>  * 功能解析\n>    \n>    * 右键点击集合，选择 add Folder,新建文件夹，存放一个集合下的一个整体请求\n> \n>  * 快捷键\n>    \n>    * ctrl,+ 放大页面； ctrl,— 缩小页面\n> \n>  * 路径请求\n>    \n>    * get请求参数在 Params\n>    * post请求在Body\n>      * 表单 ----x-www-form-urlencoded\n>      * 数据----raw\n>        * json --- JSON\n>    * PUT请求\n>    * DELETE请求',normalizedContent:'这里只提供一些原springmvc没有学到的功能\n\n\n# 学习目标\n\n 1. 掌握基于springmvc获取请求参数与响应json数据操作\n\n 2. 熟练应用基于rest风格的请求路径设置与参数传递\n\n 3. 能够根据实际业务建立前后端开发通信协议并进行实现\n\n 4. 基于ssm整合技术开发任意业务模板功能\n\n 5. 现阶段开发流程\n    \n    * spring mvc负责实现controller 和json\n      \n      \n\n\n# 配置转换\n\n\n# 原始xml文件\n\n# spring-mvc.xml\n\n> \x3c!-- s1.controller 组件扫描--\x3e\n> <context:component-scan base-package="com.diana.controller"/>\n> \n>    \n> \x3c!-- s2.配置内部资源视图解析器--\x3e                                            \x3c!-- 没有对应的配置类--\x3e\n> <bean id="viewresolver" class="org.springframework.web.servlet.view.internalresourceviewresolver">\n>     \x3c!-- 设置默认前缀和后缀 --\x3e\n>     \x3c!--        <property name="prefix" value="/jsp/"/>--\x3e\n>     <property name="prefix" value=""/>\n>     <property name="suffix" value=".jsp"/>\n> </bean>\n> \n> \n> \n> \n> \x3c!-- s4.mvc的注解驱动--\x3e\n> <mvc:annotation-driven conversion-service="conversionservice"/>\n> <mvc:annotation-driven/>\n> \n> \x3c!-- s5.1开放资源的访问 （一般是静态资源）--\x3e\n> \x3c!--    <mvc:resources mapping="/js/**" location="/js/"/>--\x3e\n> \x3c!--    <mvc:resources mapping="/img/**" location="/img/"/>--\x3e\n> \x3c!--    <mvc:resources mapping="/html/**" location="/html/"/>--\x3e\n> \x3c!-- s5.2使用原始tomcat  找静态资源  与上面虽然过程不一样，但是功能一样--\x3e\n> <mvc:default-servlet-handler/>\n> \n> \n> \x3c!-- s6.声明转换器--\x3e  \x3c!-- 然后在mvc的注解驱动中声明--\x3e                       \x3c!-- 没有对应的配置类--\x3e\n> <bean id="conversionservice" class="org.springframework.context.support.conversionservicefactorybean">\n>  <property name="converters">\n>      <list>\n>          <bean class="com.diana.converter.dateconverter"/>\n>      </list>\n>  </property>\n> </bean>\n> \n>  \n>    \x3c!-- s7.配置文件上传解析器--\x3e                                                 \x3c!-- 没有对应的配置类--\x3e\n>    <bean id="multipartresolver" class="org.springframework.web.multipart.commons.commonsmultipartresolver">\n>     <property name="defaultencoding" value="utf-8"/>\n>     <property name="maxuploadsize" value="50000"/>  \x3c!-- 上传文件总大小--\x3e\n>     <property name="maxuploadsizeperfile" value="5000"/>  \x3c!-- 上传文件总大小--\x3e\n> </bean>\n> \n> \n> \x3c!-- s8.配置拦截器--\x3e\n> <mvc:interceptors>\n>     \x3c!--拦截器1--\x3e\n>     <mvc:interceptor>\n>         \x3c!--对哪些资源进行拦截--\x3e\n>      <mvc:mapping path="/**"/>\n>      <bean class="com.diana.interceptor.myinterceptor1"/>\n>  </mvc:interceptor>\n>  \x3c!--拦截器2--\x3e\n>  <mvc:interceptor>\n>         \x3c!--对哪些资源进行拦截--\x3e\n>         <mvc:mapping path="/**"/>\n>         <bean class="com.diana.interceptor.myinterceptor2"/>\n>     </mvc:interceptor>\n>    </mvc:interceptors>\n>    \n>    \n>    \x3c!-- s9.1配置简单映射异常处理器--\x3e\n>    <bean class="org.springframework.web.servlet.handler.simplemappingexceptionresolver">\n>     \x3c!-- 默认错误视图    value 是视图名称（自己随便起名）  省略了前后缀--\x3e\n>     <property name="defaulterrorview" value="/exception/error"/>\n>     \x3c!-- 异常错误映射  优先判断里面的异常类型，里面没有时 才会进入默认错误视图--\x3e\n>  <property name="exceptionmappings">\n>      <map>\n>          <entry key="com.diana.exception.myexception" value="/exception/myerror"/>\x3c!-- 自定义异常--\x3e\n>          <entry key="java.lang.classcastexception" value="/exception/error_class"/>\x3c!-- 类型转换异常--\x3e\n>      </map>\n>     </property>\n>    </bean>\n>    \x3c!-- s9.2配置自定义异常处理器--\x3e\n>    <bean class="com.diana.resolver.myexceptionresolver"/>\n\n# web.xml\n\n> \x3c!-- w1.配置全局过滤的filter--\x3e\n> <filter>\n>   <filter-name>characterencodingfilter</filter-name>\n>   <filter-class>org.springframework.web.filter.characterencodingfilter</filter-class>\n>   <init-param>   \x3c!-- 解决乱码问题  post --\x3e\n>     <param-name>encoding</param-name>\n>     <param-value>utf-8</param-value>\n>   </init-param>\n> </filter>\n> <filter-mapping>\n>   <filter-name>characterencodingfilter</filter-name>\n>   <url-pattern>/*</url-pattern>\n> </filter-mapping>\n> \n> \n> \x3c!-- w2.配置springmvc的前端控制器--\x3e\n> <servlet>\n>   <servlet-name>dispatcherservlet</servlet-name>\n>   <servlet-class>org.springframework.web.servlet.dispatcherservlet</servlet-class>\n>   <init-param> \x3c!-- 加载spring-mvc 配置文件--\x3e\n>     <param-name>contextconfiglocation</param-name>\n>     <param-value>classpath:spring-mvc.xml</param-value>\n>   </init-param>\n>   <load-on-startup>1</load-on-startup>\n> </servlet>\n> <servlet-mapping>\n>   <servlet-name>dispatcherservlet</servlet-name>\n>   <url-pattern>/</url-pattern> \x3c!-- 所有请求都要走 这个--\x3e\n> </servlet-mapping>\n> \n> \n> \x3c!-- w3.全局初始化参数   加载spring配置文件 解耦--\x3e\n> <context-param>\n>   <param-name>contextconfiglocation</param-name>\n>   <param-value>classpath:applicationcontext.xml</param-value>\n> </context-param>\n> \n> \n> \x3c!--w4.配置监听器  来自动加载spring应用上下文--\x3e\n> <listener>\n>   <listener-class>org.springframework.web.context.contextloaderlistener</listener-class>\n> </listener>\n\n\n# 配置类\n\n# spring配置类\n\n> 在该类中要排除对controller包的扫描，推荐第一种方法\n> \n> @configuration\n> \n> // 精准加载个别包\n> //@componentscan({"com.diana.service","com.diana.mapper"}) //推荐第一种\n> \n> //扫描所有包，排除controller包\n> @componentscan(value="com.diana",\n>         excludefilters = @componentscan.filter(\n>                 type= filtertype.annotation,  //按注解过滤\n>                 classes = controller.class    //过滤controller\n> ))\n> \n> public class springconfig {\n> }\n\n# springmvc配置类\n\n> //springmvc 配置文件\n> \n> @configuration\n> //专门扫描controller包   \x3c!-- s1.controller 组件扫描--\x3e\n> @componentscan({"com.diana.controller","com.diana.config"})  //要通过扫描的方式加入springmvc支持类\n> \n> //springmvc 开启注解驱动   \x3c!-- s4.mvc的注解驱动--\x3e\n> //可以自动加载  处理映射器(requestmappinghandlermapping)，处理适配器(requestmappinghandleradapter)\n> //由json数据转换成对象的功能 在处理适配器中\n> @enablewebmvc\n> \n> public class springmvcconfig {\n> }\n\n# springmvc支持类\n\n> //springmvc支持类\n> @configuration\n> \n> public class springmvcsupport extends webmvcconfigurationsupport{\n> \n>     @autowired\n>     private projectinterceptor projectinterceptor;\n> \n>     @autowired\n>     private projectinterceptor2 projectinterceptor2;\n> \n> \n>     @override //放行静态资源   \x3c!-- s5.1开放资源的访问 （一般是静态资源）--\x3e\n>     protected void addresourcehandlers(resourcehandlerregistry registry) {\n>         //当访问/pages/??? 时候， 走/pages目录下的内容\n>         registry.addresourcehandler("/pages/**").addresourcelocations("/pages/");\n>         registry.addresourcehandler("/js/**").addresourcelocations("/js/");\n>         registry.addresourcehandler("/css/**").addresourcelocations("/css/");\n>         registry.addresourcehandler("/plugins/**").addresourcelocations("/plugins/");\n>     }\n> \n> \n>     @override//添加拦截器链   在前面的先执行    \x3c!-- s8.配置拦截器--\x3e\n>     protected void addinterceptors(interceptorregistry registry) {\n>         //拦截  /books和/books/*\n>         //  /books只能拦截 /books  不能拦截/books/1\n>         registry.addinterceptor(projectinterceptor).addpathpatterns("/books","/books/*");\n>         registry.addinterceptor(projectinterceptor2).addpathpatterns("/books","/books/*");\n>     }\n> }\n\n# web容器启动类\n\n>  * 原始方法，根源 代码见下方执行流程，及配图\n> \n> //简化开发\n> public class servletcontainersinitconfig extends abstractannotationconfigdispatcherservletinitializer{\n>    //加载spring配置类  \x3c!-- w3.全局初始化参数   加载spring配置文件 解耦--\x3e   \x3c!--w4.配置监听器  来自动加载spring应用上下文--\x3e\n>    protected class<?>[] getrootconfigclasses() {\n>        return new class[]{springconfig.class};\n>    }\n>    //加载springmvc配置类 \x3c!-- w2.配置springmvc的前端控制器——加载springmvc配置文件--\x3e\n>    protected class<?>[] getservletconfigclasses() {\n>        return new class[]{springmvcconfig.class};\n>    }\n>    //哪些请求归springmvc处理  \x3c!-- w2.配置springmvc的前端控制器——设置哪些请求归springmvc处理--\x3e\n>    protected string[] getservletmappings() {\n>        return new string[]{"/"};\n>    }\n>    \n>    //全局过滤器  \x3c!-- w1.配置全局过滤的filter--\x3e\n>    @override\n>    protected filter[] getservletfilters() {\n>        //乱码处理-- 只能处理post\n>        characterencodingfilter filter = new characterencodingfilter();\n>        filter.setencoding("utf-8");\n>        return new filter[]{filter};\n>    }\n>    \n> }\n\n# 异常处理类\n\n>  * 统一异常处理\n> \n> @restcontrolleradvice //restful 异常处理    \x3c!-- s9.2配置自定义异常处理器--\x3e\n> public class projectexceptionadvice {\n> \n>     //处理系统异常\n>     @exceptionhandler(systemexception.class)\n>     public result dosystemexception(systemexception ex){\n>         //记录日志\n>         //发送特定消息给运维人员，提醒维护\n>         //发送邮件给开发人员，ex对象发送给开发人员\n>         //发送固定消息传递给用户，进行安抚\n>         return new result(ex.getcode(),null,ex.getmessage()); //这里也返回result集，保证统一\n>     }\n> \n>     //处理业务异常\n>     @exceptionhandler(businessexception.class)\n>     public result dobusinessexception(businessexception ex){\n>         //发送对应消息传递给用户，提醒操作规范\n>         return new result(ex.getcode(),null,ex.getmessage()); //这里也返回result集，保证统一\n>     }\n> \n>     //集中的、统一的处理项目中出现的异常\n>     @exceptionhandler(exception.class) //捕获所有异常\n>     public result doexception(exception ex){\n>         //记录日志\n>         //发送特定消息给运维人员，提醒维护\n>         //发送邮件给开发人员，ex对象发送给开发人员\n>         //发送固定消息传递给用户，进行安抚\n>         return new result(code.system_unknow_err,null,"系统繁忙，请稍后重试"); //这里也返回result集，保证统一\n>     }\n> }\n> \n> \n>  * 自定义异常类\n> \n> //自定义业务异常\n> \n> public class businessexception extends runtimeexception{ //继承运行时异常，这样不用每个方法后面抛异常\n> \n>     private integer code;\n> \n>     public integer getcode() {\n>         return code;\n>     }\n> \n>     public void setcode(integer code) {\n>         this.code = code;\n>     }\n> \n>     public businessexception(integer code, string message) {\n>         super(message);\n>         this.code = code;\n>     }\n> \n>     public businessexception(integer code, string message, throwable cause) {\n>         super(message, cause);\n>         this.code = code;\n>     }\n> \n>     public businessexception(integer code) {\n>         this.code = code;\n>     }\n> \n> \n>     public businessexception(throwable cause, integer code) {\n>         super(cause);\n>         this.code = code;\n>     }\n> \n>     public businessexception(string message, throwable cause, boolean enablesuppression, boolean writablestacktrace, integer code) {\n>         this.code = code;\n>     }\n> }\n> \n> \n> //自定义系统异常\n> \n> public class systemexception extends runtimeexception{ //继承运行时异常，这样不用每个方法后面抛异常\n> \n>     private integer code;\n> \n>     public integer getcode() {\n>         return code;\n>     }\n> \n>     public void setcode(integer code) {\n>         this.code = code;\n>     }\n> \n> \n>     public systemexception(integer code, string message) {\n>         super(message);\n>         this.code = code;\n>     }\n> \n>     public systemexception(integer code,string message, throwable cause) {\n>         super(message, cause);\n>         this.code = code;\n>     }\n> \n>     public systemexception(integer code) {\n>         this.code = code;\n>     }\n> \n> \n>     public systemexception(throwable cause, integer code) {\n>         super(cause);\n>         this.code = code;\n>     }\n> \n>     public systemexception(string message, throwable cause, boolean enablesuppression, boolean writablestacktrace, integer code) {\n>         this.code = code;\n>     }\n> }\n\n# 拦截器类\n\n\n# 执行流程\n\n//web容器启动类\n\n//原始方法，根源\npublic class servletcontainersinitconfig extends abstractdispatcherservletinitializer {\n\n    //加载springmvc容器对象\n    protected webapplicationcontext createservletapplicationcontext() {\n        annotationconfigwebapplicationcontext ctx = new annotationconfigwebapplicationcontext();\n        ctx.register(springmvcconfig.class);\n        return  ctx;\n    }\n\n    //哪些请求归springmvc处理\n    protected string[] getservletmappings() {\n        return new string[]{"/"}; //所有请求都归springmvc处理\n    }\n\n    //记载spring容器对象\n    protected webapplicationcontext createrootapplicationcontext() {\n        annotationconfigwebapplicationcontext ctx = new annotationconfigwebapplicationcontext();\n        ctx.register(springconfig.class);\n        return  ctx;\n    }\n}\n\n\n\n\n\n# rest风格\n\n\n# 1.rest风格简介\n\n>  * http://localhost/users 查询全部用户信息 get(查询)\n> \n>  * http://localhost/users /1 查询指定用户信息 get(查询)\n> \n>  * http://localhost/users 添加用户信息 post(新增/保存)\n> \n>  * http://localhost/users 修改用户信息 put(修改/更新)\n> \n>  * http://localhost/users /1 删除用户信息 delete(删除)\n> \n>  * 上述行为是约定方式，约定不是规范，可以打破，所以称为rest风格\n> \n>  * 描述模块的名称通常使用复数，就是加s的格式描述，表示此类资源\n> \n>  * 根据rest风格对资源进行访问称为restful\n\n\n# 2. 参数获取\n\n * json数据——>@requestbody\n   \n   //post提交 --新增数据\n   @requestmapping(value = "/users",method = requestmethod.post)\n   @responsebody //不进行跳转，return的就是响应体      return 默认的是页面跳转\n   public string save(@requestbody user user){ //接收json数据\n       system.out.println("user save "+user);\n       return "{\'info\':\'save\'}";\n   }\n   \n   \n   * 访问url http://localhost/users\n   \n   * 请求数据\n     \n     { "username":"diana",\n     \n       "password":"123"}\n     \n   \n   * 输出 user save user{username=\'diana\', password=\'123\', address=null}\n\n * 请求路径参数——>@pathvariable\n   \n   * 访问路径后面 用{}包裹参数----参数名要和形参一致\n   \n   //delete提交 --删除数据\n   @requestmapping(value = "/users/{id}",method = requestmethod.delete)\n   @responsebody\n   public string delete(@pathvariable integer id){//接收请求路径数据\n       system.out.println("user delete id="+id);\n       return "{\'info\':\'delete\'}";\n   }\n   \n   \n   * 访问url http://localhost/users/1\n   \n   * 请求参数 1 对应id=1\n   \n   * 输出 user delete id=1\n\n * url地址传参或表单传参 ——>@requestparam\n   \n   //集合类型参数1 --参数前加上@requestparam   --集合中存储普通参数\n   @requestmapping(value = "/param51")\n   @responsebody\n   public void param51(@requestparam list<string> params) throws ioexception {// void 表示响应体为空\n       // 访问url  /param51?params=diana&params=凉冰\n       system.out.println(params);//[diana, 凉冰]\n   }\n   \n   \n   * 访问url http://localhost//param51?params=diana&params=凉冰\n   * 请求参数 params=diana&params=凉冰\n   * 输出 [diana, 凉冰]\n\n * 传参方式选取\n   \n   * 如果请求参数超过1个,以json格式为主，使用@requestbody\n   * 采用restful进行开发时，当参数较少时，使用@pathvariable\n   * 如果发送非json数据，选用@requestparam接收请求数据\n\n\n# 3.简化开发\n\n----usercontroller_rest_simple.java\n\n//rest风格开发\n// 简化开发   修改\n/*\n    1. 提出公共访问路径   在类上面加   @requestmapping("/users")\n    2. 在类上面加 @responsebody   表示整个类的所有方法都 不进行页面跳转  将当前返回值作为响应体\n    3. 将@controller  @responsebody   合二为一  @restcontroller\n\t4. 将@requestmapping(method = requestmethod.get)  替换为对应的    @getmapping\n\n */\n\n\n@restcontroller  //合二为一版\n@requestmapping("/userss")\npublic class usercontroller_rest_simple {\n\n    @postmapping\n    public string save(@requestbody user user){ //接收json数据\n        system.out.println("user save "+user);\n        return "{\'info\':\'save\'}";\n    }\n\n    @deletemapping("/{id}")\n    public string delete(@pathvariable integer id){//接收请求路径数据\n        system.out.println("user delete id="+id);\n        return "{\'info\':\'delete\'}";\n    }\n\n    @putmapping\n    public string update(@requestbody user user){ //接收json数据\n        system.out.println("user update "+user);\n        return "{\'info\':\'springmvc\'}";\n    }\n\n\n    @getmapping("/{id}")\n    public string getbyid(@pathvariable integer id){//接收请求路径数据\n        // 1.@pathvariable 来自路径\n        //2. /users/{id} =====int id    两个名字对应起来\n\n        system.out.println("user getbyid");\n        system.out.println(id);\n        return "{\'info\':\'getbyid\'}";\n    }\n\n\n    @getmapping\n    public string getall(){ //接收请求路径数据\n        system.out.println("user getall");\n        return "{\'info\':\'getall\'}";\n    }\n\n}\n\n\n\n# 4.案例\n\n基于restful页面数据交互\n\n * 配置springmvcsupport支持类，放行静态资源\n\n * 回顾ajax请求写法\n   \n   //古老写法\n   var _this=this;\n   axios({\n       "method":"get",\n       "url":"/books"\n   }).then(function (resp){\n       _this.datalist=resp.data;\n   });\n   \n   \n   //中间写法    ------暂时推荐这种   使用箭头的方式，可以自动识别this，不用var _this=this\n   axios({\n       "method":"get",\n       "url":"/books"\n   }).then((res)=>{\n       this.datalist=res.data;\n   });\n   \n   \n   //最简单写法   --------等写熟了，推荐这种\n   axios.get("/books").then((res)=>{\n       this.datalist = res.data;\n   });\n   \n\n\n# postman\n\n>  * 功能解析\n>    \n>    * 右键点击集合，选择 add folder,新建文件夹，存放一个集合下的一个整体请求\n> \n>  * 快捷键\n>    \n>    * ctrl,+ 放大页面； ctrl,— 缩小页面\n> \n>  * 路径请求\n>    \n>    * get请求参数在 params\n>    * post请求在body\n>      * 表单 ----x-www-form-urlencoded\n>      * 数据----raw\n>        * json --- json\n>    * put请求\n>    * delete请求',charsets:{cjk:!0}},{title:"《SSM》",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"01.后端/40.SSM",imgUrl:"/assets/img/ssm.jpg",description:"SSM学习笔记--整理自黑马程序员，在原教程基础上添加学习笔记"}},title:"《SSM》",date:"2023-06-30T20:30:40.000Z",permalink:"/back/SSM/",article:!1,comment:!1,editLink:!1,author:{name:"凉冰",link:"https://github.com/diana-devil"}},regularPath:"/01.%E5%90%8E%E7%AB%AF/40.SSM/",relativePath:"01.后端/40.SSM/README.md",key:"v-4b3039e8",path:"/back/SSM/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"SSM整合—注解开发",frontmatter:{autoSort:94,title:"SSM整合—注解开发",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/e67134/",categories:["后端","SSM"],tags:["知识","SSM"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/40.SSM/30.SSM%E6%95%B4%E5%90%88-%E6%B3%A8%E8%A7%A3.html",relativePath:"01.后端/40.SSM/30.SSM整合-注解.md",key:"v-726095f8",path:"/pages/e67134/",headers:[{level:2,title:"SSM框架整合",slug:"ssm框架整合",normalizedTitle:"ssm框架整合",charIndex:2},{level:3,title:"导入坐标",slug:"导入坐标",normalizedTitle:"导入坐标",charIndex:14},{level:3,title:"Spring相关配置",slug:"spring相关配置",normalizedTitle:"spring相关配置",charIndex:2035},{level:3,title:"Mybatis 相关配置",slug:"mybatis-相关配置",normalizedTitle:"mybatis 相关配置",charIndex:2382},{level:3,title:"Spring-MVC相关配置",slug:"spring-mvc相关配置",normalizedTitle:"spring-mvc相关配置",charIndex:4444},{level:3,title:"Web启动配置",slug:"web启动配置",normalizedTitle:"web启动配置",charIndex:5867},{level:2,title:"表现层数据封装",slug:"表现层数据封装",normalizedTitle:"表现层数据封装",charIndex:6758},{level:3,title:"Result 类",slug:"result-类",normalizedTitle:"result 类",charIndex:6810},{level:3,title:"状态码约定类",slug:"状态码约定类",normalizedTitle:"状态码约定类",charIndex:7665},{level:3,title:"表现层封装",slug:"表现层封装",normalizedTitle:"表现层封装",charIndex:8390},{level:2,title:"异常处理",slug:"异常处理",normalizedTitle:"异常处理",charIndex:9825},{level:3,title:"常见异常",slug:"常见异常",normalizedTitle:"常见异常",charIndex:9853},{level:3,title:"项目异常分类",slug:"项目异常分类",normalizedTitle:"项目异常分类",charIndex:10065},{level:3,title:"项目异常处理方案",slug:"项目异常处理方案",normalizedTitle:"项目异常处理方案",charIndex:10270},{level:3,title:"异常处理器 类",slug:"异常处理器-类",normalizedTitle:"异常处理器 类",charIndex:10493},{level:3,title:"自定义异常类",slug:"自定义异常类",normalizedTitle:"自定义异常类",charIndex:11452},{level:3,title:"业务层模拟异常抛出",slug:"业务层模拟异常抛出",normalizedTitle:"业务层模拟异常抛出",charIndex:13250},{level:2,title:"拦截器",slug:"拦截器",normalizedTitle:"拦截器",charIndex:5489},{level:3,title:"基础知识",slug:"基础知识",normalizedTitle:"基础知识",charIndex:13848},{level:3,title:"配置及参数",slug:"配置及参数",normalizedTitle:"配置及参数",charIndex:14273},{level:3,title:"拦截器链",slug:"拦截器链",normalizedTitle:"拦截器链",charIndex:5489},{level:2,title:"案例",slug:"案例",normalizedTitle:"案例",charIndex:17012},{level:3,title:"查询全部",slug:"查询全部",normalizedTitle:"查询全部",charIndex:17019},{level:3,title:"添加",slug:"添加",normalizedTitle:"添加",charIndex:5487},{level:3,title:"修改",slug:"修改",normalizedTitle:"修改",charIndex:9038},{level:3,title:"删除",slug:"删除",normalizedTitle:"删除",charIndex:8840}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"SSM框架整合 导入坐标 Spring相关配置 Mybatis 相关配置 Spring-MVC相关配置 Web启动配置 表现层数据封装 Result 类 状态码约定类 表现层封装 异常处理 常见异常 项目异常分类 项目异常处理方案 异常处理器 类 自定义异常类 业务层模拟异常抛出 拦截器 基础知识 配置及参数 拦截器链 案例 查询全部 添加 修改 删除",content:'# SSM框架整合\n\n\n# 导入坐标\n\n<dependencies>\n  \x3c!-- spring mvc  里面包含了spring-context--\x3e\n  <dependency>\n    <groupId>org.springframework</groupId>\n    <artifactId>spring-webmvc</artifactId>\n    <version>5.3.6</version>\n  </dependency>\n\n\n  \x3c!-- spring ——junit--\x3e\n  <dependency>\n    <groupId>org.springframework</groupId>\n    <artifactId>spring-test</artifactId>\n    <version>5.3.6</version>\n  </dependency>\n  <dependency>\n    <groupId>junit</groupId>\n    <artifactId>junit</artifactId>\n    <version>4.13</version>\n    <scope>test</scope>\n  </dependency>\n\n\n  \x3c!-- spring ——mybatis--\x3e\n  <dependency>\n    <groupId>org.mybatis</groupId>\n    <artifactId>mybatis</artifactId>\n    <version>3.5.5</version>\n  </dependency>\n  <dependency>\n    <groupId>org.mybatis</groupId>\n    <artifactId>mybatis-spring</artifactId>\n    <version>2.0.5</version>\n  </dependency>\n  <dependency>\n    <groupId>org.springframework</groupId>\n    <artifactId>spring-jdbc</artifactId>\n    <version>5.3.6</version>\n  </dependency>\n  <dependency>\n    <groupId>org.springframework</groupId>\n    <artifactId>spring-tx</artifactId>\n    <version>5.3.6</version>\n    <scope>compile</scope>\n  </dependency>\n  <dependency> \x3c!-- 数据源用这个 不要用c3p0 版本有问题--\x3e\n    <groupId>com.alibaba</groupId>\n    <artifactId>druid</artifactId>\n    <version>1.1.10</version>\n    <scope>compile</scope>\n  </dependency>\n\n  \x3c!-- mysql--\x3e\n  <dependency>\n    <groupId>mysql</groupId>\n    <artifactId>mysql-connector-java</artifactId>\n    <version>5.1.46</version>\n  </dependency>\n  \x3c!-- servlet--\x3e\n  <dependency>\n    <groupId>javax.servlet</groupId>\n    <artifactId>javax.servlet-api</artifactId>\n    <version>4.0.1</version>\n    <scope>provided</scope>\n  </dependency>\n\n  \x3c!-- json--\x3e\n  <dependency>\n    <groupId>com.fasterxml.jackson.core</groupId>\n    <artifactId>jackson-databind</artifactId>\n    <version>2.11.4</version>\n  </dependency>\n\n  \x3c!-- aop--\x3e\n  <dependency>\n    <groupId>org.aspectj</groupId>\n    <artifactId>aspectjweaver</artifactId>\n    <version>1.9.4</version>\n  </dependency>\n</dependencies>\n\n\n\n# Spring相关配置\n\n * Spring 配置类\n   \n   //spinrg 配置类\n   @Configuration\n   \n   //包扫描\n   @ComponentScan({"com.diana.mapper","com.diana.service"})\n   \n   //导入配置类\n   @Import({JdbcConfig.class,MybatisConfig.class})\n   \n   //开启 切面\n   @EnableAspectJAutoProxy\n   \n   \n   //开启事务\n   @EnableTransactionManagement\n   \n   \n   public class SpringConfig {\n   }\n   \n\n\n# Mybatis 相关配置\n\n * jdbc配置文件\n   \n   jdbc.driver=com.mysql.jdbc.Driver\n   jdbc.url=jdbc:mysql:///test?useSSL=false&useServerPrepStmts=true\n   jdbc.username=root\n   jdbc.password=1234\n   \n\n * Jdbc配置类\n   \n   // Jdbc配置\n   @PropertySource("classpath:jdbc.properties")\n   \n   public class JdbcConfig {\n   \n       @Value("${jdbc.driver}")\n       private String driver;\n       @Value("${jdbc.url}")\n       private String url;\n       @Value("${jdbc.username}")\n       private String username;\n       @Value("${jdbc.password}")\n       private String password;\n   \n   \n       @Bean //得到数据源\n       public DataSource dataSource() throws Exception {\n           DruidDataSource dataSource =new DruidDataSource();\n           dataSource.setDriverClassName(driver);\n           dataSource.setUrl(url);\n           dataSource.setUsername(username);\n           dataSource.setPassword(password);\n           return  dataSource;\n       }\n   \n       @Bean //得到事务管理器\n       public PlatformTransactionManager transactionManager(DataSource dataSource){\n           DataSourceTransactionManager dataSourceTransactionManager=new DataSourceTransactionManager();\n           dataSourceTransactionManager.setDataSource(dataSource);\n           return dataSourceTransactionManager;\n       }\n   }\n   \n\n * Mybatis配置类\n   \n   //mybatis 配置类\n   public class MybatisConfig {\n   \n       @Bean //得到sqlSession 工厂\n       public SqlSessionFactoryBean sqlSessionFactory(DataSource dataSource){\n           SqlSessionFactoryBean sqlSessionFactoryBean=new SqlSessionFactoryBean();\n           sqlSessionFactoryBean.setDataSource(dataSource); //设置数据源\n           sqlSessionFactoryBean.setTypeAliasesPackage("com.diana.pojo");//设置别名\n           return sqlSessionFactoryBean;\n       }\n   \n       @Bean //得到mapper映射\n       public MapperScannerConfigurer mapperScannerConfigurer(){\n           MapperScannerConfigurer mapperScannerConfigurer=new MapperScannerConfigurer();\n           mapperScannerConfigurer.setBasePackage("com.diana.mapper");//设置mapper所在的包\n           return  mapperScannerConfigurer;\n       }\n   \n   }\n   \n\n\n# Spring-MVC相关配置\n\n * SpringMVC配置类\n   \n   //spring-mvc 配置类\n   @Configuration\n   \n   //扫描包\n   @ComponentScan({"com.diana.controller","com.diana.config"})\n   \n   //开启spring-mvc 注解驱动\n   @EnableWebMvc\n   \n   public class SpringMvcConfig {\n   }\n   \n\n * SpringMVC支持类\n   \n   //springmvc支持类\n   @Configuration\n   \n   public class SpringMvcSupport extends WebMvcConfigurationSupport{\n   \n       @Autowired\n       private ProjectInterceptor projectInterceptor;\n       @Autowired\n       private ProjectInterceptor2 projectInterceptor2;\n   \n       @Override //放行静态资源\n       protected void addResourceHandlers(ResourceHandlerRegistry registry) {\n           //当访问/pages/??? 时候， 走/pages目录下的内容\n           registry.addResourceHandler("/pages/**").addResourceLocations("/pages/");\n           registry.addResourceHandler("/js/**").addResourceLocations("/js/");\n           registry.addResourceHandler("/css/**").addResourceLocations("/css/");\n           registry.addResourceHandler("/plugins/**").addResourceLocations("/plugins/");\n       }\n   \n   \n       @Override//添加拦截器链   在前面的先执行\n       protected void addInterceptors(InterceptorRegistry registry) {\n           //拦截  /books和/books/*\n           //  /books只能拦截 /books  不能拦截/books/1\n           registry.addInterceptor(projectInterceptor).addPathPatterns("/books","/books/*");\n           registry.addInterceptor(projectInterceptor2).addPathPatterns("/books","/books/*");\n       }\n   }\n   \n   \n\n\n# Web启动配置\n\n * web启动类\n   \n   // web启动配置类\n   public class WebConfig extends AbstractAnnotationConfigDispatcherServletInitializer {\n       //加载spring配置类\n       protected Class<?>[] getRootConfigClasses() {\n           return new Class[]{SpringConfig.class};\n       }\n   \n       //  spring-mvc的容器可以访问spring的容器，spring的容器不能访问spring-mvc的容器\n   \n       //加载spring-mvc配置类\n       protected Class<?>[] getServletConfigClasses() {\n           return new Class[]{SpringMvcConfig.class};\n       }\n       //设置spring-mvc拦截哪些请求\n       protected String[] getServletMappings() {\n           return new String[]{"/"};\n       }\n       //全局过滤器\n       @Override\n       protected Filter[] getServletFilters() {\n           //乱码处理-- 只能处理post\n           CharacterEncodingFilter filter = new CharacterEncodingFilter();\n           filter.setEncoding("UTF-8");\n           return new Filter[]{filter};\n       }\n   \n   }\n   \n\n\n# 表现层数据封装\n\n为了所有的方法返回相同类型的数据，要将返回结果封装成Result类,便于开发\n\n\n# Result 类\n\n// 重要\n//前端和后台的 约定   ----数据传输协议   里面的属性 自定义\n//提供几种 构造方法 方便使用\n\n\n//表现层结果封装类\npublic class Result {\n    private int code; //存放状态码\n    private Object data; //查询存放数据，增删改存放true/flase\n    private String msg; //存放消息\n\n    public Result(){\n    }\n\n    public Result(int code,Object data){\n        this.data=data;\n        this.code=code;\n    }\n\n    public Result(int code,Object data,String msg){\n        this.data=data;\n        this.code=code;\n        this.msg=msg;\n    }\n\n\n    public Object getData() {\n        return data;\n    }\n\n    public void setData(Object data) {\n        this.data = data;\n    }\n\n    public int getCode() {\n        return code;\n    }\n\n    public void setCode(int code) {\n        this.code = code;\n    }\n\n    public String getMsg() {\n        return msg;\n    }\n\n    public void setMsg(String msg) {\n        this.msg = msg;\n    }\n}\n\n\n\n# 状态码约定类\n\n// 定义协议码\n//根据项目 和前端自行协商\npublic class Code {\n\n    //成功码值 -- 1结尾\n    public static final Integer SAVE_OK=20011;\n    public static final Integer DELETE_OK=20021;\n    public static final Integer UPDATE_OK=20031;\n    public static final Integer GET_OK=20041;\n\n\n    //失败码值 -- 0结尾\n    public static final Integer SAVE_ERR=20010;\n    public static final Integer DELETE_ERR=20020;\n    public static final Integer UPDATE_ERR=20030;\n    public static final Integer GET_ERR=20040;\n\n    //系统异常\n    public static final Integer SYSTEM_ERR=50001;\n    public static final Integer SYSTEM_TIMEOUT_ERR=50002;\n    public static final Integer SYSTEM_UNKNOW_ERR=59999;\n    //业务异常\n    public static final Integer BUSNIESS_ERR=60001;\n\n}\n\n\n\n# 表现层封装\n\n**三元运算符 A?B:C A为true取B,为false取C **\n\n//结果集返回结果\n//表现层数据封装\n\n@RestController\n@RequestMapping("/books")\npublic class BookController_Result {\n    @Autowired\n    private BookService bookService;\n\n    @PostMapping //增加数据\n    public Result save(@RequestBody Book book){\n       boolean flag = bookService.save(book);\n       return new Result(flag ? Code.SAVE_OK : Code.SAVE_ERR,flag);//三元运算符 A?B:C  A为true取B,为false取C\n    }\n\n    @DeleteMapping("/{id}")//删除数据\n    public Result delete(@PathVariable int id){\n        boolean flag = bookService.delete(id);\n        return  new Result(flag ? Code.DELETE_OK : Code.DELETE_ERR,flag);\n    }\n\n    @PutMapping//修改数据\n    public Result updete(@RequestBody Book book){\n        boolean flag = bookService.update(book);\n        return new Result(flag ? Code.UPDATE_OK : Code.UPDATE_ERR,flag);\n    }\n\n\n    @GetMapping("/{id}")//根据id查询数据\n    public Result getById(@PathVariable int id){\n        //int i=1/0;  模拟异常\n        Book book = bookService.selectById(id);\n        Integer code= book != null ? Code.GET_OK : Code.GET_ERR;\n        String msg= book !=null ? "":"查询失败,请重试！";\n        return new Result(code,book,msg);\n    }\n\n\n    @GetMapping//查询所有数据\n    public Result getAll(){\n        List<Book> books = bookService.selectAll();\n        Integer code= books != null ? Code.GET_OK : Code.GET_ERR;\n        String msg= books != null ? "":"查询失败,请重试！";\n        return new Result(code,books,msg);\n    }\n}\n\n\n\n# 异常处理\n\n所有的异常全都抛出到表现层进行处理\n\n\n# 常见异常\n\n>  * 框架内部抛出的异常：因使用不合规导致\n>  * 数据层抛出的异常：因外部服务器故障导致（例如：服务器访问超时）\n>  * 业务层抛出的异常：因业务逻辑书写错误导致（例如：遍历业务书写操作，导致索引异常等）\n>  * 表现层抛出的异常：因数据收集、校验等规则导致（例如：不匹配的数据类型间导致异常）\n>  * 工具类抛出的异常：因工具类书写不严谨不够健壮导致（例如：必要释放的连接长期未释放等）\n\n\n# 项目异常分类\n\n>  * 业务异常\n>    * 规范的用户行为产生的异常——>(用户手滑打错了 age=diana ……）\n>    * 不规范的用户行为操作产生的异常——>(专业用户 故意找茬 ……)\n>  * 系统异常\n>    * 项目运行过程中可预计且无法避免的异常——>(数据库宕机，操作系统宕机，停电……)\n>  * 其他异常\n>    * 编程任意未预期到的异常——>(文件错误……)\n\n\n# 项目异常处理方案\n\n>  * 业务异常\n>    * 发送对应消息传递给用户，提醒操作规范\n>  * 系统异常\n>    * 记录日志\n>    * 发送特定消息给运维人员，提醒维护\n>    * 发送邮件给开发人员，包括ex对象\n>    * 发送固定消息传递给用户，进行安抚\n>  * 其他异常\n>    * 发送固定消息传递给用户，安抚用户\n>    * 发送特定消息给编程人员，提醒维护（纳入预期范围内）\n>    * 记录日志\n\n\n# 异常处理器 类\n\n@RestControllerAdvice //RESTful 异常处理\npublic class ProjectExceptionAdvice {\n\n    //处理系统异常\n    @ExceptionHandler(SystemException.class)\n    public Result doSystemException(SystemException ex){\n        //记录日志\n        //发送特定消息给运维人员，提醒维护\n        //发送邮件给开发人员，ex对象发送给开发人员\n        \n        //发送固定消息传递给用户，进行安抚\n        return new Result(ex.getCode(),null,ex.getMessage()); //这里也返回Result集，保证统一\n    }\n\n    //处理业务异常\n    @ExceptionHandler(BusinessException.class)\n    public Result doBusinessException(BusinessException ex){\n        //发送对应消息传递给用户，提醒操作规范\n        return new Result(ex.getCode(),null,ex.getMessage()); //这里也返回Result集，保证统一\n    }\n\n    //集中的、统一的处理项目中出现的异常\n    @ExceptionHandler(Exception.class) //捕获所有异常\n    public Result doException(Exception ex){\n        //记录日志\n        //发送特定消息给运维人员，提醒维护\n        //发送邮件给开发人员，ex对象发送给开发人员\n\n        //发送固定消息传递给用户，进行安抚\n        return new Result(Code.SYSTEM_UNKNOW_ERR,null,"系统繁忙，请稍后重试"); //这里也返回Result集，保证统一\n    }\n}\n\n\n\n# 自定义异常类\n\n定义异常时，继承运行异常RuntimeException，这样每个方法就不用在后面抛异常\n\n//自定义业务异常\n\npublic class BusinessException extends RuntimeException{ //继承运行时异常，这样不用每个方法后面抛异常\n\n    private Integer code;\n\n    public Integer getCode() {\n        return code;\n    }\n\n    public void setCode(Integer code) {\n        this.code = code;\n    }\n\n    public BusinessException(Integer code, String message) {\n        super(message);\n        this.code = code;\n    }\n\n    public BusinessException(Integer code, String message, Throwable cause) {\n        super(message, cause);\n        this.code = code;\n    }\n\n    public BusinessException(Integer code) {\n        this.code = code;\n    }\n\n\n    public BusinessException(Throwable cause, Integer code) {\n        super(cause);\n        this.code = code;\n    }\n\n    public BusinessException(String message, Throwable cause, boolean enableSuppression, boolean writableStackTrace, Integer code) {\n        this.code = code;\n    }\n}\n\n\n//自定义系统异常\n\npublic class SystemException extends RuntimeException{ //继承运行时异常，这样不用每个方法后面抛异常\n\n    private Integer code;\n\n    public Integer getCode() {\n        return code;\n    }\n\n    public void setCode(Integer code) {\n        this.code = code;\n    }\n\n    public SystemException(Integer code, String message) {\n        super(message);\n        this.code = code;\n    }\n\n    public SystemException(Integer code,String message, Throwable cause) {\n        super(message, cause);\n        this.code = code;\n    }\n\n    public SystemException(Integer code) {\n        this.code = code;\n    }\n\n\n    public SystemException(Throwable cause, Integer code) {\n        super(cause);\n        this.code = code;\n    }\n\n    public SystemException(String message, Throwable cause, boolean enableSuppression, boolean writableStackTrace, Integer code) {\n        this.code = code;\n    }\n}\n\n\n\n# 业务层模拟异常抛出\n\n将可能出现异常的进行包装，可以利用AOP\n\n@Service\npublic class BookServiceImpl implements BookService {\n\n    @Autowired\n    private BookMapper bookMapper;\n\n    //模拟业务层异常\n    public Book selectById(int id) {\n        if(id==2){//模拟业务异常\n            throw new BusinessException(Code.BUSNIESS_ERR,"请不要用你的技术挑战我的底线！！");\n        }\n\n        try{\n            int i=1/0;//这里放入的是可能出现异常的代码,转换成自定义异常\n            \n        }catch(Exception e){ //模拟系统异常-----服务器超时异常\n            throw new SystemException(Code.SYSTEM_TIMEOUT_ERR,"服务器超时,请重试！！",e);\n        }\n        return  bookMapper.selectById(id);\n    }\n  \n}\n\n\n\n# 拦截器\n\n\n# 基础知识\n\n * 概念\n   \n   拦截器是一种动态拦截方法调用的机制，在SpringMVC中动态拦截控制器方法的执行\n\n * 作用\n   \n   * 在指定的方法调用前后执行预先设定的代码\n   * 阻止原始方法的执行\n\n * 拦截器与过滤器的区别\n   \n   * 归属不同\n     \n     * Filter 属于Servlet技术，Interceptor属于SpringMVC技术\n   \n   * 拦截内容不同\n     \n     * Filter对所有访问进行增强，Interceptor仅针对SpringMVC的访问进行增强\n       \n       //设置spring-mvc拦截哪些请求\n       protected String[] getServletMappings() {\n           return new String[]{"/"};\n       }\n       \n   \n   \n\n\n# 配置及参数\n\n//拦截器 配置\n@Component\npublic class ProjectInterceptor implements HandlerInterceptor {\n\n    //在目标方法执行之前  执行   可以用来进行数据校验，权限判断 等\n    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {\n        //网络参数\n        String contentType = request.getHeader("Content-Type"); //可以获取request和response中的数据\n\n        //对象参数\n        HandlerMethod hm=(HandlerMethod) handler;\n        hm.getMethod();// 这里可以得到原始执行的对象，至于可以干什么，先学反射\n\n\n        System.out.println("前1"+contentType);\n\n\n        return true;\n        //return false;// 表示终止访问\n    }\n\n    //在目标方法执行之后 视图对象返回之前执行\n    public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception {\n        //modelAndView  可以用来修改视图和模型    但是现在异步请求,返回的是json数据，所以用的不多\n        //modelAndView.addObject("user1","凉冰");//将user1的参数由diana改成了凉冰\n\n        System.out.println("后1");\n    }\n\n    //在所有流程执行完毕后\n    public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception {\n       //ex\n        //这里可以得到 方法运行的异常 ，不过异常都已经进行了统一处理，所以用处也不大。\n\n        System.out.println("最后1");\n    }\n}\n\n\n在SpringMVC中声明拦截器，并配置拦截器链\n\n@Configuration\npublic class SpringMvcSupport extends WebMvcConfigurationSupport{\n\n    @Autowired\n    private ProjectInterceptor projectInterceptor;\n    @Autowired\n    private ProjectInterceptor2 projectInterceptor2;\n\n    @Override //放行静态资源\n    protected void addResourceHandlers(ResourceHandlerRegistry registry) {\n        //当访问/pages/??? 时候， 走/pages目录下的内容\n        registry.addResourceHandler("/pages/**").addResourceLocations("/pages/");\n        registry.addResourceHandler("/js/**").addResourceLocations("/js/");\n        registry.addResourceHandler("/css/**").addResourceLocations("/css/");\n        registry.addResourceHandler("/plugins/**").addResourceLocations("/plugins/");\n    }\n\n    @Override//添加拦截器链   在前面的先执行\n    protected void addInterceptors(InterceptorRegistry registry) {\n        //拦截  /books和/books/*\n        //  /books只能拦截 /books  不能拦截/books/1\n        registry.addInterceptor(projectInterceptor).addPathPatterns("/books","/books/*");\n        registry.addInterceptor(projectInterceptor2).addPathPatterns("/books","/books/*");\n    }\n}\n\n\n\n# 拦截器链\n\n * 单拦截器执行流程图\n   \n   \n\n * 拦截器链执行流程图\n\n\n\n * 拦截器链执行流程 文字版描述\n   \n   /**\n    * 拦截器链的执行顺序\n    *  preHandle1——>preHandle2——>postHandle2——>postHandle1——>afterCompletion2——>afterCompletion1\n    *   1.preHandle1 必定运行，若返回false，则直接结束\n    *   2.preHandle1 返回为true，则preHandle2必定运行，若preHandle2返回false，则直接跳转到afterCompletion1\n    *   3.preHandle1,preHandle2 均返回true,则按顺序进行\n    *   4.只要有一个preHandle返回false,postHandle都不会运行,对应的afterCompletion也不会执行\n    *\n    */\n   \n\n\n# 案例\n\n\n# 查询全部\n\ngetAll() {\n    //发送ajax请求  查询所有\n    axios.get("/books").then(resp=>{\n        this.dataList=resp.data.data;\n    });\n},\n\n\n\n# 添加\n\n * 按照后台给予的状态码 进行判断，并进行相应的操作\n * 注意 .finally(()=>{})的用法，表示最后一定执行这个，把前排重复的操作拿出来放到里面，简化开发。\n * 注意this.$message.success(); 弹出绿色弹框\n * 注意this.$message.error(); 弹出绿色弹框\n\n//弹出添加窗口\nhandleCreate() {\n    //显示弹窗\n    this.dialogFormVisible=true;\n    //重置表单\n    this.resetForm();\n},\n\n//重置表单\nresetForm() {\n    this.formData={};\n},\n\n//添加\nhandleAdd () {\n    //发送ajax请求  添加数据\n    axios.post("/books",this.formData).then(resp=>{\n        console.log(resp.data); //控制台打印data\n\n        if(resp.data.code===20011){//如果操作成功\n            // 关闭弹窗\n            this.dialogFormVisible=false;\n            //成功提示\n            this.$message.success("添加成功！！");\n        }else if(resp.data.code===20010){\n            //失败提示\n            this.$message.error("添加失败！！");\n        }else{\n            //异常提示\n            this.$message.error(resp.data.msg);\n        }\n    }).finally(()=>{ //无论哪种情况，最后都走这个\n        this.getAll(); //查询所有\n    })\n},\n\n\n\n# 修改\n\n//弹出编辑窗口\nhandleUpdate(row) {\n\n    //得到编辑数据的id     row.id\n    //数据回显   根据id查询\n    axios.get("/books/"+row.id).then(resp=>{\n        if(resp.data.code===20041){//如果操作成功\n            this.dialogFormVisible4Edit=true; //显示编辑弹窗\n            this.formData=resp.data.data; //数据回显\n        }else{\n            //异常提示\n            this.$message.error(resp.data.msg);\n        }\n    });\n},\n\n//编辑\nhandleEdit() {\n    //发送ajax请求  修改数据\n    axios.put("/books",this.formData).then(resp=>{\n        // console.log(resp.data); //控制台打印data\n\n        if(resp.data.code===20031){//如果操作成功\n            // 关闭弹窗\n            this.dialogFormVisible4Edit=false;\n            //成功提示\n            this.$message.success("修改成功！！");\n        }else if(resp.data.code===20030){\n            //失败提示\n            this.$message.error("修改失败！！");\n        }else{\n            //异常提示\n            this.$message.error(resp.data.msg);\n        }\n    }).finally(()=>{ //无论哪种情况，最后都走这个\n        this.getAll(); //查询所有\n    })\n},\n\n\n\n# 删除\n\n * this.$confirm().then().catch() 弹出提示框,确定走then，取消走catch\n * 注意这个finally(()=>{})的位置，跟在内层then后面，呆在外层then里面\n\n/ 删除\nhandleDelete(row) {\n    //1.弹出提示框\n    this.$confirm("此操作将永久删除当前数据,是否继续?","提示",{\n        type:\'info\'\n    }).then(()=>{\n        //点击确定  删除业务\n        //得到编辑数据的id     row.id\n        axios.delete("/books/"+row.id).then(resp=>{\n            if(resp.data.code===20021){//如果操作成功\n                this.$message.success("删除成功！");//信息提示\n            }else{\n                //异常提示\n                this.$message.error("删除失败！");\n            }\n        }).finally(()=>{//无论哪种情况，最后都走这个\n            this.getAll();\n        });\n    }).catch(()=>{\n        //点击取消 异常提示\n        this.$message.info("取消删除操作！");\n    });\n}\n',normalizedContent:'# ssm框架整合\n\n\n# 导入坐标\n\n<dependencies>\n  \x3c!-- spring mvc  里面包含了spring-context--\x3e\n  <dependency>\n    <groupid>org.springframework</groupid>\n    <artifactid>spring-webmvc</artifactid>\n    <version>5.3.6</version>\n  </dependency>\n\n\n  \x3c!-- spring ——junit--\x3e\n  <dependency>\n    <groupid>org.springframework</groupid>\n    <artifactid>spring-test</artifactid>\n    <version>5.3.6</version>\n  </dependency>\n  <dependency>\n    <groupid>junit</groupid>\n    <artifactid>junit</artifactid>\n    <version>4.13</version>\n    <scope>test</scope>\n  </dependency>\n\n\n  \x3c!-- spring ——mybatis--\x3e\n  <dependency>\n    <groupid>org.mybatis</groupid>\n    <artifactid>mybatis</artifactid>\n    <version>3.5.5</version>\n  </dependency>\n  <dependency>\n    <groupid>org.mybatis</groupid>\n    <artifactid>mybatis-spring</artifactid>\n    <version>2.0.5</version>\n  </dependency>\n  <dependency>\n    <groupid>org.springframework</groupid>\n    <artifactid>spring-jdbc</artifactid>\n    <version>5.3.6</version>\n  </dependency>\n  <dependency>\n    <groupid>org.springframework</groupid>\n    <artifactid>spring-tx</artifactid>\n    <version>5.3.6</version>\n    <scope>compile</scope>\n  </dependency>\n  <dependency> \x3c!-- 数据源用这个 不要用c3p0 版本有问题--\x3e\n    <groupid>com.alibaba</groupid>\n    <artifactid>druid</artifactid>\n    <version>1.1.10</version>\n    <scope>compile</scope>\n  </dependency>\n\n  \x3c!-- mysql--\x3e\n  <dependency>\n    <groupid>mysql</groupid>\n    <artifactid>mysql-connector-java</artifactid>\n    <version>5.1.46</version>\n  </dependency>\n  \x3c!-- servlet--\x3e\n  <dependency>\n    <groupid>javax.servlet</groupid>\n    <artifactid>javax.servlet-api</artifactid>\n    <version>4.0.1</version>\n    <scope>provided</scope>\n  </dependency>\n\n  \x3c!-- json--\x3e\n  <dependency>\n    <groupid>com.fasterxml.jackson.core</groupid>\n    <artifactid>jackson-databind</artifactid>\n    <version>2.11.4</version>\n  </dependency>\n\n  \x3c!-- aop--\x3e\n  <dependency>\n    <groupid>org.aspectj</groupid>\n    <artifactid>aspectjweaver</artifactid>\n    <version>1.9.4</version>\n  </dependency>\n</dependencies>\n\n\n\n# spring相关配置\n\n * spring 配置类\n   \n   //spinrg 配置类\n   @configuration\n   \n   //包扫描\n   @componentscan({"com.diana.mapper","com.diana.service"})\n   \n   //导入配置类\n   @import({jdbcconfig.class,mybatisconfig.class})\n   \n   //开启 切面\n   @enableaspectjautoproxy\n   \n   \n   //开启事务\n   @enabletransactionmanagement\n   \n   \n   public class springconfig {\n   }\n   \n\n\n# mybatis 相关配置\n\n * jdbc配置文件\n   \n   jdbc.driver=com.mysql.jdbc.driver\n   jdbc.url=jdbc:mysql:///test?usessl=false&useserverprepstmts=true\n   jdbc.username=root\n   jdbc.password=1234\n   \n\n * jdbc配置类\n   \n   // jdbc配置\n   @propertysource("classpath:jdbc.properties")\n   \n   public class jdbcconfig {\n   \n       @value("${jdbc.driver}")\n       private string driver;\n       @value("${jdbc.url}")\n       private string url;\n       @value("${jdbc.username}")\n       private string username;\n       @value("${jdbc.password}")\n       private string password;\n   \n   \n       @bean //得到数据源\n       public datasource datasource() throws exception {\n           druiddatasource datasource =new druiddatasource();\n           datasource.setdriverclassname(driver);\n           datasource.seturl(url);\n           datasource.setusername(username);\n           datasource.setpassword(password);\n           return  datasource;\n       }\n   \n       @bean //得到事务管理器\n       public platformtransactionmanager transactionmanager(datasource datasource){\n           datasourcetransactionmanager datasourcetransactionmanager=new datasourcetransactionmanager();\n           datasourcetransactionmanager.setdatasource(datasource);\n           return datasourcetransactionmanager;\n       }\n   }\n   \n\n * mybatis配置类\n   \n   //mybatis 配置类\n   public class mybatisconfig {\n   \n       @bean //得到sqlsession 工厂\n       public sqlsessionfactorybean sqlsessionfactory(datasource datasource){\n           sqlsessionfactorybean sqlsessionfactorybean=new sqlsessionfactorybean();\n           sqlsessionfactorybean.setdatasource(datasource); //设置数据源\n           sqlsessionfactorybean.settypealiasespackage("com.diana.pojo");//设置别名\n           return sqlsessionfactorybean;\n       }\n   \n       @bean //得到mapper映射\n       public mapperscannerconfigurer mapperscannerconfigurer(){\n           mapperscannerconfigurer mapperscannerconfigurer=new mapperscannerconfigurer();\n           mapperscannerconfigurer.setbasepackage("com.diana.mapper");//设置mapper所在的包\n           return  mapperscannerconfigurer;\n       }\n   \n   }\n   \n\n\n# spring-mvc相关配置\n\n * springmvc配置类\n   \n   //spring-mvc 配置类\n   @configuration\n   \n   //扫描包\n   @componentscan({"com.diana.controller","com.diana.config"})\n   \n   //开启spring-mvc 注解驱动\n   @enablewebmvc\n   \n   public class springmvcconfig {\n   }\n   \n\n * springmvc支持类\n   \n   //springmvc支持类\n   @configuration\n   \n   public class springmvcsupport extends webmvcconfigurationsupport{\n   \n       @autowired\n       private projectinterceptor projectinterceptor;\n       @autowired\n       private projectinterceptor2 projectinterceptor2;\n   \n       @override //放行静态资源\n       protected void addresourcehandlers(resourcehandlerregistry registry) {\n           //当访问/pages/??? 时候， 走/pages目录下的内容\n           registry.addresourcehandler("/pages/**").addresourcelocations("/pages/");\n           registry.addresourcehandler("/js/**").addresourcelocations("/js/");\n           registry.addresourcehandler("/css/**").addresourcelocations("/css/");\n           registry.addresourcehandler("/plugins/**").addresourcelocations("/plugins/");\n       }\n   \n   \n       @override//添加拦截器链   在前面的先执行\n       protected void addinterceptors(interceptorregistry registry) {\n           //拦截  /books和/books/*\n           //  /books只能拦截 /books  不能拦截/books/1\n           registry.addinterceptor(projectinterceptor).addpathpatterns("/books","/books/*");\n           registry.addinterceptor(projectinterceptor2).addpathpatterns("/books","/books/*");\n       }\n   }\n   \n   \n\n\n# web启动配置\n\n * web启动类\n   \n   // web启动配置类\n   public class webconfig extends abstractannotationconfigdispatcherservletinitializer {\n       //加载spring配置类\n       protected class<?>[] getrootconfigclasses() {\n           return new class[]{springconfig.class};\n       }\n   \n       //  spring-mvc的容器可以访问spring的容器，spring的容器不能访问spring-mvc的容器\n   \n       //加载spring-mvc配置类\n       protected class<?>[] getservletconfigclasses() {\n           return new class[]{springmvcconfig.class};\n       }\n       //设置spring-mvc拦截哪些请求\n       protected string[] getservletmappings() {\n           return new string[]{"/"};\n       }\n       //全局过滤器\n       @override\n       protected filter[] getservletfilters() {\n           //乱码处理-- 只能处理post\n           characterencodingfilter filter = new characterencodingfilter();\n           filter.setencoding("utf-8");\n           return new filter[]{filter};\n       }\n   \n   }\n   \n\n\n# 表现层数据封装\n\n为了所有的方法返回相同类型的数据，要将返回结果封装成result类,便于开发\n\n\n# result 类\n\n// 重要\n//前端和后台的 约定   ----数据传输协议   里面的属性 自定义\n//提供几种 构造方法 方便使用\n\n\n//表现层结果封装类\npublic class result {\n    private int code; //存放状态码\n    private object data; //查询存放数据，增删改存放true/flase\n    private string msg; //存放消息\n\n    public result(){\n    }\n\n    public result(int code,object data){\n        this.data=data;\n        this.code=code;\n    }\n\n    public result(int code,object data,string msg){\n        this.data=data;\n        this.code=code;\n        this.msg=msg;\n    }\n\n\n    public object getdata() {\n        return data;\n    }\n\n    public void setdata(object data) {\n        this.data = data;\n    }\n\n    public int getcode() {\n        return code;\n    }\n\n    public void setcode(int code) {\n        this.code = code;\n    }\n\n    public string getmsg() {\n        return msg;\n    }\n\n    public void setmsg(string msg) {\n        this.msg = msg;\n    }\n}\n\n\n\n# 状态码约定类\n\n// 定义协议码\n//根据项目 和前端自行协商\npublic class code {\n\n    //成功码值 -- 1结尾\n    public static final integer save_ok=20011;\n    public static final integer delete_ok=20021;\n    public static final integer update_ok=20031;\n    public static final integer get_ok=20041;\n\n\n    //失败码值 -- 0结尾\n    public static final integer save_err=20010;\n    public static final integer delete_err=20020;\n    public static final integer update_err=20030;\n    public static final integer get_err=20040;\n\n    //系统异常\n    public static final integer system_err=50001;\n    public static final integer system_timeout_err=50002;\n    public static final integer system_unknow_err=59999;\n    //业务异常\n    public static final integer busniess_err=60001;\n\n}\n\n\n\n# 表现层封装\n\n**三元运算符 a?b:c a为true取b,为false取c **\n\n//结果集返回结果\n//表现层数据封装\n\n@restcontroller\n@requestmapping("/books")\npublic class bookcontroller_result {\n    @autowired\n    private bookservice bookservice;\n\n    @postmapping //增加数据\n    public result save(@requestbody book book){\n       boolean flag = bookservice.save(book);\n       return new result(flag ? code.save_ok : code.save_err,flag);//三元运算符 a?b:c  a为true取b,为false取c\n    }\n\n    @deletemapping("/{id}")//删除数据\n    public result delete(@pathvariable int id){\n        boolean flag = bookservice.delete(id);\n        return  new result(flag ? code.delete_ok : code.delete_err,flag);\n    }\n\n    @putmapping//修改数据\n    public result updete(@requestbody book book){\n        boolean flag = bookservice.update(book);\n        return new result(flag ? code.update_ok : code.update_err,flag);\n    }\n\n\n    @getmapping("/{id}")//根据id查询数据\n    public result getbyid(@pathvariable int id){\n        //int i=1/0;  模拟异常\n        book book = bookservice.selectbyid(id);\n        integer code= book != null ? code.get_ok : code.get_err;\n        string msg= book !=null ? "":"查询失败,请重试！";\n        return new result(code,book,msg);\n    }\n\n\n    @getmapping//查询所有数据\n    public result getall(){\n        list<book> books = bookservice.selectall();\n        integer code= books != null ? code.get_ok : code.get_err;\n        string msg= books != null ? "":"查询失败,请重试！";\n        return new result(code,books,msg);\n    }\n}\n\n\n\n# 异常处理\n\n所有的异常全都抛出到表现层进行处理\n\n\n# 常见异常\n\n>  * 框架内部抛出的异常：因使用不合规导致\n>  * 数据层抛出的异常：因外部服务器故障导致（例如：服务器访问超时）\n>  * 业务层抛出的异常：因业务逻辑书写错误导致（例如：遍历业务书写操作，导致索引异常等）\n>  * 表现层抛出的异常：因数据收集、校验等规则导致（例如：不匹配的数据类型间导致异常）\n>  * 工具类抛出的异常：因工具类书写不严谨不够健壮导致（例如：必要释放的连接长期未释放等）\n\n\n# 项目异常分类\n\n>  * 业务异常\n>    * 规范的用户行为产生的异常——>(用户手滑打错了 age=diana ……）\n>    * 不规范的用户行为操作产生的异常——>(专业用户 故意找茬 ……)\n>  * 系统异常\n>    * 项目运行过程中可预计且无法避免的异常——>(数据库宕机，操作系统宕机，停电……)\n>  * 其他异常\n>    * 编程任意未预期到的异常——>(文件错误……)\n\n\n# 项目异常处理方案\n\n>  * 业务异常\n>    * 发送对应消息传递给用户，提醒操作规范\n>  * 系统异常\n>    * 记录日志\n>    * 发送特定消息给运维人员，提醒维护\n>    * 发送邮件给开发人员，包括ex对象\n>    * 发送固定消息传递给用户，进行安抚\n>  * 其他异常\n>    * 发送固定消息传递给用户，安抚用户\n>    * 发送特定消息给编程人员，提醒维护（纳入预期范围内）\n>    * 记录日志\n\n\n# 异常处理器 类\n\n@restcontrolleradvice //restful 异常处理\npublic class projectexceptionadvice {\n\n    //处理系统异常\n    @exceptionhandler(systemexception.class)\n    public result dosystemexception(systemexception ex){\n        //记录日志\n        //发送特定消息给运维人员，提醒维护\n        //发送邮件给开发人员，ex对象发送给开发人员\n        \n        //发送固定消息传递给用户，进行安抚\n        return new result(ex.getcode(),null,ex.getmessage()); //这里也返回result集，保证统一\n    }\n\n    //处理业务异常\n    @exceptionhandler(businessexception.class)\n    public result dobusinessexception(businessexception ex){\n        //发送对应消息传递给用户，提醒操作规范\n        return new result(ex.getcode(),null,ex.getmessage()); //这里也返回result集，保证统一\n    }\n\n    //集中的、统一的处理项目中出现的异常\n    @exceptionhandler(exception.class) //捕获所有异常\n    public result doexception(exception ex){\n        //记录日志\n        //发送特定消息给运维人员，提醒维护\n        //发送邮件给开发人员，ex对象发送给开发人员\n\n        //发送固定消息传递给用户，进行安抚\n        return new result(code.system_unknow_err,null,"系统繁忙，请稍后重试"); //这里也返回result集，保证统一\n    }\n}\n\n\n\n# 自定义异常类\n\n定义异常时，继承运行异常runtimeexception，这样每个方法就不用在后面抛异常\n\n//自定义业务异常\n\npublic class businessexception extends runtimeexception{ //继承运行时异常，这样不用每个方法后面抛异常\n\n    private integer code;\n\n    public integer getcode() {\n        return code;\n    }\n\n    public void setcode(integer code) {\n        this.code = code;\n    }\n\n    public businessexception(integer code, string message) {\n        super(message);\n        this.code = code;\n    }\n\n    public businessexception(integer code, string message, throwable cause) {\n        super(message, cause);\n        this.code = code;\n    }\n\n    public businessexception(integer code) {\n        this.code = code;\n    }\n\n\n    public businessexception(throwable cause, integer code) {\n        super(cause);\n        this.code = code;\n    }\n\n    public businessexception(string message, throwable cause, boolean enablesuppression, boolean writablestacktrace, integer code) {\n        this.code = code;\n    }\n}\n\n\n//自定义系统异常\n\npublic class systemexception extends runtimeexception{ //继承运行时异常，这样不用每个方法后面抛异常\n\n    private integer code;\n\n    public integer getcode() {\n        return code;\n    }\n\n    public void setcode(integer code) {\n        this.code = code;\n    }\n\n    public systemexception(integer code, string message) {\n        super(message);\n        this.code = code;\n    }\n\n    public systemexception(integer code,string message, throwable cause) {\n        super(message, cause);\n        this.code = code;\n    }\n\n    public systemexception(integer code) {\n        this.code = code;\n    }\n\n\n    public systemexception(throwable cause, integer code) {\n        super(cause);\n        this.code = code;\n    }\n\n    public systemexception(string message, throwable cause, boolean enablesuppression, boolean writablestacktrace, integer code) {\n        this.code = code;\n    }\n}\n\n\n\n# 业务层模拟异常抛出\n\n将可能出现异常的进行包装，可以利用aop\n\n@service\npublic class bookserviceimpl implements bookservice {\n\n    @autowired\n    private bookmapper bookmapper;\n\n    //模拟业务层异常\n    public book selectbyid(int id) {\n        if(id==2){//模拟业务异常\n            throw new businessexception(code.busniess_err,"请不要用你的技术挑战我的底线！！");\n        }\n\n        try{\n            int i=1/0;//这里放入的是可能出现异常的代码,转换成自定义异常\n            \n        }catch(exception e){ //模拟系统异常-----服务器超时异常\n            throw new systemexception(code.system_timeout_err,"服务器超时,请重试！！",e);\n        }\n        return  bookmapper.selectbyid(id);\n    }\n  \n}\n\n\n\n# 拦截器\n\n\n# 基础知识\n\n * 概念\n   \n   拦截器是一种动态拦截方法调用的机制，在springmvc中动态拦截控制器方法的执行\n\n * 作用\n   \n   * 在指定的方法调用前后执行预先设定的代码\n   * 阻止原始方法的执行\n\n * 拦截器与过滤器的区别\n   \n   * 归属不同\n     \n     * filter 属于servlet技术，interceptor属于springmvc技术\n   \n   * 拦截内容不同\n     \n     * filter对所有访问进行增强，interceptor仅针对springmvc的访问进行增强\n       \n       //设置spring-mvc拦截哪些请求\n       protected string[] getservletmappings() {\n           return new string[]{"/"};\n       }\n       \n   \n   \n\n\n# 配置及参数\n\n//拦截器 配置\n@component\npublic class projectinterceptor implements handlerinterceptor {\n\n    //在目标方法执行之前  执行   可以用来进行数据校验，权限判断 等\n    public boolean prehandle(httpservletrequest request, httpservletresponse response, object handler) throws exception {\n        //网络参数\n        string contenttype = request.getheader("content-type"); //可以获取request和response中的数据\n\n        //对象参数\n        handlermethod hm=(handlermethod) handler;\n        hm.getmethod();// 这里可以得到原始执行的对象，至于可以干什么，先学反射\n\n\n        system.out.println("前1"+contenttype);\n\n\n        return true;\n        //return false;// 表示终止访问\n    }\n\n    //在目标方法执行之后 视图对象返回之前执行\n    public void posthandle(httpservletrequest request, httpservletresponse response, object handler, modelandview modelandview) throws exception {\n        //modelandview  可以用来修改视图和模型    但是现在异步请求,返回的是json数据，所以用的不多\n        //modelandview.addobject("user1","凉冰");//将user1的参数由diana改成了凉冰\n\n        system.out.println("后1");\n    }\n\n    //在所有流程执行完毕后\n    public void aftercompletion(httpservletrequest request, httpservletresponse response, object handler, exception ex) throws exception {\n       //ex\n        //这里可以得到 方法运行的异常 ，不过异常都已经进行了统一处理，所以用处也不大。\n\n        system.out.println("最后1");\n    }\n}\n\n\n在springmvc中声明拦截器，并配置拦截器链\n\n@configuration\npublic class springmvcsupport extends webmvcconfigurationsupport{\n\n    @autowired\n    private projectinterceptor projectinterceptor;\n    @autowired\n    private projectinterceptor2 projectinterceptor2;\n\n    @override //放行静态资源\n    protected void addresourcehandlers(resourcehandlerregistry registry) {\n        //当访问/pages/??? 时候， 走/pages目录下的内容\n        registry.addresourcehandler("/pages/**").addresourcelocations("/pages/");\n        registry.addresourcehandler("/js/**").addresourcelocations("/js/");\n        registry.addresourcehandler("/css/**").addresourcelocations("/css/");\n        registry.addresourcehandler("/plugins/**").addresourcelocations("/plugins/");\n    }\n\n    @override//添加拦截器链   在前面的先执行\n    protected void addinterceptors(interceptorregistry registry) {\n        //拦截  /books和/books/*\n        //  /books只能拦截 /books  不能拦截/books/1\n        registry.addinterceptor(projectinterceptor).addpathpatterns("/books","/books/*");\n        registry.addinterceptor(projectinterceptor2).addpathpatterns("/books","/books/*");\n    }\n}\n\n\n\n# 拦截器链\n\n * 单拦截器执行流程图\n   \n   \n\n * 拦截器链执行流程图\n\n\n\n * 拦截器链执行流程 文字版描述\n   \n   /**\n    * 拦截器链的执行顺序\n    *  prehandle1——>prehandle2——>posthandle2——>posthandle1——>aftercompletion2——>aftercompletion1\n    *   1.prehandle1 必定运行，若返回false，则直接结束\n    *   2.prehandle1 返回为true，则prehandle2必定运行，若prehandle2返回false，则直接跳转到aftercompletion1\n    *   3.prehandle1,prehandle2 均返回true,则按顺序进行\n    *   4.只要有一个prehandle返回false,posthandle都不会运行,对应的aftercompletion也不会执行\n    *\n    */\n   \n\n\n# 案例\n\n\n# 查询全部\n\ngetall() {\n    //发送ajax请求  查询所有\n    axios.get("/books").then(resp=>{\n        this.datalist=resp.data.data;\n    });\n},\n\n\n\n# 添加\n\n * 按照后台给予的状态码 进行判断，并进行相应的操作\n * 注意 .finally(()=>{})的用法，表示最后一定执行这个，把前排重复的操作拿出来放到里面，简化开发。\n * 注意this.$message.success(); 弹出绿色弹框\n * 注意this.$message.error(); 弹出绿色弹框\n\n//弹出添加窗口\nhandlecreate() {\n    //显示弹窗\n    this.dialogformvisible=true;\n    //重置表单\n    this.resetform();\n},\n\n//重置表单\nresetform() {\n    this.formdata={};\n},\n\n//添加\nhandleadd () {\n    //发送ajax请求  添加数据\n    axios.post("/books",this.formdata).then(resp=>{\n        console.log(resp.data); //控制台打印data\n\n        if(resp.data.code===20011){//如果操作成功\n            // 关闭弹窗\n            this.dialogformvisible=false;\n            //成功提示\n            this.$message.success("添加成功！！");\n        }else if(resp.data.code===20010){\n            //失败提示\n            this.$message.error("添加失败！！");\n        }else{\n            //异常提示\n            this.$message.error(resp.data.msg);\n        }\n    }).finally(()=>{ //无论哪种情况，最后都走这个\n        this.getall(); //查询所有\n    })\n},\n\n\n\n# 修改\n\n//弹出编辑窗口\nhandleupdate(row) {\n\n    //得到编辑数据的id     row.id\n    //数据回显   根据id查询\n    axios.get("/books/"+row.id).then(resp=>{\n        if(resp.data.code===20041){//如果操作成功\n            this.dialogformvisible4edit=true; //显示编辑弹窗\n            this.formdata=resp.data.data; //数据回显\n        }else{\n            //异常提示\n            this.$message.error(resp.data.msg);\n        }\n    });\n},\n\n//编辑\nhandleedit() {\n    //发送ajax请求  修改数据\n    axios.put("/books",this.formdata).then(resp=>{\n        // console.log(resp.data); //控制台打印data\n\n        if(resp.data.code===20031){//如果操作成功\n            // 关闭弹窗\n            this.dialogformvisible4edit=false;\n            //成功提示\n            this.$message.success("修改成功！！");\n        }else if(resp.data.code===20030){\n            //失败提示\n            this.$message.error("修改失败！！");\n        }else{\n            //异常提示\n            this.$message.error(resp.data.msg);\n        }\n    }).finally(()=>{ //无论哪种情况，最后都走这个\n        this.getall(); //查询所有\n    })\n},\n\n\n\n# 删除\n\n * this.$confirm().then().catch() 弹出提示框,确定走then，取消走catch\n * 注意这个finally(()=>{})的位置，跟在内层then后面，呆在外层then里面\n\n/ 删除\nhandledelete(row) {\n    //1.弹出提示框\n    this.$confirm("此操作将永久删除当前数据,是否继续?","提示",{\n        type:\'info\'\n    }).then(()=>{\n        //点击确定  删除业务\n        //得到编辑数据的id     row.id\n        axios.delete("/books/"+row.id).then(resp=>{\n            if(resp.data.code===20021){//如果操作成功\n                this.$message.success("删除成功！");//信息提示\n            }else{\n                //异常提示\n                this.$message.error("删除失败！");\n            }\n        }).finally(()=>{//无论哪种情况，最后都走这个\n            this.getall();\n        });\n    }).catch(()=>{\n        //点击取消 异常提示\n        this.$message.info("取消删除操作！");\n    });\n}\n',charsets:{cjk:!0}},{title:"开发实用篇",frontmatter:{title:"开发实用篇",date:"2023-07-03T21:01:35.000Z",permalink:"/pages/164be2/",categories:["后端","SpringBoot"],tags:["知识","SpringBoot"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/50.SpringBoot/10.%E5%BC%80%E5%8F%91%E5%AE%9E%E7%94%A8%E7%AF%87.html",relativePath:"01.后端/50.SpringBoot/10.开发实用篇.md",key:"v-6a2f133e",path:"/pages/164be2/",headers:[{level:2,title:"热部署",slug:"热部署",normalizedTitle:"热部署",charIndex:2},{level:2,title:"配置高级",slug:"配置高级",normalizedTitle:"配置高级",charIndex:1147},{level:2,title:"测试",slug:"测试",normalizedTitle:"测试",charIndex:4055},{level:2,title:"监控",slug:"监控",normalizedTitle:"监控",charIndex:11219}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688392125e3,headersStr:"热部署 配置高级 测试 监控",content:'# 热部署\n\n# 热部署基础知识\n\n * 修改完功能后，能立即生效，而不需要重启服务器\n * 关于热部署\n   * 重启(Restart): 自定义开发代码，包含类、页面、配置文件等，加载位置restart类加载器\n   * 重载(ReLoad):jar包，加载位置base类加载器\n * 热部署仅仅是重启功能，即仅加载 当前开发者自定义的资源，不重新加载引入的jar包。\n\n# 手动启动热部署\n\n * 开启开发者工具\n   \n   \x3c!-- 热部署 工具--\x3e\n   <dependency>\n      <groupId>org.springframework.boot</groupId>\n      <artifactId>spring-boot-devtools</artifactId>\n   </dependency>\n   \n\n * 激活热部署\n   \n   Ctrl+F9 或者 构建中的构建项目。\n\n# 自动启动热部署\n\n * 配置设置\n   \n   \n   \n   \n\n * 当idea 失去焦点5秒后会自动启动热部署\n\n# 热部署范围配置\n\n * 默认热部署的排除文件\n   \n   \n\n * 手动干预\n   \n   devtools:\n     restart:\n       # 设置不参与热部署的文件\n       exclude: application.yml,static/**\n   \n\n# 关闭热部署\n\n * 配置文件关闭————3\n   \n   devtools:\n     restart:\n       # 设置不参与热部署的文件\n       exclude: application.yml,static/**\n       enabled: false  #关闭热部署\n   \n\n * java系统环境设置 关闭 ————5—— 这个优先级要高很多\n   \n   @SpringBootApplication\n   public class HotDeployApplication {\n   \n      public static void main(String[] args) {\n         //设置高优先级属性禁用热部署   java系统 环境设置 比配置文件优先级要高\n         System.setProperty("spring.devtools.restart.enabled","true");\n         SpringApplication.run(HotDeployApplication.class);//为了安全\n      }\n   }\n   \n\n * 参数优先级\n   \n   \n\n\n# 配置高级\n\n# @ConfigurationProperties\n\n#自定义配置\nservers:\n  ipAdress: 192.168.1.1\n  port: 2345\n  timeout: -1\n\n#自定义数据源配置\ndatasource:\n  driverClassName: com.mysql.jdbc.driver1234\n  \n\n\n * 使用注解设置关联后，代码中只需要设置好set方法，即可自动读取\n\n * 为自定义Bean关联配置\n   \n   @Data\n   @ConfigurationProperties(prefix = "servers")\n   public class ServletConfig {\n       private String ipAdress;\n       private int port;\n       private Long timeout;\n   }\n   \n\n * 为第三方Bean关联配置\n   \n   @Bean\n   @ConfigurationProperties(prefix = "datasource")  //读取配置文件属性  \n   //直接写这个就行，不用在下方代码中进行配置了\n   public DruidDataSource dataSource(){\n       DruidDataSource dataSource=new DruidDataSource();\n       //dataSource.setDriverClassName("com.mysql.jdbc.Driver");  //直接写死\n       return  dataSource;\n   }\n   \n\n * @EnableConfigurationProperties ———— 将对应类加入Spring容器\n   \n   //使用该注解将对应的类加入spring容器 与在该类上直接加@Component是一样的(两者不能同时存在)，但是这个更容易管理\n   @EnableConfigurationProperties({ServletConfig.class})\n   \n   @SpringBootApplication\n   public class ConfigurationApplication {\n   }\n   \n   \n   //@EnableConfigurationProperties 有了这个 就不需要在这里使用  @Component了\n   //老版本会报错，新版本好像不会报错\n   //@Component //加入bean 容器\n   \n   @Data  //自动生成set get tostring 等方法\n   @ConfigurationProperties(prefix = "servers")\n   public class ServletConfig {\n   \n\n# 宽松绑定/松散绑定\n\n * @ConfigurationProperties 支持宽松绑定\n * @Value 不支持\n\n\n\n * 前缀命名\n   \n   \n\n# 常用计量单位绑定\n\n * 时间与空间的计量单位\n   \n   @DurationUnit(ChronoUnit.HOURS)//可以修改默认单位 —— 修改为小时  PT3H\n   private Duration serverTimeout; //默认是毫秒 ——PT0.003S\n   \n   //@DataSizeUnit(DataUnit.MEGABYTES)// 可以修改默认单位 —— 修改为MB  //104857600B=1024*1024*10=100MB\n   private DataSize dataSize;//默认是 B —— 100B\n   \n   \n   serverTimeout: 3\n   #dataSize: 100   #可以使用@DataSizeUnit  从参数定义时指定 单位\n   dataSize: 100MB  #也可以在这里直接指定单位   不需要使用 @DataSizeUnit\n   \n\n# 数据校验\n\n * 引入JR303与Hibernate校验框架坐标\n   \n   \x3c!-- 引入JSR303规范，用作数据校验   算是个接口--\x3e\n   <dependency>\n       <groupId>javax.validation</groupId>\n       <artifactId>validation-api</artifactId>\n   </dependency>\n   \n   \x3c!-- 使用 hibernate 框架提供的校验器做实现类   算是个实现类--\x3e\n   <dependency>\n       <groupId>org.hibernate.validator</groupId>\n       <artifactId>hibernate-validator</artifactId>\n   </dependency>\n   \n\n * 使用@Validated注解启用校验功能\n   \n   //开启对当前Bean的属性注入校验\n   @Validated\n   public class ServletConfig {\n   }\n   \n\n * 使用具体校验规则规范数据校验格式\n   \n   @Max(value = 8888,message = "最大值不能超过8888") //设置具体的规则\n   private int port;\n   @NotEmpty\n   private long timeout;\n   \n   \n   * 通过点击@Max——validation,和@NotEmpty--hibernate进入对应源文件，然后点击源文件最上方的包，可以查看具体的规则有哪些。\n\n# 小bug\n\n * 在yml文件中 ，定义数字，读取结果与预期定义不符合\n   \n   #自定义数据源配置\n   datasource:\n     driverClassName: com.mysql.jdbc.driver1234\n     #password: 0127   #这里当成了8进制数，翻译成了87   #0(1-7) 表示8进制      #0x(1-9,a-f) 表示16进制\n     password: "0127"   #最好用引号包裹，这样 就是0127  不会有进制错误\n   \n   \n   @Value("${datasource.password}")\n   private String password;\n   \n   // 0127——87\n   //"0127"——0127\n   System.out.println(password); \n   \n\n * 所以尽量使用引号来传入字符串\n\n\n# 测试\n\n# 加载测试专用属性\n\nproperties&&args\n\n//使用 properties属性，来增加临时属性   仅在测试文件中生效， 优先级较高，\n@SpringBootTest(properties = {"test.prop=凉冰"})\n\n//使用 args属性，命令行模式 来增加临时属性   仅在测试文件中生效， 优先级较高，\n//这个一般不写  如果写的话就是用来模拟命令行参数  \n@SpringBootTest(args = {"--test.prop=戴安娜"})  \n\n//两个一块有的话， args 优先级高， 属于命令行参数，不过一般不会两个一块出来\n@SpringBootTest(args = {"--test.prop=戴安娜"},properties = {"test.prop=凉冰"})\nclass PropertiesAndArgsTest {\n\n    @Value("${test.prop}")\n    private String msg;\n\n    @Test\n    void testProperties() {\n        System.out.println(msg);\n    }\n}\n\n\n# 记载测试专用配置\n\n * 测试要使用的临时的配置Bean\n\n//这个加载的外部Bean  仅供测试使用  所有要定义在test包下\n@Configuration\npublic class MsgConfig {\n\n    @Bean  //String 类型也可以作为一个Bean\n    public String msg(){\n        return "diana";\n    }\n\n}\n\n\n * 在测试类中，使用@Import导入配置类 ———— 该配置类可以仅供该测试文件使用\n\n//测试---仅在测试文件中生效的---外部Bean的加载\n@SpringBootTest\n@Import({MsgConfig.class})\npublic class ConfigTest {\n\n    @Autowired\n    private String msg;\n\n    @Test\n    void testConfig(){\n        System.out.println(msg);\n\n    }\n}\n\n\n# Web环境模拟测试\n\n * 使用属性webEnvironment开启web测试环境\n   \n   * DEFINED_PORT ——使用自定义的端口\n   * RANDOM_PORT ——使用随机端口\n   * NONE ——不使用web 环境，，默认值\n   * DEFINED_PORT\n   \n   //测试Web 环境  即 表现层 --controller\n   \n   //使用提前定义好的端口 运行web环境\n   @SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.DEFINED_PORT)\n   public class WebTest {\n   \n       @Test\n       void test(){\n   \n       }\n   }\n   \n\n * 模拟请求发送——@AutoConfigureMockMvc开启虚拟MVC调用\n   \n   //测试Web 环境  即 表现层 --controller\n   \n   //使用提前定义好的端口 运行web环境\n   @SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.DEFINED_PORT)\n   //开启虚拟MVC调用\n   @AutoConfigureMockMvc\n   public class WebTest {\n   \n       @Test\n       void testWeb(@Autowired  MockMvc mvc) throws Exception { //可以通过这种方式进行参数注入\n   \n           //创建了一个虚拟请求   当前访问的是 /books\n           //左边是接口  右边是实现类\n           RequestBuilder requestBuilder= MockMvcRequestBuilders.get("/books");\n           //执行对应请求\n           mvc.perform(requestBuilder);\n       }\n   }\n   \n\n * 匹配链接状态\n   \n   @Test //链接状态匹配\n   void testStatus(@Autowired  MockMvc mvc) throws Exception { //可以通过这种方式进行参数注入\n       RequestBuilder requestBuilder= MockMvcRequestBuilders.get("/books1");\n       ResultActions perform = mvc.perform(requestBuilder); //取得 测试值\n       \n       //设置预定值，与真实值进行比较，成功则测试通过，失败则测试失败\n       //定义本次调用的预期值\n       StatusResultMatchers status = MockMvcResultMatchers.status();\n       //预计本次调用是成功的： 状态 200\n       ResultMatcher ok=status.isOk();\n   \n       //添加预计值到本次调用过程中进行匹配\n       perform.andExpect(ok);  \n       //预期:200 ------- ok \n       //实际:404 ------- perform\n       \n   }\n   \n\n * 匹配执行结果 —— 字符串匹配\n   \n   @Test //匹配执行结果———— 字符串匹配\n   void testBody(@Autowired  MockMvc mvc) throws Exception { //可以通过这种方式进行参数注入\n       RequestBuilder requestBuilder= MockMvcRequestBuilders.get("/books");\n       ResultActions perform = mvc.perform(requestBuilder); //取得 测试值\n   \n       //设置预定值，与真实值进行比较，成功则测试通过，失败则测试失败\n       //定义本次调用的预期值 —— 请求体\n       ContentResultMatchers content = MockMvcResultMatchers.content();\n       //预计本次调用的返回结果是 "springBoot"\n       ResultMatcher str = content.string("springBoot2");\n   \n       //添加预计值到本次调用过程中进行匹配\n       perform.andExpect(str);\n   \n       //错误匹配\n       //预期:springBoot2 ------- str\n       //实际:springBoot ------- perform\n   }\n   \n\n * 匹配执行结果 —— json数据匹配\n   \n   @Test //匹配执行结果———— json数据匹配\n   void testBodyJson(@Autowired  MockMvc mvc) throws Exception { //可以通过这种方式进行参数注入\n       RequestBuilder requestBuilder= MockMvcRequestBuilders.post("/books");\n       ResultActions perform = mvc.perform(requestBuilder); //取得 测试值\n   \n       //设置预定值，与真实值进行比较，成功则测试通过，失败则测试失败\n       //定义本次调用的预期值 —— 请求体\n       ContentResultMatchers content = MockMvcResultMatchers.content();\n       //预计本次调用的返回结果是 一个json串（手游--修改为端游）\n       ResultMatcher json = content.json("{\\n" +\n               "    \\"id\\": 1,\\n" +\n               "    \\"name\\": \\"崩坏三\\",\\n" +\n               "    \\"type\\": \\"端游\\",\\n" +\n               "    \\"description\\": \\"好玩\\"\\n" +\n               "}");\n   \n       //添加预计值到本次调用过程中进行匹配\n       perform.andExpect(json);\n       \n       //java.lang.AssertionError: type\n       //Expected: 端游游\n       //     got: 手游\n   }\n   \n\n * 匹配响应头\n   \n   @Test //匹配执行结果———— 响应头\n   void testHeader(@Autowired  MockMvc mvc) throws Exception { //可以通过这种方式进行参数注入\n       RequestBuilder requestBuilder= MockMvcRequestBuilders.post("/books");\n       ResultActions perform = mvc.perform(requestBuilder); //取得 测试值\n   \n       //设置预定值，与真实值进行比较，成功则测试通过，失败则测试失败\n       //定义本次调用的预期值 —— 请求体\n       HeaderResultMatchers header = MockMvcResultMatchers.header();\n       //预计本次调用的返回结果是  application/text（原为：application/json）\n       ResultMatcher string = header.string("Content-type", "application/text");\n   \n       //添加预计值到本次调用过程中进行匹配\n       perform.andExpect(string);\n   \n       //java.lang.AssertionError: Response header \'Content-type\' expected:<application/text> but was:<application/json>\n       //预期:application/text\n       //实际:application/json\n   }\n   \n\n * 完整测试用例\n   \n   @Test //完整测试用例\n   void testsave(@Autowired  MockMvc mvc) throws Exception {\n   \n       //0.发送虚拟请求，并获得返回值\n       RequestBuilder requestBuilder= MockMvcRequestBuilders.post("/books");\n       ResultActions perform = mvc.perform(requestBuilder); //取得 测试值\n   \n   \n       //1.测试连接状态\n       StatusResultMatchers status = MockMvcResultMatchers.status();\n       ResultMatcher ok=status.isOk();\n       perform.andExpect(ok);\n   \n       //2.测试响应头\n       HeaderResultMatchers header = MockMvcResultMatchers.header();\n       ResultMatcher string = header.string("Content-type", "application/json");\n       perform.andExpect(string);\n   \n       //3.测试响应数据\n       ContentResultMatchers content = MockMvcResultMatchers.content();\n       ResultMatcher json = content.json("{\\n" +\n               "    \\"id\\": 1,\\n" +\n               "    \\"name\\": \\"崩坏三\\",\\n" +\n               "    \\"type\\": \\"手游\\",\\n" +\n               "    \\"description\\": \\"好玩\\"\\n" +\n               "}");\n       perform.andExpect(json);\n   \n   }\n   \n\n# 业务层测试回滚\n\n使用maven打包时，会执行测试，测试产生的数据会影响原有的数据库\n\n * 可以提前测试好，然后打包的时候，跳过测试，见文档 ,3种方法\n\n * 使用回滚的方式\n   \n   * 在测试类上开事务，会默认回滚，即 数据不会提交到数据库\n   \n   //数据层测试回滚\n   \n   @SpringBootTest\n   @Transactional //开启事务  在测试类上开事务，会默认回滚，即\n   @Rollback(true)//默认就是true  会回滚，即不提交事务  如果想在测试过程中提交事务，可以设置为false\n   \n   public class Daotest {\n   \n       @Autowired\n       private BookService bookService;\n   \n       @Test\n       void testSave(){\n   \n           Book book=new Book();\n           book.setId(1);\n           book.setName("崩坏三");\n           book.setType("手游");\n           book.setDescription("好玩");\n   \n           bookService.save(book);\n   \n       }\n   }\n   \n\n# 测试用例数据设定\n\n使用随机产生的数据来代替之前写好的测试数据\n\n * 产生随机数据\n   \n   #随机生成数据\n   testcase:\n     book:\n       id: ${random.int}\n       id2: ${random.int(10)} # 指定10以内的int数\n       type: ${random.int(5,10)} #指定5到10之间的数\n       name: diana${random.value}  #前面数据时固定的，后面是随机的   #随机字符串 MD5字符串 32位\n       uuid: ${random.uuid} #随机uuid\n       publishTime: ${random.long}\n   \n\n * 读取测试数据\n   \n   //测试随机数据的产生\n   @SpringBootTest()\n   public class RandomTest {\n       \n       @Autowired\n       private Book2 book;\n       \n       @Test\n       void testProperties() {\n           System.out.println(book);\n       }\n   }\n   \n\n\n# 监控\n\n# 监控基础\n\n * 意义\n   \n   * 监控服务状态是否宕机\n   * 监控服务运行指标（内存，虚拟机，线程，请求）\n   * 监控日志\n   * 监控服务（服务下线）\n\n * 监控方式\n   \n   \n\n# 可视化监控平台\n\n * 配置监控服务端\n   \n   * 导入坐标—— 版本号要尽量与SpringBoot版本号一致\n     \n     <dependency>\n         <groupId>de.codecentric</groupId>\n         <artifactId>spring-boot-admin-starter-server</artifactId>\n         <version>2.6.5</version>\n     </dependency>\n     \n   \n   * 配置yml\n     \n     #配置端口\n     server:\n       port: 8080\n     \n   \n   * 代码\n     \n     * 使用注解 @EnableAdminServer 开启adminserver\n     \n     @SpringBootApplication\n     @EnableAdminServer //开启adminserver\n     public class AdminServerApplication {\n     }\n     \n\n * 将web应用设置成被监控客户端\n   \n   * 引入坐标\n     \n     <dependency>\n         <groupId>de.codecentric</groupId>\n         <artifactId>spring-boot-admin-starter-client</artifactId>\n         <version>2.6.5</version>\n     </dependency>\n     \n   \n   * 配置yml（重要）\n     \n     spring:\n       boot:\n         admin:\n           client:\n             url: http://localhost:8080   #我这个应用要注册在这个server上\n     \n     \n     management:\n       #配置对外是否开放  默认13个都是对外开放的\n       endpoint:\n         health: #默认开放，必须开放\n           show-details: always #总是展示health的细节\n         info:\n           enabled: false #不开放info信息展示—————————— 上面禁止后，即使下面是*，也只有12个端点开放\n     \n     \n       endpoints:\n         #配置在web端能否看到\n         web:\n           exposure:\n             include: "*"  #开放所有——可对外开放的信息——    最多13个端点\n             #include: health,info  #开放2个端点\n     \n   \n   * 代码\n     \n     > 正常书写\n\n# 监控原理\n\n * Actuator提供了SpringBoot生产就绪功能，通过端点的配置与访问，获取端点信息\n * 端点描述了一组监控信息，SpringBoot提供了多个内置端点，也可以根据需要自定义端点信息\n * 访问当前应用所有端点信息：/actuator\n * 访问端点详细信息：/actuator/端点名称——(/actuator/beans)(/actuator/mappings)\n * 比较重要的指标\n   * /actuator/info——显示通知信息——Insights-细节-信息\n   * /actuator/health——显示应用程序的健康信息——Insights-细节-健康\n   * /actuator/loggers——显示和修改应用程序中日志记录器的配置——日志配置\n   * /actuator/metrics——显示当前应用程序的指标度量信息——Insights-性能\n\n# 自定义监控指标\n\n# info端点\n\n——在项目admin_client中\n\n * 自定义信息显示方式一\n   \n   * 配置yml——只能实现一些静态的简单的信息\n     \n     #显示info信息的地方——springboot版本太高不好，显示不出来\n     info:\n       author: diana\n       appName: @artifactId@ #读取pom文件的值 使用 @ @\n     \n\n * 自定义信息显示方式二\n   \n   * 编程实现接口——InfoContributor\n   * 两种放信息的方式，见下方程序\n   \n   public class InfoConfig implements InfoContributor {\n       @Override\n       public void contribute(Info.Builder builder) {\n           //放入一个runtime，后面是值\n           builder.withDetail("runtime",System.currentTimeMillis());\n   \n           //放入一个map集合--要求map格式--Map<String,Object>\n           Map<String,Object> infomap=new HashMap<String,Object>();\n           infomap.put("buildTime","2022");\n           infomap.put("崩坏三","琪亚娜");\n           builder.withDetails(infomap);\n   \n       }\n   }\n   \n\n# health端点\n\n——在项目admin_client中\n\n * 说明\n   \n   * health 展示的是程序内部组件（数据库，redis，……）的工作状态，如果有一个为down，那么整个health为down\n\n * 自定义小组件在health中显示\n   \n   * --继承父类AbstractHealthIndicator\n   \n   \n   \n   \n   \n   public class HealthConfig extends AbstractHealthIndicator {\n   \n       @Override\n       protected void doHealthCheck(Health.Builder builder) throws Exception {\n   \n           boolean condition =false;//对应自己组件的状态信息\n   \n           if(condition){\n               builder.status(Status.UP);//设置为up状态--推荐\n               //builder.up(); 设置为up状态\n   \n               //放入一个map集合--要求map格式--Map<String,Object>\n               Map<String,Object> infomap=new HashMap<String,Object>();\n               infomap.put("buildTime","2022");\n               infomap.put("崩坏三","琪亚娜");\n               builder.withDetails(infomap);\n           }else{\n               builder.status(Status.DOWN);\n               builder.withDetail("上线了么?","你做梦");\n           }\n       }\n   }\n   \n\n# metrics端点\n\n—— 在项目hot_deploy中\n\n * 添加了一项 监控用户删除次数的服务\n * “用户删除次数”为key，counter为值——import io.micrometer.core.instrument.Counter;\n\nprivate Counter counter;\n//提供构造函数\npublic BookServiceImpl(MeterRegistry meterRegistry){\n    counter = meterRegistry.counter("用户删除次数：");\n}\n\npublic boolean delete(int id) {\n    //添加 自定义监控信息---metrics\n    counter.increment(); //自增操作\n    return bookMapper.delete(id)>0;\n}\n\n\n# 自定义端点\n\n\n\n * 说明是自定义端点 @Endpoint(id="diana",enableByDefault = true)\n * 当读取端点时，执行具体方法 @ReadOperation\n\n//添加自定义端点\n@Component\n@Endpoint(id="diana",enableByDefault = true)\npublic class MyPoint {\n\n    @ReadOperation //当读取端点时，执行这个操作\n    public Object getpay(){\n        System.out.println("pay-------------------------");\n\n        Map<String,String> payMap=new HashMap();\n        payMap.put("崩坏三","琪亚娜");\n        payMap.put("英雄联盟","皎月女神");\n        payMap.put("天使","diana");\n\n        return payMap;\n\n    }\n}\n',normalizedContent:'# 热部署\n\n# 热部署基础知识\n\n * 修改完功能后，能立即生效，而不需要重启服务器\n * 关于热部署\n   * 重启(restart): 自定义开发代码，包含类、页面、配置文件等，加载位置restart类加载器\n   * 重载(reload):jar包，加载位置base类加载器\n * 热部署仅仅是重启功能，即仅加载 当前开发者自定义的资源，不重新加载引入的jar包。\n\n# 手动启动热部署\n\n * 开启开发者工具\n   \n   \x3c!-- 热部署 工具--\x3e\n   <dependency>\n      <groupid>org.springframework.boot</groupid>\n      <artifactid>spring-boot-devtools</artifactid>\n   </dependency>\n   \n\n * 激活热部署\n   \n   ctrl+f9 或者 构建中的构建项目。\n\n# 自动启动热部署\n\n * 配置设置\n   \n   \n   \n   \n\n * 当idea 失去焦点5秒后会自动启动热部署\n\n# 热部署范围配置\n\n * 默认热部署的排除文件\n   \n   \n\n * 手动干预\n   \n   devtools:\n     restart:\n       # 设置不参与热部署的文件\n       exclude: application.yml,static/**\n   \n\n# 关闭热部署\n\n * 配置文件关闭————3\n   \n   devtools:\n     restart:\n       # 设置不参与热部署的文件\n       exclude: application.yml,static/**\n       enabled: false  #关闭热部署\n   \n\n * java系统环境设置 关闭 ————5—— 这个优先级要高很多\n   \n   @springbootapplication\n   public class hotdeployapplication {\n   \n      public static void main(string[] args) {\n         //设置高优先级属性禁用热部署   java系统 环境设置 比配置文件优先级要高\n         system.setproperty("spring.devtools.restart.enabled","true");\n         springapplication.run(hotdeployapplication.class);//为了安全\n      }\n   }\n   \n\n * 参数优先级\n   \n   \n\n\n# 配置高级\n\n# @configurationproperties\n\n#自定义配置\nservers:\n  ipadress: 192.168.1.1\n  port: 2345\n  timeout: -1\n\n#自定义数据源配置\ndatasource:\n  driverclassname: com.mysql.jdbc.driver1234\n  \n\n\n * 使用注解设置关联后，代码中只需要设置好set方法，即可自动读取\n\n * 为自定义bean关联配置\n   \n   @data\n   @configurationproperties(prefix = "servers")\n   public class servletconfig {\n       private string ipadress;\n       private int port;\n       private long timeout;\n   }\n   \n\n * 为第三方bean关联配置\n   \n   @bean\n   @configurationproperties(prefix = "datasource")  //读取配置文件属性  \n   //直接写这个就行，不用在下方代码中进行配置了\n   public druiddatasource datasource(){\n       druiddatasource datasource=new druiddatasource();\n       //datasource.setdriverclassname("com.mysql.jdbc.driver");  //直接写死\n       return  datasource;\n   }\n   \n\n * @enableconfigurationproperties ———— 将对应类加入spring容器\n   \n   //使用该注解将对应的类加入spring容器 与在该类上直接加@component是一样的(两者不能同时存在)，但是这个更容易管理\n   @enableconfigurationproperties({servletconfig.class})\n   \n   @springbootapplication\n   public class configurationapplication {\n   }\n   \n   \n   //@enableconfigurationproperties 有了这个 就不需要在这里使用  @component了\n   //老版本会报错，新版本好像不会报错\n   //@component //加入bean 容器\n   \n   @data  //自动生成set get tostring 等方法\n   @configurationproperties(prefix = "servers")\n   public class servletconfig {\n   \n\n# 宽松绑定/松散绑定\n\n * @configurationproperties 支持宽松绑定\n * @value 不支持\n\n\n\n * 前缀命名\n   \n   \n\n# 常用计量单位绑定\n\n * 时间与空间的计量单位\n   \n   @durationunit(chronounit.hours)//可以修改默认单位 —— 修改为小时  pt3h\n   private duration servertimeout; //默认是毫秒 ——pt0.003s\n   \n   //@datasizeunit(dataunit.megabytes)// 可以修改默认单位 —— 修改为mb  //104857600b=1024*1024*10=100mb\n   private datasize datasize;//默认是 b —— 100b\n   \n   \n   servertimeout: 3\n   #datasize: 100   #可以使用@datasizeunit  从参数定义时指定 单位\n   datasize: 100mb  #也可以在这里直接指定单位   不需要使用 @datasizeunit\n   \n\n# 数据校验\n\n * 引入jr303与hibernate校验框架坐标\n   \n   \x3c!-- 引入jsr303规范，用作数据校验   算是个接口--\x3e\n   <dependency>\n       <groupid>javax.validation</groupid>\n       <artifactid>validation-api</artifactid>\n   </dependency>\n   \n   \x3c!-- 使用 hibernate 框架提供的校验器做实现类   算是个实现类--\x3e\n   <dependency>\n       <groupid>org.hibernate.validator</groupid>\n       <artifactid>hibernate-validator</artifactid>\n   </dependency>\n   \n\n * 使用@validated注解启用校验功能\n   \n   //开启对当前bean的属性注入校验\n   @validated\n   public class servletconfig {\n   }\n   \n\n * 使用具体校验规则规范数据校验格式\n   \n   @max(value = 8888,message = "最大值不能超过8888") //设置具体的规则\n   private int port;\n   @notempty\n   private long timeout;\n   \n   \n   * 通过点击@max——validation,和@notempty--hibernate进入对应源文件，然后点击源文件最上方的包，可以查看具体的规则有哪些。\n\n# 小bug\n\n * 在yml文件中 ，定义数字，读取结果与预期定义不符合\n   \n   #自定义数据源配置\n   datasource:\n     driverclassname: com.mysql.jdbc.driver1234\n     #password: 0127   #这里当成了8进制数，翻译成了87   #0(1-7) 表示8进制      #0x(1-9,a-f) 表示16进制\n     password: "0127"   #最好用引号包裹，这样 就是0127  不会有进制错误\n   \n   \n   @value("${datasource.password}")\n   private string password;\n   \n   // 0127——87\n   //"0127"——0127\n   system.out.println(password); \n   \n\n * 所以尽量使用引号来传入字符串\n\n\n# 测试\n\n# 加载测试专用属性\n\nproperties&&args\n\n//使用 properties属性，来增加临时属性   仅在测试文件中生效， 优先级较高，\n@springboottest(properties = {"test.prop=凉冰"})\n\n//使用 args属性，命令行模式 来增加临时属性   仅在测试文件中生效， 优先级较高，\n//这个一般不写  如果写的话就是用来模拟命令行参数  \n@springboottest(args = {"--test.prop=戴安娜"})  \n\n//两个一块有的话， args 优先级高， 属于命令行参数，不过一般不会两个一块出来\n@springboottest(args = {"--test.prop=戴安娜"},properties = {"test.prop=凉冰"})\nclass propertiesandargstest {\n\n    @value("${test.prop}")\n    private string msg;\n\n    @test\n    void testproperties() {\n        system.out.println(msg);\n    }\n}\n\n\n# 记载测试专用配置\n\n * 测试要使用的临时的配置bean\n\n//这个加载的外部bean  仅供测试使用  所有要定义在test包下\n@configuration\npublic class msgconfig {\n\n    @bean  //string 类型也可以作为一个bean\n    public string msg(){\n        return "diana";\n    }\n\n}\n\n\n * 在测试类中，使用@import导入配置类 ———— 该配置类可以仅供该测试文件使用\n\n//测试---仅在测试文件中生效的---外部bean的加载\n@springboottest\n@import({msgconfig.class})\npublic class configtest {\n\n    @autowired\n    private string msg;\n\n    @test\n    void testconfig(){\n        system.out.println(msg);\n\n    }\n}\n\n\n# web环境模拟测试\n\n * 使用属性webenvironment开启web测试环境\n   \n   * defined_port ——使用自定义的端口\n   * random_port ——使用随机端口\n   * none ——不使用web 环境，，默认值\n   * defined_port\n   \n   //测试web 环境  即 表现层 --controller\n   \n   //使用提前定义好的端口 运行web环境\n   @springboottest(webenvironment = springboottest.webenvironment.defined_port)\n   public class webtest {\n   \n       @test\n       void test(){\n   \n       }\n   }\n   \n\n * 模拟请求发送——@autoconfiguremockmvc开启虚拟mvc调用\n   \n   //测试web 环境  即 表现层 --controller\n   \n   //使用提前定义好的端口 运行web环境\n   @springboottest(webenvironment = springboottest.webenvironment.defined_port)\n   //开启虚拟mvc调用\n   @autoconfiguremockmvc\n   public class webtest {\n   \n       @test\n       void testweb(@autowired  mockmvc mvc) throws exception { //可以通过这种方式进行参数注入\n   \n           //创建了一个虚拟请求   当前访问的是 /books\n           //左边是接口  右边是实现类\n           requestbuilder requestbuilder= mockmvcrequestbuilders.get("/books");\n           //执行对应请求\n           mvc.perform(requestbuilder);\n       }\n   }\n   \n\n * 匹配链接状态\n   \n   @test //链接状态匹配\n   void teststatus(@autowired  mockmvc mvc) throws exception { //可以通过这种方式进行参数注入\n       requestbuilder requestbuilder= mockmvcrequestbuilders.get("/books1");\n       resultactions perform = mvc.perform(requestbuilder); //取得 测试值\n       \n       //设置预定值，与真实值进行比较，成功则测试通过，失败则测试失败\n       //定义本次调用的预期值\n       statusresultmatchers status = mockmvcresultmatchers.status();\n       //预计本次调用是成功的： 状态 200\n       resultmatcher ok=status.isok();\n   \n       //添加预计值到本次调用过程中进行匹配\n       perform.andexpect(ok);  \n       //预期:200 ------- ok \n       //实际:404 ------- perform\n       \n   }\n   \n\n * 匹配执行结果 —— 字符串匹配\n   \n   @test //匹配执行结果———— 字符串匹配\n   void testbody(@autowired  mockmvc mvc) throws exception { //可以通过这种方式进行参数注入\n       requestbuilder requestbuilder= mockmvcrequestbuilders.get("/books");\n       resultactions perform = mvc.perform(requestbuilder); //取得 测试值\n   \n       //设置预定值，与真实值进行比较，成功则测试通过，失败则测试失败\n       //定义本次调用的预期值 —— 请求体\n       contentresultmatchers content = mockmvcresultmatchers.content();\n       //预计本次调用的返回结果是 "springboot"\n       resultmatcher str = content.string("springboot2");\n   \n       //添加预计值到本次调用过程中进行匹配\n       perform.andexpect(str);\n   \n       //错误匹配\n       //预期:springboot2 ------- str\n       //实际:springboot ------- perform\n   }\n   \n\n * 匹配执行结果 —— json数据匹配\n   \n   @test //匹配执行结果———— json数据匹配\n   void testbodyjson(@autowired  mockmvc mvc) throws exception { //可以通过这种方式进行参数注入\n       requestbuilder requestbuilder= mockmvcrequestbuilders.post("/books");\n       resultactions perform = mvc.perform(requestbuilder); //取得 测试值\n   \n       //设置预定值，与真实值进行比较，成功则测试通过，失败则测试失败\n       //定义本次调用的预期值 —— 请求体\n       contentresultmatchers content = mockmvcresultmatchers.content();\n       //预计本次调用的返回结果是 一个json串（手游--修改为端游）\n       resultmatcher json = content.json("{\\n" +\n               "    \\"id\\": 1,\\n" +\n               "    \\"name\\": \\"崩坏三\\",\\n" +\n               "    \\"type\\": \\"端游\\",\\n" +\n               "    \\"description\\": \\"好玩\\"\\n" +\n               "}");\n   \n       //添加预计值到本次调用过程中进行匹配\n       perform.andexpect(json);\n       \n       //java.lang.assertionerror: type\n       //expected: 端游游\n       //     got: 手游\n   }\n   \n\n * 匹配响应头\n   \n   @test //匹配执行结果———— 响应头\n   void testheader(@autowired  mockmvc mvc) throws exception { //可以通过这种方式进行参数注入\n       requestbuilder requestbuilder= mockmvcrequestbuilders.post("/books");\n       resultactions perform = mvc.perform(requestbuilder); //取得 测试值\n   \n       //设置预定值，与真实值进行比较，成功则测试通过，失败则测试失败\n       //定义本次调用的预期值 —— 请求体\n       headerresultmatchers header = mockmvcresultmatchers.header();\n       //预计本次调用的返回结果是  application/text（原为：application/json）\n       resultmatcher string = header.string("content-type", "application/text");\n   \n       //添加预计值到本次调用过程中进行匹配\n       perform.andexpect(string);\n   \n       //java.lang.assertionerror: response header \'content-type\' expected:<application/text> but was:<application/json>\n       //预期:application/text\n       //实际:application/json\n   }\n   \n\n * 完整测试用例\n   \n   @test //完整测试用例\n   void testsave(@autowired  mockmvc mvc) throws exception {\n   \n       //0.发送虚拟请求，并获得返回值\n       requestbuilder requestbuilder= mockmvcrequestbuilders.post("/books");\n       resultactions perform = mvc.perform(requestbuilder); //取得 测试值\n   \n   \n       //1.测试连接状态\n       statusresultmatchers status = mockmvcresultmatchers.status();\n       resultmatcher ok=status.isok();\n       perform.andexpect(ok);\n   \n       //2.测试响应头\n       headerresultmatchers header = mockmvcresultmatchers.header();\n       resultmatcher string = header.string("content-type", "application/json");\n       perform.andexpect(string);\n   \n       //3.测试响应数据\n       contentresultmatchers content = mockmvcresultmatchers.content();\n       resultmatcher json = content.json("{\\n" +\n               "    \\"id\\": 1,\\n" +\n               "    \\"name\\": \\"崩坏三\\",\\n" +\n               "    \\"type\\": \\"手游\\",\\n" +\n               "    \\"description\\": \\"好玩\\"\\n" +\n               "}");\n       perform.andexpect(json);\n   \n   }\n   \n\n# 业务层测试回滚\n\n使用maven打包时，会执行测试，测试产生的数据会影响原有的数据库\n\n * 可以提前测试好，然后打包的时候，跳过测试，见文档 ,3种方法\n\n * 使用回滚的方式\n   \n   * 在测试类上开事务，会默认回滚，即 数据不会提交到数据库\n   \n   //数据层测试回滚\n   \n   @springboottest\n   @transactional //开启事务  在测试类上开事务，会默认回滚，即\n   @rollback(true)//默认就是true  会回滚，即不提交事务  如果想在测试过程中提交事务，可以设置为false\n   \n   public class daotest {\n   \n       @autowired\n       private bookservice bookservice;\n   \n       @test\n       void testsave(){\n   \n           book book=new book();\n           book.setid(1);\n           book.setname("崩坏三");\n           book.settype("手游");\n           book.setdescription("好玩");\n   \n           bookservice.save(book);\n   \n       }\n   }\n   \n\n# 测试用例数据设定\n\n使用随机产生的数据来代替之前写好的测试数据\n\n * 产生随机数据\n   \n   #随机生成数据\n   testcase:\n     book:\n       id: ${random.int}\n       id2: ${random.int(10)} # 指定10以内的int数\n       type: ${random.int(5,10)} #指定5到10之间的数\n       name: diana${random.value}  #前面数据时固定的，后面是随机的   #随机字符串 md5字符串 32位\n       uuid: ${random.uuid} #随机uuid\n       publishtime: ${random.long}\n   \n\n * 读取测试数据\n   \n   //测试随机数据的产生\n   @springboottest()\n   public class randomtest {\n       \n       @autowired\n       private book2 book;\n       \n       @test\n       void testproperties() {\n           system.out.println(book);\n       }\n   }\n   \n\n\n# 监控\n\n# 监控基础\n\n * 意义\n   \n   * 监控服务状态是否宕机\n   * 监控服务运行指标（内存，虚拟机，线程，请求）\n   * 监控日志\n   * 监控服务（服务下线）\n\n * 监控方式\n   \n   \n\n# 可视化监控平台\n\n * 配置监控服务端\n   \n   * 导入坐标—— 版本号要尽量与springboot版本号一致\n     \n     <dependency>\n         <groupid>de.codecentric</groupid>\n         <artifactid>spring-boot-admin-starter-server</artifactid>\n         <version>2.6.5</version>\n     </dependency>\n     \n   \n   * 配置yml\n     \n     #配置端口\n     server:\n       port: 8080\n     \n   \n   * 代码\n     \n     * 使用注解 @enableadminserver 开启adminserver\n     \n     @springbootapplication\n     @enableadminserver //开启adminserver\n     public class adminserverapplication {\n     }\n     \n\n * 将web应用设置成被监控客户端\n   \n   * 引入坐标\n     \n     <dependency>\n         <groupid>de.codecentric</groupid>\n         <artifactid>spring-boot-admin-starter-client</artifactid>\n         <version>2.6.5</version>\n     </dependency>\n     \n   \n   * 配置yml（重要）\n     \n     spring:\n       boot:\n         admin:\n           client:\n             url: http://localhost:8080   #我这个应用要注册在这个server上\n     \n     \n     management:\n       #配置对外是否开放  默认13个都是对外开放的\n       endpoint:\n         health: #默认开放，必须开放\n           show-details: always #总是展示health的细节\n         info:\n           enabled: false #不开放info信息展示—————————— 上面禁止后，即使下面是*，也只有12个端点开放\n     \n     \n       endpoints:\n         #配置在web端能否看到\n         web:\n           exposure:\n             include: "*"  #开放所有——可对外开放的信息——    最多13个端点\n             #include: health,info  #开放2个端点\n     \n   \n   * 代码\n     \n     > 正常书写\n\n# 监控原理\n\n * actuator提供了springboot生产就绪功能，通过端点的配置与访问，获取端点信息\n * 端点描述了一组监控信息，springboot提供了多个内置端点，也可以根据需要自定义端点信息\n * 访问当前应用所有端点信息：/actuator\n * 访问端点详细信息：/actuator/端点名称——(/actuator/beans)(/actuator/mappings)\n * 比较重要的指标\n   * /actuator/info——显示通知信息——insights-细节-信息\n   * /actuator/health——显示应用程序的健康信息——insights-细节-健康\n   * /actuator/loggers——显示和修改应用程序中日志记录器的配置——日志配置\n   * /actuator/metrics——显示当前应用程序的指标度量信息——insights-性能\n\n# 自定义监控指标\n\n# info端点\n\n——在项目admin_client中\n\n * 自定义信息显示方式一\n   \n   * 配置yml——只能实现一些静态的简单的信息\n     \n     #显示info信息的地方——springboot版本太高不好，显示不出来\n     info:\n       author: diana\n       appname: @artifactid@ #读取pom文件的值 使用 @ @\n     \n\n * 自定义信息显示方式二\n   \n   * 编程实现接口——infocontributor\n   * 两种放信息的方式，见下方程序\n   \n   public class infoconfig implements infocontributor {\n       @override\n       public void contribute(info.builder builder) {\n           //放入一个runtime，后面是值\n           builder.withdetail("runtime",system.currenttimemillis());\n   \n           //放入一个map集合--要求map格式--map<string,object>\n           map<string,object> infomap=new hashmap<string,object>();\n           infomap.put("buildtime","2022");\n           infomap.put("崩坏三","琪亚娜");\n           builder.withdetails(infomap);\n   \n       }\n   }\n   \n\n# health端点\n\n——在项目admin_client中\n\n * 说明\n   \n   * health 展示的是程序内部组件（数据库，redis，……）的工作状态，如果有一个为down，那么整个health为down\n\n * 自定义小组件在health中显示\n   \n   * --继承父类abstracthealthindicator\n   \n   \n   \n   \n   \n   public class healthconfig extends abstracthealthindicator {\n   \n       @override\n       protected void dohealthcheck(health.builder builder) throws exception {\n   \n           boolean condition =false;//对应自己组件的状态信息\n   \n           if(condition){\n               builder.status(status.up);//设置为up状态--推荐\n               //builder.up(); 设置为up状态\n   \n               //放入一个map集合--要求map格式--map<string,object>\n               map<string,object> infomap=new hashmap<string,object>();\n               infomap.put("buildtime","2022");\n               infomap.put("崩坏三","琪亚娜");\n               builder.withdetails(infomap);\n           }else{\n               builder.status(status.down);\n               builder.withdetail("上线了么?","你做梦");\n           }\n       }\n   }\n   \n\n# metrics端点\n\n—— 在项目hot_deploy中\n\n * 添加了一项 监控用户删除次数的服务\n * “用户删除次数”为key，counter为值——import io.micrometer.core.instrument.counter;\n\nprivate counter counter;\n//提供构造函数\npublic bookserviceimpl(meterregistry meterregistry){\n    counter = meterregistry.counter("用户删除次数：");\n}\n\npublic boolean delete(int id) {\n    //添加 自定义监控信息---metrics\n    counter.increment(); //自增操作\n    return bookmapper.delete(id)>0;\n}\n\n\n# 自定义端点\n\n\n\n * 说明是自定义端点 @endpoint(id="diana",enablebydefault = true)\n * 当读取端点时，执行具体方法 @readoperation\n\n//添加自定义端点\n@component\n@endpoint(id="diana",enablebydefault = true)\npublic class mypoint {\n\n    @readoperation //当读取端点时，执行这个操作\n    public object getpay(){\n        system.out.println("pay-------------------------");\n\n        map<string,string> paymap=new hashmap();\n        paymap.put("崩坏三","琪亚娜");\n        paymap.put("英雄联盟","皎月女神");\n        paymap.put("天使","diana");\n\n        return paymap;\n\n    }\n}\n',charsets:{cjk:!0}},{title:"数据层解决方案",frontmatter:{title:"数据层解决方案",date:"2023-07-03T21:20:29.000Z",permalink:"/pages/0446de/",categories:["后端","SpringBoot"],tags:["知识","SpringBoot"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/50.SpringBoot/15.%E6%95%B0%E6%8D%AE%E5%B1%82%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.html",relativePath:"01.后端/50.SpringBoot/15.数据层解决方案.md",key:"v-3df805f8",path:"/pages/0446de/",headers:[{level:2,title:"SQL",slug:"sql",normalizedTitle:"sql",charIndex:2},{level:3,title:"数据层解决方案技术选型",slug:"数据层解决方案技术选型",normalizedTitle:"数据层解决方案技术选型",charIndex:73},{level:3,title:"数据源配置",slug:"数据源配置",normalizedTitle:"数据源配置",charIndex:232},{level:3,title:"持久化技术",slug:"持久化技术",normalizedTitle:"持久化技术",charIndex:145},{level:3,title:"数据库",slug:"数据库",normalizedTitle:"数据库",charIndex:17},{level:2,title:"NoSQL",slug:"nosql",normalizedTitle:"nosql",charIndex:2843},{level:3,title:"Redis",slug:"redis",normalizedTitle:"redis",charIndex:2853},{level:3,title:"Mongo",slug:"mongo",normalizedTitle:"mongo",charIndex:11394},{level:3,title:"ES",slug:"es",normalizedTitle:"es",charIndex:13241}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688392125e3,headersStr:"SQL 数据层解决方案技术选型 数据源配置 持久化技术 数据库 NoSQL Redis Mongo ES",content:'# SQL\n\n**sql 是关系型数据库，可以操作结构化数据，但是访问速度不是很快 **\n\n**mysql 默认端口号： 8080 **\n\n\n# 数据层解决方案技术选型\n\nDruid+MyBatis-Plus+MySQL\n\n * 数据源\n   \n   * Druid+Hikari\n\n * 持久化技术\n   \n   * MyBatis-Plus/MyBatis+JdbcTemplate\n\n * 数据库\n   \n   * MySQL+H2\n   \n   \n\n\n# 数据源配置\n\n * 引入Druid的三种方式\n   \n   * 直接引入Druid\n     \n     <dependency>\n        <groupId>com.alibaba</groupId>\n        <artifactId>druid</artifactId>\n        <version>1.2.9</version>\n     </dependency>\n     \n     \n     spring:\n       datasource:\n         driver-class-name: com.mysql.cj.jdbc.Driver\n         url: jdbc:mysql:///test?useSSL=false&useServerPrepStmts=true\n         username: root\n         password: 1234\n         type: com.alibaba.druid.pool.DruidDataSource\n     \n   \n   * 引入Druid-starter\n     \n     <dependency>\n        <groupId>com.alibaba</groupId>\n        <artifactId>druid-spring-boot-starter</artifactId>\n        <version>1.2.6</version>\n     </dependency>\n     \n     \n     #引入Druid-starter\n     spring:\n       datasource:\n         druid:\n             driver-class-name: com.mysql.cj.jdbc.Driver\n             url: jdbc:mysql:///test?useSSL=false&useServerPrepStmts=true\n             username: root\n             password: 1234\n     \n   \n   * 使用自动配置\n     \n     只要引入了Druid的坐标，SpirngBoot会自动配置Druid数据源，在yml文件中不需要体现出任何Druid的信息，也可以装配上\n\n * 不引入Druid，使用内置数据源\n   \n   * HikariCP————轻量级的，速度很快，可以考虑\n     \n     > SpringBoot默认内置数据源对象———— 最佳轻量级数据库\n     > \n     > com.zaxxer.hikari.HikariDataSource\n   \n   * Tomcat 提供DataSource\n     \n     > 在HikariCP不可用的情况下，且在web环境中，可以使用tomcat服务器内置的数据源对象\n   \n   * Commons DBCP\n     \n     > 一般不会用这个，除非上面两个用不了\n\n * 配置代码\n   \n   通用配置最好写在外面，让数据源自己去读即可\n   \n   HikariCP 的url写在里面会报错，其他三个不会报错，所以尽量把通用配置写在最外面即可\n   \n   #使用默认的 HikariCP  不导入任何 druid坐标\n   spring:\n     datasource:\n       driver-class-name: com.mysql.cj.jdbc.Driver\n       url: jdbc:mysql:///test?useSSL=false&useServerPrepStmts=true\n       username: root\n       password: 1234\n       hikari:\n         maximum-pool-size: 50 \n   \n\n\n# 持久化技术\n\nJdbcTemplate\n\n\n\n\n# 数据库\n\n * 内嵌数据库—— 内存级数据库，方便小巧，可用来测试\n   \n   * H2 ————测试的时候 快很多，可以考虑使用，但是不适合上线\n     \n     * 导入坐标\n       \n       \x3c!-- h2 数据库--\x3e\n       <dependency>\n          <groupId>com.h2database</groupId>\n          <artifactId>h2</artifactId>\n       </dependency>\n       <dependency>\n          <groupId>org.springframework.boot</groupId>\n          <artifactId>spring-boot-starter-data-jpa</artifactId>\n       </dependency>\n       \n     \n     * yml配置\n       \n       在上线时，一定记得吧 enabled属性设置为false\n       \n       loacalhost/h2 即可访问，进入h2数据库操作页面\n       \n       #h2 数据库\n       spring:\n         h2:\n           console:\n             enabled: true\n             path: /h2\n       \n         datasource:\n           driver-class-name: org.h2.Driver\n           url: jdbc:h2:~/test\n           username: sa\n           password: 123456\n       \n     \n     * 测试的时候，跟使用mysql一模一样\n   \n   * HSQL\n   \n   * Derby\n\n\n# NoSQL\n\n\n# Redis\n\nRedis 是一款 key-value 存储结构的 内存级 NoSQL 数据库，访问速度极快\n\n * 命令行\n   \n   * 端口号：6379\n   * 启动服务\n   \n   \n   \n   * 运行指令\n     \n     * set key value ——》 一个key 对 应一个value\n     * set keys key value——》 一个keys 里面存有多个key-value键值对\n     \n     \n\n * redis 基础\n   \n   * 数据类型\n   \n   \n   \n   * redis中文网\n   \n   * 基本命令——String\n     \n     * set key value——设置指定key的值\n     \n     * get key——获取指定key的值\n     \n     * setex key seconds value——设置指定key的值，并将key的过期时间设置为seconds秒\n     \n     * setnx key value——只有在key不存在时设置key，用于分布式锁\n   \n   * 基本命令——Hash\n     \n     * hset key field value——将哈希表key中的字段field的值设为value\n   \n   * hget key field——获取存储在哈希表中指定字段的值\n     \n     * hdel key field——删除存储在hash表中的指定字段\n     * hkeys key——获取哈希表中所有字段\n     * hvals key——获取哈希表中所有值\n   \n   * hgetall key——获取在哈希表中指定key的所有字段和值\n   \n   * 基本命令——List（顺序，string类型）（可重复）\n     \n     * lpush key value[value2]——将一个或多个值插入到列表头部\n     \n     * lrange key start stop——获取列表指定范围内的元素\n       \n       * lrange key 0 -1——获取全部元素\n     \n     * rpop key——移除并获取列表最后一个元素\n     \n     * llen key——获取列表长度\n     \n     * brpop key1 [key2] timeout——移出并获取最后一个元素，如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止\n   \n   * 基本命令——Set（String）（唯一，不能重复）\n     \n     * sadd key member1 [member2]——向集合添加一个或多个成员\n     \n     * smembers key——返回集合中的所有成员\n     \n     * scard key——获取集合的成员数\n     \n     * sinter key1 [key2]——返回给定所有集合的交集\n     \n     * sunion key [key2]——返回给定所有集合的并集\n     \n     * sdiff key1 [key2]——返回给定所有集合的差集（key1-key2）\n     \n     * srem key member1 [member2]——移除集合中一个或多个成员\n   \n   * 基本命令——sorted set（String）（有序）（唯一，不重复）\n     \n     * 每个元素都会关联一个double类型分数（score），通过分数对集合中的成员进行从小到大的排序，成员唯一，分数可重复\n     \n     * zadd key score1 member1 [score2 member2]——向有序集合中添加一个或多个成员，或者更新已存在成员的分数\n     \n     * zrange key start stop [withscores]——通过索引区间返回有序集合中指定区间内的成员（携带分数返回）\n     \n     * zincrby key increment member——有序集合中对指定成员的分数加上增量increment(一个数，原分数+增量=现分数)\n       \n       * zincrby user 20 b\n     \n     * zrem key member [member]——移除有序集合中的一个或多个成员\n   \n   * 通用命令\n     \n     * keys pattern——查找所有复合给定模式（pattern）的可以\n       * keys *——得到所有key\n     * exists key——检测给定key是否存在\n     * type key——返回key所存储值的类型\n     * ttl key——返回给定key的剩余时间（time to live） ，以秒为单位\n     * del key——该命令用于在可以存在时删除key\n     * select 1——选择1号数据库\n\n * SpringBoot整合Redis\n   \n   * 导入坐标\n     \n     \x3c!-- 引入redis--\x3e\n     <dependency>\n         <groupId>org.springframework.boot</groupId>\n         <artifactId>spring-boot-starter-data-redis</artifactId>\n     </dependency>\n     \n   \n   * 配置yml\n     \n     #配置redis\n     spring:\n       redis:\n         host: localhost\n         port: 6379\n     \n   \n   * 提供操作Redis接口对象RedisTemplate\n     \n     * 先使用opsFor-xx获取操作类型，然后在进行读取操作\n     * \n     * 对象型模板————以这种方式存入的数据库，在命令行窗口直接读取不出来\n     \n     //需要先启动redis服务\n     //测试 Redis    ------ RedisTemplate 传入的是对象\n     //以这种方式存入的数据库，在命令行窗口直接读取不出来\n     \n     @SpringBootTest\n     class RedisTest {\n     \n         @Autowired\n         private RedisTemplate redisTemplate;\n     \n         @Test//普通值\n         void set() {\n             ValueOperations ops = redisTemplate.opsForValue();\n             ops.set("age",41);\n         }\n         @Test//普通值\n         void get() {\n             ValueOperations ops = redisTemplate.opsForValue();\n             System.out.println(ops.get("age"));\n         }\n     \n         \n         @Test//哈希值\n         void hset() {\n             HashOperations ops = redisTemplate.opsForHash();\n             ops.put("info","value","diana");\n             ops.put("info","name","diane");\n             ops.put("info","chinese","凉冰");\n         }\n         @Test//哈希值\n         void hget() {\n             HashOperations ops = redisTemplate.opsForHash();\n             System.out.println(ops.get("info","value"));\n             System.out.println(ops.get("info","chinese"));\n         }\n     }\n     \n     \n     * 字符串型模板 ————常用————在命令行窗口可以直接读取出来\n     \n      //需要先启动redis服务\n      //测试 Redis    ------ StringRedisTemplate 传入的是字符串\n      //以这种方式存入的数据库，在命令行窗口可以直接读取出来 —————— 常用\n      \n      @SpringBootTest\n      class RedisTest2 {\n      \n          @Autowired\n          private StringRedisTemplate stringRedisTemplate;\n      \n          @Test//普通值\n          void set() {\n              ValueOperations<String,String> ops = stringRedisTemplate.opsForValue();\n              ops.set("age","41");\n          }\n          @Test//普通值\n          void get() {\n              ValueOperations<String,String> ops = stringRedisTemplate.opsForValue();\n              System.out.println(ops.get("k2"));\n          }\n      \n          @Test//哈希值\n          void hset() {\n              HashOperations<String,String,String> ops = stringRedisTemplate.opsForHash();\n              ops.put("info","value","diana");\n              ops.put("info","name","diane");\n              ops.put("info","chinese","凉冰");\n          }\n          @Test//哈希值\n          void hget() {\n              HashOperations<String,String,String> ops = stringRedisTemplate.opsForHash();\n              System.out.println(ops.get("info","value"));\n              System.out.println(ops.get("info","chinese"));\n          }\n          /**\n          * 操作List类型的数据\n          */\n         @Test\n         public void testList(){\n             ListOperations<String, String> listOperations = stringRedisTemplate.opsForList();\n             //存值\n             listOperations.leftPush("mylist","a");//存一个\n             listOperations.leftPushAll("mylist","b","c","d");//存一堆\n             //取值(东西还在队列里)\n             List<String> mylist = listOperations.range("mylist", 0, -1);\n             for (String s : mylist) {\n                 System.out.println(s);\n             }\n             //获取列表长度\n             Long size = listOperations.size("mylist");\n             int isize = size.intValue();\n             for (int i=0;i<isize;i++){\n                 //出队列\n                 listOperations.rightPop("mylist");//出一个\n             }\n     \n     \n         }\n     \n         /**\n          * 操作Set类型的数据 ——不能有重复值\n          */\n         @Test\n         public void testSet(){\n             SetOperations<String, String> setOperations = stringRedisTemplate.opsForSet();\n             //存值\n             setOperations.add("myset","diana","diane");\n             //取值\n             Set<String> myset = setOperations.members("myset");\n             for (String s : myset) {\n                 System.out.println(s);\n             }\n             //删除\n             setOperations.remove("myset","diana");\n         }\n     \n     \n         /**\n          * 操作Zset类型  sorted set\n          */\n         @Test\n         public void testZset(){\n             ZSetOperations<String, String> zSet = stringRedisTemplate.opsForZSet();\n             //存值\n             zSet.add("myzset","a",100);\n             zSet.add("myzset","b",90);\n             zSet.add("myzset","c",80);\n             zSet.add("myzset","d",70);\n             //取值\n             Set<String> myzset = zSet.range("myzset", 0, -1);\n             for (String s : myzset) {\n                 System.out.println(s);\n             }\n             //修改分数(增量)\n             zSet.incrementScore("myzset","b",10);\n             //删除\n             zSet.remove("myzset","a","c");\n             //取值\n             Set<String> myzset1 = zSet.range("myzset", 0, -1);\n             for (String s : myzset1) {\n                 System.out.println(s);\n             }\n     \n         }\n     \n     \n         /**\n          * 通用操作\n          */\n         @Test\n         public void testCommon(){\n             //获取redis中的所有key\n             Set<String> keys = stringRedisTemplate.keys("*");\n             for (String key : keys) {\n                 System.out.println(key);\n             }\n     \n             //判断某个key是否存在\n             Boolean aBoolean = stringRedisTemplate.hasKey("mylist");\n             System.out.println(aBoolean);\n     \n             //删除指定key\n             stringRedisTemplate.delete("mylist");\n     \n             //获取指定key对应的value的数据类型\n             DataType dataType = stringRedisTemplate.type("myset");\n             System.out.println(dataType.name());\n         }   \n      \n      }\n     \n\n * Redis客户端方式选择 ——两种方式的操作API是一样的\n   \n   * lettuce (默认)\n   \n   * jedis\n     \n     * 导入坐标\n       \n       \x3c!-- 引入jedis--\x3e\n       <dependency>\n           <groupId>redis.clients</groupId>\n           <artifactId>jedis</artifactId>\n       </dependency>\n       \n     \n     * 配置\n       \n       #配置redis\n       spring:\n         redis:\n           host: localhost\n           port: 6379\n           client-type:  jedis   #lettuce 默认  #两种模式操作API是一样的\n           #配置  lettuce\n           lettuce:\n             pool:\n               max-active: 16\n           #配置  jedis\n           jedis:\n             pool:\n               max-active: 8\n       \n\n * 二者区别\n   \n   \n\n\n# Mongo\n\nMongo 介于SQL 与Redis之间，既可以操作结构性数据，访问速度也很快\n\n * 应用场景\n\n * 命令行\n   \n   * 端口号 27017\n   \n   * 启动服务\n     \n     * mongod --dbpath=..\\data\\db\n   \n   * 执行客户端\n     \n     * mongo\n   \n   ![mongo 启动](/assets/后端/springboot/mongo 启动.png)\n\n * 基本操作\n   \n   * Robo 3T 1.4 —— 可视化操作客户端\n   \n   * 基本的增删改查\n     \n     //添加数据(文档)  数据类型可以不统一\n     db.book.save({"name":"SpringBoot"})\n     db.book.save({"name":"SpringBoot","type":"code"})\n     \n     //删除操作\n     db.book.remove({type:"code"})\n     \n     //修改单个操作  只修改满足条件的第一条数据    第一个{}里是条件，第二个{}里是要修改的值\n     db.book.update({name:"SpringBoot22"},{$set:{name:"SpringBoot"}})\n     //修改多个操作  可以修改所有满足条件的数据\n     db.book.updateMany({name:"SpringBoot"},{$set:{name:"SpringBoot22"}})\n     \n     //查询所有\n     db.book.find()\n     //按条件查询\n     db.book.find({type:"code"})\n     \n   \n   * 常用命令\n     \n     \n\n * SpringBoot整合Mongo\n   \n   * 导入坐标\n     \n     \x3c!-- 引入mongo--\x3e\n     <dependency>\n         <groupId>org.springframework.boot</groupId>\n         <artifactId>spring-boot-starter-data-mongodb</artifactId>\n     </dependency>\n     \n   \n   * 配置文件\n     \n     #配置mongo\n     spring:\n       data:\n         mongodb:\n           uri: mongodb://localhost/diana\n     \n   \n   * 测试———— mongoTemplate 每一个操作，都对应一个接口\n     \n     //需要先启动mongo服务\n     //测试 Mongo\n     @SpringBootTest\n     class MongoTest {\n         \n         @Autowired\n         private MongoTemplate mongoTemplate;\n         \n         @Test  //插入\n         void testSave(){\n             Book book=new Book();\n             book.setId(2);\n             book.setName("崩坏三");\n             book.setType("手游");\n             book.setDescription("好玩");\n             mongoTemplate.save(book);\n         }\n     \n         @Test  //查询所有\n         void testFindAll(){\n             List<Book> all = mongoTemplate.findAll(Book.class);\n             System.out.println(all);\n         }\n     \n     }\n     \n\n\n# ES\n\nElasticsearch 是一个分布式全文搜索引擎；；；；全文搜索—— 牺牲空间，换取时间\n\n * 全文搜索 概念\n   \n   * 索引 ——id就是一个索引，根据id找数据的过程称为索引—— 红框的右半部分\n   * 倒排索引—— 根据内容（Spring）找到id的过程称为倒排索引—— 红框的左半部分\n   * 创建文档——根据spring-1-1xx（即id为1的部分数据） 创建一个文档 （实战-1-1xx）\n   * 使用文档—— 根据文档去查出所有数据的过程\n\n * 命令行\n   \n   * 端口号 9200\n   * 直接双击 elasticsearch.bat，开启服务\n\n * 创建索引\n   \n   使用postman给es创建索引，全面支持restful\n   \n   * 使用get请求,得到索引 http://localhost:9200/books ——GET\n   \n   * 使用delete请求，删除索引 http://localhost:9200/books ——DELETE\n   \n   * 使用put请求，带上下面的json字符串，创建索引 http://localhost:9200/books——PUT\n     \n     * type\n       * keyword: 作为被查询的关键字\n       * text: 普通文本\n     * analyzer —— ik_max_word\n       * 表示使用ik分词器，es文件夹要存放在不带空格的目录下，要不然插件用不了，会报错\n     * copy_to\n       * 表示将搜索内容传给 all一份\n     * all\n       * all 作为一个虚拟的属性，用来保证可以同时查询name和description ，和copy_to 配合使用\n   \n   {\n       "mappings":{\n           "properties":{\n               "id":{\n                   "type":"keyword"\n               },\n               "name":{\n                   "type":"text",\n                   "analyzer":"ik_max_word",\n                   "copy_to":"all"\n               },\n               "type":{\n                   "type":"keyword"\n               },\n               "description":{\n                   "type":"text",\n                   "analyzer":"ik_max_word",\n                   "copy_to":"all"\n               },\n               "all":{\n                   "type":"text",\n                   "analyzer":"ik_max_word"\n               }\n           }\n       }\n   }\n   \n\n * 创建文档\n   \n   * POST http://localhost:9200/books/_doc ——使用系统生成id\n   * POST http://localhost:9200/books/_doc/10 ——使用指定id，不存在则创建，存在则修改\n   * POST http://localhost:9200/books/_create/1 ——使用系统生成id ，只能创建\n\n * 删除文档\n   \n   * DELETE http://localhost:9200/books/_doc/1 ——按照id删除\n\n * 更新文档\n   \n   * PUT http://localhost:9200/books/_doc/1 ——全覆盖修改，提交的json数据会全面覆盖原来的数据\n     \n     {\n         "name":"springBoot very good",\n         "type":"diana",\n         "description":"pringBoot2 very good"\n     }\n     \n   \n   * POST http://localhost:9200/books/_update/1 ——修改单个属性 /_doc/id 也可以\n     \n     {\n         "doc":{\n             "name":"springBoot"\n         }\n     }\n     \n\n * 查询文档\n   \n   * GET http://localhost:9200/books/_search ——查询全部\n   * GET http://localhost:9200/books/_doc/1 ——按照id查询\n   * GET http://localhost:9200/books/_search?q=description:good ——按照参数值进行条件查询\n\n * SpringBoot整合es\n   \n   * 引入坐标\n     \n     \x3c!-- 引入es high-level--\x3e\n     <dependency>\n         <groupId>org.elasticsearch.client</groupId>\n         <artifactId>elasticsearch-rest-high-level-client</artifactId>\n     </dependency>\n     \n   \n   * 配置yml\n     \n     SpringBoot没有整合，所以不能直接在这里配置了\n   \n   * 创建简单索引\n     \n     要手动创建对象，手动关闭\n     \n     * 写在一起 不推荐\n     \n     @Test //使用es高版本客户端，因为springboot没有整合，所以 要使用硬编码的方式 来进行配置\n     void testCreateIndex() throws IOException {\n         //创建客户端\n         HttpHost host=HttpHost.create("http://localhost:9200");\n         RestClientBuilder builder= RestClient.builder(host);\n         client=new RestHighLevelClient(builder);\n     \n         //创建索引\n         CreateIndexRequest request=new CreateIndexRequest("diana");\n         client.indices().create(request, RequestOptions.DEFAULT);\n     \n         client.close();\n     }\n     \n     \n     * 使用AOP思想，实现，前置和后置方法 ————》推荐\n     \n     private RestHighLevelClient client;\n     \n     @BeforeEach //所有方法执行前 --AOP思想\n     void setUp() {\n         //创建客户端\n         HttpHost host=HttpHost.create("http://localhost:9200");\n         RestClientBuilder builder= RestClient.builder(host);\n         client=new RestHighLevelClient(builder);\n     }\n     \n     @AfterEach //所有方法执行后  --AOP思想\n     void tearDown() throws IOException {\n         //关闭客户端\n         client.close();\n     }\n     \n     @Test //使用es高版本客户端，因为springboot没有整合，所以 要使用硬编码的方式 来进行配置\n     void testCreateIndex() throws IOException {\n         //创建客户端\n         //创建索引\n         CreateIndexRequest request=new CreateIndexRequest("users");\n         \n         client.indices().create(request, RequestOptions.DEFAULT);\n         //关闭客户端\n     }\n     \n   \n   * 创建带参索引\n     \n     * request.source(json, XContentType.JSON);//前面是参数，后面是类型\n     \n     @Test //带上配置创建索引\n     void testCreateIndexJson() throws IOException {\n         //创建客户端\n         String json="{\\n" +\n                 "    \\"mappings\\":{\\n" +\n                 "        \\"properties\\":{\\n" +\n                 "            \\"id\\":{\\n" +\n                 "                \\"type\\":\\"keyword\\"\\n" +\n                 "            },\\n" +\n                 "            \\"name\\":{\\n" +\n                 "                \\"type\\":\\"text\\",\\n" +\n                 "                \\"analyzer\\":\\"ik_max_word\\",\\n" +\n                 "                \\"copy_to\\":\\"all\\"\\n" +\n                 "            },\\n" +\n                 "            \\"type\\":{\\n" +\n                 "                \\"type\\":\\"keyword\\"\\n" +\n                 "            },\\n" +\n                 "            \\"description\\":{\\n" +\n                 "                \\"type\\":\\"text\\",\\n" +\n                 "                \\"analyzer\\":\\"ik_max_word\\",\\n" +\n                 "                \\"copy_to\\":\\"all\\"\\n" +\n                 "            },\\n" +\n                 "            \\"all\\":{\\n" +\n                 "                \\"type\\":\\"text\\",\\n" +\n                 "                \\"analyzer\\":\\"ik_max_word\\"\\n" +\n                 "            }\\n" +\n                 "        }\\n" +\n                 "    }\\n" +\n                 "}";\n     \n         //创建索引\n         CreateIndexRequest request=new CreateIndexRequest("books");\n         \n         request.source(json, XContentType.JSON);//前面是参数，后面是类型\n         \n         client.indices().create(request, RequestOptions.DEFAULT);\n         //关闭客户端\n     }\n     \n   \n   * 创建单条文档\n     \n     @Test //创建一条文档\n     void testCreateOneDoc() throws IOException {\n         Book book = mapper.selectById(3);\n         String id=book.getId().toString(); //获取id   Integer 类型 的数据 有toString 方法  因为他是个对象\n         String json = JSON.toJSONString(book); //获取json数据\n     \n         //创建请求   //   /users/_doc/1\n         IndexRequest request=new IndexRequest("books").id(id);\n         //携带参数\n         request.source(json,XContentType.JSON);\n         // 创建文档\n         client.index(request,RequestOptions.DEFAULT);\n     }\n     \n   \n   * 批量创建所有文档\n     \n     * BulkRequest bulkRequest=new BulkRequest();——创建批处理请求容器\n     * bulkRequest.add(request); ———— 将单个请求加入容器\n     \n     @Test //创建所有数据的文档\n     void testCreateAllDoc() throws IOException {\n         List<Book> books = mapper.selectList(null);\n         //创建批处理请求容器\n         BulkRequest bulkRequest=new BulkRequest();\n     \n         for(Book book:books){\n             String id=book.getId().toString(); //获取id   Integer 类型 的数据 有toString 方法  因为他是个对象\n             String json = JSON.toJSONString(book); //获取json数据\n     \n             //创建请求   //   /users/_doc/1\n             IndexRequest request=new IndexRequest("books").id(id);\n             //携带参数\n             request.source(json,XContentType.JSON);\n             //将请求加入容器\n             bulkRequest.add(request);\n     \n         }\n         //创建批量文档   一次性完成多个请求\n         client.bulk(bulkRequest,RequestOptions.DEFAULT);\n     }\n     \n   \n   * 按id查询文档\n     \n     @Test //按id查询\n     void testGetById() throws IOException {\n         GetRequest request= new GetRequest("books","2");//索引名称，id\n         GetResponse res = client.get(request, RequestOptions.DEFAULT);\n         //得到返回数据中的source————里面存储的是数据信息\n         String json = res.getSourceAsString(); \n         System.out.println(json);\n     \n     }\n     \n   \n   * 按条件查询文档\n     \n     @Test //按条件查询\n     void testGetByCon() throws IOException {\n     \n         SearchRequest request= new SearchRequest("books");\n         SearchSourceBuilder builder=new SearchSourceBuilder();\n         //插入查询条件\n         builder.query(QueryBuilders.termQuery("name","丽"));//查找name中包含丽字的词条\n         request.source(builder);\n         SearchResponse response = client.search(request, RequestOptions.DEFAULT);\n     \n     \n         //查询得到所有的hits  命中\n         SearchHits hits = response.getHits();\n         //遍历得到的所有hits，从中取出Source 并转换成字符串\n         for(SearchHit hit:hits){\n             String json = hit.getSourceAsString();\n             Book book = JSON.parseObject(json, Book.class);\n             System.out.println(book);\n         }\n     \n     }\n     \n   \n   * 更新文档（修改单个属性）\n     \n     @Test //更新文档———— 修改单个属性\n     void testUpdateAll() throws IOException {\n         UpdateRequest request=new UpdateRequest("books","3");\n         String json=" {\\n" +\n                 "        \\"name\\":\\"diane\\"\\n" +\n                 "    }";\n         request.doc(json, XContentType.JSON);\n         client.update(request, RequestOptions.DEFAULT);\n     }\n     \n   \n   * 删除文档\n     \n     @Test //删除文档\n     void testDelete() throws IOException {\n     \n         DeleteRequest request=new DeleteRequest("books","4");\n         client.delete(request, RequestOptions.DEFAULT);\n     \n     }\n     ',normalizedContent:'# sql\n\n**sql 是关系型数据库，可以操作结构化数据，但是访问速度不是很快 **\n\n**mysql 默认端口号： 8080 **\n\n\n# 数据层解决方案技术选型\n\ndruid+mybatis-plus+mysql\n\n * 数据源\n   \n   * druid+hikari\n\n * 持久化技术\n   \n   * mybatis-plus/mybatis+jdbctemplate\n\n * 数据库\n   \n   * mysql+h2\n   \n   \n\n\n# 数据源配置\n\n * 引入druid的三种方式\n   \n   * 直接引入druid\n     \n     <dependency>\n        <groupid>com.alibaba</groupid>\n        <artifactid>druid</artifactid>\n        <version>1.2.9</version>\n     </dependency>\n     \n     \n     spring:\n       datasource:\n         driver-class-name: com.mysql.cj.jdbc.driver\n         url: jdbc:mysql:///test?usessl=false&useserverprepstmts=true\n         username: root\n         password: 1234\n         type: com.alibaba.druid.pool.druiddatasource\n     \n   \n   * 引入druid-starter\n     \n     <dependency>\n        <groupid>com.alibaba</groupid>\n        <artifactid>druid-spring-boot-starter</artifactid>\n        <version>1.2.6</version>\n     </dependency>\n     \n     \n     #引入druid-starter\n     spring:\n       datasource:\n         druid:\n             driver-class-name: com.mysql.cj.jdbc.driver\n             url: jdbc:mysql:///test?usessl=false&useserverprepstmts=true\n             username: root\n             password: 1234\n     \n   \n   * 使用自动配置\n     \n     只要引入了druid的坐标，spirngboot会自动配置druid数据源，在yml文件中不需要体现出任何druid的信息，也可以装配上\n\n * 不引入druid，使用内置数据源\n   \n   * hikaricp————轻量级的，速度很快，可以考虑\n     \n     > springboot默认内置数据源对象———— 最佳轻量级数据库\n     > \n     > com.zaxxer.hikari.hikaridatasource\n   \n   * tomcat 提供datasource\n     \n     > 在hikaricp不可用的情况下，且在web环境中，可以使用tomcat服务器内置的数据源对象\n   \n   * commons dbcp\n     \n     > 一般不会用这个，除非上面两个用不了\n\n * 配置代码\n   \n   通用配置最好写在外面，让数据源自己去读即可\n   \n   hikaricp 的url写在里面会报错，其他三个不会报错，所以尽量把通用配置写在最外面即可\n   \n   #使用默认的 hikaricp  不导入任何 druid坐标\n   spring:\n     datasource:\n       driver-class-name: com.mysql.cj.jdbc.driver\n       url: jdbc:mysql:///test?usessl=false&useserverprepstmts=true\n       username: root\n       password: 1234\n       hikari:\n         maximum-pool-size: 50 \n   \n\n\n# 持久化技术\n\njdbctemplate\n\n\n\n\n# 数据库\n\n * 内嵌数据库—— 内存级数据库，方便小巧，可用来测试\n   \n   * h2 ————测试的时候 快很多，可以考虑使用，但是不适合上线\n     \n     * 导入坐标\n       \n       \x3c!-- h2 数据库--\x3e\n       <dependency>\n          <groupid>com.h2database</groupid>\n          <artifactid>h2</artifactid>\n       </dependency>\n       <dependency>\n          <groupid>org.springframework.boot</groupid>\n          <artifactid>spring-boot-starter-data-jpa</artifactid>\n       </dependency>\n       \n     \n     * yml配置\n       \n       在上线时，一定记得吧 enabled属性设置为false\n       \n       loacalhost/h2 即可访问，进入h2数据库操作页面\n       \n       #h2 数据库\n       spring:\n         h2:\n           console:\n             enabled: true\n             path: /h2\n       \n         datasource:\n           driver-class-name: org.h2.driver\n           url: jdbc:h2:~/test\n           username: sa\n           password: 123456\n       \n     \n     * 测试的时候，跟使用mysql一模一样\n   \n   * hsql\n   \n   * derby\n\n\n# nosql\n\n\n# redis\n\nredis 是一款 key-value 存储结构的 内存级 nosql 数据库，访问速度极快\n\n * 命令行\n   \n   * 端口号：6379\n   * 启动服务\n   \n   \n   \n   * 运行指令\n     \n     * set key value ——》 一个key 对 应一个value\n     * set keys key value——》 一个keys 里面存有多个key-value键值对\n     \n     \n\n * redis 基础\n   \n   * 数据类型\n   \n   \n   \n   * redis中文网\n   \n   * 基本命令——string\n     \n     * set key value——设置指定key的值\n     \n     * get key——获取指定key的值\n     \n     * setex key seconds value——设置指定key的值，并将key的过期时间设置为seconds秒\n     \n     * setnx key value——只有在key不存在时设置key，用于分布式锁\n   \n   * 基本命令——hash\n     \n     * hset key field value——将哈希表key中的字段field的值设为value\n   \n   * hget key field——获取存储在哈希表中指定字段的值\n     \n     * hdel key field——删除存储在hash表中的指定字段\n     * hkeys key——获取哈希表中所有字段\n     * hvals key——获取哈希表中所有值\n   \n   * hgetall key——获取在哈希表中指定key的所有字段和值\n   \n   * 基本命令——list（顺序，string类型）（可重复）\n     \n     * lpush key value[value2]——将一个或多个值插入到列表头部\n     \n     * lrange key start stop——获取列表指定范围内的元素\n       \n       * lrange key 0 -1——获取全部元素\n     \n     * rpop key——移除并获取列表最后一个元素\n     \n     * llen key——获取列表长度\n     \n     * brpop key1 [key2] timeout——移出并获取最后一个元素，如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止\n   \n   * 基本命令——set（string）（唯一，不能重复）\n     \n     * sadd key member1 [member2]——向集合添加一个或多个成员\n     \n     * smembers key——返回集合中的所有成员\n     \n     * scard key——获取集合的成员数\n     \n     * sinter key1 [key2]——返回给定所有集合的交集\n     \n     * sunion key [key2]——返回给定所有集合的并集\n     \n     * sdiff key1 [key2]——返回给定所有集合的差集（key1-key2）\n     \n     * srem key member1 [member2]——移除集合中一个或多个成员\n   \n   * 基本命令——sorted set（string）（有序）（唯一，不重复）\n     \n     * 每个元素都会关联一个double类型分数（score），通过分数对集合中的成员进行从小到大的排序，成员唯一，分数可重复\n     \n     * zadd key score1 member1 [score2 member2]——向有序集合中添加一个或多个成员，或者更新已存在成员的分数\n     \n     * zrange key start stop [withscores]——通过索引区间返回有序集合中指定区间内的成员（携带分数返回）\n     \n     * zincrby key increment member——有序集合中对指定成员的分数加上增量increment(一个数，原分数+增量=现分数)\n       \n       * zincrby user 20 b\n     \n     * zrem key member [member]——移除有序集合中的一个或多个成员\n   \n   * 通用命令\n     \n     * keys pattern——查找所有复合给定模式（pattern）的可以\n       * keys *——得到所有key\n     * exists key——检测给定key是否存在\n     * type key——返回key所存储值的类型\n     * ttl key——返回给定key的剩余时间（time to live） ，以秒为单位\n     * del key——该命令用于在可以存在时删除key\n     * select 1——选择1号数据库\n\n * springboot整合redis\n   \n   * 导入坐标\n     \n     \x3c!-- 引入redis--\x3e\n     <dependency>\n         <groupid>org.springframework.boot</groupid>\n         <artifactid>spring-boot-starter-data-redis</artifactid>\n     </dependency>\n     \n   \n   * 配置yml\n     \n     #配置redis\n     spring:\n       redis:\n         host: localhost\n         port: 6379\n     \n   \n   * 提供操作redis接口对象redistemplate\n     \n     * 先使用opsfor-xx获取操作类型，然后在进行读取操作\n     * \n     * 对象型模板————以这种方式存入的数据库，在命令行窗口直接读取不出来\n     \n     //需要先启动redis服务\n     //测试 redis    ------ redistemplate 传入的是对象\n     //以这种方式存入的数据库，在命令行窗口直接读取不出来\n     \n     @springboottest\n     class redistest {\n     \n         @autowired\n         private redistemplate redistemplate;\n     \n         @test//普通值\n         void set() {\n             valueoperations ops = redistemplate.opsforvalue();\n             ops.set("age",41);\n         }\n         @test//普通值\n         void get() {\n             valueoperations ops = redistemplate.opsforvalue();\n             system.out.println(ops.get("age"));\n         }\n     \n         \n         @test//哈希值\n         void hset() {\n             hashoperations ops = redistemplate.opsforhash();\n             ops.put("info","value","diana");\n             ops.put("info","name","diane");\n             ops.put("info","chinese","凉冰");\n         }\n         @test//哈希值\n         void hget() {\n             hashoperations ops = redistemplate.opsforhash();\n             system.out.println(ops.get("info","value"));\n             system.out.println(ops.get("info","chinese"));\n         }\n     }\n     \n     \n     * 字符串型模板 ————常用————在命令行窗口可以直接读取出来\n     \n      //需要先启动redis服务\n      //测试 redis    ------ stringredistemplate 传入的是字符串\n      //以这种方式存入的数据库，在命令行窗口可以直接读取出来 —————— 常用\n      \n      @springboottest\n      class redistest2 {\n      \n          @autowired\n          private stringredistemplate stringredistemplate;\n      \n          @test//普通值\n          void set() {\n              valueoperations<string,string> ops = stringredistemplate.opsforvalue();\n              ops.set("age","41");\n          }\n          @test//普通值\n          void get() {\n              valueoperations<string,string> ops = stringredistemplate.opsforvalue();\n              system.out.println(ops.get("k2"));\n          }\n      \n          @test//哈希值\n          void hset() {\n              hashoperations<string,string,string> ops = stringredistemplate.opsforhash();\n              ops.put("info","value","diana");\n              ops.put("info","name","diane");\n              ops.put("info","chinese","凉冰");\n          }\n          @test//哈希值\n          void hget() {\n              hashoperations<string,string,string> ops = stringredistemplate.opsforhash();\n              system.out.println(ops.get("info","value"));\n              system.out.println(ops.get("info","chinese"));\n          }\n          /**\n          * 操作list类型的数据\n          */\n         @test\n         public void testlist(){\n             listoperations<string, string> listoperations = stringredistemplate.opsforlist();\n             //存值\n             listoperations.leftpush("mylist","a");//存一个\n             listoperations.leftpushall("mylist","b","c","d");//存一堆\n             //取值(东西还在队列里)\n             list<string> mylist = listoperations.range("mylist", 0, -1);\n             for (string s : mylist) {\n                 system.out.println(s);\n             }\n             //获取列表长度\n             long size = listoperations.size("mylist");\n             int isize = size.intvalue();\n             for (int i=0;i<isize;i++){\n                 //出队列\n                 listoperations.rightpop("mylist");//出一个\n             }\n     \n     \n         }\n     \n         /**\n          * 操作set类型的数据 ——不能有重复值\n          */\n         @test\n         public void testset(){\n             setoperations<string, string> setoperations = stringredistemplate.opsforset();\n             //存值\n             setoperations.add("myset","diana","diane");\n             //取值\n             set<string> myset = setoperations.members("myset");\n             for (string s : myset) {\n                 system.out.println(s);\n             }\n             //删除\n             setoperations.remove("myset","diana");\n         }\n     \n     \n         /**\n          * 操作zset类型  sorted set\n          */\n         @test\n         public void testzset(){\n             zsetoperations<string, string> zset = stringredistemplate.opsforzset();\n             //存值\n             zset.add("myzset","a",100);\n             zset.add("myzset","b",90);\n             zset.add("myzset","c",80);\n             zset.add("myzset","d",70);\n             //取值\n             set<string> myzset = zset.range("myzset", 0, -1);\n             for (string s : myzset) {\n                 system.out.println(s);\n             }\n             //修改分数(增量)\n             zset.incrementscore("myzset","b",10);\n             //删除\n             zset.remove("myzset","a","c");\n             //取值\n             set<string> myzset1 = zset.range("myzset", 0, -1);\n             for (string s : myzset1) {\n                 system.out.println(s);\n             }\n     \n         }\n     \n     \n         /**\n          * 通用操作\n          */\n         @test\n         public void testcommon(){\n             //获取redis中的所有key\n             set<string> keys = stringredistemplate.keys("*");\n             for (string key : keys) {\n                 system.out.println(key);\n             }\n     \n             //判断某个key是否存在\n             boolean aboolean = stringredistemplate.haskey("mylist");\n             system.out.println(aboolean);\n     \n             //删除指定key\n             stringredistemplate.delete("mylist");\n     \n             //获取指定key对应的value的数据类型\n             datatype datatype = stringredistemplate.type("myset");\n             system.out.println(datatype.name());\n         }   \n      \n      }\n     \n\n * redis客户端方式选择 ——两种方式的操作api是一样的\n   \n   * lettuce (默认)\n   \n   * jedis\n     \n     * 导入坐标\n       \n       \x3c!-- 引入jedis--\x3e\n       <dependency>\n           <groupid>redis.clients</groupid>\n           <artifactid>jedis</artifactid>\n       </dependency>\n       \n     \n     * 配置\n       \n       #配置redis\n       spring:\n         redis:\n           host: localhost\n           port: 6379\n           client-type:  jedis   #lettuce 默认  #两种模式操作api是一样的\n           #配置  lettuce\n           lettuce:\n             pool:\n               max-active: 16\n           #配置  jedis\n           jedis:\n             pool:\n               max-active: 8\n       \n\n * 二者区别\n   \n   \n\n\n# mongo\n\nmongo 介于sql 与redis之间，既可以操作结构性数据，访问速度也很快\n\n * 应用场景\n\n * 命令行\n   \n   * 端口号 27017\n   \n   * 启动服务\n     \n     * mongod --dbpath=..\\data\\db\n   \n   * 执行客户端\n     \n     * mongo\n   \n   ![mongo 启动](/assets/后端/springboot/mongo 启动.png)\n\n * 基本操作\n   \n   * robo 3t 1.4 —— 可视化操作客户端\n   \n   * 基本的增删改查\n     \n     //添加数据(文档)  数据类型可以不统一\n     db.book.save({"name":"springboot"})\n     db.book.save({"name":"springboot","type":"code"})\n     \n     //删除操作\n     db.book.remove({type:"code"})\n     \n     //修改单个操作  只修改满足条件的第一条数据    第一个{}里是条件，第二个{}里是要修改的值\n     db.book.update({name:"springboot22"},{$set:{name:"springboot"}})\n     //修改多个操作  可以修改所有满足条件的数据\n     db.book.updatemany({name:"springboot"},{$set:{name:"springboot22"}})\n     \n     //查询所有\n     db.book.find()\n     //按条件查询\n     db.book.find({type:"code"})\n     \n   \n   * 常用命令\n     \n     \n\n * springboot整合mongo\n   \n   * 导入坐标\n     \n     \x3c!-- 引入mongo--\x3e\n     <dependency>\n         <groupid>org.springframework.boot</groupid>\n         <artifactid>spring-boot-starter-data-mongodb</artifactid>\n     </dependency>\n     \n   \n   * 配置文件\n     \n     #配置mongo\n     spring:\n       data:\n         mongodb:\n           uri: mongodb://localhost/diana\n     \n   \n   * 测试———— mongotemplate 每一个操作，都对应一个接口\n     \n     //需要先启动mongo服务\n     //测试 mongo\n     @springboottest\n     class mongotest {\n         \n         @autowired\n         private mongotemplate mongotemplate;\n         \n         @test  //插入\n         void testsave(){\n             book book=new book();\n             book.setid(2);\n             book.setname("崩坏三");\n             book.settype("手游");\n             book.setdescription("好玩");\n             mongotemplate.save(book);\n         }\n     \n         @test  //查询所有\n         void testfindall(){\n             list<book> all = mongotemplate.findall(book.class);\n             system.out.println(all);\n         }\n     \n     }\n     \n\n\n# es\n\nelasticsearch 是一个分布式全文搜索引擎；；；；全文搜索—— 牺牲空间，换取时间\n\n * 全文搜索 概念\n   \n   * 索引 ——id就是一个索引，根据id找数据的过程称为索引—— 红框的右半部分\n   * 倒排索引—— 根据内容（spring）找到id的过程称为倒排索引—— 红框的左半部分\n   * 创建文档——根据spring-1-1xx（即id为1的部分数据） 创建一个文档 （实战-1-1xx）\n   * 使用文档—— 根据文档去查出所有数据的过程\n\n * 命令行\n   \n   * 端口号 9200\n   * 直接双击 elasticsearch.bat，开启服务\n\n * 创建索引\n   \n   使用postman给es创建索引，全面支持restful\n   \n   * 使用get请求,得到索引 http://localhost:9200/books ——get\n   \n   * 使用delete请求，删除索引 http://localhost:9200/books ——delete\n   \n   * 使用put请求，带上下面的json字符串，创建索引 http://localhost:9200/books——put\n     \n     * type\n       * keyword: 作为被查询的关键字\n       * text: 普通文本\n     * analyzer —— ik_max_word\n       * 表示使用ik分词器，es文件夹要存放在不带空格的目录下，要不然插件用不了，会报错\n     * copy_to\n       * 表示将搜索内容传给 all一份\n     * all\n       * all 作为一个虚拟的属性，用来保证可以同时查询name和description ，和copy_to 配合使用\n   \n   {\n       "mappings":{\n           "properties":{\n               "id":{\n                   "type":"keyword"\n               },\n               "name":{\n                   "type":"text",\n                   "analyzer":"ik_max_word",\n                   "copy_to":"all"\n               },\n               "type":{\n                   "type":"keyword"\n               },\n               "description":{\n                   "type":"text",\n                   "analyzer":"ik_max_word",\n                   "copy_to":"all"\n               },\n               "all":{\n                   "type":"text",\n                   "analyzer":"ik_max_word"\n               }\n           }\n       }\n   }\n   \n\n * 创建文档\n   \n   * post http://localhost:9200/books/_doc ——使用系统生成id\n   * post http://localhost:9200/books/_doc/10 ——使用指定id，不存在则创建，存在则修改\n   * post http://localhost:9200/books/_create/1 ——使用系统生成id ，只能创建\n\n * 删除文档\n   \n   * delete http://localhost:9200/books/_doc/1 ——按照id删除\n\n * 更新文档\n   \n   * put http://localhost:9200/books/_doc/1 ——全覆盖修改，提交的json数据会全面覆盖原来的数据\n     \n     {\n         "name":"springboot very good",\n         "type":"diana",\n         "description":"pringboot2 very good"\n     }\n     \n   \n   * post http://localhost:9200/books/_update/1 ——修改单个属性 /_doc/id 也可以\n     \n     {\n         "doc":{\n             "name":"springboot"\n         }\n     }\n     \n\n * 查询文档\n   \n   * get http://localhost:9200/books/_search ——查询全部\n   * get http://localhost:9200/books/_doc/1 ——按照id查询\n   * get http://localhost:9200/books/_search?q=description:good ——按照参数值进行条件查询\n\n * springboot整合es\n   \n   * 引入坐标\n     \n     \x3c!-- 引入es high-level--\x3e\n     <dependency>\n         <groupid>org.elasticsearch.client</groupid>\n         <artifactid>elasticsearch-rest-high-level-client</artifactid>\n     </dependency>\n     \n   \n   * 配置yml\n     \n     springboot没有整合，所以不能直接在这里配置了\n   \n   * 创建简单索引\n     \n     要手动创建对象，手动关闭\n     \n     * 写在一起 不推荐\n     \n     @test //使用es高版本客户端，因为springboot没有整合，所以 要使用硬编码的方式 来进行配置\n     void testcreateindex() throws ioexception {\n         //创建客户端\n         httphost host=httphost.create("http://localhost:9200");\n         restclientbuilder builder= restclient.builder(host);\n         client=new resthighlevelclient(builder);\n     \n         //创建索引\n         createindexrequest request=new createindexrequest("diana");\n         client.indices().create(request, requestoptions.default);\n     \n         client.close();\n     }\n     \n     \n     * 使用aop思想，实现，前置和后置方法 ————》推荐\n     \n     private resthighlevelclient client;\n     \n     @beforeeach //所有方法执行前 --aop思想\n     void setup() {\n         //创建客户端\n         httphost host=httphost.create("http://localhost:9200");\n         restclientbuilder builder= restclient.builder(host);\n         client=new resthighlevelclient(builder);\n     }\n     \n     @aftereach //所有方法执行后  --aop思想\n     void teardown() throws ioexception {\n         //关闭客户端\n         client.close();\n     }\n     \n     @test //使用es高版本客户端，因为springboot没有整合，所以 要使用硬编码的方式 来进行配置\n     void testcreateindex() throws ioexception {\n         //创建客户端\n         //创建索引\n         createindexrequest request=new createindexrequest("users");\n         \n         client.indices().create(request, requestoptions.default);\n         //关闭客户端\n     }\n     \n   \n   * 创建带参索引\n     \n     * request.source(json, xcontenttype.json);//前面是参数，后面是类型\n     \n     @test //带上配置创建索引\n     void testcreateindexjson() throws ioexception {\n         //创建客户端\n         string json="{\\n" +\n                 "    \\"mappings\\":{\\n" +\n                 "        \\"properties\\":{\\n" +\n                 "            \\"id\\":{\\n" +\n                 "                \\"type\\":\\"keyword\\"\\n" +\n                 "            },\\n" +\n                 "            \\"name\\":{\\n" +\n                 "                \\"type\\":\\"text\\",\\n" +\n                 "                \\"analyzer\\":\\"ik_max_word\\",\\n" +\n                 "                \\"copy_to\\":\\"all\\"\\n" +\n                 "            },\\n" +\n                 "            \\"type\\":{\\n" +\n                 "                \\"type\\":\\"keyword\\"\\n" +\n                 "            },\\n" +\n                 "            \\"description\\":{\\n" +\n                 "                \\"type\\":\\"text\\",\\n" +\n                 "                \\"analyzer\\":\\"ik_max_word\\",\\n" +\n                 "                \\"copy_to\\":\\"all\\"\\n" +\n                 "            },\\n" +\n                 "            \\"all\\":{\\n" +\n                 "                \\"type\\":\\"text\\",\\n" +\n                 "                \\"analyzer\\":\\"ik_max_word\\"\\n" +\n                 "            }\\n" +\n                 "        }\\n" +\n                 "    }\\n" +\n                 "}";\n     \n         //创建索引\n         createindexrequest request=new createindexrequest("books");\n         \n         request.source(json, xcontenttype.json);//前面是参数，后面是类型\n         \n         client.indices().create(request, requestoptions.default);\n         //关闭客户端\n     }\n     \n   \n   * 创建单条文档\n     \n     @test //创建一条文档\n     void testcreateonedoc() throws ioexception {\n         book book = mapper.selectbyid(3);\n         string id=book.getid().tostring(); //获取id   integer 类型 的数据 有tostring 方法  因为他是个对象\n         string json = json.tojsonstring(book); //获取json数据\n     \n         //创建请求   //   /users/_doc/1\n         indexrequest request=new indexrequest("books").id(id);\n         //携带参数\n         request.source(json,xcontenttype.json);\n         // 创建文档\n         client.index(request,requestoptions.default);\n     }\n     \n   \n   * 批量创建所有文档\n     \n     * bulkrequest bulkrequest=new bulkrequest();——创建批处理请求容器\n     * bulkrequest.add(request); ———— 将单个请求加入容器\n     \n     @test //创建所有数据的文档\n     void testcreatealldoc() throws ioexception {\n         list<book> books = mapper.selectlist(null);\n         //创建批处理请求容器\n         bulkrequest bulkrequest=new bulkrequest();\n     \n         for(book book:books){\n             string id=book.getid().tostring(); //获取id   integer 类型 的数据 有tostring 方法  因为他是个对象\n             string json = json.tojsonstring(book); //获取json数据\n     \n             //创建请求   //   /users/_doc/1\n             indexrequest request=new indexrequest("books").id(id);\n             //携带参数\n             request.source(json,xcontenttype.json);\n             //将请求加入容器\n             bulkrequest.add(request);\n     \n         }\n         //创建批量文档   一次性完成多个请求\n         client.bulk(bulkrequest,requestoptions.default);\n     }\n     \n   \n   * 按id查询文档\n     \n     @test //按id查询\n     void testgetbyid() throws ioexception {\n         getrequest request= new getrequest("books","2");//索引名称，id\n         getresponse res = client.get(request, requestoptions.default);\n         //得到返回数据中的source————里面存储的是数据信息\n         string json = res.getsourceasstring(); \n         system.out.println(json);\n     \n     }\n     \n   \n   * 按条件查询文档\n     \n     @test //按条件查询\n     void testgetbycon() throws ioexception {\n     \n         searchrequest request= new searchrequest("books");\n         searchsourcebuilder builder=new searchsourcebuilder();\n         //插入查询条件\n         builder.query(querybuilders.termquery("name","丽"));//查找name中包含丽字的词条\n         request.source(builder);\n         searchresponse response = client.search(request, requestoptions.default);\n     \n     \n         //查询得到所有的hits  命中\n         searchhits hits = response.gethits();\n         //遍历得到的所有hits，从中取出source 并转换成字符串\n         for(searchhit hit:hits){\n             string json = hit.getsourceasstring();\n             book book = json.parseobject(json, book.class);\n             system.out.println(book);\n         }\n     \n     }\n     \n   \n   * 更新文档（修改单个属性）\n     \n     @test //更新文档———— 修改单个属性\n     void testupdateall() throws ioexception {\n         updaterequest request=new updaterequest("books","3");\n         string json=" {\\n" +\n                 "        \\"name\\":\\"diane\\"\\n" +\n                 "    }";\n         request.doc(json, xcontenttype.json);\n         client.update(request, requestoptions.default);\n     }\n     \n   \n   * 删除文档\n     \n     @test //删除文档\n     void testdelete() throws ioexception {\n     \n         deleterequest request=new deleterequest("books","4");\n         client.delete(request, requestoptions.default);\n     \n     }\n     ',charsets:{cjk:!0}},{title:"基础篇",frontmatter:{title:"基础篇",date:"2023-07-03T20:59:58.000Z",permalink:"/pages/d74c9d/",categories:["后端","SpringBoot"],tags:["知识","SpringBoot"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/50.SpringBoot/05.%E5%9F%BA%E7%A1%80%E7%AF%87.html",relativePath:"01.后端/50.SpringBoot/05.基础篇.md",key:"v-057fb4a2",path:"/pages/d74c9d/",headers:[{level:3,title:"SpringBoot基础",slug:"springboot基础",normalizedTitle:"springboot基础",charIndex:10},{level:3,title:"基础配置",slug:"基础配置",normalizedTitle:"基础配置",charIndex:2339},{level:3,title:"SpringBoot整合",slug:"springboot整合",normalizedTitle:"springboot整合",charIndex:12386},{level:3,title:"SSM整合",slug:"ssm整合",normalizedTitle:"ssm整合",charIndex:15032},{level:3,title:"SSMP 整合",slug:"ssmp-整合",normalizedTitle:"ssmp 整合",charIndex:15761},{level:3,title:"教你一招",slug:"教你一招",normalizedTitle:"教你一招",charIndex:19338}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688392125e3,headersStr:"SpringBoot基础 基础配置 SpringBoot整合 SSM整合 SSMP 整合 教你一招",content:'# 基础篇\n\n\n# SpringBoot基础\n\n# 新建项目\n\n * 设置maven版本 （使用默认设置）\n * 测试一下https://start.spring.io/的连接情况， setting——>http——>检测连接，连接上才可以创建工程\n * 一般来说连接上都需要翻墙，新建项目都需要联网\n * 使用阿里云网站 https://start.aliyun.com/\n\n# 执行项目\n\n * 使用maven——package打一个jar包\n\n * 在命令行输入 java -jar jar包名 例如 java -jar demo1-0.0.1-SNAPSHOT.jar\n\n * ctrl+c 退出服务\n\n * springBoot主启动类\n   \n   //springBoot 主启动类\n   \n   @SpringBootApplication\n   public class Demo1Application {\n   \n       public static void main(String[] args) {\n           SpringApplication.run(Demo1Application.class, args);\n       }\n   \n   }\n   \n\n * 内嵌Tomcat服务器\n   \n   * 内嵌Tomcat服务器的工作原理是将Tomcat服务器作为对象运行，并将对象交给Spirng容器管理。\n\n * jar 包\n   \n   * maven 打包插件\n     \n     <plugins>\n        \x3c!-- Maven 打包插件--\x3e\n        <plugin>\n           <groupId>org.springframework.boot</groupId>\n           <artifactId>spring-boot-maven-plugin</artifactId>\n        </plugin>\n     </plugins>\n     \n   \n   * jar包描述文件\n\n * 端口占用解决\n   \n   \n   \n   \n\n * idea中配置临时属性 （一般不用，了解即可）\n   \n   public static void main(String[] args) {\n   \n      //可以通过修改args来 增加一些临时配置\n      String[] arg=new String[1];\n      arg[0]="--server.port=8082";\n      SpringApplication.run(SsmApplication.class, arg);\n   \n      //可以在启动boot程序时断开读取外部临时配置对应的入口，也就是去掉读取外部参数的形参 args\n      //SpringApplication.run(SsmApplication.class);\n   }\n   \n\n * idea中的配置覆盖\n   \n   \n\n# Linux下运行项目\n\n * 等学会Linux，再来看 视频\n\n# 起步依赖\n\n * starter\n   \n   * SpringBoot中常见的项目名称，定义了当前项目使用的所有项目坐标，以达到减少依赖配置的目的\n\n * parent\n   \n   * 所有SpringBoot要继承的项目，定义了若干个坐标版本号，以达到减少依赖冲突的目的\n\n * 实际开发\n   \n   * 使用任意坐标时，仅书写GVA中的G和A，V由SpirngBoot提供\n   * 如果发生坐标错误(可能性很小)，在指定version\n   * G: groupId A: artifactId V:version\n\n * 换技术\n   \n   不想用 tomcat，想用jetty\n   \n   jetty更轻量级一些，内部功能很少，自己用到什么配什么；而tomcat 里面集成了相当多的功能\n   \n   * 排除tomcat依赖\n     \n     <dependency>\n         <groupId>org.springframework.boot</groupId>\n         <artifactId>spring-boot-starter-web</artifactId>\n         <exclusions>\n             \x3c!-- 排除 tomcat 服务器--\x3e\n             <exclusion>\n                 <groupId>org.springframework.boot</groupId>\n                 <artifactId>spring-boot-starter-tomcat</artifactId>\n             </exclusion>\n         </exclusions>\n     </dependency>\n     \n   \n   * 引入jetty依赖\n     \n     <dependency>\x3c!-- 使用jetty服务器--\x3e\n         <groupId>org.springframework.boot</groupId>\n         <artifactId>spring-boot-starter-jetty</artifactId>\n     </dependency>\n     \n\n\n# 基础配置\n\n# 官方配置\n\n# 配件文件格式\n\n * appplication.properties\n   \n   * server.port=82\n\n * application.yml\n   \n   * 主写这个，\n   \n   * server:\n       port: 82   #设置端口号\n       servlet:\n         context-path: /123  #增加访问前缀\n       \n     #Tomcat started on port(s): 82 (http) with context path \'/123\'\n     #访问时，需要增加前缀才可以访问到    \n     \n     #原访问路径    http://localhost:82/books/2\n     #现访问路径    http://localhost:82/123/books/2\n     \n\n * application.yaml\n   \n   * 这个好像不支持\n   \n   * server:\n       port: 81\n     \n\n * 配置文件优先级\n   \n   * appplication.properties > application.yml > application.yaml\n\n# yaml\n\n一种 数据序列化格式\n\n * 优点\n   \n   * 容易阅读\n   * 容易与脚本语言交互\n   * 以数据为核心，重数据轻格式\n\n * 文件扩展名\n   \n   * .yml （主流）\n   * .yaml\n\n * 语法规则\n   \n   * 大小写敏感\n   \n   * 属性层级关系使用多行描述，每行结尾使用冒号结束（或者数据结束）\n   \n   * 使用缩进表示层级关系，同层级左侧对齐，只允许使用空格(不允许使用Tab键)——>(跟python类似)\n   \n   * 属性值前面添加空格（属性名与属性值之间使用冒号+空格作为分隔）\n   \n   * # 表示注释\n     \n     logging:\n       level:\n         root: info   #debug,info,warn   从前往后日志信息依次减少，info是默认\n     \n   \n   * 数组数据在数据书写位置的下方使用减号作为数据的开始符合，减号和数据之间用空格分离\n     \n     likes:\n       - game\n       - code\n     \n\n * yaml文件中的变量引用\n   \n   n: 崩坏三\n   game:\n     #  name: 崩坏三\n     name: ${n}  #引用文件中的变量    #这句话和上面的意思是一样的\n   \n\n * yaml文件解析转义字符\n   \n   lesson: "SpringBoot\\tss"   #用引号引起来，可以读取转义字符  \\t 表示制表符tab\n   # 读出lesson的值为  SpringBoot\tss\n   \n\n * 数据读取\n   \n   server:\n     port: 81\n   \n   logging:\n     level:\n       root: info   #debug,info,warn   从前往后日志信息依次减少，info是默认\n   \n   game:\n     name: 崩坏三\n     age: 4\n     likes:\n       - 丽塔\n       - 爱丽希雅\n       - diana\n   \n   lesson: SpringBoot\n   \n   \n   * 单个读取\n     \n     //1.挨个配置，读取配置文件中的数据\n     @Value("${lesson}")\n     private String lesson;\n     \n     @Value("${game.likes[0]}")\n     private String like_0;\n     \n     @Value("${logging.level.root}")\n     private String log;\n     \n   \n   * 读取全部\n     \n     //2.直接读取所有\n     @Autowired\n     private Environment environment;\n     \n     \n     System.out.println(environment.getProperty("lesson"));\n     System.out.println(environment.getProperty("game.likes[0]"));\n     System.out.println(environment.getProperty("logging.level.root"));\n     \n   \n   * 自定义对象读取\n     \n     * 这个用的多，可以在里面配置 mybatis 数据等，框架内部有定义好的对象，会读取咱们配置的数据，所有格式一定要对\n     \n     * 自定义对象数据警告解决方案\n       \n       \x3c!-- 自定义对象封装数据警告解决方案--\x3e\n       <dependency>\n           <groupId>org.springframework.boot</groupId>\n           <artifactId>spring-boot-configuration-processor</artifactId>\n           <optional>true</optional>\n       </dependency>\n       \n     \n     * 自定义对象写法\n       \n       @Component\n       @ConfigurationProperties(prefix = "game") //表明读取配置文件中的game\n       public class Game {\n           private String name;\n           private Integer age;\n           private String[] likes;\n       }\n       //3.使用java对象读取\n       @Autowired\n       private Game game;\n       System.out.println(game);\n       \n       \n\n# 多环境启动\n\n# yml\n\n * 单文件配置 demo1项目中\n   \n   * 设置不同的环境，它们之间用---分隔开\n   \n   * 他们之间的参数都是独立的，都是只归定义的环境所有，比如在 pro下定义的参数，使用test环境运行，就读取不到该参数。\n   \n   * 但是写在最上面的参数，是它们公有的，都可以读取到\n     \n     # 设置启用的环境\n     spring:\n       profiles:\n         active: pro\n     \n     # 这些参数是公共的，都可以使用\n     logging:\n       level:\n         root: info   #debug,info,warn   从前往后日志信息依次减少，info是默认\n     \n     game:\n       name: 崩坏三\n       age: 4\n       likes:\n         - 丽塔\n         - 爱丽希雅\n         - diana\n     \n     lesson: SpringBoot\n     \n     ---\n     #开发\n     spring:\n       config:\n         activate:\n           on-profile: dev\n     server:\n       port: 81\n     \n     ---\n     \n     #生产\n     spring:\n       config:\n         activate:\n           on-profile: pro\n     server:\n       port: 82\n     \n     ---\n     \n     #测试\n     spring:\n       config:\n         activate:\n           on-profile: test\n     server:\n       port: 83\n     \n\n * 多文件配置 demo2项目中\n   \n   * 可以将一个文件分拆成4个，一个主的，一个开发，一个生产，一个测试\n     * 注意 文件命名 参照图片\n   \n   \n\n * 多环境开发独立配置文件\n   \n   \n   \n   \n\n# properties\n\ndemo1项目中\n\n * 只支持多文件配置\n\n * 存到了bak 备份文件夹中\n\n# Maven与SpringBoot\n\n * 运行jar包的话，是maven打的包，所以maven优先级要高\n\n * 在配置文件application.yml中引用pom文件中定义的属性\n   \n   * pom文件 要使用这个插件 帮助maven解析资源文件中的占位符\n     \n     \x3c!-- 告诉maven 我要处理资源文件——资源文件中可以用${}读取maven的属性值  默认是resources文件夹--\x3e\n     <plugin>\n         <groupId>org.apache.maven.plugins</groupId>\n         <artifactId>maven-resources-plugin</artifactId>\n         <version>3.2.0</version>\n         <configuration>\n             <encoding>UTF-8</encoding>\n             <useDefaultDelimiters>true</useDefaultDelimiters>\n         </configuration>\n     </plugin>\n     \n     \n     \x3c!-- maven 多环境 配置--\x3e\n     \x3c!-- 配置多环境--\x3e\n     <profiles>\n         \x3c!-- 开发环境--\x3e\n         <profile>\n             \x3c!-- id 随便起--\x3e\n             <id>dev</id>\n             <properties>\n                 \x3c!-- 自定义 属性 名字随便起 --\x3e\n                 <profile.active>dev</profile.active>\n             </properties>\n             \x3c!-- 设定 是否为默认环境--\x3e\n             <activation>\n                 <activeByDefault> true </activeByDefault>\n             </activation>\n         </profile>\n     \n         \x3c!-- 生产环境--\x3e\n         <profile>\n             \x3c!-- id 随便起--\x3e\n             <id>pro</id>\n             <properties>\n                 \x3c!-- 自定义 属性 名字随便起 --\x3e\n                 <profile.active>pro</profile.active>\n             </properties>\n         </profile>\n     \n         \x3c!-- 测试环境--\x3e\n         <profile>\n             \x3c!-- id 随便起--\x3e\n             <id>test</id>\n             <properties>\n                 \x3c!-- 自定义 属性 名字随便起 --\x3e\n                 <profile.active>test</profile.active>\n             </properties>\n         </profile>\n     </profiles>\n     \n   \n   * application.yml 文件\n     \n     # 设置启用的环境\n     spring:\n       profiles:\n         #active: dev   #直接使用下面定义的 数据\n         active: ${profile.active} #使用pom文件中的 自定义的属性\n     \n\n# 参数配置\n\n * 打jar包\n   \n   * 把多余的配置文件 放入bak文件夹，留作备份，不用影响正常测试\n   * 把项目的语言环境设置为utf-8\n   * 使用 maven-clean ，清除之前的包\n   * 使用maven-package,打包\n\n * 运行jar包 （可以使用命令行参数输入，来覆盖默认的参数）\n   \n   * java jar -demo1-0.0.1-SNAPSHOT.jar 按照默认参数运行\n   * java jar -demo1-0.0.1-SNAPSHOT.jar --spring.profiles.active=test 修改运行环境为 test环境\n   * java jar -demo1-0.0.1-SNAPSHOT.jar --server.port=88 修改端口号为 88\n   * java jar -demo1-0.0.1-SNAPSHOT.jar --spring.profiles.active=test --server.port=88 修改运行环境为 test，同时修改端口号为88\n\n * 参数加载优先级\n   \n   \n\n# 配置文件分类\n\n1. 共有4种配置文件，1级 优先级最高，4级优先级最低 ———— 项目demo1中\n\n * file —— 一般做测试，或者生产的时候用这些来配置 —— 留作系统打包后设置通用属性\n   \n   * 1级 file: config/application.yml 在jar所在目录下新建一个config包，里面放入配置文件，优先级最高\n   \n   * 2级 file: application.yml 在jar所在目录下新建一个配置文件，优先级为第二\n\n * classpath —— 在做开发的时候，用这些配置 —— 留作系统开发阶段设置通用属性\n   \n   * 3级 classpath : config/application.yml 在resource文件下新建一个config包，里面放入配置文件\n   \n   * 4级 classpath : application.yml 在resource文件下新建一个配置文件，优先级最低\n\n 2. 自定义配置文件 ———— 项目ssm中\n    \n    * 默认配置文件名称为 application.yml,可以使用配置覆盖的方式，来自定义配置文件名称\n    * spring.config.name 只写名称，不写后缀名\n    \n    \n    \n    * 可以在一个值内，同时配置多个文件，后面会覆盖前面一样的，前面的其他配置正常生效。\n      * 例 diana——server.port=80 , diana——server.port=82 ,执行结果是 82\n    \n    \n\n 3. 基于微服务（SpringCloud）开发时，配置文件将使用配置中心进行管理，2 所说的自定义配置文件，用的地方不多，用处不大，了解即可。\n\n# 日志\n\ndemo2项目中\n\n# 代码\n\n * 代码1 —— 使用日志来记录 比直接打印输出要好\n   \n   import org.slf4j.LoggerFactory;\n   import org.slf4j.Logger;\n   \n   public class BookController {\n   \n       //创建记录日志的对象\n       private static final Logger log= LoggerFactory.getLogger(BookController.class);\n   \n   \n       @GetMapping("/{id}")\n       public String getById(@PathVariable int id){\n   \n           //日志的使用    比使用打印输出要好\n           log.debug("debug..."); //默认不开启\n           log.info("info...");\n           log.warn("warn...");\n           log.error("error...");\n   \n           return "hello springBoot!";\n       }\n   }\n   \n\n * 代码2—— 使用注解——使用 @Slf4j 来代替 创建日志对象的代码 （需要导入 lombok 的包）\n   \n   //日志的使用\n   @Slf4j\n   @RestController\n   @RequestMapping("/books")\n   public class BookController {\n   \n       //创建记录日志的对象\n   //    private static final Logger log= LoggerFactory.getLogger(BookController.class);\n       //可以使用   @Slf4j 来代替 这一行代码    需要导入 lombok 的包\n   \n       \n       @GetMapping("/{id}")\n       public String getById(@PathVariable int id){\n   \n           //日志的使用    比使用打印输出要好\n           log.debug("debug...");\n           log.info("info...");\n           log.warn("warn...");\n           log.error("error...");\n   \n           return "hello springBoot!";\n       }\n   }\n   \n\n# 总*配置文件\n\n * 可以设置总的日志级别 ——root\n * 可以设置某个组的日志级别 ——group\n\n#日志设置\nlogging:\n  #设置分组\n  group:\n    my: com.diana\n    isservice: com.alibaba\n  level:\n    #设置总的日志级别\n    root: info   #debug,info,warn   从前往后日志信息依次减少，info是默认\n\n    #设置某个包的日志级别\n    #com.diana.controller: debug    #不推荐使用太麻烦\n\n    #设置某个组设置日志级别\n    my: debug\n\n\n  #设置日志的模板格式\n  #pattern:\n    #console: "%d--%m %n" # %d 表示时间； %m 表示消息；  %n  表示换行\n    \n    #console: "%d--%clr(%5p) %n" # %5p 表示消息，5表示 统一长度，p表示日志级别 ；  %clr表示彩色，加在谁上面用括号包裹即可\n    \n    #console: "%d--%clr(%5p) -----[%16t] %-40c %n" # %t表示线程名称  %c表示类名  %-40c 表示长度限制40，负号表示左对齐\n    \n    #console: "%d--%clr(%5p) -----[%16t] %clr(%-40.40c){red} : %m %n" #%-40.40c 中的 .40 表示截取40位，就剩下40位\n    \n    # %clr(){red} 表示设置日志颜色  red-红   cyan-青色\n\n  #保存日志\n  file:\n    name: server.log # 名字随便起\n    #path:             #默认路径是项目最外部\n\n  #设置日志存储格式\n  logback:\n    rollingpolicy:\n      file-name-pattern: server.%d{yyyy-MM-dd}.%i.log  \n      #设置文件名格式 server.2022-04-30.1.log  %d 表示日期，{}里可以指定格式； %i表示第几个，从0开始\n      max-file-size: 4KB   #设置最大上限\n\n\n# 日志格式\n\n * 默认格式\n   \n   \n   \n   * 配置文件 修改日志输出格式\n     \n     * 一般不用自己写，了解即可\n     \n     #设置日志的模板格式\n     pattern:\n       #console: "%d--%m %n" # %d 表示时间； %m 表示消息；  %n  表示换行\n       \n       #console: "%d--%clr(%5p) %n" # %5p 表示消息，5表示 统一长度，p表示日志级别 ；  %clr表示彩色，加在谁上面用括号包裹即可\n       \n       #console: "%d--%clr(%5p) -----[%16t] %-40c %n" # %t表示线程名称  %c表示类名  %-40c 表示长度限制40，负号表示左对齐\n       \n       console: "%d--%clr(%5p) -----[%16t] %clr(%-40.40c){red} : %m %n" #%-40.40c 中的 .40 表示截取40位，就剩下40位\n       \n       # %clr(){red} 表示设置日志颜色  red-红   cyan-青色\n     \n\n# 日志存储\n\n#保存日志(单一日志，所有的日志都存入里面)\nfile:\n  name: server.log # 名字随便起\n  #path:             #默认路径是项目最外部\n\n#设置日志存储格式\nlogback:\n  rollingpolicy: #(设置滚动日志，日志存储到达上限后，新建一个日志)\n    file-name-pattern: server.%d{yyyy-MM-dd}.%i.log  \n    #设置文件名格式 server.2022-04-30.1.log  %d 表示日期，{}里可以指定格式； %i表示第几个，从0开始\n    max-file-size: 4KB   #设置最大上限\n\n\n\n# SpringBoot整合\n\n# 整合 junit\n\n * @SpringBootTest 设置Junit加载SpringBoot启动类（MybatisJunitApplication.java）\n * 测试类和主启动类，要在一个包下，不然加载不了配置\n\npackage com.diana;\n\n@SpringBootTest\nclass MybatisJunitApplicationTests {\n   @Autowired\n   private BookService bookService;\n    \n   @Test\n   public void test1(){\n      bookService.save();\n   }\n}\n\n\n * 测试类和引导类不在一个包下时，要使用classes属性，声明引导类 @SpringBootTest(classes = JunitMybatisApplication.class)\n   \n   package com.test;\n   \n   //测试类和引导类不在一个包下时，要使用classes属性，声明引导类\n   @SpringBootTest(classes = JunitMybatisApplication.class)\n   public class OneTest {\n       @Autowired\n       BookMapper bookMapper;\n   \n       @Test\n       public void testMapper() {\n           System.out.println(bookMapper.getById(2));\n       }\n   }\n   \n\n# 整合mybatis\n\n * 配置文件\n   \n   * springboot 2.4.2 以前会有时区问题， 需要加上 serverTimezone=UTC\n   \n   * 如果要设置数据源，需要导入 druid，导入这个坐标的话，只能用下面配置数据源的方式引入druid\n     \n     <dependency>\n        <groupId>com.alibaba</groupId>\n        <artifactId>druid</artifactId>\n        <version>1.2.9</version>\n     </dependency>\n     \n   \n   #配置数据源 ————通用型\n   spring:\n     datasource:\n       driver-class-name: com.mysql.cj.jdbc.Driver\n       url: jdbc:mysql:///test?useSSL=false&useServerPrepStmts=true&serverTimezone=UTC #2.4.2以前需要设置时区  最后一个\n       username: root\n       password: 1234\n   \n       type: com.alibaba.druid.pool.DruidDataSource     #设置数据源\n   \n\n * Mapper类\n   \n   加上@Mapper 注解\n   \n   @Mapper\n   public interface BookMapper {\n   \n       @Select("select * from book where id=#{id}")\n       Book getById(int id);\n   }\n   \n\n * 测试\n   \n   @SpringBootTest\n   class JunitMybatisApplicationTests {\n   \n      @Autowired\n      BookService bookService;\n   \n      @Autowired\n      BookMapper bookMapper;\n   \n      @Test\n      public void testService() {\n         bookService.save();\n      }\n   \n      @Test\n      public void testMapper() {\n         System.out.println(bookMapper.getById(2));\n      }\n   \n   }\n   \n\n# 整合 mybatisplus\n\n * 项目在 mybatisplus中。\n * md文件\n\n# 整合druid\n\n * 导入druid 简单使用，参照 整合mybatis中druid的导入\n\n * 整合druid\n   \n   * 导入坐标 --（整合druid专用坐标）\n     \n     <dependency>\n        <groupId>com.alibaba</groupId>\n        <artifactId>druid-spring-boot-starter</artifactId>\n        <version>1.2.9</version>\n     </dependency>\n     \n   \n   * 完成配置 --（只有导入了上面的坐标，才能这样配置）\n     \n     #整合druid\n     spring:\n       datasource:\n         druid:\n           driver-class-name: com.mysql.cj.jdbc.Driver\n           url: jdbc:mysql:///test?useSSL=false&useServerPrepStmts=true&serverTimezone=UTC #2.4.2以前需要设置时区  最后一个\n           username: root\n           password: 1234\n     \n\n * 这两种方式 一样用，差不多\n\n# 整合三方技术\n\n * 导入对应的starter\n * 根据提供的配置格式，配置非默认值对应的配置项\n\n\n# SSM整合\n\n * 引入数据源\n   \n   \x3c!--TODO 引入druid数据源--\x3e\n   <dependency>\n      <groupId>com.alibaba</groupId>\n      <artifactId>druid</artifactId>\n      <version>1.2.9</version>\n   </dependency>\n   \n   \n   # TODO 配置数据源相关信息\n   spring:\n     datasource:\n       type: com.alibaba.druid.pool.DruidDataSource\n       driver-class-name: com.mysql.cj.jdbc.Driver\n       url: jdbc:mysql:///test?useSSL=false&useServerPrepStmts=true\n       username: root\n       password: 1234\n   \n   server:\n     port: 80\n   \n\n * 添加@Mapper\n   \n   //TODO 添加@Mapper\n   @Mapper\n   public interface BookMapper {}\n   \n\n * 删除配置文件\n\n * 将web静态资源放入resources下的static中\n   \n   * 这样也不需要进行静态资源的放行\n\n * TODO 标签\n   \n   * 使用该标签 标注，可以在下方通过点击的方式，进行快速定位\n\n * Ctrl+F12\n   \n   * 快速打开文件结构\n\n\n# SSMP 整合\n\n# 分页查询\n\n * Mapper\n   \n   //使用mybatisPlus\n   @Mapper\n   public interface BookMapper2 extends BaseMapper<Book> {}\n   //内部已经实现了一些基本的增删改查，不用自己手动实现了\n   //只需要自己手动实现一些比较复杂的多表操作即可\n   \n\n * IService\n   \n   //TODO Ctrl+F12  快速打开 文件目录\n   //使用mybatisPlus\n   //提供了很多基础的 CRUD操作  这些简单的不用自己再实现了\n   //自己去定义一些比较复杂的，切合项目需求的方法，尽量不要覆盖原有的方法\n   public interface IBookService extends IService<Book> {\n       \n       //自己定义方法\n       IPage<Book> getPage(int current, int pagesize);   //分页操作 \n   \n   }\n   \n\n * IServiceImpl\n   \n   //里面有很多基本的增删改操作\n   @Service ////使用mybatisPlus\n   public class BookServiceImp2 extends ServiceImpl<BookMapper2,Book>   implements IBookService {\n   \n       @Autowired\n       private BookMapper2 mapper;\n   \n       //实现自己定义的方法\n       //分页操作\n       public IPage<Book> getPage(int current,int pagesize){\n           IPage<Book> page=new Page<Book>(current,pagesize);\n           mapper.selectPage(page,null);\n           return page;\n       }\n   }\n   \n   \n   //IBookServiceTest\n   \n   //基本的增删改查操作对应 ————Book book=new Book();\n   // 添加  save(book)\n   // 更新  updateById(book)    \n   // 按照id删除  removeById(9)\n   // 查询单个  getById(3)\n   // 查询全部  list()\n   \n   \n   \n\n * BookController\n   \n   @GetMapping("/{currentPage}/{pageSize}") //分页操作\n   public Result bookIpage(@PathVariable int currentPage,@PathVariable int pageSize){\n       IPage<Book> page = bookService2.getPage(currentPage, pageSize);\n       //如果当前页码值大于了总页码值，那么重新执行查询操作，使用最大页码值作为当前页码值\n       if(page.getCurrent()>page.getPages()){\n            page = bookService2.getPage((int)page.getPages(), pageSize);\n       }\n   \n       //page里的数据不为null 也不为空--isEmpty()\n       Integer code= (page.getRecords()!=null && !page.getRecords().isEmpty()) ? Code.GET_OK : Code.GET_ERR;\n   \n       return new Result(code,page);\n   \n   }\n   \n   \n   * 基于业务需求维护删除功能\n     \n     //如果当前页码值大于了总页码值，那么重新执行查询操作，使用最大页码值作为当前页码值\n     if(page.getCurrent()>page.getPages()){\n          page = bookService2.getPage((int)page.getPages(), pageSize);\n     }\n     \n\n# 分页条件查询\n\n * IService\n   \n   //TODO Ctrl+F12  快速打开 文件目录\n   //使用mybatisPlus\n   //提供了很多基础的 CRUD操作  这些简单的不用自己再实现了\n   //自己去定义一些比较复杂的，切合项目需求的方法，尽量不要覆盖原有的方法\n   public interface IBookService extends IService<Book> {\n   \n       //自己定义方法\n       IPage<Book> getPage(int currentPage, int pageSize,Book book);   //分页操作\n   \n   }\n   \n\n * IServiceImpl ———— 在业务层完成 条件对象的创建及查询操作\n   \n   //分页操作\n   public IPage<Book> getPage(int currentPage, int pageSize, Book book){\n       //创建查询对象\n       LambdaQueryWrapper<Book> lqw=new LambdaQueryWrapper<Book>();\n       lqw.like(book.getType()!=null,Book::getType,book.getType())\n               .like(book.getName()!=null,Book::getName,book.getName())\n               .like(book.getDescription()!=null,Book::getDescription,book.getDescription());\n   \n       IPage<Book> page=new Page<Book>(currentPage,pageSize);\n       mapper.selectPage(page,lqw);\n       return page;\n   }\n   \n\n * BookController\n   \n   @GetMapping("/{currentPage}/{pageSize}") //分页条件查询\n   public Result bookIpage(@PathVariable int currentPage,@PathVariable int pageSize,Book book){//使用Book对象直接获取get方式的请求参数\n   \n       //分页条件查询\n       IPage<Book> page = bookService2.getPage(currentPage, pageSize,book);//条件查询的事情交给业务层去做\n   \n       //如果当前页码值大于了总页码值，那么重新执行查询操作，使用最大页码值作为当前页码值\n       if(page.getCurrent()>page.getPages()){\n           page = bookService2.getPage((int)page.getPages(), pageSize,book);\n       }\n       //page里的数据不为null 也不为空--isEmpty()\n       Integer code= (page.getRecords()!=null && !page.getRecords().isEmpty()) ? Code.GET_OK : Code.GET_ERR;\n       return new Result(code,page);\n   \n   }\n   \n\n\n# 教你一招\n\n# 阿里云建立项目\n\n * 使用阿里云网站 https://start.aliyun.com/ 创建SpringBoot工程\n\n# 隐藏指定文件\n\n',normalizedContent:'# 基础篇\n\n\n# springboot基础\n\n# 新建项目\n\n * 设置maven版本 （使用默认设置）\n * 测试一下https://start.spring.io/的连接情况， setting——>http——>检测连接，连接上才可以创建工程\n * 一般来说连接上都需要翻墙，新建项目都需要联网\n * 使用阿里云网站 https://start.aliyun.com/\n\n# 执行项目\n\n * 使用maven——package打一个jar包\n\n * 在命令行输入 java -jar jar包名 例如 java -jar demo1-0.0.1-snapshot.jar\n\n * ctrl+c 退出服务\n\n * springboot主启动类\n   \n   //springboot 主启动类\n   \n   @springbootapplication\n   public class demo1application {\n   \n       public static void main(string[] args) {\n           springapplication.run(demo1application.class, args);\n       }\n   \n   }\n   \n\n * 内嵌tomcat服务器\n   \n   * 内嵌tomcat服务器的工作原理是将tomcat服务器作为对象运行，并将对象交给spirng容器管理。\n\n * jar 包\n   \n   * maven 打包插件\n     \n     <plugins>\n        \x3c!-- maven 打包插件--\x3e\n        <plugin>\n           <groupid>org.springframework.boot</groupid>\n           <artifactid>spring-boot-maven-plugin</artifactid>\n        </plugin>\n     </plugins>\n     \n   \n   * jar包描述文件\n\n * 端口占用解决\n   \n   \n   \n   \n\n * idea中配置临时属性 （一般不用，了解即可）\n   \n   public static void main(string[] args) {\n   \n      //可以通过修改args来 增加一些临时配置\n      string[] arg=new string[1];\n      arg[0]="--server.port=8082";\n      springapplication.run(ssmapplication.class, arg);\n   \n      //可以在启动boot程序时断开读取外部临时配置对应的入口，也就是去掉读取外部参数的形参 args\n      //springapplication.run(ssmapplication.class);\n   }\n   \n\n * idea中的配置覆盖\n   \n   \n\n# linux下运行项目\n\n * 等学会linux，再来看 视频\n\n# 起步依赖\n\n * starter\n   \n   * springboot中常见的项目名称，定义了当前项目使用的所有项目坐标，以达到减少依赖配置的目的\n\n * parent\n   \n   * 所有springboot要继承的项目，定义了若干个坐标版本号，以达到减少依赖冲突的目的\n\n * 实际开发\n   \n   * 使用任意坐标时，仅书写gva中的g和a，v由spirngboot提供\n   * 如果发生坐标错误(可能性很小)，在指定version\n   * g: groupid a: artifactid v:version\n\n * 换技术\n   \n   不想用 tomcat，想用jetty\n   \n   jetty更轻量级一些，内部功能很少，自己用到什么配什么；而tomcat 里面集成了相当多的功能\n   \n   * 排除tomcat依赖\n     \n     <dependency>\n         <groupid>org.springframework.boot</groupid>\n         <artifactid>spring-boot-starter-web</artifactid>\n         <exclusions>\n             \x3c!-- 排除 tomcat 服务器--\x3e\n             <exclusion>\n                 <groupid>org.springframework.boot</groupid>\n                 <artifactid>spring-boot-starter-tomcat</artifactid>\n             </exclusion>\n         </exclusions>\n     </dependency>\n     \n   \n   * 引入jetty依赖\n     \n     <dependency>\x3c!-- 使用jetty服务器--\x3e\n         <groupid>org.springframework.boot</groupid>\n         <artifactid>spring-boot-starter-jetty</artifactid>\n     </dependency>\n     \n\n\n# 基础配置\n\n# 官方配置\n\n# 配件文件格式\n\n * appplication.properties\n   \n   * server.port=82\n\n * application.yml\n   \n   * 主写这个，\n   \n   * server:\n       port: 82   #设置端口号\n       servlet:\n         context-path: /123  #增加访问前缀\n       \n     #tomcat started on port(s): 82 (http) with context path \'/123\'\n     #访问时，需要增加前缀才可以访问到    \n     \n     #原访问路径    http://localhost:82/books/2\n     #现访问路径    http://localhost:82/123/books/2\n     \n\n * application.yaml\n   \n   * 这个好像不支持\n   \n   * server:\n       port: 81\n     \n\n * 配置文件优先级\n   \n   * appplication.properties > application.yml > application.yaml\n\n# yaml\n\n一种 数据序列化格式\n\n * 优点\n   \n   * 容易阅读\n   * 容易与脚本语言交互\n   * 以数据为核心，重数据轻格式\n\n * 文件扩展名\n   \n   * .yml （主流）\n   * .yaml\n\n * 语法规则\n   \n   * 大小写敏感\n   \n   * 属性层级关系使用多行描述，每行结尾使用冒号结束（或者数据结束）\n   \n   * 使用缩进表示层级关系，同层级左侧对齐，只允许使用空格(不允许使用tab键)——>(跟python类似)\n   \n   * 属性值前面添加空格（属性名与属性值之间使用冒号+空格作为分隔）\n   \n   * # 表示注释\n     \n     logging:\n       level:\n         root: info   #debug,info,warn   从前往后日志信息依次减少，info是默认\n     \n   \n   * 数组数据在数据书写位置的下方使用减号作为数据的开始符合，减号和数据之间用空格分离\n     \n     likes:\n       - game\n       - code\n     \n\n * yaml文件中的变量引用\n   \n   n: 崩坏三\n   game:\n     #  name: 崩坏三\n     name: ${n}  #引用文件中的变量    #这句话和上面的意思是一样的\n   \n\n * yaml文件解析转义字符\n   \n   lesson: "springboot\\tss"   #用引号引起来，可以读取转义字符  \\t 表示制表符tab\n   # 读出lesson的值为  springboot\tss\n   \n\n * 数据读取\n   \n   server:\n     port: 81\n   \n   logging:\n     level:\n       root: info   #debug,info,warn   从前往后日志信息依次减少，info是默认\n   \n   game:\n     name: 崩坏三\n     age: 4\n     likes:\n       - 丽塔\n       - 爱丽希雅\n       - diana\n   \n   lesson: springboot\n   \n   \n   * 单个读取\n     \n     //1.挨个配置，读取配置文件中的数据\n     @value("${lesson}")\n     private string lesson;\n     \n     @value("${game.likes[0]}")\n     private string like_0;\n     \n     @value("${logging.level.root}")\n     private string log;\n     \n   \n   * 读取全部\n     \n     //2.直接读取所有\n     @autowired\n     private environment environment;\n     \n     \n     system.out.println(environment.getproperty("lesson"));\n     system.out.println(environment.getproperty("game.likes[0]"));\n     system.out.println(environment.getproperty("logging.level.root"));\n     \n   \n   * 自定义对象读取\n     \n     * 这个用的多，可以在里面配置 mybatis 数据等，框架内部有定义好的对象，会读取咱们配置的数据，所有格式一定要对\n     \n     * 自定义对象数据警告解决方案\n       \n       \x3c!-- 自定义对象封装数据警告解决方案--\x3e\n       <dependency>\n           <groupid>org.springframework.boot</groupid>\n           <artifactid>spring-boot-configuration-processor</artifactid>\n           <optional>true</optional>\n       </dependency>\n       \n     \n     * 自定义对象写法\n       \n       @component\n       @configurationproperties(prefix = "game") //表明读取配置文件中的game\n       public class game {\n           private string name;\n           private integer age;\n           private string[] likes;\n       }\n       //3.使用java对象读取\n       @autowired\n       private game game;\n       system.out.println(game);\n       \n       \n\n# 多环境启动\n\n# yml\n\n * 单文件配置 demo1项目中\n   \n   * 设置不同的环境，它们之间用---分隔开\n   \n   * 他们之间的参数都是独立的，都是只归定义的环境所有，比如在 pro下定义的参数，使用test环境运行，就读取不到该参数。\n   \n   * 但是写在最上面的参数，是它们公有的，都可以读取到\n     \n     # 设置启用的环境\n     spring:\n       profiles:\n         active: pro\n     \n     # 这些参数是公共的，都可以使用\n     logging:\n       level:\n         root: info   #debug,info,warn   从前往后日志信息依次减少，info是默认\n     \n     game:\n       name: 崩坏三\n       age: 4\n       likes:\n         - 丽塔\n         - 爱丽希雅\n         - diana\n     \n     lesson: springboot\n     \n     ---\n     #开发\n     spring:\n       config:\n         activate:\n           on-profile: dev\n     server:\n       port: 81\n     \n     ---\n     \n     #生产\n     spring:\n       config:\n         activate:\n           on-profile: pro\n     server:\n       port: 82\n     \n     ---\n     \n     #测试\n     spring:\n       config:\n         activate:\n           on-profile: test\n     server:\n       port: 83\n     \n\n * 多文件配置 demo2项目中\n   \n   * 可以将一个文件分拆成4个，一个主的，一个开发，一个生产，一个测试\n     * 注意 文件命名 参照图片\n   \n   \n\n * 多环境开发独立配置文件\n   \n   \n   \n   \n\n# properties\n\ndemo1项目中\n\n * 只支持多文件配置\n\n * 存到了bak 备份文件夹中\n\n# maven与springboot\n\n * 运行jar包的话，是maven打的包，所以maven优先级要高\n\n * 在配置文件application.yml中引用pom文件中定义的属性\n   \n   * pom文件 要使用这个插件 帮助maven解析资源文件中的占位符\n     \n     \x3c!-- 告诉maven 我要处理资源文件——资源文件中可以用${}读取maven的属性值  默认是resources文件夹--\x3e\n     <plugin>\n         <groupid>org.apache.maven.plugins</groupid>\n         <artifactid>maven-resources-plugin</artifactid>\n         <version>3.2.0</version>\n         <configuration>\n             <encoding>utf-8</encoding>\n             <usedefaultdelimiters>true</usedefaultdelimiters>\n         </configuration>\n     </plugin>\n     \n     \n     \x3c!-- maven 多环境 配置--\x3e\n     \x3c!-- 配置多环境--\x3e\n     <profiles>\n         \x3c!-- 开发环境--\x3e\n         <profile>\n             \x3c!-- id 随便起--\x3e\n             <id>dev</id>\n             <properties>\n                 \x3c!-- 自定义 属性 名字随便起 --\x3e\n                 <profile.active>dev</profile.active>\n             </properties>\n             \x3c!-- 设定 是否为默认环境--\x3e\n             <activation>\n                 <activebydefault> true </activebydefault>\n             </activation>\n         </profile>\n     \n         \x3c!-- 生产环境--\x3e\n         <profile>\n             \x3c!-- id 随便起--\x3e\n             <id>pro</id>\n             <properties>\n                 \x3c!-- 自定义 属性 名字随便起 --\x3e\n                 <profile.active>pro</profile.active>\n             </properties>\n         </profile>\n     \n         \x3c!-- 测试环境--\x3e\n         <profile>\n             \x3c!-- id 随便起--\x3e\n             <id>test</id>\n             <properties>\n                 \x3c!-- 自定义 属性 名字随便起 --\x3e\n                 <profile.active>test</profile.active>\n             </properties>\n         </profile>\n     </profiles>\n     \n   \n   * application.yml 文件\n     \n     # 设置启用的环境\n     spring:\n       profiles:\n         #active: dev   #直接使用下面定义的 数据\n         active: ${profile.active} #使用pom文件中的 自定义的属性\n     \n\n# 参数配置\n\n * 打jar包\n   \n   * 把多余的配置文件 放入bak文件夹，留作备份，不用影响正常测试\n   * 把项目的语言环境设置为utf-8\n   * 使用 maven-clean ，清除之前的包\n   * 使用maven-package,打包\n\n * 运行jar包 （可以使用命令行参数输入，来覆盖默认的参数）\n   \n   * java jar -demo1-0.0.1-snapshot.jar 按照默认参数运行\n   * java jar -demo1-0.0.1-snapshot.jar --spring.profiles.active=test 修改运行环境为 test环境\n   * java jar -demo1-0.0.1-snapshot.jar --server.port=88 修改端口号为 88\n   * java jar -demo1-0.0.1-snapshot.jar --spring.profiles.active=test --server.port=88 修改运行环境为 test，同时修改端口号为88\n\n * 参数加载优先级\n   \n   \n\n# 配置文件分类\n\n1. 共有4种配置文件，1级 优先级最高，4级优先级最低 ———— 项目demo1中\n\n * file —— 一般做测试，或者生产的时候用这些来配置 —— 留作系统打包后设置通用属性\n   \n   * 1级 file: config/application.yml 在jar所在目录下新建一个config包，里面放入配置文件，优先级最高\n   \n   * 2级 file: application.yml 在jar所在目录下新建一个配置文件，优先级为第二\n\n * classpath —— 在做开发的时候，用这些配置 —— 留作系统开发阶段设置通用属性\n   \n   * 3级 classpath : config/application.yml 在resource文件下新建一个config包，里面放入配置文件\n   \n   * 4级 classpath : application.yml 在resource文件下新建一个配置文件，优先级最低\n\n 2. 自定义配置文件 ———— 项目ssm中\n    \n    * 默认配置文件名称为 application.yml,可以使用配置覆盖的方式，来自定义配置文件名称\n    * spring.config.name 只写名称，不写后缀名\n    \n    \n    \n    * 可以在一个值内，同时配置多个文件，后面会覆盖前面一样的，前面的其他配置正常生效。\n      * 例 diana——server.port=80 , diana——server.port=82 ,执行结果是 82\n    \n    \n\n 3. 基于微服务（springcloud）开发时，配置文件将使用配置中心进行管理，2 所说的自定义配置文件，用的地方不多，用处不大，了解即可。\n\n# 日志\n\ndemo2项目中\n\n# 代码\n\n * 代码1 —— 使用日志来记录 比直接打印输出要好\n   \n   import org.slf4j.loggerfactory;\n   import org.slf4j.logger;\n   \n   public class bookcontroller {\n   \n       //创建记录日志的对象\n       private static final logger log= loggerfactory.getlogger(bookcontroller.class);\n   \n   \n       @getmapping("/{id}")\n       public string getbyid(@pathvariable int id){\n   \n           //日志的使用    比使用打印输出要好\n           log.debug("debug..."); //默认不开启\n           log.info("info...");\n           log.warn("warn...");\n           log.error("error...");\n   \n           return "hello springboot!";\n       }\n   }\n   \n\n * 代码2—— 使用注解——使用 @slf4j 来代替 创建日志对象的代码 （需要导入 lombok 的包）\n   \n   //日志的使用\n   @slf4j\n   @restcontroller\n   @requestmapping("/books")\n   public class bookcontroller {\n   \n       //创建记录日志的对象\n   //    private static final logger log= loggerfactory.getlogger(bookcontroller.class);\n       //可以使用   @slf4j 来代替 这一行代码    需要导入 lombok 的包\n   \n       \n       @getmapping("/{id}")\n       public string getbyid(@pathvariable int id){\n   \n           //日志的使用    比使用打印输出要好\n           log.debug("debug...");\n           log.info("info...");\n           log.warn("warn...");\n           log.error("error...");\n   \n           return "hello springboot!";\n       }\n   }\n   \n\n# 总*配置文件\n\n * 可以设置总的日志级别 ——root\n * 可以设置某个组的日志级别 ——group\n\n#日志设置\nlogging:\n  #设置分组\n  group:\n    my: com.diana\n    isservice: com.alibaba\n  level:\n    #设置总的日志级别\n    root: info   #debug,info,warn   从前往后日志信息依次减少，info是默认\n\n    #设置某个包的日志级别\n    #com.diana.controller: debug    #不推荐使用太麻烦\n\n    #设置某个组设置日志级别\n    my: debug\n\n\n  #设置日志的模板格式\n  #pattern:\n    #console: "%d--%m %n" # %d 表示时间； %m 表示消息；  %n  表示换行\n    \n    #console: "%d--%clr(%5p) %n" # %5p 表示消息，5表示 统一长度，p表示日志级别 ；  %clr表示彩色，加在谁上面用括号包裹即可\n    \n    #console: "%d--%clr(%5p) -----[%16t] %-40c %n" # %t表示线程名称  %c表示类名  %-40c 表示长度限制40，负号表示左对齐\n    \n    #console: "%d--%clr(%5p) -----[%16t] %clr(%-40.40c){red} : %m %n" #%-40.40c 中的 .40 表示截取40位，就剩下40位\n    \n    # %clr(){red} 表示设置日志颜色  red-红   cyan-青色\n\n  #保存日志\n  file:\n    name: server.log # 名字随便起\n    #path:             #默认路径是项目最外部\n\n  #设置日志存储格式\n  logback:\n    rollingpolicy:\n      file-name-pattern: server.%d{yyyy-mm-dd}.%i.log  \n      #设置文件名格式 server.2022-04-30.1.log  %d 表示日期，{}里可以指定格式； %i表示第几个，从0开始\n      max-file-size: 4kb   #设置最大上限\n\n\n# 日志格式\n\n * 默认格式\n   \n   \n   \n   * 配置文件 修改日志输出格式\n     \n     * 一般不用自己写，了解即可\n     \n     #设置日志的模板格式\n     pattern:\n       #console: "%d--%m %n" # %d 表示时间； %m 表示消息；  %n  表示换行\n       \n       #console: "%d--%clr(%5p) %n" # %5p 表示消息，5表示 统一长度，p表示日志级别 ；  %clr表示彩色，加在谁上面用括号包裹即可\n       \n       #console: "%d--%clr(%5p) -----[%16t] %-40c %n" # %t表示线程名称  %c表示类名  %-40c 表示长度限制40，负号表示左对齐\n       \n       console: "%d--%clr(%5p) -----[%16t] %clr(%-40.40c){red} : %m %n" #%-40.40c 中的 .40 表示截取40位，就剩下40位\n       \n       # %clr(){red} 表示设置日志颜色  red-红   cyan-青色\n     \n\n# 日志存储\n\n#保存日志(单一日志，所有的日志都存入里面)\nfile:\n  name: server.log # 名字随便起\n  #path:             #默认路径是项目最外部\n\n#设置日志存储格式\nlogback:\n  rollingpolicy: #(设置滚动日志，日志存储到达上限后，新建一个日志)\n    file-name-pattern: server.%d{yyyy-mm-dd}.%i.log  \n    #设置文件名格式 server.2022-04-30.1.log  %d 表示日期，{}里可以指定格式； %i表示第几个，从0开始\n    max-file-size: 4kb   #设置最大上限\n\n\n\n# springboot整合\n\n# 整合 junit\n\n * @springboottest 设置junit加载springboot启动类（mybatisjunitapplication.java）\n * 测试类和主启动类，要在一个包下，不然加载不了配置\n\npackage com.diana;\n\n@springboottest\nclass mybatisjunitapplicationtests {\n   @autowired\n   private bookservice bookservice;\n    \n   @test\n   public void test1(){\n      bookservice.save();\n   }\n}\n\n\n * 测试类和引导类不在一个包下时，要使用classes属性，声明引导类 @springboottest(classes = junitmybatisapplication.class)\n   \n   package com.test;\n   \n   //测试类和引导类不在一个包下时，要使用classes属性，声明引导类\n   @springboottest(classes = junitmybatisapplication.class)\n   public class onetest {\n       @autowired\n       bookmapper bookmapper;\n   \n       @test\n       public void testmapper() {\n           system.out.println(bookmapper.getbyid(2));\n       }\n   }\n   \n\n# 整合mybatis\n\n * 配置文件\n   \n   * springboot 2.4.2 以前会有时区问题， 需要加上 servertimezone=utc\n   \n   * 如果要设置数据源，需要导入 druid，导入这个坐标的话，只能用下面配置数据源的方式引入druid\n     \n     <dependency>\n        <groupid>com.alibaba</groupid>\n        <artifactid>druid</artifactid>\n        <version>1.2.9</version>\n     </dependency>\n     \n   \n   #配置数据源 ————通用型\n   spring:\n     datasource:\n       driver-class-name: com.mysql.cj.jdbc.driver\n       url: jdbc:mysql:///test?usessl=false&useserverprepstmts=true&servertimezone=utc #2.4.2以前需要设置时区  最后一个\n       username: root\n       password: 1234\n   \n       type: com.alibaba.druid.pool.druiddatasource     #设置数据源\n   \n\n * mapper类\n   \n   加上@mapper 注解\n   \n   @mapper\n   public interface bookmapper {\n   \n       @select("select * from book where id=#{id}")\n       book getbyid(int id);\n   }\n   \n\n * 测试\n   \n   @springboottest\n   class junitmybatisapplicationtests {\n   \n      @autowired\n      bookservice bookservice;\n   \n      @autowired\n      bookmapper bookmapper;\n   \n      @test\n      public void testservice() {\n         bookservice.save();\n      }\n   \n      @test\n      public void testmapper() {\n         system.out.println(bookmapper.getbyid(2));\n      }\n   \n   }\n   \n\n# 整合 mybatisplus\n\n * 项目在 mybatisplus中。\n * md文件\n\n# 整合druid\n\n * 导入druid 简单使用，参照 整合mybatis中druid的导入\n\n * 整合druid\n   \n   * 导入坐标 --（整合druid专用坐标）\n     \n     <dependency>\n        <groupid>com.alibaba</groupid>\n        <artifactid>druid-spring-boot-starter</artifactid>\n        <version>1.2.9</version>\n     </dependency>\n     \n   \n   * 完成配置 --（只有导入了上面的坐标，才能这样配置）\n     \n     #整合druid\n     spring:\n       datasource:\n         druid:\n           driver-class-name: com.mysql.cj.jdbc.driver\n           url: jdbc:mysql:///test?usessl=false&useserverprepstmts=true&servertimezone=utc #2.4.2以前需要设置时区  最后一个\n           username: root\n           password: 1234\n     \n\n * 这两种方式 一样用，差不多\n\n# 整合三方技术\n\n * 导入对应的starter\n * 根据提供的配置格式，配置非默认值对应的配置项\n\n\n# ssm整合\n\n * 引入数据源\n   \n   \x3c!--todo 引入druid数据源--\x3e\n   <dependency>\n      <groupid>com.alibaba</groupid>\n      <artifactid>druid</artifactid>\n      <version>1.2.9</version>\n   </dependency>\n   \n   \n   # todo 配置数据源相关信息\n   spring:\n     datasource:\n       type: com.alibaba.druid.pool.druiddatasource\n       driver-class-name: com.mysql.cj.jdbc.driver\n       url: jdbc:mysql:///test?usessl=false&useserverprepstmts=true\n       username: root\n       password: 1234\n   \n   server:\n     port: 80\n   \n\n * 添加@mapper\n   \n   //todo 添加@mapper\n   @mapper\n   public interface bookmapper {}\n   \n\n * 删除配置文件\n\n * 将web静态资源放入resources下的static中\n   \n   * 这样也不需要进行静态资源的放行\n\n * todo 标签\n   \n   * 使用该标签 标注，可以在下方通过点击的方式，进行快速定位\n\n * ctrl+f12\n   \n   * 快速打开文件结构\n\n\n# ssmp 整合\n\n# 分页查询\n\n * mapper\n   \n   //使用mybatisplus\n   @mapper\n   public interface bookmapper2 extends basemapper<book> {}\n   //内部已经实现了一些基本的增删改查，不用自己手动实现了\n   //只需要自己手动实现一些比较复杂的多表操作即可\n   \n\n * iservice\n   \n   //todo ctrl+f12  快速打开 文件目录\n   //使用mybatisplus\n   //提供了很多基础的 crud操作  这些简单的不用自己再实现了\n   //自己去定义一些比较复杂的，切合项目需求的方法，尽量不要覆盖原有的方法\n   public interface ibookservice extends iservice<book> {\n       \n       //自己定义方法\n       ipage<book> getpage(int current, int pagesize);   //分页操作 \n   \n   }\n   \n\n * iserviceimpl\n   \n   //里面有很多基本的增删改操作\n   @service ////使用mybatisplus\n   public class bookserviceimp2 extends serviceimpl<bookmapper2,book>   implements ibookservice {\n   \n       @autowired\n       private bookmapper2 mapper;\n   \n       //实现自己定义的方法\n       //分页操作\n       public ipage<book> getpage(int current,int pagesize){\n           ipage<book> page=new page<book>(current,pagesize);\n           mapper.selectpage(page,null);\n           return page;\n       }\n   }\n   \n   \n   //ibookservicetest\n   \n   //基本的增删改查操作对应 ————book book=new book();\n   // 添加  save(book)\n   // 更新  updatebyid(book)    \n   // 按照id删除  removebyid(9)\n   // 查询单个  getbyid(3)\n   // 查询全部  list()\n   \n   \n   \n\n * bookcontroller\n   \n   @getmapping("/{currentpage}/{pagesize}") //分页操作\n   public result bookipage(@pathvariable int currentpage,@pathvariable int pagesize){\n       ipage<book> page = bookservice2.getpage(currentpage, pagesize);\n       //如果当前页码值大于了总页码值，那么重新执行查询操作，使用最大页码值作为当前页码值\n       if(page.getcurrent()>page.getpages()){\n            page = bookservice2.getpage((int)page.getpages(), pagesize);\n       }\n   \n       //page里的数据不为null 也不为空--isempty()\n       integer code= (page.getrecords()!=null && !page.getrecords().isempty()) ? code.get_ok : code.get_err;\n   \n       return new result(code,page);\n   \n   }\n   \n   \n   * 基于业务需求维护删除功能\n     \n     //如果当前页码值大于了总页码值，那么重新执行查询操作，使用最大页码值作为当前页码值\n     if(page.getcurrent()>page.getpages()){\n          page = bookservice2.getpage((int)page.getpages(), pagesize);\n     }\n     \n\n# 分页条件查询\n\n * iservice\n   \n   //todo ctrl+f12  快速打开 文件目录\n   //使用mybatisplus\n   //提供了很多基础的 crud操作  这些简单的不用自己再实现了\n   //自己去定义一些比较复杂的，切合项目需求的方法，尽量不要覆盖原有的方法\n   public interface ibookservice extends iservice<book> {\n   \n       //自己定义方法\n       ipage<book> getpage(int currentpage, int pagesize,book book);   //分页操作\n   \n   }\n   \n\n * iserviceimpl ———— 在业务层完成 条件对象的创建及查询操作\n   \n   //分页操作\n   public ipage<book> getpage(int currentpage, int pagesize, book book){\n       //创建查询对象\n       lambdaquerywrapper<book> lqw=new lambdaquerywrapper<book>();\n       lqw.like(book.gettype()!=null,book::gettype,book.gettype())\n               .like(book.getname()!=null,book::getname,book.getname())\n               .like(book.getdescription()!=null,book::getdescription,book.getdescription());\n   \n       ipage<book> page=new page<book>(currentpage,pagesize);\n       mapper.selectpage(page,lqw);\n       return page;\n   }\n   \n\n * bookcontroller\n   \n   @getmapping("/{currentpage}/{pagesize}") //分页条件查询\n   public result bookipage(@pathvariable int currentpage,@pathvariable int pagesize,book book){//使用book对象直接获取get方式的请求参数\n   \n       //分页条件查询\n       ipage<book> page = bookservice2.getpage(currentpage, pagesize,book);//条件查询的事情交给业务层去做\n   \n       //如果当前页码值大于了总页码值，那么重新执行查询操作，使用最大页码值作为当前页码值\n       if(page.getcurrent()>page.getpages()){\n           page = bookservice2.getpage((int)page.getpages(), pagesize,book);\n       }\n       //page里的数据不为null 也不为空--isempty()\n       integer code= (page.getrecords()!=null && !page.getrecords().isempty()) ? code.get_ok : code.get_err;\n       return new result(code,page);\n   \n   }\n   \n\n\n# 教你一招\n\n# 阿里云建立项目\n\n * 使用阿里云网站 https://start.aliyun.com/ 创建springboot工程\n\n# 隐藏指定文件\n\n',charsets:{cjk:!0}},{title:"原理篇",frontmatter:{title:"原理篇",date:"2023-07-03T21:01:41.000Z",permalink:"/pages/cd183e/",categories:["后端","SpringBoot"],tags:["知识","SpringBoot"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/50.SpringBoot/50.%E5%8E%9F%E7%90%86%E7%AF%87.html",relativePath:"01.后端/50.SpringBoot/50.原理篇.md",key:"v-e327c356",path:"/pages/cd183e/",headers:[{level:3,title:"学习目标",slug:"学习目标",normalizedTitle:"学习目标",charIndex:10},{level:3,title:"自动配置",slug:"自动配置",normalizedTitle:"自动配置",charIndex:89},{level:3,title:"自定义starter",slug:"自定义starter",normalizedTitle:"自定义starter",charIndex:5409},{level:3,title:"核心原理",slug:"核心原理",normalizedTitle:"核心原理",charIndex:13354}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688392125e3,headersStr:"学习目标 自动配置 自定义starter 核心原理",content:'# 原理篇\n\n\n# 学习目标\n\n * 掌握SpringBoot内部工作流程\n * 理解SpringBoot整合第三方技术的原理\n * 实现自定义开发整合第三方技术的组件\n\n\n# 自动配置\n\n# bean 加载方式\n\n * 1.2\n\n * 3.从spring配置中调用方法，创建对象\n   \n   * 默认配置时true，保证每次从配置类中执行方法时，操作的都是一个对象，直接从容器拿对象，而不会创建新的对象\n   * 若配置为false，则每次执行方法，都会创建一个新的对象\n\n * 4.1在spring配置类中加载原xml配置的东西\n   \n   * @ImportResource({"applicationContext-account.xml","applicationContext-account_ZJ.xml"})//导入配置文件的信息\n   * 当导入多个配置时，若有同名bean，加载哪个？\n     * xml优先级高于配置类，不管谁在前，永远加载xml\n     * 同级别配置，后加载的覆盖先加载的 前面使用c3p0 数据源，后面使用druid数据源——>druid 数据源生效\n   \n   @Test//测试从不同的地方加载两个同名bean，哪个bean生效\n   //两个dataSource 同名，一个c3p的，一个durid的\n   //1.以xml配置文件加载的bean优先\n   //2.同级别配置，后加载的覆盖先加载的\n   //前面使用c3p0 数据源，后面使用druid数据源——>druid 数据源生效\n   public void test_bean(){\n       AnnotationConfigApplicationContext app = new AnnotationConfigApplicationContext(SpringConfiguration.class);\n       Object dataSource = app.getBean("dataSource");\n       System.out.println(dataSource);\n   \n   }\n   \n\n * 4.2使用@Import({aaa.class})导入类\n   \n   * 导入普通类，会以全路径名称的方式导入类——com.diana.pojo.DataSource\n   * 导入配置类，不仅会以全路径名称的方式导入配置类，还会将类中的@Bean定义的bean加载。\n     * 且被加载的配置类，头上可以不用挂@Configuration\n   \n   \n\n * 5.在spring上下文中加载bean\n   \n   * 仅仅能在spring配置类中使用，使用xml配置的不可以使用\n   \n   \n\n * 6.导入实现ImportSelector（选择器）接口的类，实现对导入源的编程式处理\n   \n   * 可以使用AnnotationMetadata根据各式各样的条件判断元数据，来选择加载哪个bean————高端\n   * 返回类名加载bean——low\n   \n   \n\n * 7.导入实现@ImportBeanDefinitionRegistrar接口的类\n   \n   * 可以使用AnnotationMetadata根据各式各样的条件判断元数据，来选择加载哪个bean————高端\n   * 还可以使用BeanDefinitionRegistry去注册bean——高端\n   \n   \n\n * 8.导入实现@BeanDefinitionRegistryPostProcessor接口的类\n   \n   * 加载优先级最高，可以作为最终裁定\n   * 可以使用BeanDefinitionRegistry去注册bean——高端\n   \n   \n\n * 8种方式总结\n   \n   \n\n# bean加载控制\n\n * 5.6.7.8 都可以进行对bean加载进行控制（5,7,8——注册方式）(6——返回类名)\n   \n   * 使用if-else的判断形式\n   \n   public class MyImportSelector implements ImportSelector {\n   \n       public String[] selectImports(AnnotationMetadata annotationMetadata) {\n   \n           try {\n               //读取环境中的类名\n               Class<?> aClass = Class.forName("com.diana.bean.Mouse");\n               //做判断\n               if(aClass!=null){//如果存在老鼠类，则返回猫类\n                   //返回实体类\n                   return new String[]{"com.diana.bean.Cat"};\n               }\n           } catch (ClassNotFoundException e) {\n               //出异常返回狗类\n               return new String[]{"com.diana.bean.Dog"};\n           }\n   \n   //        return new String[]{"com.diana.bean.Dog"};\n           return null;\n   \n       }\n   }\n   \n\n * 使用Spring-Boot提供的注解@ConditionalOn...,来进行条件判断——springboot\n   \n   * 如果有容器中有mouse这个bean，且没有dog这个bean，就加载我的cat-tom\n   \n   @Component("tom")\n   @ConditionalOnBean(name="jerry")//如果有mouse这个bean，按bean名称匹配\n   @ConditionalOnMissingBean(Dog.class)//如果没有狗这个bean,按字节码对象匹配，与上面组合使用\n   public class Cat {\n   }\n   \n   \n   * 如果环境中有数据库这个类——导入了这个jar包，那么就加载我的dataSource\n   \n   @Bean\n   @ConditionalOnClass(name="com.mysql.jdbc.Driver")//如果有数据库这个类(加了pom坐标)，我在加载数据源的类\n   public DruidDataSource dataSource(){\n       return new DruidDataSource();\n   }\n   \n\n * 用途:\n   \n   > 可以将所有的配置抽取出来，配置各种限制条件，当需要使用这个技术的时候，对应的依赖Bean就进行加载，不用的时候，就不加载Bean，程序丝毫感知不到。\n\n# bean依赖属性配置\n\n * 将原定义在类中与yml文件关联的配置抽取出来，单独做成一个配置类，用配置类读取信息\n\n * 不要将类强制声明为spring管控的类—— 去掉@Component——使用@EnableConfigurationProperties\n   \n   //1.将原定义在类中与yml文件关联的配置抽取出来，单独做成一个配置类\n   //2.不要将类强制声明为spring管控的类—— 去掉@Component——使用@EnableConfigurationProperties\n   //@Component\n   @Data\n   @ConfigurationProperties(prefix = "cartoon")\n   public class CartoonCatAndMouseProperties {\n       private Cat cat;\n       private Mouse mouse;\n   }\n   \n\n * 使用@EnableConfigurationProperties来替代配置类的@Component\n\n * 在主启动类中 使用@Import来替代这个类中的@Component\n\n * 通过三目运算符读取配置类中的属性，实现，有配置就用配置值，无配置就用默认值\n   \n   //不要将类强制声明为spring管控的类—— 去掉@Component——使用@Import\n   //@Component\n   @Data\n   //@ConfigurationProperties(prefix = "cartoon")——抽取成配置类\n   //使用这个注解来替代上面那个，一是解了与配置文件的耦，二是配置类不需要强制声明称Bean\n   @EnableConfigurationProperties(CartoonCatAndMouseProperties.class)\n   \n   public class CartoonCatAndMouse {\n       private Cat cat;\n       private Mouse mouse;\n       private CartoonCatAndMouseProperties properties;\n   \n   \n       //将配置类注入进来\n       public CartoonCatAndMouse(CartoonCatAndMouseProperties properties){\n           this.properties=properties;\n           cat=new Cat();\n           //有值吗？ 有就用，没有就用默认\n           cat.setAge(properties.getCat()!=null && properties.getCat().getAge()!=null ? properties.getCat().getAge():1);\n           cat.setName(properties.getCat()!=null && StringUtils.hasText(properties.getCat().getName())? properties.getCat().getName():"Tom");\n   \n           mouse=new Mouse();\n           mouse.setAge(properties.getMouse()!=null && properties.getMouse().getAge()!=null ? properties.getMouse().getAge():2);\n           mouse.setName(properties.getMouse()!=null && StringUtils.hasText(properties.getMouse().getName())? properties.getMouse().getName():"jerry");\n       }\n   \n       public void play(){\n           System.out.println(cat.getAge()+"岁的"+cat.getName()+"和"+mouse.getAge()+"岁的"+mouse.getName()+"打起来了");\n       }\n   \n   }\n   \n\n# 自动配置原理\n\n * 自动配置原理\n\n\n\n * 技术集A定义位置， 26-156，共130个\n   \n   org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\\n\n\n\n * @ConditionalOn...设置条件判断，是否加载\n * 设置集B的某个类——读取配置文件的.properties类，有很多属性，有的有默认值，有的没有。\n * 在.yml中配置对应属性，可以覆盖原先定义的值，如果不设置值，则按照默认值来。\n\n# 变更自动配置\n\n * 自定义的自动配置类——抽取properties的类，在配置文件中进行配置即可\n * 配置文件中也可以放入一些自定义的Bean，启动时会自动导入\n   * 不需要在类上加//@Component\n   * 也不需要使用@Import(CartoonCatAndMouse.class)\n   * 程序启动后，自动进入容器，可以和定义的Bean一样使用，自动注入等\n\n\n\n\n# 自定义starter\n\n * 案例---统计独立IP访问次数\n   \n   * 功能\n     \n     * 每次进行访问网站行为均进行统计\n     * 后台每几秒输出一次监控信息（格式：IP+访问次数）\n   \n   * 需求分析\n     \n     \n\n * 自定义starter\n   \n   * 开发一个业务功能——IPCountService\n     \n     * 1.使用Map存储数据，一般来说会使用redis，这里进做演示使用\n     * 2.获取ip地址时，用到了HttpServletRequest对象，但是自定义starter中没有这个对象，由调用starter的工程负责注入\n     * 3.使用el表达式获取属性值——#{beanId.属性}\n       * @Scheduled(cron = "0/#{ipproperties.cycle} * * * * ?")//使用el表达式--/#{beanID.属性}\n     * 4.String.format()用来格式化输出字符串\n     \n     //统计ip访问次数\n     public class IPCountService {\n     \n         //定义存储Map\n         HashMap<String,Integer> ipCountMap=new HashMap<>();\n     \n         @Autowired//当前的request对象的注入工作 由使用当前starter的工程提供自动装配\n         HttpServletRequest httpServletRequest;\n     \n         @Autowired\n         private IpProperties ipProperties;\n     \n     \n         //每次调用当前操作，就记录当前访问ip，然后累加访问次数\n         public void count(){\n             //1.得到ip\n             String ip=httpServletRequest.getRemoteAddr();\n     //        System.out.println("________________________________"+ip);\n             //2.根据ip地址从Map取值，并递增\n             Integer ipCount = ipCountMap.get(ip);\n             if(ipCount==null){\n                 //如果是空，就放入1\n                 ipCountMap.put(ip,1);\n             }else{\n                 //如果有ip，就自增一次\n                 ipCountMap.put(ip,ipCount+1);\n             }\n         }\n     \n         //展示map中的存储数据\n     //    @Scheduled(cron = "0/5 * * * * ?")//写死 5秒\n         @Scheduled(cron = "0/#{ipproperties.cycle} * * * * ?")//使用el表达式--/#{beanID.属性}\n         public void print(){\n             System.out.println("IP访问监控");\n             if(ipProperties.getModel().equals(IpProperties.LogModel.DETAIL.getValue())){\n                 //详细模式\n                 System.out.println("+-----ip-adress-----+--num--+");\n     \n                 for (Map.Entry<String, Integer> entry : ipCountMap.entrySet()) {\n                     String key= entry.getKey();\n                     Integer value=entry.getValue();\n                     System.out.println(String.format("|%18s  |%5d  |",key,value));\n                 }\n                 System.out.println("+-------------------+-------+");\n             }else if(ipProperties.getModel().equals(IpProperties.LogModel.SIMPLE.getValue())){\n                 //极简模式\n                 System.out.println("+-----ip-adress-----+");\n     \n                 for (String key : ipCountMap.keySet()) {\n                     System.out.println(String.format("|%18s  |",key));\n                 }\n                 System.out.println("+-------------------+");\n             }\n     \n     \n             //如果设置为true 则情况数据\n             if(ipProperties.getCycleRest()){\n                 ipCountMap.clear();\n             }\n         }\n     }\n     \n   \n   * 自定一个自启动类——导入功能类，配置类\n     \n     * 理论来说，是要用标准方法，@EnableConfigurationProperties(IpProperties.class)来进入配置类的，但是在定时器部分，需要读属性名称，需要用到beanId，所以要换成@Import的方法。\n     * 虽然不标准，但是可以解决实际问题。\n     \n     //自启动类\n     @Import({IPCountService.class,IpProperties.class, SpringWebConfig.class})//导入功能类\n     @EnableScheduling //开启定时功能\n     \n     //@EnableConfigurationProperties(IpProperties.class)//导入配置类\n     //理论上来说，上面这个方法是标准的，但是 影响了我的使用，所以我不能按照标准的来\n     //使用定义bean，再导入bean的方式\n     public class IpAutoConfig {\n         \n     }\n     \n   \n   * 配置自启动文件——spring.factories\n     \n     # Auto Configure\n     org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\\n     com.liangbing.autoconfig.IpAutoConfig\n     \n   \n   * 定义一个读取yml配置文件的配置类\n     \n     * 1.对每个属性上面都要加注释说明，一个是方便自己和他人查阅，在就是Springboot可以读到文档，给出用户提示\n     * 2.定义了内部枚举类——如果有选择几类，最好用枚举的方式，更专业\n     * 3.使用@Component("ipproperties")声明成了 一个bean，为了在业务层中调用el表达式，读取beanID的值\n     \n     @Component("ipproperties")\n     //配置可设置的属性\n     @ConfigurationProperties(prefix = "tools.ip")\n     public class IpProperties {\n     \n         /**\n          * 日志显示周期\n          * 即每几秒刷新一次显示\n          */\n         private Long cycle=5L;\n     \n         /**\n          * 是否周期内重置数据\n          */\n         private Boolean cycleRest=false;\n     \n     \n         /**\n          * 日志输出格式\n          *  detail 详细模式\n          *  simple 极简模式\n          */\n         private String model=LogModel.DETAIL.value;\n     \n     \n         public Long getCycle() {\n             return cycle;\n         }\n     \n         public void setCycle(Long cycle) {\n             this.cycle = cycle;\n         }\n     \n         public Boolean getCycleRest() {\n             return cycleRest;\n         }\n     \n         public void setCycleRest(Boolean cycleRest) {\n             this.cycleRest = cycleRest;\n         }\n     \n         public String getModel() {\n             return model;\n         }\n     \n         public void setModel(String model) {\n             this.model = model;\n         }\n     \n         //定义内部枚举类\n         public enum LogModel{\n     \n             SIMPLE("simple"),\n             DETAIL("detail");\n     \n             private String value;\n             LogModel(String value){\n                 this.value=value;\n             }\n             public String getValue(){\n                 return value;\n             }\n         }\n     }\n     \n   \n   * 配置拦截器和springmvc配置\n     \n     * 要将网络层的东西也在starter中做好，这样用户只需导入starter，不需要动原来的代码，即可实现功能，方便用户。\n     \n     * 在springmvc配置中，导入拦截器配置类很关键，没有这句话，无法注入拦截器\n     \n     //拦截器 配置\n     public class ProjectInterceptor implements HandlerInterceptor {\n     \n         @Autowired\n         private IPCountService ipCountService;\n     \n         //在目标方法执行之前  执行   可以用来进行数据校验，权限判断 等\n         public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {\n             ipCountService.count();//调用自定义starter方法\n             return true;\n         }\n     }\n     \n     \n     //SpringWeb 配置\n     @Import(ProjectInterceptor.class)//这一个特别重要，没有这个外部调用，无法注入projectInterceptor\n     public class SpringWebConfig implements WebMvcConfigurer {\n     \n         @Autowired\n         public ProjectInterceptor projectInterceptor;\n     \n         @Override\n         public void addInterceptors(InterceptorRegistry registry) {\n             //添加拦截器，并拦截所有请求\n             registry.addInterceptor(projectInterceptor).addPathPatterns("/**");\n         }\n     }\n     \n   \n   * 开启yml提示功能\n     \n     * 导入坐标——它的唯一作用就是生成配置文件，生成后去掉即可，对外发布，不要带着这个坐标\n       \n       <dependency>\n           <groupId>org.springframework.boot</groupId>\n           <artifactId>spring-boot-configuration-processor</artifactId>\n       </dependency>\n       \n     \n     * 配置json文件——spring-configuration-metada.json\n       \n       * 需要自己配置的是 hints\n         * 里面是给出选择提示\n       \n       {\n         "groups": [\n           {\n             "name": "tools.ip",\n             "type": "com.liangbing.properties.IpProperties",\n             "sourceType": "com.liangbing.properties.IpProperties"\n           }\n         ],\n         "properties": [\n           {\n             "name": "tools.ip.cycle",\n             "type": "java.lang.Long",\n             "description": "日志显示周期 即每几秒刷新一次显示",\n             "sourceType": "com.liangbing.properties.IpProperties",\n             "defaultValue": 5\n           },\n           {\n             "name": "tools.ip.cycle-rest",\n             "type": "java.lang.Boolean",\n             "description": "是否周期内重置数据",\n             "sourceType": "com.liangbing.properties.IpProperties",\n             "defaultValue": false\n           },\n           {\n             "name": "tools.ip.model",\n             "type": "java.lang.String",\n             "description": "日志输出格式  detail 详细模式  simple 极简模式",\n             "sourceType": "com.liangbing.properties.IpProperties"\n           }\n         ],\n         "hints": [\n           {\n             "name": "tools.ip.model",\n             "values": [\n               {\n                 "value": "detail",\n                 "description": "详细模式."\n               },\n               {\n                 "value": "simple",\n                 "description": "简单模式."\n               }\n             ]\n           }\n         ]\n       }\n       \n   \n   * clean清除，install安装到本地仓库\n   \n   * 工程导入对应的pom坐标，即可实现业务功能\n\n\n# 核心原理\n\n# SpringBoot启动流程\n\n\n\n# 容器类型选择\n\n# 监听器\n\n',normalizedContent:'# 原理篇\n\n\n# 学习目标\n\n * 掌握springboot内部工作流程\n * 理解springboot整合第三方技术的原理\n * 实现自定义开发整合第三方技术的组件\n\n\n# 自动配置\n\n# bean 加载方式\n\n * 1.2\n\n * 3.从spring配置中调用方法，创建对象\n   \n   * 默认配置时true，保证每次从配置类中执行方法时，操作的都是一个对象，直接从容器拿对象，而不会创建新的对象\n   * 若配置为false，则每次执行方法，都会创建一个新的对象\n\n * 4.1在spring配置类中加载原xml配置的东西\n   \n   * @importresource({"applicationcontext-account.xml","applicationcontext-account_zj.xml"})//导入配置文件的信息\n   * 当导入多个配置时，若有同名bean，加载哪个？\n     * xml优先级高于配置类，不管谁在前，永远加载xml\n     * 同级别配置，后加载的覆盖先加载的 前面使用c3p0 数据源，后面使用druid数据源——>druid 数据源生效\n   \n   @test//测试从不同的地方加载两个同名bean，哪个bean生效\n   //两个datasource 同名，一个c3p的，一个durid的\n   //1.以xml配置文件加载的bean优先\n   //2.同级别配置，后加载的覆盖先加载的\n   //前面使用c3p0 数据源，后面使用druid数据源——>druid 数据源生效\n   public void test_bean(){\n       annotationconfigapplicationcontext app = new annotationconfigapplicationcontext(springconfiguration.class);\n       object datasource = app.getbean("datasource");\n       system.out.println(datasource);\n   \n   }\n   \n\n * 4.2使用@import({aaa.class})导入类\n   \n   * 导入普通类，会以全路径名称的方式导入类——com.diana.pojo.datasource\n   * 导入配置类，不仅会以全路径名称的方式导入配置类，还会将类中的@bean定义的bean加载。\n     * 且被加载的配置类，头上可以不用挂@configuration\n   \n   \n\n * 5.在spring上下文中加载bean\n   \n   * 仅仅能在spring配置类中使用，使用xml配置的不可以使用\n   \n   \n\n * 6.导入实现importselector（选择器）接口的类，实现对导入源的编程式处理\n   \n   * 可以使用annotationmetadata根据各式各样的条件判断元数据，来选择加载哪个bean————高端\n   * 返回类名加载bean——low\n   \n   \n\n * 7.导入实现@importbeandefinitionregistrar接口的类\n   \n   * 可以使用annotationmetadata根据各式各样的条件判断元数据，来选择加载哪个bean————高端\n   * 还可以使用beandefinitionregistry去注册bean——高端\n   \n   \n\n * 8.导入实现@beandefinitionregistrypostprocessor接口的类\n   \n   * 加载优先级最高，可以作为最终裁定\n   * 可以使用beandefinitionregistry去注册bean——高端\n   \n   \n\n * 8种方式总结\n   \n   \n\n# bean加载控制\n\n * 5.6.7.8 都可以进行对bean加载进行控制（5,7,8——注册方式）(6——返回类名)\n   \n   * 使用if-else的判断形式\n   \n   public class myimportselector implements importselector {\n   \n       public string[] selectimports(annotationmetadata annotationmetadata) {\n   \n           try {\n               //读取环境中的类名\n               class<?> aclass = class.forname("com.diana.bean.mouse");\n               //做判断\n               if(aclass!=null){//如果存在老鼠类，则返回猫类\n                   //返回实体类\n                   return new string[]{"com.diana.bean.cat"};\n               }\n           } catch (classnotfoundexception e) {\n               //出异常返回狗类\n               return new string[]{"com.diana.bean.dog"};\n           }\n   \n   //        return new string[]{"com.diana.bean.dog"};\n           return null;\n   \n       }\n   }\n   \n\n * 使用spring-boot提供的注解@conditionalon...,来进行条件判断——springboot\n   \n   * 如果有容器中有mouse这个bean，且没有dog这个bean，就加载我的cat-tom\n   \n   @component("tom")\n   @conditionalonbean(name="jerry")//如果有mouse这个bean，按bean名称匹配\n   @conditionalonmissingbean(dog.class)//如果没有狗这个bean,按字节码对象匹配，与上面组合使用\n   public class cat {\n   }\n   \n   \n   * 如果环境中有数据库这个类——导入了这个jar包，那么就加载我的datasource\n   \n   @bean\n   @conditionalonclass(name="com.mysql.jdbc.driver")//如果有数据库这个类(加了pom坐标)，我在加载数据源的类\n   public druiddatasource datasource(){\n       return new druiddatasource();\n   }\n   \n\n * 用途:\n   \n   > 可以将所有的配置抽取出来，配置各种限制条件，当需要使用这个技术的时候，对应的依赖bean就进行加载，不用的时候，就不加载bean，程序丝毫感知不到。\n\n# bean依赖属性配置\n\n * 将原定义在类中与yml文件关联的配置抽取出来，单独做成一个配置类，用配置类读取信息\n\n * 不要将类强制声明为spring管控的类—— 去掉@component——使用@enableconfigurationproperties\n   \n   //1.将原定义在类中与yml文件关联的配置抽取出来，单独做成一个配置类\n   //2.不要将类强制声明为spring管控的类—— 去掉@component——使用@enableconfigurationproperties\n   //@component\n   @data\n   @configurationproperties(prefix = "cartoon")\n   public class cartooncatandmouseproperties {\n       private cat cat;\n       private mouse mouse;\n   }\n   \n\n * 使用@enableconfigurationproperties来替代配置类的@component\n\n * 在主启动类中 使用@import来替代这个类中的@component\n\n * 通过三目运算符读取配置类中的属性，实现，有配置就用配置值，无配置就用默认值\n   \n   //不要将类强制声明为spring管控的类—— 去掉@component——使用@import\n   //@component\n   @data\n   //@configurationproperties(prefix = "cartoon")——抽取成配置类\n   //使用这个注解来替代上面那个，一是解了与配置文件的耦，二是配置类不需要强制声明称bean\n   @enableconfigurationproperties(cartooncatandmouseproperties.class)\n   \n   public class cartooncatandmouse {\n       private cat cat;\n       private mouse mouse;\n       private cartooncatandmouseproperties properties;\n   \n   \n       //将配置类注入进来\n       public cartooncatandmouse(cartooncatandmouseproperties properties){\n           this.properties=properties;\n           cat=new cat();\n           //有值吗？ 有就用，没有就用默认\n           cat.setage(properties.getcat()!=null && properties.getcat().getage()!=null ? properties.getcat().getage():1);\n           cat.setname(properties.getcat()!=null && stringutils.hastext(properties.getcat().getname())? properties.getcat().getname():"tom");\n   \n           mouse=new mouse();\n           mouse.setage(properties.getmouse()!=null && properties.getmouse().getage()!=null ? properties.getmouse().getage():2);\n           mouse.setname(properties.getmouse()!=null && stringutils.hastext(properties.getmouse().getname())? properties.getmouse().getname():"jerry");\n       }\n   \n       public void play(){\n           system.out.println(cat.getage()+"岁的"+cat.getname()+"和"+mouse.getage()+"岁的"+mouse.getname()+"打起来了");\n       }\n   \n   }\n   \n\n# 自动配置原理\n\n * 自动配置原理\n\n\n\n * 技术集a定义位置， 26-156，共130个\n   \n   org.springframework.boot.autoconfigure.enableautoconfiguration=\\\n\n\n\n * @conditionalon...设置条件判断，是否加载\n * 设置集b的某个类——读取配置文件的.properties类，有很多属性，有的有默认值，有的没有。\n * 在.yml中配置对应属性，可以覆盖原先定义的值，如果不设置值，则按照默认值来。\n\n# 变更自动配置\n\n * 自定义的自动配置类——抽取properties的类，在配置文件中进行配置即可\n * 配置文件中也可以放入一些自定义的bean，启动时会自动导入\n   * 不需要在类上加//@component\n   * 也不需要使用@import(cartooncatandmouse.class)\n   * 程序启动后，自动进入容器，可以和定义的bean一样使用，自动注入等\n\n\n\n\n# 自定义starter\n\n * 案例---统计独立ip访问次数\n   \n   * 功能\n     \n     * 每次进行访问网站行为均进行统计\n     * 后台每几秒输出一次监控信息（格式：ip+访问次数）\n   \n   * 需求分析\n     \n     \n\n * 自定义starter\n   \n   * 开发一个业务功能——ipcountservice\n     \n     * 1.使用map存储数据，一般来说会使用redis，这里进做演示使用\n     * 2.获取ip地址时，用到了httpservletrequest对象，但是自定义starter中没有这个对象，由调用starter的工程负责注入\n     * 3.使用el表达式获取属性值——#{beanid.属性}\n       * @scheduled(cron = "0/#{ipproperties.cycle} * * * * ?")//使用el表达式--/#{beanid.属性}\n     * 4.string.format()用来格式化输出字符串\n     \n     //统计ip访问次数\n     public class ipcountservice {\n     \n         //定义存储map\n         hashmap<string,integer> ipcountmap=new hashmap<>();\n     \n         @autowired//当前的request对象的注入工作 由使用当前starter的工程提供自动装配\n         httpservletrequest httpservletrequest;\n     \n         @autowired\n         private ipproperties ipproperties;\n     \n     \n         //每次调用当前操作，就记录当前访问ip，然后累加访问次数\n         public void count(){\n             //1.得到ip\n             string ip=httpservletrequest.getremoteaddr();\n     //        system.out.println("________________________________"+ip);\n             //2.根据ip地址从map取值，并递增\n             integer ipcount = ipcountmap.get(ip);\n             if(ipcount==null){\n                 //如果是空，就放入1\n                 ipcountmap.put(ip,1);\n             }else{\n                 //如果有ip，就自增一次\n                 ipcountmap.put(ip,ipcount+1);\n             }\n         }\n     \n         //展示map中的存储数据\n     //    @scheduled(cron = "0/5 * * * * ?")//写死 5秒\n         @scheduled(cron = "0/#{ipproperties.cycle} * * * * ?")//使用el表达式--/#{beanid.属性}\n         public void print(){\n             system.out.println("ip访问监控");\n             if(ipproperties.getmodel().equals(ipproperties.logmodel.detail.getvalue())){\n                 //详细模式\n                 system.out.println("+-----ip-adress-----+--num--+");\n     \n                 for (map.entry<string, integer> entry : ipcountmap.entryset()) {\n                     string key= entry.getkey();\n                     integer value=entry.getvalue();\n                     system.out.println(string.format("|%18s  |%5d  |",key,value));\n                 }\n                 system.out.println("+-------------------+-------+");\n             }else if(ipproperties.getmodel().equals(ipproperties.logmodel.simple.getvalue())){\n                 //极简模式\n                 system.out.println("+-----ip-adress-----+");\n     \n                 for (string key : ipcountmap.keyset()) {\n                     system.out.println(string.format("|%18s  |",key));\n                 }\n                 system.out.println("+-------------------+");\n             }\n     \n     \n             //如果设置为true 则情况数据\n             if(ipproperties.getcyclerest()){\n                 ipcountmap.clear();\n             }\n         }\n     }\n     \n   \n   * 自定一个自启动类——导入功能类，配置类\n     \n     * 理论来说，是要用标准方法，@enableconfigurationproperties(ipproperties.class)来进入配置类的，但是在定时器部分，需要读属性名称，需要用到beanid，所以要换成@import的方法。\n     * 虽然不标准，但是可以解决实际问题。\n     \n     //自启动类\n     @import({ipcountservice.class,ipproperties.class, springwebconfig.class})//导入功能类\n     @enablescheduling //开启定时功能\n     \n     //@enableconfigurationproperties(ipproperties.class)//导入配置类\n     //理论上来说，上面这个方法是标准的，但是 影响了我的使用，所以我不能按照标准的来\n     //使用定义bean，再导入bean的方式\n     public class ipautoconfig {\n         \n     }\n     \n   \n   * 配置自启动文件——spring.factories\n     \n     # auto configure\n     org.springframework.boot.autoconfigure.enableautoconfiguration=\\\n     com.liangbing.autoconfig.ipautoconfig\n     \n   \n   * 定义一个读取yml配置文件的配置类\n     \n     * 1.对每个属性上面都要加注释说明，一个是方便自己和他人查阅，在就是springboot可以读到文档，给出用户提示\n     * 2.定义了内部枚举类——如果有选择几类，最好用枚举的方式，更专业\n     * 3.使用@component("ipproperties")声明成了 一个bean，为了在业务层中调用el表达式，读取beanid的值\n     \n     @component("ipproperties")\n     //配置可设置的属性\n     @configurationproperties(prefix = "tools.ip")\n     public class ipproperties {\n     \n         /**\n          * 日志显示周期\n          * 即每几秒刷新一次显示\n          */\n         private long cycle=5l;\n     \n         /**\n          * 是否周期内重置数据\n          */\n         private boolean cyclerest=false;\n     \n     \n         /**\n          * 日志输出格式\n          *  detail 详细模式\n          *  simple 极简模式\n          */\n         private string model=logmodel.detail.value;\n     \n     \n         public long getcycle() {\n             return cycle;\n         }\n     \n         public void setcycle(long cycle) {\n             this.cycle = cycle;\n         }\n     \n         public boolean getcyclerest() {\n             return cyclerest;\n         }\n     \n         public void setcyclerest(boolean cyclerest) {\n             this.cyclerest = cyclerest;\n         }\n     \n         public string getmodel() {\n             return model;\n         }\n     \n         public void setmodel(string model) {\n             this.model = model;\n         }\n     \n         //定义内部枚举类\n         public enum logmodel{\n     \n             simple("simple"),\n             detail("detail");\n     \n             private string value;\n             logmodel(string value){\n                 this.value=value;\n             }\n             public string getvalue(){\n                 return value;\n             }\n         }\n     }\n     \n   \n   * 配置拦截器和springmvc配置\n     \n     * 要将网络层的东西也在starter中做好，这样用户只需导入starter，不需要动原来的代码，即可实现功能，方便用户。\n     \n     * 在springmvc配置中，导入拦截器配置类很关键，没有这句话，无法注入拦截器\n     \n     //拦截器 配置\n     public class projectinterceptor implements handlerinterceptor {\n     \n         @autowired\n         private ipcountservice ipcountservice;\n     \n         //在目标方法执行之前  执行   可以用来进行数据校验，权限判断 等\n         public boolean prehandle(httpservletrequest request, httpservletresponse response, object handler) throws exception {\n             ipcountservice.count();//调用自定义starter方法\n             return true;\n         }\n     }\n     \n     \n     //springweb 配置\n     @import(projectinterceptor.class)//这一个特别重要，没有这个外部调用，无法注入projectinterceptor\n     public class springwebconfig implements webmvcconfigurer {\n     \n         @autowired\n         public projectinterceptor projectinterceptor;\n     \n         @override\n         public void addinterceptors(interceptorregistry registry) {\n             //添加拦截器，并拦截所有请求\n             registry.addinterceptor(projectinterceptor).addpathpatterns("/**");\n         }\n     }\n     \n   \n   * 开启yml提示功能\n     \n     * 导入坐标——它的唯一作用就是生成配置文件，生成后去掉即可，对外发布，不要带着这个坐标\n       \n       <dependency>\n           <groupid>org.springframework.boot</groupid>\n           <artifactid>spring-boot-configuration-processor</artifactid>\n       </dependency>\n       \n     \n     * 配置json文件——spring-configuration-metada.json\n       \n       * 需要自己配置的是 hints\n         * 里面是给出选择提示\n       \n       {\n         "groups": [\n           {\n             "name": "tools.ip",\n             "type": "com.liangbing.properties.ipproperties",\n             "sourcetype": "com.liangbing.properties.ipproperties"\n           }\n         ],\n         "properties": [\n           {\n             "name": "tools.ip.cycle",\n             "type": "java.lang.long",\n             "description": "日志显示周期 即每几秒刷新一次显示",\n             "sourcetype": "com.liangbing.properties.ipproperties",\n             "defaultvalue": 5\n           },\n           {\n             "name": "tools.ip.cycle-rest",\n             "type": "java.lang.boolean",\n             "description": "是否周期内重置数据",\n             "sourcetype": "com.liangbing.properties.ipproperties",\n             "defaultvalue": false\n           },\n           {\n             "name": "tools.ip.model",\n             "type": "java.lang.string",\n             "description": "日志输出格式  detail 详细模式  simple 极简模式",\n             "sourcetype": "com.liangbing.properties.ipproperties"\n           }\n         ],\n         "hints": [\n           {\n             "name": "tools.ip.model",\n             "values": [\n               {\n                 "value": "detail",\n                 "description": "详细模式."\n               },\n               {\n                 "value": "simple",\n                 "description": "简单模式."\n               }\n             ]\n           }\n         ]\n       }\n       \n   \n   * clean清除，install安装到本地仓库\n   \n   * 工程导入对应的pom坐标，即可实现业务功能\n\n\n# 核心原理\n\n# springboot启动流程\n\n\n\n# 容器类型选择\n\n# 监听器\n\n',charsets:{cjk:!0}},{title:"《SpringBoot》",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"01.后端/50.SpringBoot",imgUrl:"/assets/img/SpringBoot.png",description:"SpringBoot学习笔记--整理自黑马程序员，在原教程基础上添加学习笔记"}},title:"《SpringBoot》",date:"2023-06-30T20:30:40.000Z",permalink:"/back/SpringBoot/",article:!1,comment:!1,editLink:!1,author:{name:"凉冰",link:"https://github.com/diana-devil"}},regularPath:"/01.%E5%90%8E%E7%AB%AF/50.SpringBoot/",relativePath:"01.后端/50.SpringBoot/README.md",key:"v-5c23f220",path:"/back/SpringBoot/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"安装Docker",frontmatter:{autoSort:100,title:"安装Docker",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/04da56/",categories:["后端","微服务","Docker"],tags:["知识","微服务","Docker"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/10.Docker/05.%E5%AE%89%E8%A3%85Docker.html",relativePath:"01.后端/60.微服务/10.Docker/05.安装Docker.md",key:"v-6e2eff3a",path:"/pages/04da56/",headers:[{level:2,title:"CentOS安装Docker",slug:"centos安装docker",normalizedTitle:"centos安装docker",charIndex:173},{level:3,title:"卸载（可选）",slug:"卸载-可选",normalizedTitle:"卸载（可选）",charIndex:282},{level:3,title:"安装docker",slug:"安装docker",normalizedTitle:"安装docker",charIndex:709},{level:3,title:"启动docker",slug:"启动docker",normalizedTitle:"启动docker",charIndex:1173},{level:3,title:"配置镜像加速",slug:"配置镜像加速",normalizedTitle:"配置镜像加速",charIndex:1585},{level:2,title:"CentOS7安装DockerCompose",slug:"centos7安装dockercompose",normalizedTitle:"centos7安装dockercompose",charIndex:1902},{level:3,title:"下载",slug:"下载",normalizedTitle:"下载",charIndex:1929},{level:3,title:"修改文件权限",slug:"修改文件权限",normalizedTitle:"修改文件权限",charIndex:2170},{level:3,title:"Base自动补全命令：",slug:"base自动补全命令",normalizedTitle:"base自动补全命令：",charIndex:2238},{level:2,title:"Docker镜像仓库",slug:"docker镜像仓库",normalizedTitle:"docker镜像仓库",charIndex:2523},{level:3,title:"简化版镜像仓库",slug:"简化版镜像仓库",normalizedTitle:"简化版镜像仓库",charIndex:2619},{level:3,title:"带有图形化界面版本",slug:"带有图形化界面版本",normalizedTitle:"带有图形化界面版本",charIndex:2970},{level:3,title:"配置Docker信任地址",slug:"配置docker信任地址",normalizedTitle:"配置docker信任地址",charIndex:3344}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"CentOS安装Docker 卸载（可选） 安装docker 启动docker 配置镜像加速 CentOS7安装DockerCompose 下载 修改文件权限 Base自动补全命令： Docker镜像仓库 简化版镜像仓库 带有图形化界面版本 配置Docker信任地址",content:'Docker 分为 CE 和 EE 两大版本。CE 即社区版（免费，支持周期 7 个月），EE 即企业版，强调安全，付费使用，支持周期 24 个月。\n\nDocker CE 分为 stable test 和 nightly 三个更新频道。\n\n官方网站上有各种环境下的 安装指南，这里主要介绍 Docker CE 在 CentOS上的安装。\n\n\n# CentOS安装Docker\n\nDocker CE 支持 64 位版本 CentOS 7，并且要求内核版本不低于 3.10， CentOS 7 满足最低内核的要求，所以我们在CentOS 7安装Docker。\n\n\n# 卸载（可选）\n\n如果之前安装过旧版本的Docker，可以使用下面命令卸载：\n\nyum remove docker \\\n                  docker-client \\\n                  docker-client-latest \\\n                  docker-common \\\n                  docker-latest \\\n                  docker-latest-logrotate \\\n                  docker-logrotate \\\n                  docker-selinux \\\n                  docker-engine-selinux \\\n                  docker-engine \\\n                  docker-ce\n\n\n\n# 安装docker\n\n首先需要大家虚拟机联网，安装yum工具\n\nyum install -y yum-utils \\\n           device-mapper-persistent-data \\\n           lvm2 --skip-broken\n\n\n然后更新本地镜像源：\n\n# 设置docker镜像源\nyum-config-manager \\\n    --add-repo \\\n    https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n    \nsed -i \'s/download.docker.com/mirrors.aliyun.com\\/docker-ce/g\' /etc/yum.repos.d/docker-ce.repo\n\nyum makecache fast\n\n\n然后输入命令：\n\nyum install -y docker-ce\n\n\ndocker-ce为社区免费版本。稍等片刻，docker即可安装成功。\n\n\n# 启动docker\n\nDocker应用需要用到各种端口，逐一去修改防火墙设置。非常麻烦，因此建议大家直接关闭防火墙！\n\n启动docker前，一定要关闭防火墙后！！\n\n启动docker前，一定要关闭防火墙后！！\n\n启动docker前，一定要关闭防火墙后！！\n\n# 关闭\nsystemctl stop firewalld\n# 禁止开机启动防火墙\nsystemctl disable firewalld\n\n\n通过命令启动docker：\n\nsystemctl start docker  # 启动docker服务\n\nsystemctl status docker #查看docker状态\n\nsystemctl stop docker  # 停止docker服务\n\nsystemctl restart docker  # 重启docker服务\n\n\n然后输入命令，可以查看docker版本：\n\ndocker -v\n\n\n如图：\n\n\n\n\n# 配置镜像加速\n\ndocker官方镜像仓库网速较差，我们需要设置国内镜像服务：\n\n参考阿里云的镜像加速文档：https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors\n\nsudo mkdir -p /etc/docker\nsudo tee /etc/docker/daemon.json <<-\'EOF\'\n{\n  "registry-mirrors": ["https://q58amku2.mirror.aliyuncs.com"]\n}\nEOF\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n\n\n\n# CentOS7安装DockerCompose\n\n\n# 下载\n\nLinux下需要通过命令下载：\n\n# 安装\ncurl -L https://github.com/docker/compose/releases/download/1.23.1/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose\n\n\n如果下载速度较慢，或者下载失败，可以使用课前资料提供的docker-compose文件：\n\n\n\n上传到/usr/local/bin/目录也可以。\n\n\n# 修改文件权限\n\n修改文件权限：\n\n# 修改权限\nchmod +x /usr/local/bin/docker-compose\n\n\n\n# Base自动补全命令：\n\n# 补全命令---还是得翻墙才好用\ncurl -L https://raw.githubusercontent.com/docker/compose/1.29.1/contrib/completion/bash/docker-compose > /etc/bash_completion.d/docker-compose\n\n\n\n\n如果这里出现错误，需要修改自己的hosts文件：\n\necho "199.232.68.133 raw.githubusercontent.com" >> /etc/hosts\n#这里不需要，翻墙就可以了\n\n\n\n# Docker镜像仓库\n\n搭建镜像仓库可以基于Docker官方提供的DockerRegistry来实现。\n\n官网地址：https://hub.docker.com/_/registry\n\n\n# 简化版镜像仓库\n\nDocker官方的Docker Registry是一个基础版本的Docker镜像仓库，具备仓库管理的完整功能，但是没有图形化界面。\n\n搭建方式比较简单，命令如下：\n\ndocker run -d \\\n    --restart=always \\\n    --name registry\t\\\n    -p 5000:5000 \\\n    -v registry-data:/var/lib/registry \\\n    registry\n\n\n命令中挂载了一个数据卷registry-data到容器内的/var/lib/registry 目录，这是私有镜像库存放数据的目录。\n\n访问http://YourIp:5000/v2/_catalog 可以查看当前私有镜像服务中包含的镜像\n\n\n# 带有图形化界面版本\n\n使用DockerCompose部署带有图象界面的DockerRegistry，命令如下：\n\nversion: \'3.0\'\nservices:\n  registry:\n    image: registry\n    volumes:\n      - ./registry-data:/var/lib/registry\n  ui:\n    image: joxit/docker-registry-ui:static\n    ports:\n      - 8080:80\n    environment:\n      - REGISTRY_TITLE=传智教育私有仓库\n      - REGISTRY_URL=http://registry:5000\n    depends_on:\n      - registry\n\n\n\n# 配置Docker信任地址\n\n我们的私服采用的是http协议，默认不被Docker信任，所以需要做一个配置：\n\n# 打开要修改的文件\nvi /etc/docker/daemon.json\n# 添加内容：\n"insecure-registries":["http://192.168.159.100:8080"]\n# 重加载\nsystemctl daemon-reload\n# 重启docker\nsystemctl restart docker\n',normalizedContent:'docker 分为 ce 和 ee 两大版本。ce 即社区版（免费，支持周期 7 个月），ee 即企业版，强调安全，付费使用，支持周期 24 个月。\n\ndocker ce 分为 stable test 和 nightly 三个更新频道。\n\n官方网站上有各种环境下的 安装指南，这里主要介绍 docker ce 在 centos上的安装。\n\n\n# centos安装docker\n\ndocker ce 支持 64 位版本 centos 7，并且要求内核版本不低于 3.10， centos 7 满足最低内核的要求，所以我们在centos 7安装docker。\n\n\n# 卸载（可选）\n\n如果之前安装过旧版本的docker，可以使用下面命令卸载：\n\nyum remove docker \\\n                  docker-client \\\n                  docker-client-latest \\\n                  docker-common \\\n                  docker-latest \\\n                  docker-latest-logrotate \\\n                  docker-logrotate \\\n                  docker-selinux \\\n                  docker-engine-selinux \\\n                  docker-engine \\\n                  docker-ce\n\n\n\n# 安装docker\n\n首先需要大家虚拟机联网，安装yum工具\n\nyum install -y yum-utils \\\n           device-mapper-persistent-data \\\n           lvm2 --skip-broken\n\n\n然后更新本地镜像源：\n\n# 设置docker镜像源\nyum-config-manager \\\n    --add-repo \\\n    https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n    \nsed -i \'s/download.docker.com/mirrors.aliyun.com\\/docker-ce/g\' /etc/yum.repos.d/docker-ce.repo\n\nyum makecache fast\n\n\n然后输入命令：\n\nyum install -y docker-ce\n\n\ndocker-ce为社区免费版本。稍等片刻，docker即可安装成功。\n\n\n# 启动docker\n\ndocker应用需要用到各种端口，逐一去修改防火墙设置。非常麻烦，因此建议大家直接关闭防火墙！\n\n启动docker前，一定要关闭防火墙后！！\n\n启动docker前，一定要关闭防火墙后！！\n\n启动docker前，一定要关闭防火墙后！！\n\n# 关闭\nsystemctl stop firewalld\n# 禁止开机启动防火墙\nsystemctl disable firewalld\n\n\n通过命令启动docker：\n\nsystemctl start docker  # 启动docker服务\n\nsystemctl status docker #查看docker状态\n\nsystemctl stop docker  # 停止docker服务\n\nsystemctl restart docker  # 重启docker服务\n\n\n然后输入命令，可以查看docker版本：\n\ndocker -v\n\n\n如图：\n\n\n\n\n# 配置镜像加速\n\ndocker官方镜像仓库网速较差，我们需要设置国内镜像服务：\n\n参考阿里云的镜像加速文档：https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors\n\nsudo mkdir -p /etc/docker\nsudo tee /etc/docker/daemon.json <<-\'eof\'\n{\n  "registry-mirrors": ["https://q58amku2.mirror.aliyuncs.com"]\n}\neof\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n\n\n\n# centos7安装dockercompose\n\n\n# 下载\n\nlinux下需要通过命令下载：\n\n# 安装\ncurl -l https://github.com/docker/compose/releases/download/1.23.1/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose\n\n\n如果下载速度较慢，或者下载失败，可以使用课前资料提供的docker-compose文件：\n\n\n\n上传到/usr/local/bin/目录也可以。\n\n\n# 修改文件权限\n\n修改文件权限：\n\n# 修改权限\nchmod +x /usr/local/bin/docker-compose\n\n\n\n# base自动补全命令：\n\n# 补全命令---还是得翻墙才好用\ncurl -l https://raw.githubusercontent.com/docker/compose/1.29.1/contrib/completion/bash/docker-compose > /etc/bash_completion.d/docker-compose\n\n\n\n\n如果这里出现错误，需要修改自己的hosts文件：\n\necho "199.232.68.133 raw.githubusercontent.com" >> /etc/hosts\n#这里不需要，翻墙就可以了\n\n\n\n# docker镜像仓库\n\n搭建镜像仓库可以基于docker官方提供的dockerregistry来实现。\n\n官网地址：https://hub.docker.com/_/registry\n\n\n# 简化版镜像仓库\n\ndocker官方的docker registry是一个基础版本的docker镜像仓库，具备仓库管理的完整功能，但是没有图形化界面。\n\n搭建方式比较简单，命令如下：\n\ndocker run -d \\\n    --restart=always \\\n    --name registry\t\\\n    -p 5000:5000 \\\n    -v registry-data:/var/lib/registry \\\n    registry\n\n\n命令中挂载了一个数据卷registry-data到容器内的/var/lib/registry 目录，这是私有镜像库存放数据的目录。\n\n访问http://yourip:5000/v2/_catalog 可以查看当前私有镜像服务中包含的镜像\n\n\n# 带有图形化界面版本\n\n使用dockercompose部署带有图象界面的dockerregistry，命令如下：\n\nversion: \'3.0\'\nservices:\n  registry:\n    image: registry\n    volumes:\n      - ./registry-data:/var/lib/registry\n  ui:\n    image: joxit/docker-registry-ui:static\n    ports:\n      - 8080:80\n    environment:\n      - registry_title=传智教育私有仓库\n      - registry_url=http://registry:5000\n    depends_on:\n      - registry\n\n\n\n# 配置docker信任地址\n\n我们的私服采用的是http协议，默认不被docker信任，所以需要做一个配置：\n\n# 打开要修改的文件\nvi /etc/docker/daemon.json\n# 添加内容：\n"insecure-registries":["http://192.168.159.100:8080"]\n# 重加载\nsystemctl daemon-reload\n# 重启docker\nsystemctl restart docker\n',charsets:{cjk:!0}},{title:"第三方技术整合",frontmatter:{title:"第三方技术整合",date:"2023-07-03T21:15:58.000Z",permalink:"/pages/e76fff/",categories:["后端","SpringBoot"],tags:["知识","SpringBoot"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/50.SpringBoot/20.%E7%AC%AC%E4%B8%89%E6%96%B9%E6%8A%80%E6%9C%AF%E6%95%B4%E5%90%88.html",relativePath:"01.后端/50.SpringBoot/20.第三方技术整合.md",key:"v-626f8d1c",path:"/pages/e76fff/",headers:[{level:2,title:"缓存",slug:"缓存",normalizedTitle:"缓存",charIndex:2},{level:3,title:"Simple",slug:"simple",normalizedTitle:"simple",charIndex:1126},{level:3,title:"Ehcache",slug:"ehcache",normalizedTitle:"ehcache",charIndex:3674},{level:3,title:"Redis",slug:"redis",normalizedTitle:"redis",charIndex:4001},{level:3,title:"memcached",slug:"memcached",normalizedTitle:"memcached",charIndex:4628},{level:3,title:"jetcache",slug:"jetcache",normalizedTitle:"jetcache",charIndex:6965},{level:3,title:"j2cache",slug:"j2cache",normalizedTitle:"j2cache",charIndex:10487},{level:2,title:"任务",slug:"任务",normalizedTitle:"任务",charIndex:12228},{level:3,title:"SpringBoot整合Quartz",slug:"springboot整合quartz",normalizedTitle:"springboot整合quartz",charIndex:12679},{level:3,title:"SpringBoot 定时器",slug:"springboot-定时器",normalizedTitle:"springboot 定时器",charIndex:14028},{level:2,title:"邮件",slug:"邮件",normalizedTitle:"邮件",charIndex:14737},{level:3,title:"SpirngBoot整合JavaMail",slug:"spirngboot整合javamail",normalizedTitle:"spirngboot整合javamail",charIndex:14744},{level:2,title:"消息",slug:"消息",normalizedTitle:"消息",charIndex:14848},{level:3,title:"基础概念",slug:"基础概念",normalizedTitle:"基础概念",charIndex:17988},{level:3,title:"手工模拟消息队列",slug:"手工模拟消息队列",normalizedTitle:"手工模拟消息队列",charIndex:18354},{level:3,title:"ActiveMQ",slug:"activemq",normalizedTitle:"activemq",charIndex:18302},{level:3,title:"RabbitMQ",slug:"rabbitmq",normalizedTitle:"rabbitmq",charIndex:18316},{level:3,title:"RocketMQ",slug:"rocketmq",normalizedTitle:"rocketmq",charIndex:18330},{level:3,title:"Kafka",slug:"kafka",normalizedTitle:"kafka",charIndex:18344}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688392125e3,headersStr:"缓存 Simple Ehcache Redis memcached jetcache j2cache 任务 SpringBoot整合Quartz SpringBoot 定时器 邮件 SpirngBoot整合JavaMail 消息 基础概念 手工模拟消息队列 ActiveMQ RabbitMQ RocketMQ Kafka",content:'# 缓存\n\n\n\n * 缓存模拟——与数据库交互\n   \n   @Autowired\n   private BookMapper bookMapper;\n   //创建一个HashMap，用来模拟缓存，一个id对应一个book\n   private HashMap<Integer,Book> cache=new HashMap<>();\n   \n   \n   @Override //使用HashMap 模拟缓存 实现\n   public Book getById(int id) {\n       Book book = cache.get(id);\n       //如果缓存中有数据，则从缓存中读取，如果缓存中没有数据，则从数据库读取到缓存\n       if(book == null){\n           Book queryBook = bookMapper.selectById(id);\n           cache.put(id,queryBook);\n           return queryBook;\n       }\n       return cache.get(id);\n   }\n   \n\n * 缓存模拟——与外部数据交互(简单版)\n   \n   @Service\n   public class MsgServiceImpl implements IMsgService {\n   \n       private HashMap<String,String> cache=new HashMap<>();\n   \n       @Override //生成验证码\n       public String get(String tel) {\n           String code = tel.substring(tel.length() - 6);//指定开始，默认到结尾——取 从第6为开始取-- 不包括开头\n           cache.put(tel,code);//将生成的验证码放入缓存\n           return code;\n       }\n   \n       @Override //将输入的验证码与查询的验证码相匹配\n       public boolean check(String tel, String code) {//从问号得到数据\n           String QueryCode = cache.get(tel);\n           return QueryCode.equals(code);\n       }\n   }\n   \n\n\n# Simple\n\nSpringBoot整合缓存——默认缓存技术（Simple）\n\n * 引入坐标\n   \n   \x3c!-- 引入缓存坐标--\x3e\n   <dependency>\n       <groupId>org.springframework.boot</groupId>\n       <artifactId>spring-boot-starter-cache</artifactId>\n   </dependency>\n   \n\n * 开启缓存 @EnableCaching\n   \n   @SpringBootApplication\n   @EnableCaching //开启缓存功能\n   public class CacheApplication {\n   }\n   \n\n * 设置缓存 @Cacheable\n   \n   //使用springBoot整合的缓存技术\n   //缓存空间中没有对应值时，执行return 将数据存入缓存\n   //缓存空间有值时，直接从缓存空间取值，不执行return 后的查询操作\n   @Cacheable(value = "cacheSpace",key = "#id") //value 随便起名，定义一片空间，存储缓存 //id作为一个键值，对应一个数据\n   public Book getById(int id) {\n       return bookMapper.selectById(id);\n   }\n   \n\n * 与外部数据交互(专业版—— 验证码案例)\n   \n   * 工具类\n     \n     * 生成验证码——用到了数据结构的思想来简化程序\n       \n       private final String[] patch={"000000","00000","0000","000","00","0",""};\n       \n       //生成验证码 6位\n       public String generator(String tel){\n           int hash =tel.hashCode();\n           int encryption=20206666;\n           long result=hash^encryption;//做异或\n           long nowTime=System.currentTimeMillis();\n           result = nowTime ^ result;\n           long code = result % 1000000;//取余数，小于等于6位\n           code=code < 0 ? -code:code;//全部取正数\n           String codeStr=code+"";//转换成字符串\n           int length = codeStr.length();//根据长度补0,1位补5个0,2为补4个0 ，等等\n           return patch[length]+codeStr;//返回6为验证码\n       }\n       \n     \n     * 从缓存中获得验证码\n       \n       如果直接将方法定义在service内容，从内部调用方法，不走spring容器，那它头上注解不会生效，即不会开启缓存\n       \n       @Cacheable使用时，若想只取数据，不存数据，则返回null即可\n       \n       //get方法 不能直接放在service中，因为它需要经过容器管理，才能启用缓存\n       //如果直接内部调用方法，不走spring容器，那它头上注解不会生效，即不会开启缓存。\n       \n       @Cacheable(value = "SMSCode",key="#tel") //从缓存中拿验证码，只拿不放，返回null即可\n       public String getCode(String tel){\n           return null;\n       }\n       \n   \n   * service\n     \n     @CachePut将数据存入缓存，只存不取，每次调用方法都会产生新数据\n     \n     @Cacheable不光将数据存入缓存，而且当再次执行方法时，会先查询缓存中的数据，如果有直接拿，不产生新数据\n     \n     .equals()将可能为null的值放在后面\n     \n     @Autowired\n      private CodeUtils codeUtils;\n     \n      @Override\n     // @Cacheable(value = "SMSCode",key="#tel") //存入缓存，若key相同，则从存缓存中取数据\n      @CachePut(value = "SMSCode",key="#tel") //存入缓存,只存 不取，新数据会覆盖旧数据\n      public String SendCode(String tel) {\n          String code = codeUtils.generator(tel);//调用方法，产生验证码\n          return code;\n      }\n     \n     \n      @Override\n      public boolean CheckCode(SMSCode smsCode) {\n          //比对验证码\n          String queryCode = codeUtils.getCode(smsCode.getTel());\n          return smsCode.getCode().equals(queryCode);//将可能为null的值放在后面\n      }\n     \n\n\n# Ehcache\n\nSpringBoot整合缓存——Ehcache——不需要修改代码\n\n * 导入坐标\n   \n   \x3c!-- 引入缓存 Ehcache--\x3e\n   <dependency>\n       <groupId>net.sf.ehcache</groupId>\n       <artifactId>ehcache</artifactId>\n   </dependency>\n   \n\n * 配置yml——告诉springBoot 用那种缓存方式\n   \n   #配置cache  不配是默认的 simple\n   cache:\n     type: ehcache\n   \n\n * 导入配置文件\n   \n   ehcache.xml\n\n\n# Redis\n\nSpringBoot整合缓存——Redis——不需要修改代码\n\n * 导入坐标\n   \n   \x3c!-- 引入 redis--\x3e\n   <dependency>\n       <groupId>org.springframework.boot</groupId>\n       <artifactId>spring-boot-starter-data-redis</artifactId>\n   </dependency>\n   \n\n * 配置yml\n   \n     #配置redis\n     cache:\n       type: redis\n       redis:\n   #      cache-null-values: false   #是否缓存空值\n   #      enable-statistics: false\n         time-to-live: 20s    #生存时间 一定要带单位\n   #      use-key-prefix: false  #是否使用键前缀   使用——"SMSCode::17854201283"  不使用——"17854201283" 默认是true，最好是true\n   #      key-prefix: sms_  #设置键前缀是啥\n   \n     redis:\n       host: localhost\n       port: 6379\n   \n\n\n# memcached\n\nSpringBoot整合缓存——memcached——需要修改代码\n\n * 安装服务 ——第一个是安装，第二个是启动服务\n   \n   \n\n * 导入坐标\n   \n   \x3c!-- 引入memcached--\x3e\n   <dependency>\n       <groupId>com.googlecode.xmemcached</groupId>\n       <artifactId>xmemcached</artifactId>\n       <version>2.4.7</version>\n   </dependency>\n   \n\n * 配置yml\n   \n   * 这里配置的仅仅是 memcached的链接属性，是自定义的名称，和官方没关系，springBoot没有整合，所有没有正规配置。\n   \n   #配置memcache\n   memcached:\n     servers: localhost:11211\n     poolSize: 10\n     opTimeout: 3000   #3秒钟没有读到就认为超时\n   \n\n * 代码\n   \n   * 设置属性\n     \n     //配置memcached的连接属性\n     @Component\n     @Data\n     @ConfigurationProperties(prefix = "memcached")\n     public class XmemcacheProperties {\n         private String servers;\n         private int poolSize;\n         private long opTimeout;\n     }\n     \n   \n   * 配置客户端\n     \n     @Bean\n     public MemcachedClient getMemcacaheClient() throws IOException {\n         //创建Builder  传入服务器地址\n         MemcachedClientBuilder memcachedClientBuilder= new XMemcachedClientBuilder(xmemcacheProperties.getServers());\n         //设置参数\n         memcachedClientBuilder.setConnectionPoolSize(xmemcacheProperties.getPoolSize());//连接池数量\n         memcachedClientBuilder.setOpTimeout(xmemcacheProperties.getOpTimeout());//超时时间\n         //创建客户端\n         MemcachedClient memcachedClient=memcachedClientBuilder.build();\n         return memcachedClient;\n     }\n     \n   \n   * Service层代码修改\n     \n     * 这里不能使用之前的@Cacheable,因为没有整合，所有只能自己set，自己put\n     * 当一个接口被两个类实现的时候，使用@Autowired不能注入属性，需要使用@Resource(name="")的方式注入\n     \n     @Autowired\n     private CodeUtils codeUtils;\n     \n     @Override\n     public String SendCode(String tel) {\n         System.out.println(111);\n         String code = codeUtils.generator(tel);//调用方法，产生验证码\n         try {\n             memcachedClient.set(tel,10,code);//第一个参数是key，第二个是0表示永不过期,是10表示保留10s,第三个是值\n         } catch (Exception e) {\n             e.printStackTrace();\n         }\n         return code;\n     }\n     \n     \n     @Override\n     public boolean CheckCode(SMSCode smsCode) {\n         //比对验证码\n         String queryCode= null;\n         try {\n             //如果验证码过期了，这里会报空指针异常\n             queryCode = memcachedClient.get(smsCode.getTel()).toString();\n         } catch (Exception e) {\n             e.printStackTrace();\n         }\n         return smsCode.getCode().equals(queryCode);//将可能为null的值放在后面\n     }\n     \n\n\n# jetcache\n\n\n\n * 引入坐标\n   \n   <dependency>\n       <groupId>com.alicp.jetcache</groupId>\n       <artifactId>jetcache-starter-redis</artifactId>\n       <version>2.6.4</version>\n   </dependency>\n   \n\n * 配置yml\n   \n   #配置jetcache\n   jetcache:\n     statIntervalMinutes: 1  #每过1分钟 控制台显示一次统计数据\n     areaInCacheName: false  #不使用前缀  #不使用前缀"book_2"      ---使用前缀"default_book_2"\n     #本地方案\n     local:\n       default:\n         type: linkedhashmap\n         keyConvertor: fastjson   #key转换成字符串的方式\n   \n     #远程方案\n     remote:\n       default:\n         type: redis\n         host: localhost\n         port: 6379\n         keyConvertor: fastjson   #key转换成字符串的方式\n         valueEncode: java        #将java对象转换成编码，存入redis\n         valueDecode: java        #从redis中取出编码，将编码转换成java对象\n         poolConfig:  #必须要配一个，不然会报错\n           maxTotal: 50\n       sms:\n         type: redis\n         host: localhost\n         port: 6379\n         keyConvertor: fastjson   #key转换成字符串的方式\n         valueEncode: java        #将java对象转换成编码，存入redis\n         valueDecode: java        #从redis中取出编码，将编码转换成java对象\n         poolConfig: #必须要配一个，不然会报错\n           maxTotal: 50\n   \n   \n   \n\n * 代码\n   \n   * 开启缓存\n   \n   @SpringBootApplication\n   //jetcache启用缓存的主开关\n   @EnableCreateCacheAnnotation\n   //开启注解方法缓存———— 与上面那个注解配合使用\n   @EnableMethodCache(basePackages = "com.diana")\n   public class JetcacheApplication {\n   }\n   \n   \n   * 硬编码方式使用缓存，put，get\n   \n    //自定义缓存空间————remote\n   //第0个参数是使用哪个配置,默认可以不写；第一个参数是命名空间；第二个是过期时间，默认是秒\n   @CreateCache(area = "sms",name="jetCache_",expire = 10)\n   private Cache<String,String> jetCache_;\n   \n   \n   //    //定义第二套缓存空间————remote  可以分开管理\n   //    @CreateCache(area = "sms",name="jetCache2_",expire = 10)\n   //    private Cache<String,String> jetCache2_;\n   \n   \n   //自定义缓存空间————local\n   //使用本地缓存-- 只存在本地，不存入reids  默认是both，本地和redis都使用\n   @CreateCache(name="jetCache_local_",expire = 100,cacheType = CacheType.LOCAL)\n   private Cache<String,String> jetCache_local_;\n   \n   \n   @Override\n   public String SendCode(String tel) {\n       String code = codeUtils.generator(tel);//调用方法，产生验证码\n       //        jetCache_.put(tel,code);//放入远程缓存\n       jetCache_local_.put(tel,code);//放入本地缓存\n       return code;\n   }\n   \n   \n   @Override\n   public boolean CheckCode(SMSCode smsCode) {\n       //比对验证码\n       //        String queryCode = jetCache_.get(smsCode.getTel());//从远程缓存中取数据\n       String queryCode = jetCache_local_.get(smsCode.getTel());//从本地缓存中取数据\n       return smsCode.getCode().equals(queryCode);//将可能为null的值放在后面\n   }\n   \n   \n   * 缓存对象必须保证可以序列化\n     \n     public class Book implements Serializable {}  //实现接口，以便对象可以进行序列化操作\n     \n   \n   * 注解方式使用缓存\n     \n     * @Cached 用来存，取数据\n     * @CacheRefresh,用来刷新缓存，保证缓存和数据库数据保持一致，一般不用\n     * @CacheInvalidate,用在delete方法上，保证删除数据后，清除对应缓存\n     * @CacheUpdate用在update方法上，保证修改数据后，修改对应缓存\n   \n   @Override\n   //使用jetcache 技术，默认使用redis存储\n   @Cached(name="book_",key = "#id",expire = 3600)\n   //@CacheRefresh(refresh = 10)//10s刷新一次缓存 一般不用，就是为了保证缓存和数据库数据保持一致\n   public Book getById(int id) {\n       return bookMapper.selectById(id);\n   }\n   \n   \n   @Override\n   public boolean save(Book book) {\n       return bookMapper.insert(book)>0;\n   }\n   \n   @Override\n   @CacheInvalidate(name="book_",key = "#id")//删除缓存，当调用删除操作删除数据时，缓存会同步删除\n   public boolean delete(int id ) {\n       return bookMapper.deleteById(id)>0;\n   }\n   \n   @Override\n   @CacheUpdate(name="book_",key="#book.id",value = "#book")  //更新缓存，当调用更新操作更新数据时，缓存会同步更新\n   public boolean update(Book book) {\n       return bookMapper.updateById(book)>0;\n   }  \n   \n\n\n# j2cache\n\nj2cache是一个缓存整合框架，可以提供缓存的整合方案，使用各种缓存搭配使用，自身不提供缓存功能\n\n案例： 使用reids和ehcache实现2级缓存\n\n * 引入坐标\n   \n   \x3c!-- 引入j2cache--\x3e\n   <dependency>\n       <groupId>net.oschina.j2cache</groupId>\n       <artifactId>j2cache-core</artifactId>\n       <version>2.8.5-release</version>\n   </dependency>\n   <dependency>\x3c!-- 默认导入了一个redis,建议使用你使用--\x3e\n       <groupId>net.oschina.j2cache</groupId>\n       <artifactId>j2cache-spring-boot2-starter</artifactId>\n       <version>2.8.0-release</version>\n   </dependency>\n   \n\n * 配置yml\n   \n   j2cache:\n     config-location: j2cache.properties  #声明配置文件\n   \n\n * 配置properties\n   \n   #配置1级缓存\n   j2cache.L1.provider_class=ehcache\n   ehcache.configXml=ehcache.xml\n   \n   \n   #设置是否启用二级缓存  默认是true\n   j2cache.l2-cache-open=true \n   \n   \n   #配置2级缓存\n   #这是是官方提供的配置 类\n   j2cache.L2.provider_class=net.oschina.j2cache.cache.support.redis.SpringRedisProvider\n   j2cache.L2.config_section=redis\n   redis.hosts=localhost:6379\n   #模式\n   redis.mode=single\n   #前缀名———— 想写入更多配置，查询bkg文件下的j2cache.properties\n   redis.namespace=SMSCode\n   #1) "SMSCode:sms:17854201285" ——带前缀\n   #2) "sms:17854201285" —— 不带前缀\n   \n   \n   #1级缓存中的数据怎样到达二级缓存  广播模式\n   #这个类提供的是  redis 去别的地方拿数据\n   j2cache.broadcast= net.oschina.j2cache.cache.support.redis.SpringRedisPubSubPolicy\n   \n\n * 代码\n   \n   @Autowired//注入一个缓存对象\n   private CacheChannel cacheChannel;\n   \n   @Override\n   public String SendCode(String tel) {\n       String code = codeUtils.generator(tel);//调用方法，产生验证码\n       cacheChannel.set("sms",tel,code);\n       return code;\n   }\n   \n   \n   @Override\n   public boolean CheckCode(SMSCode smsCode) {\n       String queryCode = cacheChannel.get("sms",smsCode.getTel()).asString();\n       return smsCode.getCode().equals(queryCode);//将可能为null的值放在后面\n   }\n   \n\n\n# 任务\n\n主要是做定时任务\n\n * java 实现简单定时器\n   \n   public class TimerTaskApp {\n   \n       public static void main(String[] args){\n           Timer timer=new Timer(); //定时器\n           TimerTask task=new TimerTask() {\n               @Override\n               public void run() {\n                   System.out.println("timer is running ");\n               }\n           };//定时任务\n   \n           timer.schedule(task,0,2000); //从当前开始，每2s执行一次任务\n       }\n   \n   \n   }\n   \n\n\n# SpringBoot整合Quartz\n\n——复杂，不推荐\n\n * 基本概念\n   \n   * 工作(Job)：用于定义具体执行的工作\n   * 工作明细(JobDetail): 用于描述定时工作相关的信息\n   * 触发器（Trigger）：用于描述触发工作的规则，通常使用cron表达式定义调度规则\n   * 调度器（Scheduler）：描述了工作明细与触发器的对应关系\n\n * 引入坐标\n   \n   \x3c!-- 引入quartz坐标--\x3e\n   <dependency>\n       <groupId>org.springframework.boot</groupId>\n       <artifactId>spring-boot-starter-quartz</artifactId>\n   </dependency>\n   \n\n * 定义要实现的任务，继承QuartzJobBean\n   \n   //定义要实现的任务\n   public class Myquartz extends QuartzJobBean {\n       @Override//context是 工作执行的上下文对象\n       protected void executeInternal(JobExecutionContext context) throws JobExecutionException {\n           System.out.println("task is running");\n       }\n   }\n   \n\n * 定时器实现\n   \n   //quartz 定时器配置\n   @Configuration\n   public class QuartzConfig {\n       @Bean\n       public JobDetail printJobDetail(){\n           //绑定具体的工作\n           return JobBuilder.newJob(Myquartz.class).storeDurably().build();\n       }\n   \n       @Bean\n       public Trigger printJobTrigger(){\n           //绑定对应的工作明细\n           //("0/3 10 6 11 4 ?"); 4月11日6点10分0秒开始，每3秒执行一次\n           //————问号表示星期几，一般星期几何几月几日不同时规定，问号表示跟随前面\n           ScheduleBuilder scheduleBuilder=CronScheduleBuilder.cronSchedule("0/3 * * * * ?");//任意时间，0s开始，每3秒执行一次\n           return TriggerBuilder.newTrigger().forJob(printJobDetail()).withSchedule(scheduleBuilder).build();\n       }\n   \n   }\n   \n\n\n# SpringBoot 定时器\n\n——简单，推荐\n\n * 开启定时任务开关\n   \n   @EnableScheduling\n   public class TaskApplication {\n   }\n   \n\n * 配置yml\n   \n   spring:\n     task:\n       scheduling:\n         #任务调度线程池大小  默认1\n         pool:\n           size: 1\n         #调度线程名称前缀\n         thread-name-prefix: spring_task_\n         shutdown:\n           #线程池关闭时是否等待所有任务完成\n           await-termination: false\n           #调度线程关闭前最大等待时间，确保最后一定关闭\n           await-termination-period: 10s\n   \n\n * 编写定时任务——加注解——使用cron表达式定义调度规则\n   \n   @Scheduled(cron = "0/6 * * * * ?")\n   public void print(){\n       System.out.println("spring task run...");\n       //前面是调用线程的名字\n        System.out.println(Thread.currentThread().getName()+":spring task run...");\n   }\n   \n\n\n# 邮件\n\n\n# SpirngBoot整合JavaMail\n\n * 传输协议\n   \n   * SMTP：简单邮件传输协议，用于发送电子邮件的传输协议\n   * POP3：用于接收电子邮件的标准协议\n   * IMAP：互联网消息协议，是POP3的替代协议(比POP3要好那么一丢丢)\n\n * 引入坐标\n   \n   \x3c!-- 引入mail 坐标--\x3e\n   <dependency>\n       <groupId>org.springframework.boot</groupId>\n       <artifactId>spring-boot-starter-mail</artifactId>\n   </dependency>\n   \n    \x3c!-- 解决配置实体类关联配置文件 警告问题--\x3e\n   <dependency>\n       <groupId>org.springframework.boot</groupId>\n       <artifactId>spring-boot-configuration-processor</artifactId>\n       <optional>true</optional>\n   </dependency>\n   \n\n * 配置文件\n   \n   #配置email\n   spring:\n     mail:\n       host: smtp.qq.com #使用qq邮箱服务  smtp.126.com#使用126邮箱服务\n       username: 1693849288@qq.com\n       password: pmpartwhndcrfcjg\n   \n   \n   #配置email 信息\n   mail:\n     mail_from: 1693849288@qq.com\n     mail_to: diana_liangbing@126.com\n     mail_title: "SpringBoot 整合 JAVAMail"\n     mail_msg: "hello javaMail!!"\n     ahref: "<a href=\'https://www.baidu.com\'>点击前往百度</a>"\n     img: "<img src=\'https://img1.baidu.com/it/u=4065318963,1219732686&fm=253&fmt=auto&app=138&f=JPEG?w=500&h=675\'>"\n   \n\n * 发送简单邮件\n   \n   * mail.getMail_from()+"(diana)"——//加个括号作为发件人，否则默认是邮箱前缀\n   \n   @Autowired\n   private JavaMailSender javaMailSender;\n   \n   @Autowired\n   private Mail mail;\n   \n   public  void sendMail(){\n       SimpleMailMessage message=new SimpleMailMessage();\n       System.out.println(mail);\n       message.setFrom(mail.getMail_from()+"(diana)");//加个括号，发件人，否则默认是邮箱前缀\n       message.setTo(mail.getMail_to());\n       message.setSubject(mail.getMail_title());\n       message.setText(mail.getMail_msg());\n       javaMailSender.send(message);\n   }\n   \n\n * 发送复杂邮件\n   \n   * 可以发送html的标签来加载网站和图片——\n     * 1.setText(mail.getAhref(),true);——true 表示发送html 标签信息，可以解析出来，默认是false，这样的话只能发送文本消息\n     * 2.具体的html标签可参照上面的参数配置\n     * 3.使用多个setText()命令，最后一个会覆盖前面的。如果想发送多个内容，需要将所有的内容写入到一个string中\n   * 可以发送附件\n     * 1.MimeMessageHelper helper=new MimeMessageHelper(message,true);——true 表示可以发送附件`\n     * 2.使用多个addAttachment()命令，可以直接发送多个文件。\n   \n   public  void sendMail()  {\n   \n       //附件信息\n       File f1=new File("D:\\\\生活\\\\学习\\\\西电杭研院\\\\云共享\\\\OneDrive\\\\文档\\\\云共享\\\\java\\\\SpringBoot\\\\实用篇\\\\mail\\\\pom.xml");\n       File f2=new File("D:\\\\生活\\\\学习\\\\西电杭研院\\\\云共享\\\\OneDrive\\\\文档\\\\云共享\\\\java\\\\SpringBoot\\\\实用篇\\\\mail\\\\src\\\\main\\\\resources\\\\凉冰.jpg");\n   \n       try {\n           MimeMessage message=javaMailSender.createMimeMessage();\n           MimeMessageHelper helper=new MimeMessageHelper(message,true); //true 表示可以发送附件\n           helper.setFrom(mail.getMail_from()+"(diana)");//加个括号，发件人，否则默认是邮箱前缀\n           helper.setTo(mail.getMail_to());\n           helper.setSubject(mail.getMail_title());\n   \n           //发送正文 ——多个setText 后面的会覆盖前面的——要想发送多个内容，要把内容放到一个字符串中\n           helper.setText(mail.getAhref());//默认是false,显示文本   "<a href=\'https://www.baidu.com\'>点击前往百度</a>"\n           helper.setText(mail.getAhref(),true);//用true 表示发送的html文件    --点击前往百度--\n           helper.setText(mail.getImg(),true);//多个setText 后面的会覆盖前面的\n   \n           //发送附件 ——多个addAttachment 会一起发过去，添加多个附件\n           helper.addAttachment(f1.getName(),f1);\n           helper.addAttachment(f2.getName(),f2);\n   \n           javaMailSender.send(message);\n   \n       } catch (Exception e) {\n           e.printStackTrace();\n       }\n   \n   }\n   \n\n\n# 消息\n\n\n# 基础概念\n\n * 消息发送方——生产者\n\n * 消息接收方——消费者\n\n * 同步消息——需要等待回复，然后继续进行\n\n * 异步消息——不需要等待回复，你回不回复，不影响我的运行\n\n * 消息队列——主服务器接收大量请求后，将服务提交给消息队列，然后让子服务器去消息队列中去取。\n   \n   \n\n * 企业中的三种异步消息传递技术（规范）\n   \n   * JMS(规范)\n     \n     \n   \n   * AMQP(协议)\n     \n     \n   \n   * MQTT(小型)\n     \n     > 消息队列遥测传输，专为小设备设计，是物联网生态系统中主要成分之一。\n\n * 技术实现\n   \n   * ActiveMQ\n   * RabbitMQ\n   * RocketMQ\n   * Kafka\n\n\n# 手工模拟消息队列\n\n * 使用ArrayList集合，模拟简单的消息队列\n\n//手动模拟消息队列实现\n@Service\npublic class MessageServiceImpl implements IMessageService {\n\n    private ArrayList<String> msgList=new ArrayList<String>();\n\n    @Override //往消息队列中传入订单\n    public void sendMessage(String id) {\n\n        System.out.println("待发送短信的订单已纳入处理队列，id："+id);\n        msgList.add(id);\n\n    }\n\n    @Override//从消息队列中取出订单\n    public String doMessage() {\n        String id = msgList.remove(0);//接收从队列中移除的订单id\n        System.out.println("已完成短信发送业务，id："+id);\n        return id;\n    }\n}\n\n\n\n# ActiveMQ\n\n * 启动服务\n   \n   * D:\\Program Files\\mq\\activemq\\apache-activemq-5.16.3\\bin\\win64\\activemq.bat 双击即可\n   * 网页管理 http://127.0.0.1:8161/\n\n * 引入坐标\n   \n   \x3c!-- ActiveMQ--\x3e\n   <dependency>\n       <groupId>org.springframework.boot</groupId>\n       <artifactId>spring-boot-starter-activemq</artifactId>\n   </dependency>\n   \n\n * 配置yml\n   \n   #配置ActiveMQ\n   spring:\n     activemq:\n       broker-url: tcp://localhost:61616\n     jms:\n       template:\n         default-destination: diana #指定一个destination，相当于是给消息队列起名字\n       pub-sub-domain: true #是否开启发布订阅模型，默认为false，即默认为点对点模型\n   \n\n * 代码\n   \n   * 使用接口规范定义，不需要使用具体的实现类，就像使用JDBC模板那样\n   * 推荐使用监听功能，实现自动消费。\n   \n   @Autowired\n   private JmsMessagingTemplate messagingTemplate; //接口规范  不需要使用实现类定义\n   \n   @Override //往消息队列中传入订单\n   public void sendMessage(String id) {\n   \n       System.out.println("待发送短信的订单已纳入处理队列，id："+id);\n       //messagingTemplate.convertAndSend(id);  //放到默认位置   ——default-destination: diana\n       messagingTemplate.convertAndSend("order.queue.id",id);//指定 destination\n   }\n   \n   @Override//从消息队列中取出订单 --------手动从消息队列中取出信息--手动消费\n   public String doMessage() {\n       //        String id = messagingTemplate.receiveAndConvert(String.class);//从默认destination——diana 中取\n       String id = messagingTemplate.receiveAndConvert("order.queue.id",String.class);\n       System.out.println("已手动完成短信发送业务，id："+id);\n       return id;\n   }\n   \n   \n   //作为监听器存在，一旦发现消息队列中有消息进来，立即消费掉\n   @Component\n   public class MessageListener {\n   \n      @JmsListener(destination = "order.queue.id")//--------自动从消息队列中取出信息--自动消费\n      @SendTo("order.other.queue.id")//将方法的返回值自动发送到另外一个队列——可有可无\n       public String receive(String id) {\n           System.out.println("已自动完成短信发送业务，id："+id);\n           return "new:"+id;\n       }\n   \n   \n   }\n   \n\n\n# RabbitMQ\n\n * 启动服务\n   \n   D:\\Program Files\\mq\\rabbitmq\\rabbitmq\\rabbitmq\\rabbitmq_server-3.9.13\\sbin下cmd\n   \n   * rabbitmq-service.bat strat——启动服务\n   * rabbitmq-service.bat stop——关闭服务\n   * rabbitmq-service.bat staus——查看启动状态\n   * rabbitmq-plugins.bat list——查看所有插件\n   * rabbitmq-plugins.bat enable rabbitmq_management—启动客户端插件\n\n * 启动rabbitmq_management——默认端口 15672\n   \n   * 访问网站 guest，guest\n   \n   * 报错——原因是用户名是中文\n   \n   \n\n * 导入坐标\n   \n   \x3c!-- rabbitmq--\x3e\n   <dependency>\n       <groupId>org.springframework.boot</groupId>\n       <artifactId>spring-boot-starter-amqp</artifactId>\n   </dependency>\n   \n\n * 配置yml\n   \n   #配置 rabbitmq\n   rabbitmq:\n     host: localhost\n     port: 5672  #服务端口5672，对外管理端口 15672\n   \n\n * 两种模式\n   \n   * 直连模式配置——Direct\n     \n     // rabbitmq 直连 配置\n     @Configuration\n     public class RabbitmqConfigDirect {\n     \n         @Bean//新建消息队列对象\n         public Queue directQueue(){\n             return new Queue("direct_queue");\n         }\n         @Bean//新建消息队列对象\n         public Queue directQueue2(){\n             //durable: 是否持久化，默认false\n             //exclusive： 是否当前连接专用，默认false；如果设为true,则连接关闭后队列即被删除\n             //autoDelete: 是否自动删除，当生产者或消费者不在使用此队列，自动删除\n             return new Queue("direct_queue2",true,false,false);\n         }\n     \n         @Bean//新建交换机对象\n         public DirectExchange directExchange(){\n             return new DirectExchange("directExchange");\n         }\n     \n         @Bean//绑定交换机对象和消息队列对象\n         public Binding bindingDirect(){\n             return BindingBuilder.bind(directQueue()).to(directExchange()).with("direct");\n         }\n         @Bean//一个交换机对象可以绑定多个消息队列对象\n         public Binding bindingDirect2(){\n             return BindingBuilder.bind(directQueue2()).to(directExchange()).with("direct2");\n         }\n     \n     }\n     \n   \n   * 分发模式——Topic\n     \n     * 与Direct 不同的地方在于 分发功能，可以根据绑定时的名字，根据匹配规则，分发到不同服务\n     \n     ![rabbit 匹配](/assets/后端/springboot/rabbit 匹配.png)\n     \n     * 当一个服务符合两个绑定的匹配时，会同时将消息发送到两个不同的队列\n     \n     @Bean//绑定交换机对象和消息队列对象\n     public Binding bindingTopic(){\n         //*表示匹配到任意，这样就算有个分发功能，一个绑定名称可以对应多个 到\n         return BindingBuilder.bind(topicQueue()).to(topicExchange()).with("topic.*.id");\n     }\n     @Bean//一个交换机对象可以绑定多个消息队列对象\n     public Binding bindingTopic2(){\n         //当一个业务匹配到多个绑定对象时，会将消息分发到多个队列\n         return BindingBuilder.bind(topicQueue2()).to(topicExchange()).with("topic.orders.*");\n     }\n     \n   \n   * 主服务代码——两种方式代码是一样的——绑定名称不一样\n     \n     @Autowired\n     private AmqpTemplate amqpTemplate;\n     \n     @Override—————————————————————————————————————————————————————直连模式———————————一一对应\n     public void sendMessage(String id) {\n         System.out.println("待发送短信的订单已纳入处理队列，id："+id);\n         //directExchange交换机对象，direct名字--对应绑定名称，一一对应\n         amqpTemplate.convertAndSend("directExchange","direct",id);\n     }\n     \n     @Override——————————————————————————————————————————————————topic模式——————————————匹配规则对应\n     public void sendMessage(String id) {\n         System.out.println("待发送短信的订单已纳入处理队列，id："+id);\n         //topicExchange交换机对象名称，topic.orders.id --对应绑定名称，有分发模式，匹配到即可\n         amqpTemplate.convertAndSend("topicExchange","topic.orders.id",id);\n     }\n     \n   \n   * 监听器——两种方式代码是一样的——队列名称不一样\n     \n     * @RabbitListener(queues = "direct_queue")\n     \n     @RabbitListener(queues = "direct_queue")//-----直连模式---自动从消息队列中取出信息--自动消费\n     public void receive(String id) {\n         System.out.println("a1已自动完成短信发送业务，id："+id);\n     }\n     \n     @RabbitListener(queues = "topic_queue")//----topic模式----自动从消息队列中取出信息--自动消费\n     public void receive(String id) {\n         System.out.println("queue1已自动完成短信发送业务，id："+id);\n     }\n     \n     \n     * 当多个监听器同时监听一个队列时，多个监听器会轮询操作\n\n\n# RocketMQ\n\n * 启动服务\n   \n   * 解决java安装路径带空格问题\n     \n     修改runserver.cmd和runbroker.cmd\n     \n     * 1.将JAVA_HOME定义成字符串 ——set JAVAHOME="%JAVA_HOME%"\n     * 2.在倒数第三行中使用%JAVAHOME%替换掉原来的%JAVA_HOME%——set "JAVA_OPT=%JAVA_OPT% -Djava.ext.dirs=%BASE_DIR%lib;%JAVAHOME%\\jre\\lib\\ext"\n     * 3.在倒数第二行中，将%CLASSPATH%修改为"%CLASSPATH%",加了一对英文双引号\n   \n   * 启动流程\n     \n     * 先启动mqnamesrv.cmd,双击即可启动命名服务器\n     * 再启动mqbroker.cmd，双击即可启动业务服务器\n     * 服务器功能测试：生产者tools org.apache.rocketmq.exemple.quickstart.Producer\n     * 服务器功能测试：消费者tools org.apache.rocketmq.exemple.quickstart.Consumer\n\n * 导入坐标\n   \n   \x3c!-- rocketmq--\x3e\n   <dependency>\n       <groupId>org.apache.rocketmq</groupId>\n       <artifactId>rocketmq-spring-boot-starter</artifactId>\n       <version>2.2.2</version>\n   </dependency>\n   \n\n * 配置yml\n   \n   #配置rocketmq\n   rocketmq:\n     name-server: localhost:9876\n     producer:\n       group: group_rocketmq #初始化生产者组名（随便起）\n   \n\n * 代码\n   \n   * service\n   \n   @Autowired\n   private RocketMQTemplate rocketMQTemplate;\n   \n   @Override\n   public void sendMessage(String id) {\n       System.out.println("待发送短信的订单已纳入处理队列，id："+id);\n       //        rocketMQTemplate.convertAndSend("order_id",id);//同步消息\n   \n       //定义回调方法\n       SendCallback callback=new SendCallback() {\n           @Override\n           public void onSuccess(SendResult sendResult) {\n               System.out.println("发送成功！！");\n           }\n   \n           @Override\n           public void onException(Throwable throwable) {\n               System.out.println("发送失败！！");\n   \n           }\n       };//发送异步消息\n       rocketMQTemplate.asyncSend("order_id",id,callback);\n   }\n   \n   \n   * listener ——实现接口，注解写在上方\n   \n   @Component//监听器\n   //destination 的名称（同存入消息时的名称），   消费者组的名称(随便起)，这里让消费者和生产者同组\n   @RocketMQMessageListener(topic = "order_id",consumerGroup = "group_rocketmq")\n   public class MessageListener implements RocketMQListener<String> {\n   \n       @Override\n       public void onMessage(String id) {\n           System.out.println("已自动完成短信发送业务，id："+id);\n       }\n   \n   }\n   \n\n\n# Kafka\n\n * 启动服务\n   \n   * 启动zookeeper zookeeper-server-start.bat ../../config/zookeeper.properties\n     * 默认端口 2181\n   * 启动kafka kafka-server-start ..\\..\\config\\server.properties\n     * 默认端口9092\n   * 创建topic kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic diana\n   * 查看topic kafka-topics --zookeeper localhost:2181 --list\n   * 删除topic kafka-topics --delete --zookeeper localhost:2181 --topic diana\n   * 生产者功能测试 kafka-console-producer --broker-list localhost:9092 --topic diana\n   * 消费者功能测试 kafka-console-consumer --bootstrap-server localhost:9092 --topic diana --from-beginning\n\n * 导入坐标\n   \n   \x3c!-- kafka--\x3e\n   <dependency>\n       <groupId>org.springframework.kafka</groupId>\n       <artifactId>spring-kafka</artifactId>\n   </dependency>\n   \n\n * 配置yml\n   \n   #配置kafka\n   kafka:\n     bootstrap-servers: localhost:9092\n     consumer:\n       group-id: order #配置消费者组id\n   \n\n * 代码\n   \n   * service\n   \n   @Autowired\n   private KafkaTemplate<String,String> kafkaTemplate;\n   \n   @Override\n   public void sendMessage(String id) {\n       System.out.println("待发送短信的订单已纳入处理队列(kafka)，id："+id);\n       kafkaTemplate.send("diana",id);//前面是topic 名称，后面是要送入队列的消息\n   }\n   \n   \n   * listener\n   \n   @KafkaListener(topics = "diana")//设置topic名称\n   public void onMessage(ConsumerRecord<String,String> record) {\n       System.out.println("已自动完成短信发送业务(kafaka)，id："+record.value());\n   }\n   ',normalizedContent:'# 缓存\n\n\n\n * 缓存模拟——与数据库交互\n   \n   @autowired\n   private bookmapper bookmapper;\n   //创建一个hashmap，用来模拟缓存，一个id对应一个book\n   private hashmap<integer,book> cache=new hashmap<>();\n   \n   \n   @override //使用hashmap 模拟缓存 实现\n   public book getbyid(int id) {\n       book book = cache.get(id);\n       //如果缓存中有数据，则从缓存中读取，如果缓存中没有数据，则从数据库读取到缓存\n       if(book == null){\n           book querybook = bookmapper.selectbyid(id);\n           cache.put(id,querybook);\n           return querybook;\n       }\n       return cache.get(id);\n   }\n   \n\n * 缓存模拟——与外部数据交互(简单版)\n   \n   @service\n   public class msgserviceimpl implements imsgservice {\n   \n       private hashmap<string,string> cache=new hashmap<>();\n   \n       @override //生成验证码\n       public string get(string tel) {\n           string code = tel.substring(tel.length() - 6);//指定开始，默认到结尾——取 从第6为开始取-- 不包括开头\n           cache.put(tel,code);//将生成的验证码放入缓存\n           return code;\n       }\n   \n       @override //将输入的验证码与查询的验证码相匹配\n       public boolean check(string tel, string code) {//从问号得到数据\n           string querycode = cache.get(tel);\n           return querycode.equals(code);\n       }\n   }\n   \n\n\n# simple\n\nspringboot整合缓存——默认缓存技术（simple）\n\n * 引入坐标\n   \n   \x3c!-- 引入缓存坐标--\x3e\n   <dependency>\n       <groupid>org.springframework.boot</groupid>\n       <artifactid>spring-boot-starter-cache</artifactid>\n   </dependency>\n   \n\n * 开启缓存 @enablecaching\n   \n   @springbootapplication\n   @enablecaching //开启缓存功能\n   public class cacheapplication {\n   }\n   \n\n * 设置缓存 @cacheable\n   \n   //使用springboot整合的缓存技术\n   //缓存空间中没有对应值时，执行return 将数据存入缓存\n   //缓存空间有值时，直接从缓存空间取值，不执行return 后的查询操作\n   @cacheable(value = "cachespace",key = "#id") //value 随便起名，定义一片空间，存储缓存 //id作为一个键值，对应一个数据\n   public book getbyid(int id) {\n       return bookmapper.selectbyid(id);\n   }\n   \n\n * 与外部数据交互(专业版—— 验证码案例)\n   \n   * 工具类\n     \n     * 生成验证码——用到了数据结构的思想来简化程序\n       \n       private final string[] patch={"000000","00000","0000","000","00","0",""};\n       \n       //生成验证码 6位\n       public string generator(string tel){\n           int hash =tel.hashcode();\n           int encryption=20206666;\n           long result=hash^encryption;//做异或\n           long nowtime=system.currenttimemillis();\n           result = nowtime ^ result;\n           long code = result % 1000000;//取余数，小于等于6位\n           code=code < 0 ? -code:code;//全部取正数\n           string codestr=code+"";//转换成字符串\n           int length = codestr.length();//根据长度补0,1位补5个0,2为补4个0 ，等等\n           return patch[length]+codestr;//返回6为验证码\n       }\n       \n     \n     * 从缓存中获得验证码\n       \n       如果直接将方法定义在service内容，从内部调用方法，不走spring容器，那它头上注解不会生效，即不会开启缓存\n       \n       @cacheable使用时，若想只取数据，不存数据，则返回null即可\n       \n       //get方法 不能直接放在service中，因为它需要经过容器管理，才能启用缓存\n       //如果直接内部调用方法，不走spring容器，那它头上注解不会生效，即不会开启缓存。\n       \n       @cacheable(value = "smscode",key="#tel") //从缓存中拿验证码，只拿不放，返回null即可\n       public string getcode(string tel){\n           return null;\n       }\n       \n   \n   * service\n     \n     @cacheput将数据存入缓存，只存不取，每次调用方法都会产生新数据\n     \n     @cacheable不光将数据存入缓存，而且当再次执行方法时，会先查询缓存中的数据，如果有直接拿，不产生新数据\n     \n     .equals()将可能为null的值放在后面\n     \n     @autowired\n      private codeutils codeutils;\n     \n      @override\n     // @cacheable(value = "smscode",key="#tel") //存入缓存，若key相同，则从存缓存中取数据\n      @cacheput(value = "smscode",key="#tel") //存入缓存,只存 不取，新数据会覆盖旧数据\n      public string sendcode(string tel) {\n          string code = codeutils.generator(tel);//调用方法，产生验证码\n          return code;\n      }\n     \n     \n      @override\n      public boolean checkcode(smscode smscode) {\n          //比对验证码\n          string querycode = codeutils.getcode(smscode.gettel());\n          return smscode.getcode().equals(querycode);//将可能为null的值放在后面\n      }\n     \n\n\n# ehcache\n\nspringboot整合缓存——ehcache——不需要修改代码\n\n * 导入坐标\n   \n   \x3c!-- 引入缓存 ehcache--\x3e\n   <dependency>\n       <groupid>net.sf.ehcache</groupid>\n       <artifactid>ehcache</artifactid>\n   </dependency>\n   \n\n * 配置yml——告诉springboot 用那种缓存方式\n   \n   #配置cache  不配是默认的 simple\n   cache:\n     type: ehcache\n   \n\n * 导入配置文件\n   \n   ehcache.xml\n\n\n# redis\n\nspringboot整合缓存——redis——不需要修改代码\n\n * 导入坐标\n   \n   \x3c!-- 引入 redis--\x3e\n   <dependency>\n       <groupid>org.springframework.boot</groupid>\n       <artifactid>spring-boot-starter-data-redis</artifactid>\n   </dependency>\n   \n\n * 配置yml\n   \n     #配置redis\n     cache:\n       type: redis\n       redis:\n   #      cache-null-values: false   #是否缓存空值\n   #      enable-statistics: false\n         time-to-live: 20s    #生存时间 一定要带单位\n   #      use-key-prefix: false  #是否使用键前缀   使用——"smscode::17854201283"  不使用——"17854201283" 默认是true，最好是true\n   #      key-prefix: sms_  #设置键前缀是啥\n   \n     redis:\n       host: localhost\n       port: 6379\n   \n\n\n# memcached\n\nspringboot整合缓存——memcached——需要修改代码\n\n * 安装服务 ——第一个是安装，第二个是启动服务\n   \n   \n\n * 导入坐标\n   \n   \x3c!-- 引入memcached--\x3e\n   <dependency>\n       <groupid>com.googlecode.xmemcached</groupid>\n       <artifactid>xmemcached</artifactid>\n       <version>2.4.7</version>\n   </dependency>\n   \n\n * 配置yml\n   \n   * 这里配置的仅仅是 memcached的链接属性，是自定义的名称，和官方没关系，springboot没有整合，所有没有正规配置。\n   \n   #配置memcache\n   memcached:\n     servers: localhost:11211\n     poolsize: 10\n     optimeout: 3000   #3秒钟没有读到就认为超时\n   \n\n * 代码\n   \n   * 设置属性\n     \n     //配置memcached的连接属性\n     @component\n     @data\n     @configurationproperties(prefix = "memcached")\n     public class xmemcacheproperties {\n         private string servers;\n         private int poolsize;\n         private long optimeout;\n     }\n     \n   \n   * 配置客户端\n     \n     @bean\n     public memcachedclient getmemcacaheclient() throws ioexception {\n         //创建builder  传入服务器地址\n         memcachedclientbuilder memcachedclientbuilder= new xmemcachedclientbuilder(xmemcacheproperties.getservers());\n         //设置参数\n         memcachedclientbuilder.setconnectionpoolsize(xmemcacheproperties.getpoolsize());//连接池数量\n         memcachedclientbuilder.setoptimeout(xmemcacheproperties.getoptimeout());//超时时间\n         //创建客户端\n         memcachedclient memcachedclient=memcachedclientbuilder.build();\n         return memcachedclient;\n     }\n     \n   \n   * service层代码修改\n     \n     * 这里不能使用之前的@cacheable,因为没有整合，所有只能自己set，自己put\n     * 当一个接口被两个类实现的时候，使用@autowired不能注入属性，需要使用@resource(name="")的方式注入\n     \n     @autowired\n     private codeutils codeutils;\n     \n     @override\n     public string sendcode(string tel) {\n         system.out.println(111);\n         string code = codeutils.generator(tel);//调用方法，产生验证码\n         try {\n             memcachedclient.set(tel,10,code);//第一个参数是key，第二个是0表示永不过期,是10表示保留10s,第三个是值\n         } catch (exception e) {\n             e.printstacktrace();\n         }\n         return code;\n     }\n     \n     \n     @override\n     public boolean checkcode(smscode smscode) {\n         //比对验证码\n         string querycode= null;\n         try {\n             //如果验证码过期了，这里会报空指针异常\n             querycode = memcachedclient.get(smscode.gettel()).tostring();\n         } catch (exception e) {\n             e.printstacktrace();\n         }\n         return smscode.getcode().equals(querycode);//将可能为null的值放在后面\n     }\n     \n\n\n# jetcache\n\n\n\n * 引入坐标\n   \n   <dependency>\n       <groupid>com.alicp.jetcache</groupid>\n       <artifactid>jetcache-starter-redis</artifactid>\n       <version>2.6.4</version>\n   </dependency>\n   \n\n * 配置yml\n   \n   #配置jetcache\n   jetcache:\n     statintervalminutes: 1  #每过1分钟 控制台显示一次统计数据\n     areaincachename: false  #不使用前缀  #不使用前缀"book_2"      ---使用前缀"default_book_2"\n     #本地方案\n     local:\n       default:\n         type: linkedhashmap\n         keyconvertor: fastjson   #key转换成字符串的方式\n   \n     #远程方案\n     remote:\n       default:\n         type: redis\n         host: localhost\n         port: 6379\n         keyconvertor: fastjson   #key转换成字符串的方式\n         valueencode: java        #将java对象转换成编码，存入redis\n         valuedecode: java        #从redis中取出编码，将编码转换成java对象\n         poolconfig:  #必须要配一个，不然会报错\n           maxtotal: 50\n       sms:\n         type: redis\n         host: localhost\n         port: 6379\n         keyconvertor: fastjson   #key转换成字符串的方式\n         valueencode: java        #将java对象转换成编码，存入redis\n         valuedecode: java        #从redis中取出编码，将编码转换成java对象\n         poolconfig: #必须要配一个，不然会报错\n           maxtotal: 50\n   \n   \n   \n\n * 代码\n   \n   * 开启缓存\n   \n   @springbootapplication\n   //jetcache启用缓存的主开关\n   @enablecreatecacheannotation\n   //开启注解方法缓存———— 与上面那个注解配合使用\n   @enablemethodcache(basepackages = "com.diana")\n   public class jetcacheapplication {\n   }\n   \n   \n   * 硬编码方式使用缓存，put，get\n   \n    //自定义缓存空间————remote\n   //第0个参数是使用哪个配置,默认可以不写；第一个参数是命名空间；第二个是过期时间，默认是秒\n   @createcache(area = "sms",name="jetcache_",expire = 10)\n   private cache<string,string> jetcache_;\n   \n   \n   //    //定义第二套缓存空间————remote  可以分开管理\n   //    @createcache(area = "sms",name="jetcache2_",expire = 10)\n   //    private cache<string,string> jetcache2_;\n   \n   \n   //自定义缓存空间————local\n   //使用本地缓存-- 只存在本地，不存入reids  默认是both，本地和redis都使用\n   @createcache(name="jetcache_local_",expire = 100,cachetype = cachetype.local)\n   private cache<string,string> jetcache_local_;\n   \n   \n   @override\n   public string sendcode(string tel) {\n       string code = codeutils.generator(tel);//调用方法，产生验证码\n       //        jetcache_.put(tel,code);//放入远程缓存\n       jetcache_local_.put(tel,code);//放入本地缓存\n       return code;\n   }\n   \n   \n   @override\n   public boolean checkcode(smscode smscode) {\n       //比对验证码\n       //        string querycode = jetcache_.get(smscode.gettel());//从远程缓存中取数据\n       string querycode = jetcache_local_.get(smscode.gettel());//从本地缓存中取数据\n       return smscode.getcode().equals(querycode);//将可能为null的值放在后面\n   }\n   \n   \n   * 缓存对象必须保证可以序列化\n     \n     public class book implements serializable {}  //实现接口，以便对象可以进行序列化操作\n     \n   \n   * 注解方式使用缓存\n     \n     * @cached 用来存，取数据\n     * @cacherefresh,用来刷新缓存，保证缓存和数据库数据保持一致，一般不用\n     * @cacheinvalidate,用在delete方法上，保证删除数据后，清除对应缓存\n     * @cacheupdate用在update方法上，保证修改数据后，修改对应缓存\n   \n   @override\n   //使用jetcache 技术，默认使用redis存储\n   @cached(name="book_",key = "#id",expire = 3600)\n   //@cacherefresh(refresh = 10)//10s刷新一次缓存 一般不用，就是为了保证缓存和数据库数据保持一致\n   public book getbyid(int id) {\n       return bookmapper.selectbyid(id);\n   }\n   \n   \n   @override\n   public boolean save(book book) {\n       return bookmapper.insert(book)>0;\n   }\n   \n   @override\n   @cacheinvalidate(name="book_",key = "#id")//删除缓存，当调用删除操作删除数据时，缓存会同步删除\n   public boolean delete(int id ) {\n       return bookmapper.deletebyid(id)>0;\n   }\n   \n   @override\n   @cacheupdate(name="book_",key="#book.id",value = "#book")  //更新缓存，当调用更新操作更新数据时，缓存会同步更新\n   public boolean update(book book) {\n       return bookmapper.updatebyid(book)>0;\n   }  \n   \n\n\n# j2cache\n\nj2cache是一个缓存整合框架，可以提供缓存的整合方案，使用各种缓存搭配使用，自身不提供缓存功能\n\n案例： 使用reids和ehcache实现2级缓存\n\n * 引入坐标\n   \n   \x3c!-- 引入j2cache--\x3e\n   <dependency>\n       <groupid>net.oschina.j2cache</groupid>\n       <artifactid>j2cache-core</artifactid>\n       <version>2.8.5-release</version>\n   </dependency>\n   <dependency>\x3c!-- 默认导入了一个redis,建议使用你使用--\x3e\n       <groupid>net.oschina.j2cache</groupid>\n       <artifactid>j2cache-spring-boot2-starter</artifactid>\n       <version>2.8.0-release</version>\n   </dependency>\n   \n\n * 配置yml\n   \n   j2cache:\n     config-location: j2cache.properties  #声明配置文件\n   \n\n * 配置properties\n   \n   #配置1级缓存\n   j2cache.l1.provider_class=ehcache\n   ehcache.configxml=ehcache.xml\n   \n   \n   #设置是否启用二级缓存  默认是true\n   j2cache.l2-cache-open=true \n   \n   \n   #配置2级缓存\n   #这是是官方提供的配置 类\n   j2cache.l2.provider_class=net.oschina.j2cache.cache.support.redis.springredisprovider\n   j2cache.l2.config_section=redis\n   redis.hosts=localhost:6379\n   #模式\n   redis.mode=single\n   #前缀名———— 想写入更多配置，查询bkg文件下的j2cache.properties\n   redis.namespace=smscode\n   #1) "smscode:sms:17854201285" ——带前缀\n   #2) "sms:17854201285" —— 不带前缀\n   \n   \n   #1级缓存中的数据怎样到达二级缓存  广播模式\n   #这个类提供的是  redis 去别的地方拿数据\n   j2cache.broadcast= net.oschina.j2cache.cache.support.redis.springredispubsubpolicy\n   \n\n * 代码\n   \n   @autowired//注入一个缓存对象\n   private cachechannel cachechannel;\n   \n   @override\n   public string sendcode(string tel) {\n       string code = codeutils.generator(tel);//调用方法，产生验证码\n       cachechannel.set("sms",tel,code);\n       return code;\n   }\n   \n   \n   @override\n   public boolean checkcode(smscode smscode) {\n       string querycode = cachechannel.get("sms",smscode.gettel()).asstring();\n       return smscode.getcode().equals(querycode);//将可能为null的值放在后面\n   }\n   \n\n\n# 任务\n\n主要是做定时任务\n\n * java 实现简单定时器\n   \n   public class timertaskapp {\n   \n       public static void main(string[] args){\n           timer timer=new timer(); //定时器\n           timertask task=new timertask() {\n               @override\n               public void run() {\n                   system.out.println("timer is running ");\n               }\n           };//定时任务\n   \n           timer.schedule(task,0,2000); //从当前开始，每2s执行一次任务\n       }\n   \n   \n   }\n   \n\n\n# springboot整合quartz\n\n——复杂，不推荐\n\n * 基本概念\n   \n   * 工作(job)：用于定义具体执行的工作\n   * 工作明细(jobdetail): 用于描述定时工作相关的信息\n   * 触发器（trigger）：用于描述触发工作的规则，通常使用cron表达式定义调度规则\n   * 调度器（scheduler）：描述了工作明细与触发器的对应关系\n\n * 引入坐标\n   \n   \x3c!-- 引入quartz坐标--\x3e\n   <dependency>\n       <groupid>org.springframework.boot</groupid>\n       <artifactid>spring-boot-starter-quartz</artifactid>\n   </dependency>\n   \n\n * 定义要实现的任务，继承quartzjobbean\n   \n   //定义要实现的任务\n   public class myquartz extends quartzjobbean {\n       @override//context是 工作执行的上下文对象\n       protected void executeinternal(jobexecutioncontext context) throws jobexecutionexception {\n           system.out.println("task is running");\n       }\n   }\n   \n\n * 定时器实现\n   \n   //quartz 定时器配置\n   @configuration\n   public class quartzconfig {\n       @bean\n       public jobdetail printjobdetail(){\n           //绑定具体的工作\n           return jobbuilder.newjob(myquartz.class).storedurably().build();\n       }\n   \n       @bean\n       public trigger printjobtrigger(){\n           //绑定对应的工作明细\n           //("0/3 10 6 11 4 ?"); 4月11日6点10分0秒开始，每3秒执行一次\n           //————问号表示星期几，一般星期几何几月几日不同时规定，问号表示跟随前面\n           schedulebuilder schedulebuilder=cronschedulebuilder.cronschedule("0/3 * * * * ?");//任意时间，0s开始，每3秒执行一次\n           return triggerbuilder.newtrigger().forjob(printjobdetail()).withschedule(schedulebuilder).build();\n       }\n   \n   }\n   \n\n\n# springboot 定时器\n\n——简单，推荐\n\n * 开启定时任务开关\n   \n   @enablescheduling\n   public class taskapplication {\n   }\n   \n\n * 配置yml\n   \n   spring:\n     task:\n       scheduling:\n         #任务调度线程池大小  默认1\n         pool:\n           size: 1\n         #调度线程名称前缀\n         thread-name-prefix: spring_task_\n         shutdown:\n           #线程池关闭时是否等待所有任务完成\n           await-termination: false\n           #调度线程关闭前最大等待时间，确保最后一定关闭\n           await-termination-period: 10s\n   \n\n * 编写定时任务——加注解——使用cron表达式定义调度规则\n   \n   @scheduled(cron = "0/6 * * * * ?")\n   public void print(){\n       system.out.println("spring task run...");\n       //前面是调用线程的名字\n        system.out.println(thread.currentthread().getname()+":spring task run...");\n   }\n   \n\n\n# 邮件\n\n\n# spirngboot整合javamail\n\n * 传输协议\n   \n   * smtp：简单邮件传输协议，用于发送电子邮件的传输协议\n   * pop3：用于接收电子邮件的标准协议\n   * imap：互联网消息协议，是pop3的替代协议(比pop3要好那么一丢丢)\n\n * 引入坐标\n   \n   \x3c!-- 引入mail 坐标--\x3e\n   <dependency>\n       <groupid>org.springframework.boot</groupid>\n       <artifactid>spring-boot-starter-mail</artifactid>\n   </dependency>\n   \n    \x3c!-- 解决配置实体类关联配置文件 警告问题--\x3e\n   <dependency>\n       <groupid>org.springframework.boot</groupid>\n       <artifactid>spring-boot-configuration-processor</artifactid>\n       <optional>true</optional>\n   </dependency>\n   \n\n * 配置文件\n   \n   #配置email\n   spring:\n     mail:\n       host: smtp.qq.com #使用qq邮箱服务  smtp.126.com#使用126邮箱服务\n       username: 1693849288@qq.com\n       password: pmpartwhndcrfcjg\n   \n   \n   #配置email 信息\n   mail:\n     mail_from: 1693849288@qq.com\n     mail_to: diana_liangbing@126.com\n     mail_title: "springboot 整合 javamail"\n     mail_msg: "hello javamail!!"\n     ahref: "<a href=\'https://www.baidu.com\'>点击前往百度</a>"\n     img: "<img src=\'https://img1.baidu.com/it/u=4065318963,1219732686&fm=253&fmt=auto&app=138&f=jpeg?w=500&h=675\'>"\n   \n\n * 发送简单邮件\n   \n   * mail.getmail_from()+"(diana)"——//加个括号作为发件人，否则默认是邮箱前缀\n   \n   @autowired\n   private javamailsender javamailsender;\n   \n   @autowired\n   private mail mail;\n   \n   public  void sendmail(){\n       simplemailmessage message=new simplemailmessage();\n       system.out.println(mail);\n       message.setfrom(mail.getmail_from()+"(diana)");//加个括号，发件人，否则默认是邮箱前缀\n       message.setto(mail.getmail_to());\n       message.setsubject(mail.getmail_title());\n       message.settext(mail.getmail_msg());\n       javamailsender.send(message);\n   }\n   \n\n * 发送复杂邮件\n   \n   * 可以发送html的标签来加载网站和图片——\n     * 1.settext(mail.getahref(),true);——true 表示发送html 标签信息，可以解析出来，默认是false，这样的话只能发送文本消息\n     * 2.具体的html标签可参照上面的参数配置\n     * 3.使用多个settext()命令，最后一个会覆盖前面的。如果想发送多个内容，需要将所有的内容写入到一个string中\n   * 可以发送附件\n     * 1.mimemessagehelper helper=new mimemessagehelper(message,true);——true 表示可以发送附件`\n     * 2.使用多个addattachment()命令，可以直接发送多个文件。\n   \n   public  void sendmail()  {\n   \n       //附件信息\n       file f1=new file("d:\\\\生活\\\\学习\\\\西电杭研院\\\\云共享\\\\onedrive\\\\文档\\\\云共享\\\\java\\\\springboot\\\\实用篇\\\\mail\\\\pom.xml");\n       file f2=new file("d:\\\\生活\\\\学习\\\\西电杭研院\\\\云共享\\\\onedrive\\\\文档\\\\云共享\\\\java\\\\springboot\\\\实用篇\\\\mail\\\\src\\\\main\\\\resources\\\\凉冰.jpg");\n   \n       try {\n           mimemessage message=javamailsender.createmimemessage();\n           mimemessagehelper helper=new mimemessagehelper(message,true); //true 表示可以发送附件\n           helper.setfrom(mail.getmail_from()+"(diana)");//加个括号，发件人，否则默认是邮箱前缀\n           helper.setto(mail.getmail_to());\n           helper.setsubject(mail.getmail_title());\n   \n           //发送正文 ——多个settext 后面的会覆盖前面的——要想发送多个内容，要把内容放到一个字符串中\n           helper.settext(mail.getahref());//默认是false,显示文本   "<a href=\'https://www.baidu.com\'>点击前往百度</a>"\n           helper.settext(mail.getahref(),true);//用true 表示发送的html文件    --点击前往百度--\n           helper.settext(mail.getimg(),true);//多个settext 后面的会覆盖前面的\n   \n           //发送附件 ——多个addattachment 会一起发过去，添加多个附件\n           helper.addattachment(f1.getname(),f1);\n           helper.addattachment(f2.getname(),f2);\n   \n           javamailsender.send(message);\n   \n       } catch (exception e) {\n           e.printstacktrace();\n       }\n   \n   }\n   \n\n\n# 消息\n\n\n# 基础概念\n\n * 消息发送方——生产者\n\n * 消息接收方——消费者\n\n * 同步消息——需要等待回复，然后继续进行\n\n * 异步消息——不需要等待回复，你回不回复，不影响我的运行\n\n * 消息队列——主服务器接收大量请求后，将服务提交给消息队列，然后让子服务器去消息队列中去取。\n   \n   \n\n * 企业中的三种异步消息传递技术（规范）\n   \n   * jms(规范)\n     \n     \n   \n   * amqp(协议)\n     \n     \n   \n   * mqtt(小型)\n     \n     > 消息队列遥测传输，专为小设备设计，是物联网生态系统中主要成分之一。\n\n * 技术实现\n   \n   * activemq\n   * rabbitmq\n   * rocketmq\n   * kafka\n\n\n# 手工模拟消息队列\n\n * 使用arraylist集合，模拟简单的消息队列\n\n//手动模拟消息队列实现\n@service\npublic class messageserviceimpl implements imessageservice {\n\n    private arraylist<string> msglist=new arraylist<string>();\n\n    @override //往消息队列中传入订单\n    public void sendmessage(string id) {\n\n        system.out.println("待发送短信的订单已纳入处理队列，id："+id);\n        msglist.add(id);\n\n    }\n\n    @override//从消息队列中取出订单\n    public string domessage() {\n        string id = msglist.remove(0);//接收从队列中移除的订单id\n        system.out.println("已完成短信发送业务，id："+id);\n        return id;\n    }\n}\n\n\n\n# activemq\n\n * 启动服务\n   \n   * d:\\program files\\mq\\activemq\\apache-activemq-5.16.3\\bin\\win64\\activemq.bat 双击即可\n   * 网页管理 http://127.0.0.1:8161/\n\n * 引入坐标\n   \n   \x3c!-- activemq--\x3e\n   <dependency>\n       <groupid>org.springframework.boot</groupid>\n       <artifactid>spring-boot-starter-activemq</artifactid>\n   </dependency>\n   \n\n * 配置yml\n   \n   #配置activemq\n   spring:\n     activemq:\n       broker-url: tcp://localhost:61616\n     jms:\n       template:\n         default-destination: diana #指定一个destination，相当于是给消息队列起名字\n       pub-sub-domain: true #是否开启发布订阅模型，默认为false，即默认为点对点模型\n   \n\n * 代码\n   \n   * 使用接口规范定义，不需要使用具体的实现类，就像使用jdbc模板那样\n   * 推荐使用监听功能，实现自动消费。\n   \n   @autowired\n   private jmsmessagingtemplate messagingtemplate; //接口规范  不需要使用实现类定义\n   \n   @override //往消息队列中传入订单\n   public void sendmessage(string id) {\n   \n       system.out.println("待发送短信的订单已纳入处理队列，id："+id);\n       //messagingtemplate.convertandsend(id);  //放到默认位置   ——default-destination: diana\n       messagingtemplate.convertandsend("order.queue.id",id);//指定 destination\n   }\n   \n   @override//从消息队列中取出订单 --------手动从消息队列中取出信息--手动消费\n   public string domessage() {\n       //        string id = messagingtemplate.receiveandconvert(string.class);//从默认destination——diana 中取\n       string id = messagingtemplate.receiveandconvert("order.queue.id",string.class);\n       system.out.println("已手动完成短信发送业务，id："+id);\n       return id;\n   }\n   \n   \n   //作为监听器存在，一旦发现消息队列中有消息进来，立即消费掉\n   @component\n   public class messagelistener {\n   \n      @jmslistener(destination = "order.queue.id")//--------自动从消息队列中取出信息--自动消费\n      @sendto("order.other.queue.id")//将方法的返回值自动发送到另外一个队列——可有可无\n       public string receive(string id) {\n           system.out.println("已自动完成短信发送业务，id："+id);\n           return "new:"+id;\n       }\n   \n   \n   }\n   \n\n\n# rabbitmq\n\n * 启动服务\n   \n   d:\\program files\\mq\\rabbitmq\\rabbitmq\\rabbitmq\\rabbitmq_server-3.9.13\\sbin下cmd\n   \n   * rabbitmq-service.bat strat——启动服务\n   * rabbitmq-service.bat stop——关闭服务\n   * rabbitmq-service.bat staus——查看启动状态\n   * rabbitmq-plugins.bat list——查看所有插件\n   * rabbitmq-plugins.bat enable rabbitmq_management—启动客户端插件\n\n * 启动rabbitmq_management——默认端口 15672\n   \n   * 访问网站 guest，guest\n   \n   * 报错——原因是用户名是中文\n   \n   \n\n * 导入坐标\n   \n   \x3c!-- rabbitmq--\x3e\n   <dependency>\n       <groupid>org.springframework.boot</groupid>\n       <artifactid>spring-boot-starter-amqp</artifactid>\n   </dependency>\n   \n\n * 配置yml\n   \n   #配置 rabbitmq\n   rabbitmq:\n     host: localhost\n     port: 5672  #服务端口5672，对外管理端口 15672\n   \n\n * 两种模式\n   \n   * 直连模式配置——direct\n     \n     // rabbitmq 直连 配置\n     @configuration\n     public class rabbitmqconfigdirect {\n     \n         @bean//新建消息队列对象\n         public queue directqueue(){\n             return new queue("direct_queue");\n         }\n         @bean//新建消息队列对象\n         public queue directqueue2(){\n             //durable: 是否持久化，默认false\n             //exclusive： 是否当前连接专用，默认false；如果设为true,则连接关闭后队列即被删除\n             //autodelete: 是否自动删除，当生产者或消费者不在使用此队列，自动删除\n             return new queue("direct_queue2",true,false,false);\n         }\n     \n         @bean//新建交换机对象\n         public directexchange directexchange(){\n             return new directexchange("directexchange");\n         }\n     \n         @bean//绑定交换机对象和消息队列对象\n         public binding bindingdirect(){\n             return bindingbuilder.bind(directqueue()).to(directexchange()).with("direct");\n         }\n         @bean//一个交换机对象可以绑定多个消息队列对象\n         public binding bindingdirect2(){\n             return bindingbuilder.bind(directqueue2()).to(directexchange()).with("direct2");\n         }\n     \n     }\n     \n   \n   * 分发模式——topic\n     \n     * 与direct 不同的地方在于 分发功能，可以根据绑定时的名字，根据匹配规则，分发到不同服务\n     \n     ![rabbit 匹配](/assets/后端/springboot/rabbit 匹配.png)\n     \n     * 当一个服务符合两个绑定的匹配时，会同时将消息发送到两个不同的队列\n     \n     @bean//绑定交换机对象和消息队列对象\n     public binding bindingtopic(){\n         //*表示匹配到任意，这样就算有个分发功能，一个绑定名称可以对应多个 到\n         return bindingbuilder.bind(topicqueue()).to(topicexchange()).with("topic.*.id");\n     }\n     @bean//一个交换机对象可以绑定多个消息队列对象\n     public binding bindingtopic2(){\n         //当一个业务匹配到多个绑定对象时，会将消息分发到多个队列\n         return bindingbuilder.bind(topicqueue2()).to(topicexchange()).with("topic.orders.*");\n     }\n     \n   \n   * 主服务代码——两种方式代码是一样的——绑定名称不一样\n     \n     @autowired\n     private amqptemplate amqptemplate;\n     \n     @override—————————————————————————————————————————————————————直连模式———————————一一对应\n     public void sendmessage(string id) {\n         system.out.println("待发送短信的订单已纳入处理队列，id："+id);\n         //directexchange交换机对象，direct名字--对应绑定名称，一一对应\n         amqptemplate.convertandsend("directexchange","direct",id);\n     }\n     \n     @override——————————————————————————————————————————————————topic模式——————————————匹配规则对应\n     public void sendmessage(string id) {\n         system.out.println("待发送短信的订单已纳入处理队列，id："+id);\n         //topicexchange交换机对象名称，topic.orders.id --对应绑定名称，有分发模式，匹配到即可\n         amqptemplate.convertandsend("topicexchange","topic.orders.id",id);\n     }\n     \n   \n   * 监听器——两种方式代码是一样的——队列名称不一样\n     \n     * @rabbitlistener(queues = "direct_queue")\n     \n     @rabbitlistener(queues = "direct_queue")//-----直连模式---自动从消息队列中取出信息--自动消费\n     public void receive(string id) {\n         system.out.println("a1已自动完成短信发送业务，id："+id);\n     }\n     \n     @rabbitlistener(queues = "topic_queue")//----topic模式----自动从消息队列中取出信息--自动消费\n     public void receive(string id) {\n         system.out.println("queue1已自动完成短信发送业务，id："+id);\n     }\n     \n     \n     * 当多个监听器同时监听一个队列时，多个监听器会轮询操作\n\n\n# rocketmq\n\n * 启动服务\n   \n   * 解决java安装路径带空格问题\n     \n     修改runserver.cmd和runbroker.cmd\n     \n     * 1.将java_home定义成字符串 ——set javahome="%java_home%"\n     * 2.在倒数第三行中使用%javahome%替换掉原来的%java_home%——set "java_opt=%java_opt% -djava.ext.dirs=%base_dir%lib;%javahome%\\jre\\lib\\ext"\n     * 3.在倒数第二行中，将%classpath%修改为"%classpath%",加了一对英文双引号\n   \n   * 启动流程\n     \n     * 先启动mqnamesrv.cmd,双击即可启动命名服务器\n     * 再启动mqbroker.cmd，双击即可启动业务服务器\n     * 服务器功能测试：生产者tools org.apache.rocketmq.exemple.quickstart.producer\n     * 服务器功能测试：消费者tools org.apache.rocketmq.exemple.quickstart.consumer\n\n * 导入坐标\n   \n   \x3c!-- rocketmq--\x3e\n   <dependency>\n       <groupid>org.apache.rocketmq</groupid>\n       <artifactid>rocketmq-spring-boot-starter</artifactid>\n       <version>2.2.2</version>\n   </dependency>\n   \n\n * 配置yml\n   \n   #配置rocketmq\n   rocketmq:\n     name-server: localhost:9876\n     producer:\n       group: group_rocketmq #初始化生产者组名（随便起）\n   \n\n * 代码\n   \n   * service\n   \n   @autowired\n   private rocketmqtemplate rocketmqtemplate;\n   \n   @override\n   public void sendmessage(string id) {\n       system.out.println("待发送短信的订单已纳入处理队列，id："+id);\n       //        rocketmqtemplate.convertandsend("order_id",id);//同步消息\n   \n       //定义回调方法\n       sendcallback callback=new sendcallback() {\n           @override\n           public void onsuccess(sendresult sendresult) {\n               system.out.println("发送成功！！");\n           }\n   \n           @override\n           public void onexception(throwable throwable) {\n               system.out.println("发送失败！！");\n   \n           }\n       };//发送异步消息\n       rocketmqtemplate.asyncsend("order_id",id,callback);\n   }\n   \n   \n   * listener ——实现接口，注解写在上方\n   \n   @component//监听器\n   //destination 的名称（同存入消息时的名称），   消费者组的名称(随便起)，这里让消费者和生产者同组\n   @rocketmqmessagelistener(topic = "order_id",consumergroup = "group_rocketmq")\n   public class messagelistener implements rocketmqlistener<string> {\n   \n       @override\n       public void onmessage(string id) {\n           system.out.println("已自动完成短信发送业务，id："+id);\n       }\n   \n   }\n   \n\n\n# kafka\n\n * 启动服务\n   \n   * 启动zookeeper zookeeper-server-start.bat ../../config/zookeeper.properties\n     * 默认端口 2181\n   * 启动kafka kafka-server-start ..\\..\\config\\server.properties\n     * 默认端口9092\n   * 创建topic kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic diana\n   * 查看topic kafka-topics --zookeeper localhost:2181 --list\n   * 删除topic kafka-topics --delete --zookeeper localhost:2181 --topic diana\n   * 生产者功能测试 kafka-console-producer --broker-list localhost:9092 --topic diana\n   * 消费者功能测试 kafka-console-consumer --bootstrap-server localhost:9092 --topic diana --from-beginning\n\n * 导入坐标\n   \n   \x3c!-- kafka--\x3e\n   <dependency>\n       <groupid>org.springframework.kafka</groupid>\n       <artifactid>spring-kafka</artifactid>\n   </dependency>\n   \n\n * 配置yml\n   \n   #配置kafka\n   kafka:\n     bootstrap-servers: localhost:9092\n     consumer:\n       group-id: order #配置消费者组id\n   \n\n * 代码\n   \n   * service\n   \n   @autowired\n   private kafkatemplate<string,string> kafkatemplate;\n   \n   @override\n   public void sendmessage(string id) {\n       system.out.println("待发送短信的订单已纳入处理队列(kafka)，id："+id);\n       kafkatemplate.send("diana",id);//前面是topic 名称，后面是要送入队列的消息\n   }\n   \n   \n   * listener\n   \n   @kafkalistener(topics = "diana")//设置topic名称\n   public void onmessage(consumerrecord<string,string> record) {\n       system.out.println("已自动完成短信发送业务(kafaka)，id："+record.value());\n   }\n   ',charsets:{cjk:!0}},{title:"文档操作",frontmatter:{autoSort:98,title:"文档操作",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/7624a9/",categories:["后端","微服务","ES"],tags:["知识","微服务","ES"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/20.ES/15.%E6%96%87%E6%A1%A3%E6%93%8D%E4%BD%9C.html",relativePath:"01.后端/60.微服务/20.ES/15.文档操作.md",key:"v-61a116d1",path:"/pages/7624a9/",headers:[{level:2,title:"新增文档",slug:"新增文档",normalizedTitle:"新增文档",charIndex:2},{level:2,title:"查询文档",slug:"查询文档",normalizedTitle:"查询文档",charIndex:314},{level:2,title:"删除文档",slug:"删除文档",normalizedTitle:"删除文档",charIndex:449},{level:2,title:"修改文档",slug:"修改文档",normalizedTitle:"修改文档",charIndex:565},{level:3,title:"全量修改",slug:"全量修改",normalizedTitle:"全量修改",charIndex:584},{level:3,title:"增量修改",slug:"增量修改",normalizedTitle:"增量修改",charIndex:602},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:1161}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"新增文档 查询文档 删除文档 修改文档 全量修改 增量修改 总结",content:'# 新增文档\n\n语法：\n\nPOST /索引库名/_doc/文档id\n{\n    "字段1": "值1",\n    "字段2": "值2",\n    "字段3": {\n        "子属性1": "值3",\n        "子属性2": "值4"\n    },\n    // ...\n}\n\n\n示例：\n\nPOST /heima/_doc/1\n{\n    "info": "黑马程序员Java讲师",\n    "email": "zy@itcast.cn",\n    "name": {\n        "firstName": "云",\n        "lastName": "赵"\n    }\n}\n\n\n响应：\n\n\n\n\n# 查询文档\n\n根据rest风格，新增是post，查询应该是get，不过查询一般都需要条件，这里我们把文档id带上。\n\n语法：\n\nGET /{索引库名称}/_doc/{id}\n\n\n通过kibana查看数据：\n\nGET /heima/_doc/1\n\n\n查看结果：\n\n\n\n\n# 删除文档\n\n删除使用DELETE请求，同样，需要根据id进行删除：\n\n语法：\n\nDELETE /{索引库名}/_doc/id值\n\n\n示例：\n\n## 根据id删除数据\nDELETE /heima/_doc/1\n\n\n结果：\n\n\n\n\n# 修改文档\n\n修改有两种方式：\n\n * 全量修改：直接覆盖原来的文档\n * 增量修改：修改文档中的部分字段\n\n\n# 全量修改\n\n全量修改是覆盖原来的文档，其本质是：\n\n * 根据指定的id删除文档\n * 新增一个相同id的文档\n\n注意：如果根据id删除时，id不存在，第二步的新增也会执行，也就从修改变成了新增操作了。\n\n语法：\n\nPUT /{索引库名}/_doc/文档id\n{\n    "字段1": "值1",\n    "字段2": "值2",\n    // ... 略\n}\n\n\n\n示例：\n\nPUT /heima/_doc/1\n{\n    "info": "黑马程序员高级Java讲师",\n    "email": "zy@itcast.cn",\n    "name": {\n        "firstName": "云",\n        "lastName": "赵"\n    }\n}\n\n\n\n# 增量修改\n\n增量修改是只修改指定id匹配的文档中的部分字段。\n\n语法：\n\nPOST /{索引库名}/_update/文档id\n{\n    "doc": {\n         "字段名": "新的值",\n    }\n}\n\n\n示例：\n\nPOST /heima/_update/1\n{\n  "doc": {\n    "email": "ZhaoYun@itcast.cn"\n  }\n}\n\n\n\n# 总结\n\n文档操作有哪些？\n\n * 创建文档：POST /{索引库名}/_doc/文档id { json文档 }\n * 查询文档：GET /{索引库名}/_doc/文档id\n * 删除文档：DELETE /{索引库名}/_doc/文档id\n * 修改文档：\n   * 全量修改：PUT /{索引库名}/_doc/文档id { json文档 }\n   * 增量修改：POST /{索引库名}/_update/文档id { "doc": {字段}}\n\n##新增文档\nPOST /diana/_doc/1\n{\n  "info":"皎月女神",\n  "email":"123@123.com",\n  "name": {\n    "firstName": "冰",\n    "lastName": "凉"\n  }\n}\n\n##查询文档\nGET /diana/_doc/1\n    \n##批量查询\nGET /diana/_search\n\n##删除文档\nDELETE /diana/_doc/1\n\n##修改文档--全量修改\nPUT /diana/_doc/2\n{\n  "info":"皎月女神",\n  "email":"123@1234.com",\n  "name": {\n    "firstName": "冰",\n    "lastName": "凉"\n  }\n}\n\n##修改文档--增量修改\nPOST /diana/_update/1\n{\n  "doc": {\n    "info": "diana"\n  }\n}\n\n',normalizedContent:'# 新增文档\n\n语法：\n\npost /索引库名/_doc/文档id\n{\n    "字段1": "值1",\n    "字段2": "值2",\n    "字段3": {\n        "子属性1": "值3",\n        "子属性2": "值4"\n    },\n    // ...\n}\n\n\n示例：\n\npost /heima/_doc/1\n{\n    "info": "黑马程序员java讲师",\n    "email": "zy@itcast.cn",\n    "name": {\n        "firstname": "云",\n        "lastname": "赵"\n    }\n}\n\n\n响应：\n\n\n\n\n# 查询文档\n\n根据rest风格，新增是post，查询应该是get，不过查询一般都需要条件，这里我们把文档id带上。\n\n语法：\n\nget /{索引库名称}/_doc/{id}\n\n\n通过kibana查看数据：\n\nget /heima/_doc/1\n\n\n查看结果：\n\n\n\n\n# 删除文档\n\n删除使用delete请求，同样，需要根据id进行删除：\n\n语法：\n\ndelete /{索引库名}/_doc/id值\n\n\n示例：\n\n## 根据id删除数据\ndelete /heima/_doc/1\n\n\n结果：\n\n\n\n\n# 修改文档\n\n修改有两种方式：\n\n * 全量修改：直接覆盖原来的文档\n * 增量修改：修改文档中的部分字段\n\n\n# 全量修改\n\n全量修改是覆盖原来的文档，其本质是：\n\n * 根据指定的id删除文档\n * 新增一个相同id的文档\n\n注意：如果根据id删除时，id不存在，第二步的新增也会执行，也就从修改变成了新增操作了。\n\n语法：\n\nput /{索引库名}/_doc/文档id\n{\n    "字段1": "值1",\n    "字段2": "值2",\n    // ... 略\n}\n\n\n\n示例：\n\nput /heima/_doc/1\n{\n    "info": "黑马程序员高级java讲师",\n    "email": "zy@itcast.cn",\n    "name": {\n        "firstname": "云",\n        "lastname": "赵"\n    }\n}\n\n\n\n# 增量修改\n\n增量修改是只修改指定id匹配的文档中的部分字段。\n\n语法：\n\npost /{索引库名}/_update/文档id\n{\n    "doc": {\n         "字段名": "新的值",\n    }\n}\n\n\n示例：\n\npost /heima/_update/1\n{\n  "doc": {\n    "email": "zhaoyun@itcast.cn"\n  }\n}\n\n\n\n# 总结\n\n文档操作有哪些？\n\n * 创建文档：post /{索引库名}/_doc/文档id { json文档 }\n * 查询文档：get /{索引库名}/_doc/文档id\n * 删除文档：delete /{索引库名}/_doc/文档id\n * 修改文档：\n   * 全量修改：put /{索引库名}/_doc/文档id { json文档 }\n   * 增量修改：post /{索引库名}/_update/文档id { "doc": {字段}}\n\n##新增文档\npost /diana/_doc/1\n{\n  "info":"皎月女神",\n  "email":"123@123.com",\n  "name": {\n    "firstname": "冰",\n    "lastname": "凉"\n  }\n}\n\n##查询文档\nget /diana/_doc/1\n    \n##批量查询\nget /diana/_search\n\n##删除文档\ndelete /diana/_doc/1\n\n##修改文档--全量修改\nput /diana/_doc/2\n{\n  "info":"皎月女神",\n  "email":"123@1234.com",\n  "name": {\n    "firstname": "冰",\n    "lastname": "凉"\n  }\n}\n\n##修改文档--增量修改\npost /diana/_update/1\n{\n  "doc": {\n    "info": "diana"\n  }\n}\n\n',charsets:{cjk:!0}},{title:"ES基础",frontmatter:{autoSort:100,title:"ES基础",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/e9e22c/",categories:["后端","微服务","ES"],tags:["知识","微服务","ES"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/20.ES/05.ES%E5%9F%BA%E7%A1%80.html",relativePath:"01.后端/60.微服务/20.ES/05.ES基础.md",key:"v-427aa158",path:"/pages/e9e22c/",headers:[{level:2,title:"了解ES",slug:"了解es",normalizedTitle:"了解es",charIndex:2},{level:3,title:"elasticsearch的作用",slug:"elasticsearch的作用",normalizedTitle:"elasticsearch的作用",charIndex:11},{level:3,title:"ELK技术栈",slug:"elk技术栈",normalizedTitle:"elk技术栈",charIndex:188},{level:3,title:"elasticsearch和lucene",slug:"elasticsearch和lucene",normalizedTitle:"elasticsearch和lucene",charIndex:332},{level:3,title:"为什么不是其他搜索技术？",slug:"为什么不是其他搜索技术",normalizedTitle:"为什么不是其他搜索技术？",charIndex:593},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:698},{level:2,title:"es的一些概念",slug:"es的一些概念",normalizedTitle:"es的一些概念",charIndex:902},{level:3,title:"文档和字段",slug:"文档和字段",normalizedTitle:"文档和字段",charIndex:959},{level:3,title:"索引和映射",slug:"索引和映射",normalizedTitle:"索引和映射",charIndex:1103},{level:3,title:"mysql与elasticsearch",slug:"mysql与elasticsearch",normalizedTitle:"mysql与elasticsearch",charIndex:1331}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"了解ES elasticsearch的作用 ELK技术栈 elasticsearch和lucene 为什么不是其他搜索技术？ 总结 es的一些概念 文档和字段 索引和映射 mysql与elasticsearch",content:"# 了解ES\n\n\n# elasticsearch的作用\n\nelasticsearch是一款非常强大的开源搜索引擎，具备非常多强大功能，可以帮助我们从海量数据中快速找到需要的内容\n\n例如：\n\n * 在GitHub搜索代码\n   \n   \n\n * 在电商网站搜索商品\n   \n   \n\n * 在百度搜索答案\n   \n   \n\n * 在打车软件搜索附近的车\n   \n   \n\n\n# ELK技术栈\n\nelasticsearch结合kibana、Logstash、Beats，也就是elastic stack（ELK）。被广泛应用在日志数据分析、实时监控等领域：\n\n\n\n而elasticsearch是elastic stack的核心，负责存储、搜索、分析数据。\n\n\n\n\n# elasticsearch和lucene\n\nelasticsearch底层是基于lucene来实现的。\n\nLucene是一个Java语言的搜索引擎类库，是Apache公司的顶级项目，由DougCutting于1999年研发。官网地址：https://lucene.apache.org/ 。\n\n\n\nelasticsearch的发展历史：\n\n * 2004年Shay Banon基于Lucene开发了Compass\n * 2010年Shay Banon 重写了Compass，取名为Elasticsearch。\n\n\n\n\n# 为什么不是其他搜索技术？\n\n目前比较知名的搜索引擎技术排名：\n\n\n\n虽然在早期，Apache Solr是最主要的搜索引擎技术，但随着发展elasticsearch已经渐渐超越了Solr，独占鳌头：\n\n\n\n\n# 总结\n\n什么是elasticsearch？\n\n * 一个开源的分布式搜索引擎，可以用来实现搜索、日志统计、分析、系统监控等功能\n\n什么是elastic stack（ELK）？\n\n * 是以elasticsearch为核心的技术栈，包括beats、Logstash、kibana、elasticsearch\n\n什么是Lucene？\n\n * 是Apache的开源搜索引擎类库，提供了搜索引擎的核心API\n\n\n# es的一些概念\n\nelasticsearch中有很多独有的概念，与mysql中略有差别，但也有相似之处。\n\n\n# 文档和字段\n\nelasticsearch是面向**文档（Document）**存储的，可以是数据库中的一条商品数据，一个订单信息。文档数据会被序列化为json格式后存储在elasticsearch中：\n\n\n\n而Json文档中往往包含很多的字段（Field），类似于数据库中的列。\n\n\n# 索引和映射\n\n索引（Index），就是相同类型的文档的集合。\n\n例如：\n\n * 所有用户文档，就可以组织在一起，称为用户的索引；\n * 所有商品的文档，可以组织在一起，称为商品的索引；\n * 所有订单的文档，可以组织在一起，称为订单的索引；\n\n\n\n因此，我们可以把索引当做是数据库中的表。\n\n数据库的表会有约束信息，用来定义表的结构、字段的名称、类型等信息。因此，索引库中就有映射（mapping），是索引中文档的字段约束信息，类似表的结构约束。\n\n\n# mysql与elasticsearch\n\n我们统一的把mysql与elasticsearch的概念做一下对比：\n\nMYSQL    ELASTICSEARCH   说明\nTable    Index           索引(index)，就是文档的集合，类似数据库的表(table)\nRow      Document        文档（Document），就是一条条的数据，类似数据库中的行（Row），文档都是JSON格式\nColumn   Field           字段（Field），就是JSON文档中的字段，类似数据库中的列（Column）\nSchema   Mapping         Mapping（映射）是索引中文档的约束，例如字段类型约束。类似数据库的表结构（Schema）\nSQL      DSL             DSL是elasticsearch提供的JSON风格的请求语句，用来操作elasticsearch，实现CRUD\n\n是不是说，我们学习了elasticsearch就不再需要mysql了呢？\n\n并不是如此，两者各自有自己的擅长支出：\n\n * Mysql：擅长事务类型操作，可以确保数据的安全和一致性\n\n * Elasticsearch：擅长海量数据的搜索、分析、计算\n\n因此在企业中，往往是两者结合使用：互补\n\n * 对安全性要求较高的写操作，使用mysql实现\n * 对查询性能要求较高的搜索需求，使用elasticsearch实现\n * 两者再基于某种方式，实现数据的同步，保证一致性\n\n",normalizedContent:"# 了解es\n\n\n# elasticsearch的作用\n\nelasticsearch是一款非常强大的开源搜索引擎，具备非常多强大功能，可以帮助我们从海量数据中快速找到需要的内容\n\n例如：\n\n * 在github搜索代码\n   \n   \n\n * 在电商网站搜索商品\n   \n   \n\n * 在百度搜索答案\n   \n   \n\n * 在打车软件搜索附近的车\n   \n   \n\n\n# elk技术栈\n\nelasticsearch结合kibana、logstash、beats，也就是elastic stack（elk）。被广泛应用在日志数据分析、实时监控等领域：\n\n\n\n而elasticsearch是elastic stack的核心，负责存储、搜索、分析数据。\n\n\n\n\n# elasticsearch和lucene\n\nelasticsearch底层是基于lucene来实现的。\n\nlucene是一个java语言的搜索引擎类库，是apache公司的顶级项目，由dougcutting于1999年研发。官网地址：https://lucene.apache.org/ 。\n\n\n\nelasticsearch的发展历史：\n\n * 2004年shay banon基于lucene开发了compass\n * 2010年shay banon 重写了compass，取名为elasticsearch。\n\n\n\n\n# 为什么不是其他搜索技术？\n\n目前比较知名的搜索引擎技术排名：\n\n\n\n虽然在早期，apache solr是最主要的搜索引擎技术，但随着发展elasticsearch已经渐渐超越了solr，独占鳌头：\n\n\n\n\n# 总结\n\n什么是elasticsearch？\n\n * 一个开源的分布式搜索引擎，可以用来实现搜索、日志统计、分析、系统监控等功能\n\n什么是elastic stack（elk）？\n\n * 是以elasticsearch为核心的技术栈，包括beats、logstash、kibana、elasticsearch\n\n什么是lucene？\n\n * 是apache的开源搜索引擎类库，提供了搜索引擎的核心api\n\n\n# es的一些概念\n\nelasticsearch中有很多独有的概念，与mysql中略有差别，但也有相似之处。\n\n\n# 文档和字段\n\nelasticsearch是面向**文档（document）**存储的，可以是数据库中的一条商品数据，一个订单信息。文档数据会被序列化为json格式后存储在elasticsearch中：\n\n\n\n而json文档中往往包含很多的字段（field），类似于数据库中的列。\n\n\n# 索引和映射\n\n索引（index），就是相同类型的文档的集合。\n\n例如：\n\n * 所有用户文档，就可以组织在一起，称为用户的索引；\n * 所有商品的文档，可以组织在一起，称为商品的索引；\n * 所有订单的文档，可以组织在一起，称为订单的索引；\n\n\n\n因此，我们可以把索引当做是数据库中的表。\n\n数据库的表会有约束信息，用来定义表的结构、字段的名称、类型等信息。因此，索引库中就有映射（mapping），是索引中文档的字段约束信息，类似表的结构约束。\n\n\n# mysql与elasticsearch\n\n我们统一的把mysql与elasticsearch的概念做一下对比：\n\nmysql    elasticsearch   说明\ntable    index           索引(index)，就是文档的集合，类似数据库的表(table)\nrow      document        文档（document），就是一条条的数据，类似数据库中的行（row），文档都是json格式\ncolumn   field           字段（field），就是json文档中的字段，类似数据库中的列（column）\nschema   mapping         mapping（映射）是索引中文档的约束，例如字段类型约束。类似数据库的表结构（schema）\nsql      dsl             dsl是elasticsearch提供的json风格的请求语句，用来操作elasticsearch，实现crud\n\n是不是说，我们学习了elasticsearch就不再需要mysql了呢？\n\n并不是如此，两者各自有自己的擅长支出：\n\n * mysql：擅长事务类型操作，可以确保数据的安全和一致性\n\n * elasticsearch：擅长海量数据的搜索、分析、计算\n\n因此在企业中，往往是两者结合使用：互补\n\n * 对安全性要求较高的写操作，使用mysql实现\n * 对查询性能要求较高的搜索需求，使用elasticsearch实现\n * 两者再基于某种方式，实现数据的同步，保证一致性\n\n",charsets:{cjk:!0}},{title:"操作练习",frontmatter:{autoSort:93,title:"操作练习",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/06e2e5/",categories:["后端","微服务","ES"],tags:["知识","微服务","ES"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/20.ES/100.es%E6%93%8D%E4%BD%9C%E6%96%87%E6%A1%A3.html",relativePath:"01.后端/60.微服务/20.ES/100.es操作文档.md",key:"v-56deed3f",path:"/pages/06e2e5/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:'# kibana\n\n * Dev Tools的 操作 记录\n\n\nGET _search\n{\n  "query": {\n    "match_all": {}\n  }\n}\n\n#测试es是否链接\nGET /\n\n\n#测试标准分词器\nPOST /_analyze\n{\n  "text":"黑马程序员学习java太棒了！",\n  "analyzer": "standard"\n}\n\n#测试ik分词器\nPOST /_analyze\n{\n  "text":"啊黑马程序员学习java太棒了！",\n  //"analyzer": "ik_max_word"  //最细切分\n  "analyzer": "ik_smart" //最少切分\n}\n\n#测试ik分词器+自定义字典\nPOST /_analyze\n{\n  "text":"哦嗯雄的凉冰太厉害了,比英雄联盟的皎月女神强啊！",\n  "analyzer": "ik_smart" \n}\n\n\n#创建索引库\nPUT /diana\n{\n  "mappings": {\n    "properties": {\n      "info": {\n        "type": "text",\n        "analyzer": "ik_smart"\n      },\n      "email": {\n        "type": "keyword",\n        "index": false\n      },\n      "name": {\n        "type": "object", \n        "properties": {\n          "firstName": {\n            "type": "keyword"\n          },\n          "lastName": {\n            "type": "keyword"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n#查询索引库\nGET /diana\n\n#修改索引库,只允许添加新字段，不允许修改已经存在的字段\nPUT /diana/_mapping\n{\n  "properties": {\n    "age": {\n      "type":"integer"\n    }\n  }\n}\n\n#删除索引库\nDELETE /diana\n\n\n\n\n\n\n#新增文档\nPOST /diana/_doc/1\n{\n  "info":"皎月女神",\n  "email":"123@123.com",\n  "name": {\n    "firstName": "冰",\n    "lastName": "凉"\n  }\n}\n\n#查询文档\nGET /diana/_doc/1\n\n#删除文档\nDELETE /diana/_doc/1\n\n#修改文档--全量修改\nPUT /diana/_doc/2\n{\n  "info":"皎月女神",\n  "email":"123@1234.com",\n  "name": {\n    "firstName": "冰",\n    "lastName": "凉"\n  }\n}\n\n#修改文档--增量修改\nPOST /diana/_update/1\n{\n  "doc": {\n    "info": "diana"\n  }\n}\n\n\n\n\n\n\n#创建酒店索引\nPUT /hotel\n{\n  "mappings": {\n    "properties": {\n      "id":{//keyword 类型，不是long\n        "type":"keyword"\n      },\n      "name": {\n        "type":"text",\n        "analyzer": "ik_max_word",\n        "copy_to": "all"\n      },\n      "address":{\n        "type":"text",\n        "analyzer": "ik_smart"\n      },\n      "price":{\n        "type":"integer",\n        "copy_to": "all"\n      },\n      "score":{\n        "type":"integer",\n        "copy_to": "all"\n      },\n      "brand":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "city":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "starName":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "business":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "location":{//地理位置特殊字段,包括经度和纬度\n        "type":"geo_point"\n      },\n      "pic":{//不分词，不搜索\n        "type":"keyword",\n        "index":false\n      },\n      "all": {//将许多字段的值聚在一块，方便搜索\n        "type":"text",\n        "analyzer": "ik_max_word"\n      }\n    }\n  }\n}\n\n#查看酒店索引\nGET /hotel\n\n#删除酒店索引\nDELETE /hotel\n\n#得到酒店文档信息\nGET /hotel/_doc/36934\n\n#批量查询\nGET /hotel/_search\n\n\n\n\n\n\n\n#对文档内信息的检索###############\n\n###查询所有\nGET /hotel/_search\n{\n  "query": {\n    "match_all": {}\n  }\n}\n\n\n###全文检索\n#match  单个字段查询  --性能高，推荐\nGET /hotel/_search\n{\n  "query": {\n    "match": {\n      "all": "外滩如家"\n    }\n  }\n}\n\n#multi_match  查询多个字段\nGET /hotel/_search\n{\n  "query": {\n    "multi_match": {\n      "query": "外滩如家",\n      "fields":["brand","name", "business"]\n    }\n  }\n}\n\n###精确查询\n#term  根据词条精确值查询\nGET /hotel/_search\n{\n  "query": {\n    "term": {\n      "city": {\n        "value": "上海"\n      }\n    }\n  }\n}\n\n#range 根据值的范围查询\nGET /hotel/_search\n{\n  "query": {\n    "range": {\n      "price": {\n        "gte": 100, //带e是等于  大于等于-gte；大于gt\n        "lte": 300\n      }\n    }\n  }\n}\n\n\n###地理查询\n#geo_bounding_box 矩形范围查询，左上，右下坐标\nGET /hotel/_search\n{\n  "query": {\n    "geo_bounding_box": {\n      "location": {\n        "top_left": {//左上\n          "lat": 31.1,\n          "lon": 121.5\n        },\n        "bottom_right": {//右下\n          "lat": 30.9,\n          "lon": 121.7\n        }\n      }\n     \n    }\n  }\n}\n\n\n#geo_distance 距离查询  以坐标为中心画一个圆  --常用\nGET /hotel/_search\n{\n  "query": {\n    "geo_distance": {\n      "distance": "5km",\n      "location": "31.21, 121.5"\n    }\n  }\n}\n\n\n\n###复合查询\n#默认按相关度打分，现在要修改打分数 function_score\nGET /hotel/_search\n{\n  "query": {\n    "function_score": {\n      "query": {//正常查询\n        "match": {\n          "all": "外滩"\n        }\n      },\n      "functions": [\n        {\n          "filter": {//筛选条件\n            "term": {\n              "brand": "如家"\n            }\n          },\n          "weight": 10 //算分权值  默认是乘算\n        }\n      ],\n      "boost_mode": "sum"//乘法运算\n    }\n  }\n}\n\n\n#布尔查询  一个或多个查询子句的组合\nGET /hotel/_search\n{\n  "query": {\n    "bool": {\n      "must": [//必须是如家品牌\n        {\n          "term": {\n            "brand": {\n              "value": "如家"\n            }\n          }\n        }\n      ],\n      "must_not": [//必须不大于400 -- 小于等于400\n        {\n          "range": {\n            "price": {//大于400\n              "gt": 400\n            }\n          }\n        }\n      ],\n      "filter": [//筛选范围\n        {\n          "geo_distance": {\n            "distance": "10km",\n            "location": "31.21, 121.5"\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n#搜索结果处理###############\n\n###排序\n#普通字段排序\nGET /hotel/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "sort": [//先按评分降序，在按价格升序\n    {\n      "score": "desc"\n    },\n    {\n      "price": "asc"\n    }\n  ]\n}\n#地理坐标排序\nGET /hotel/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "sort": [\n    {\n      "price": "asc"\n    },\n    {\n      "_geo_distance": {\n        "location": {\n          "lat": 40, //纬度\n          "lon": -70 //经度\n        },\n        "order": "desc",\n        "unit": "km"\n      }\n    }\n  ]\n}\n\n###分页\n#基本分页\nGET /hotel/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "sort": [\n    {\n      "price": "asc"\n    }\n  ],\n  "from": 10, //从10开始\n  "size": 20 //一页数目 20\n}\n\n#深度分页\n#after search\n\n###高亮\nGET /hotel/_search\n{\n  "query": {\n    "match": {\n      "all": "如家"\n    }\n  },\n  "highlight": {\n    "fields": {// 指定要高亮的字段\n      "name": {//默认就是加 <em></em>\n        "require_field_match": "false"//默认是要字段匹配，这是设置成不需要匹配\n      }\n    }\n  }\n}\n\n\n\n\n#添加广告标记\nPOST /hotel/_update/2056126831\n{\n    "doc": {\n        "isAD": true\n    }\n}\nPOST /hotel/_update/1989806195\n{\n    "doc": {\n        "isAD": true\n    }\n}\nPOST /hotel/_update/2056105938\n{\n    "doc": {\n        "isAD": true\n    }\n}\n\nGET /hotel/_doc/2056105938\n\n\n\n###############聚合\n\n#对品牌排序\nGET /hotel/_search\n{\n  "size": 0,\n  "aggs": {\n    "brandAggs": {\n      "terms": {\n        "field": "brand"\n        "size": 10\n      }\n    }\n  }\n}\n\n#对品牌排序 自定义排序，按升序排列\nGET /hotel/_search\n{\n  "size": 0,\n  "aggs": {\n    "brandAggs": {\n      "terms": {\n        "field": "brand",\n        "size": 10,\n        "order": {\n          "_count": "asc" \n        }\n      }\n    }\n  }\n}\n\n\n#对品牌排序 限定聚合范围-限定范围小于200\nGET /hotel/_search\n{\n  "query": {\n    "range": {\n      "price": {\n        "lte": 200\n      }\n    }\n  }, \n  "size": 0,\n  "aggs": {\n    "brandAggs": {\n      "terms": {\n        "field": "brand",\n        "size": 10,\n        "order": {\n          "_count": "asc" \n        }\n      }\n    }\n  }\n}\n\n\n#Metric聚合\nGET /hotel/_search\n{\n  "size": 0,\n  "aggs": {\n    "brandAgg": {\n      "terms": {\n        "field": "brand",\n        "size": 20,\n        "order": {\n          "scoreAgg.avg": "asc"\n        }\n      },\n      "aggs": {\n        "scoreAgg": {\n          "stats": {\n            "field": "score"\n          }\n        }\n      }\n      \n    }\n  }\n}\n\n\n\n\n# 多字段聚合\nGET /hotel/_search\n{\n  "size": 0,\n  "aggs": {\n    "brandAggs": {\n      "terms": {\n        "field": "brand",\n        "size": 3\n      }\n    },\n    "cityAgg": {\n      "terms": {\n        "field": "city",\n        "size": 3\n      }\n    },\n    "starNameAgg": {\n      "terms": {\n        "field": "starName",\n        "size": 10\n      }\n    }\n  }\n}\n\n#### 拼音分词器\nPOST /_analyze\n{\n  "text": ["如家酒店还不错"],\n  "analyzer": "pinyin"\n}\n\n\n\n## 自动补全测\n## 自动补全的索引库\nPUT test\n{\n  "mappings": {\n    "properties": {\n      "title":{\n        "type": "completion"\n      }\n    }\n  }\n}\n## 示例数据\nPOST test/_doc\n{\n  "title": ["Sony", "WH-1000XM3"]\n}\nPOST test/_doc\n{\n  "title": ["SK-II", "PITERA"]\n}\nPOST test/_doc\n{\n  "title": ["Nintendo", "switch"]\n}\n\n##自动补全查询\nPOST /test/_search\n{\n  "suggest": {\n    "title_suggest": {\n      "text": "s", // 关键字\n      "completion": {\n        "field": "title", // 补全字段\n        "skip_duplicates": true, // 跳过重复的\n        "size": 10 // 获取前10条结果\n      }\n    }\n  }\n}\n\n#创建带自动补全功能的 酒店数据索引库\nPUT /hotel\n{\n  "settings": {\n    "analysis": {\n      "analyzer": {\n        "text_anlyzer": {//自定义分词器名称\n          "tokenizer": "ik_max_word",//中间用ik分割词语\n          "filter": "py"//最后用拼音分词器处理\n        },\n        "completion_analyzer": {\n          "tokenizer": "keyword",//中间 不用词语分割\n          "filter": "py" //最后用拼音分词器处理\n        }\n      },\n      "filter": {\n        "py": {\n          "type": "pinyin",\n          "keep_full_pinyin": false,// 刘德华> [liu,de,hua],默认为true\n          "keep_joined_full_pinyin": true,//刘德华> [liudehua],默认为false\n          "keep_original": true,//保留原始的汉字输入\n          "limit_first_letter_length": 16,\n          "remove_duplicated_term": true,//移除重复，[de的]>de\n          "none_chinese_pinyin_tokenize": false//保持拼音连续，不拆开\n        }\n      }\n    }\n  },\n  "mappings": {\n    "properties": {\n      "id":{\n        "type": "keyword"\n      },\n      "name":{\n        "type": "text",\n        "analyzer": "text_anlyzer",//创建索引时 使用自定义混合分词器\n        "search_analyzer": "ik_smart",//搜索时 使用ik分词器\n        "copy_to": "all"\n      },\n      "address":{\n        "type": "keyword",\n        "index": false\n      },\n      "price":{\n        "type": "integer"\n      },\n      "score":{\n        "type": "integer"\n      },\n      "brand":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "city":{\n        "type": "keyword"\n      },\n      "starName":{\n        "type": "keyword"\n      },\n      "business":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "location":{\n        "type": "geo_point"\n      },\n      "pic":{\n        "type": "keyword",\n        "index": false\n      },\n      "all":{\n        "type": "text",\n        "analyzer": "text_anlyzer",\n        "search_analyzer": "ik_smart"\n      },\n      "suggestion":{//关键词，自动补全字段\n          "type": "completion",//必须是completion类型\n          "analyzer": "completion_analyzer"//不分词，直接转拼音\n      }\n    }\n  }\n}\n\n\n\nGET /hotel/_search\n\n\n\n#自动补全测试\nPOST /hotel/_search\n{\n  "suggest": {\n    "suggestions": {\n      "text": "ff", // 前缀词\n      "completion": {\n        "field": "suggestion", // 补全字段\n        "skip_duplicates": true, // 跳过重复的\n        "size": 10 // 获取前10条结果\n      }\n    }\n  }\n}\n\n\n\n\n\n\n\n\nDELETE /hotel/_doc/1586232717316648962\n\nGET /hotel/_search\n{\n  "query": {\n    "match": {\n      "all": "2"\n    }\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n',normalizedContent:'# kibana\n\n * dev tools的 操作 记录\n\n\nget _search\n{\n  "query": {\n    "match_all": {}\n  }\n}\n\n#测试es是否链接\nget /\n\n\n#测试标准分词器\npost /_analyze\n{\n  "text":"黑马程序员学习java太棒了！",\n  "analyzer": "standard"\n}\n\n#测试ik分词器\npost /_analyze\n{\n  "text":"啊黑马程序员学习java太棒了！",\n  //"analyzer": "ik_max_word"  //最细切分\n  "analyzer": "ik_smart" //最少切分\n}\n\n#测试ik分词器+自定义字典\npost /_analyze\n{\n  "text":"哦嗯雄的凉冰太厉害了,比英雄联盟的皎月女神强啊！",\n  "analyzer": "ik_smart" \n}\n\n\n#创建索引库\nput /diana\n{\n  "mappings": {\n    "properties": {\n      "info": {\n        "type": "text",\n        "analyzer": "ik_smart"\n      },\n      "email": {\n        "type": "keyword",\n        "index": false\n      },\n      "name": {\n        "type": "object", \n        "properties": {\n          "firstname": {\n            "type": "keyword"\n          },\n          "lastname": {\n            "type": "keyword"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n#查询索引库\nget /diana\n\n#修改索引库,只允许添加新字段，不允许修改已经存在的字段\nput /diana/_mapping\n{\n  "properties": {\n    "age": {\n      "type":"integer"\n    }\n  }\n}\n\n#删除索引库\ndelete /diana\n\n\n\n\n\n\n#新增文档\npost /diana/_doc/1\n{\n  "info":"皎月女神",\n  "email":"123@123.com",\n  "name": {\n    "firstname": "冰",\n    "lastname": "凉"\n  }\n}\n\n#查询文档\nget /diana/_doc/1\n\n#删除文档\ndelete /diana/_doc/1\n\n#修改文档--全量修改\nput /diana/_doc/2\n{\n  "info":"皎月女神",\n  "email":"123@1234.com",\n  "name": {\n    "firstname": "冰",\n    "lastname": "凉"\n  }\n}\n\n#修改文档--增量修改\npost /diana/_update/1\n{\n  "doc": {\n    "info": "diana"\n  }\n}\n\n\n\n\n\n\n#创建酒店索引\nput /hotel\n{\n  "mappings": {\n    "properties": {\n      "id":{//keyword 类型，不是long\n        "type":"keyword"\n      },\n      "name": {\n        "type":"text",\n        "analyzer": "ik_max_word",\n        "copy_to": "all"\n      },\n      "address":{\n        "type":"text",\n        "analyzer": "ik_smart"\n      },\n      "price":{\n        "type":"integer",\n        "copy_to": "all"\n      },\n      "score":{\n        "type":"integer",\n        "copy_to": "all"\n      },\n      "brand":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "city":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "starname":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "business":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "location":{//地理位置特殊字段,包括经度和纬度\n        "type":"geo_point"\n      },\n      "pic":{//不分词，不搜索\n        "type":"keyword",\n        "index":false\n      },\n      "all": {//将许多字段的值聚在一块，方便搜索\n        "type":"text",\n        "analyzer": "ik_max_word"\n      }\n    }\n  }\n}\n\n#查看酒店索引\nget /hotel\n\n#删除酒店索引\ndelete /hotel\n\n#得到酒店文档信息\nget /hotel/_doc/36934\n\n#批量查询\nget /hotel/_search\n\n\n\n\n\n\n\n#对文档内信息的检索###############\n\n###查询所有\nget /hotel/_search\n{\n  "query": {\n    "match_all": {}\n  }\n}\n\n\n###全文检索\n#match  单个字段查询  --性能高，推荐\nget /hotel/_search\n{\n  "query": {\n    "match": {\n      "all": "外滩如家"\n    }\n  }\n}\n\n#multi_match  查询多个字段\nget /hotel/_search\n{\n  "query": {\n    "multi_match": {\n      "query": "外滩如家",\n      "fields":["brand","name", "business"]\n    }\n  }\n}\n\n###精确查询\n#term  根据词条精确值查询\nget /hotel/_search\n{\n  "query": {\n    "term": {\n      "city": {\n        "value": "上海"\n      }\n    }\n  }\n}\n\n#range 根据值的范围查询\nget /hotel/_search\n{\n  "query": {\n    "range": {\n      "price": {\n        "gte": 100, //带e是等于  大于等于-gte；大于gt\n        "lte": 300\n      }\n    }\n  }\n}\n\n\n###地理查询\n#geo_bounding_box 矩形范围查询，左上，右下坐标\nget /hotel/_search\n{\n  "query": {\n    "geo_bounding_box": {\n      "location": {\n        "top_left": {//左上\n          "lat": 31.1,\n          "lon": 121.5\n        },\n        "bottom_right": {//右下\n          "lat": 30.9,\n          "lon": 121.7\n        }\n      }\n     \n    }\n  }\n}\n\n\n#geo_distance 距离查询  以坐标为中心画一个圆  --常用\nget /hotel/_search\n{\n  "query": {\n    "geo_distance": {\n      "distance": "5km",\n      "location": "31.21, 121.5"\n    }\n  }\n}\n\n\n\n###复合查询\n#默认按相关度打分，现在要修改打分数 function_score\nget /hotel/_search\n{\n  "query": {\n    "function_score": {\n      "query": {//正常查询\n        "match": {\n          "all": "外滩"\n        }\n      },\n      "functions": [\n        {\n          "filter": {//筛选条件\n            "term": {\n              "brand": "如家"\n            }\n          },\n          "weight": 10 //算分权值  默认是乘算\n        }\n      ],\n      "boost_mode": "sum"//乘法运算\n    }\n  }\n}\n\n\n#布尔查询  一个或多个查询子句的组合\nget /hotel/_search\n{\n  "query": {\n    "bool": {\n      "must": [//必须是如家品牌\n        {\n          "term": {\n            "brand": {\n              "value": "如家"\n            }\n          }\n        }\n      ],\n      "must_not": [//必须不大于400 -- 小于等于400\n        {\n          "range": {\n            "price": {//大于400\n              "gt": 400\n            }\n          }\n        }\n      ],\n      "filter": [//筛选范围\n        {\n          "geo_distance": {\n            "distance": "10km",\n            "location": "31.21, 121.5"\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n#搜索结果处理###############\n\n###排序\n#普通字段排序\nget /hotel/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "sort": [//先按评分降序，在按价格升序\n    {\n      "score": "desc"\n    },\n    {\n      "price": "asc"\n    }\n  ]\n}\n#地理坐标排序\nget /hotel/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "sort": [\n    {\n      "price": "asc"\n    },\n    {\n      "_geo_distance": {\n        "location": {\n          "lat": 40, //纬度\n          "lon": -70 //经度\n        },\n        "order": "desc",\n        "unit": "km"\n      }\n    }\n  ]\n}\n\n###分页\n#基本分页\nget /hotel/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "sort": [\n    {\n      "price": "asc"\n    }\n  ],\n  "from": 10, //从10开始\n  "size": 20 //一页数目 20\n}\n\n#深度分页\n#after search\n\n###高亮\nget /hotel/_search\n{\n  "query": {\n    "match": {\n      "all": "如家"\n    }\n  },\n  "highlight": {\n    "fields": {// 指定要高亮的字段\n      "name": {//默认就是加 <em></em>\n        "require_field_match": "false"//默认是要字段匹配，这是设置成不需要匹配\n      }\n    }\n  }\n}\n\n\n\n\n#添加广告标记\npost /hotel/_update/2056126831\n{\n    "doc": {\n        "isad": true\n    }\n}\npost /hotel/_update/1989806195\n{\n    "doc": {\n        "isad": true\n    }\n}\npost /hotel/_update/2056105938\n{\n    "doc": {\n        "isad": true\n    }\n}\n\nget /hotel/_doc/2056105938\n\n\n\n###############聚合\n\n#对品牌排序\nget /hotel/_search\n{\n  "size": 0,\n  "aggs": {\n    "brandaggs": {\n      "terms": {\n        "field": "brand"\n        "size": 10\n      }\n    }\n  }\n}\n\n#对品牌排序 自定义排序，按升序排列\nget /hotel/_search\n{\n  "size": 0,\n  "aggs": {\n    "brandaggs": {\n      "terms": {\n        "field": "brand",\n        "size": 10,\n        "order": {\n          "_count": "asc" \n        }\n      }\n    }\n  }\n}\n\n\n#对品牌排序 限定聚合范围-限定范围小于200\nget /hotel/_search\n{\n  "query": {\n    "range": {\n      "price": {\n        "lte": 200\n      }\n    }\n  }, \n  "size": 0,\n  "aggs": {\n    "brandaggs": {\n      "terms": {\n        "field": "brand",\n        "size": 10,\n        "order": {\n          "_count": "asc" \n        }\n      }\n    }\n  }\n}\n\n\n#metric聚合\nget /hotel/_search\n{\n  "size": 0,\n  "aggs": {\n    "brandagg": {\n      "terms": {\n        "field": "brand",\n        "size": 20,\n        "order": {\n          "scoreagg.avg": "asc"\n        }\n      },\n      "aggs": {\n        "scoreagg": {\n          "stats": {\n            "field": "score"\n          }\n        }\n      }\n      \n    }\n  }\n}\n\n\n\n\n# 多字段聚合\nget /hotel/_search\n{\n  "size": 0,\n  "aggs": {\n    "brandaggs": {\n      "terms": {\n        "field": "brand",\n        "size": 3\n      }\n    },\n    "cityagg": {\n      "terms": {\n        "field": "city",\n        "size": 3\n      }\n    },\n    "starnameagg": {\n      "terms": {\n        "field": "starname",\n        "size": 10\n      }\n    }\n  }\n}\n\n#### 拼音分词器\npost /_analyze\n{\n  "text": ["如家酒店还不错"],\n  "analyzer": "pinyin"\n}\n\n\n\n## 自动补全测\n## 自动补全的索引库\nput test\n{\n  "mappings": {\n    "properties": {\n      "title":{\n        "type": "completion"\n      }\n    }\n  }\n}\n## 示例数据\npost test/_doc\n{\n  "title": ["sony", "wh-1000xm3"]\n}\npost test/_doc\n{\n  "title": ["sk-ii", "pitera"]\n}\npost test/_doc\n{\n  "title": ["nintendo", "switch"]\n}\n\n##自动补全查询\npost /test/_search\n{\n  "suggest": {\n    "title_suggest": {\n      "text": "s", // 关键字\n      "completion": {\n        "field": "title", // 补全字段\n        "skip_duplicates": true, // 跳过重复的\n        "size": 10 // 获取前10条结果\n      }\n    }\n  }\n}\n\n#创建带自动补全功能的 酒店数据索引库\nput /hotel\n{\n  "settings": {\n    "analysis": {\n      "analyzer": {\n        "text_anlyzer": {//自定义分词器名称\n          "tokenizer": "ik_max_word",//中间用ik分割词语\n          "filter": "py"//最后用拼音分词器处理\n        },\n        "completion_analyzer": {\n          "tokenizer": "keyword",//中间 不用词语分割\n          "filter": "py" //最后用拼音分词器处理\n        }\n      },\n      "filter": {\n        "py": {\n          "type": "pinyin",\n          "keep_full_pinyin": false,// 刘德华> [liu,de,hua],默认为true\n          "keep_joined_full_pinyin": true,//刘德华> [liudehua],默认为false\n          "keep_original": true,//保留原始的汉字输入\n          "limit_first_letter_length": 16,\n          "remove_duplicated_term": true,//移除重复，[de的]>de\n          "none_chinese_pinyin_tokenize": false//保持拼音连续，不拆开\n        }\n      }\n    }\n  },\n  "mappings": {\n    "properties": {\n      "id":{\n        "type": "keyword"\n      },\n      "name":{\n        "type": "text",\n        "analyzer": "text_anlyzer",//创建索引时 使用自定义混合分词器\n        "search_analyzer": "ik_smart",//搜索时 使用ik分词器\n        "copy_to": "all"\n      },\n      "address":{\n        "type": "keyword",\n        "index": false\n      },\n      "price":{\n        "type": "integer"\n      },\n      "score":{\n        "type": "integer"\n      },\n      "brand":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "city":{\n        "type": "keyword"\n      },\n      "starname":{\n        "type": "keyword"\n      },\n      "business":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "location":{\n        "type": "geo_point"\n      },\n      "pic":{\n        "type": "keyword",\n        "index": false\n      },\n      "all":{\n        "type": "text",\n        "analyzer": "text_anlyzer",\n        "search_analyzer": "ik_smart"\n      },\n      "suggestion":{//关键词，自动补全字段\n          "type": "completion",//必须是completion类型\n          "analyzer": "completion_analyzer"//不分词，直接转拼音\n      }\n    }\n  }\n}\n\n\n\nget /hotel/_search\n\n\n\n#自动补全测试\npost /hotel/_search\n{\n  "suggest": {\n    "suggestions": {\n      "text": "ff", // 前缀词\n      "completion": {\n        "field": "suggestion", // 补全字段\n        "skip_duplicates": true, // 跳过重复的\n        "size": 10 // 获取前10条结果\n      }\n    }\n  }\n}\n\n\n\n\n\n\n\n\ndelete /hotel/_doc/1586232717316648962\n\nget /hotel/_search\n{\n  "query": {\n    "match": {\n      "all": "2"\n    }\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n',charsets:{cjk:!0}},{title:"Docker实用篇",frontmatter:{autoSort:99,title:"Docker实用篇",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/247ef5/",categories:["后端","微服务","Docker"],tags:["知识","微服务","Docker"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/10.Docker/10.Docker%E5%AE%9E%E7%94%A8%E7%AF%87.html",relativePath:"01.后端/60.微服务/10.Docker/10.Docker实用篇.md",key:"v-2ba678ac",path:"/pages/247ef5/",headers:[{level:2,title:"初识Docker",slug:"初识docker",normalizedTitle:"初识docker",charIndex:2},{level:2,title:"Docker的基本操作",slug:"docker的基本操作",normalizedTitle:"docker的基本操作",charIndex:3022},{level:2,title:"Dockerfile自定义镜像",slug:"dockerfile自定义镜像",normalizedTitle:"dockerfile自定义镜像",charIndex:9415},{level:2,title:"Docker-Compose",slug:"docker-compose",normalizedTitle:"docker-compose",charIndex:11826},{level:2,title:"Docker镜像仓库",slug:"docker镜像仓库",normalizedTitle:"docker镜像仓库",charIndex:15214}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"初识Docker Docker的基本操作 Dockerfile自定义镜像 Docker-Compose Docker镜像仓库",content:'# 初识Docker\n\n# 什么是Docker\n\n微服务虽然具备各种各样的优势，但服务的拆分通用给部署带来了很大的麻烦。\n\n * 分布式系统中，依赖的组件非常多，不同组件之间部署时往往会产生一些冲突。\n * 在数百上千台服务中重复部署，环境不一定一致，会遇到各种问题\n\n# 1.1.1.应用部署的环境问题\n\n大型项目组件较多，运行环境也较为复杂，部署时会碰到一些问题：\n\n * 依赖关系复杂，容易出现兼容性问题\n\n * 开发、测试、生产环境有差异\n\n\n\n例如一个项目中，部署时需要依赖于node.js、Redis、RabbitMQ、MySQL等，这些服务部署时所需要的函数库、依赖项各不相同，甚至会有冲突。给部署带来了极大的困难。\n\n# 1.1.2.Docker解决依赖兼容问题\n\n而Docker确巧妙的解决了这些问题，Docker是如何实现的呢？\n\nDocker为了解决依赖的兼容问题的，采用了两个手段：\n\n * 将应用的Libs（函数库）、Deps（依赖）、配置与应用一起打包\n\n * 将每个应用放到一个隔离容器去运行，避免互相干扰\n\n\n\n这样打包好的应用包中，既包含应用本身，也保护应用所需要的Libs、Deps，无需再操作系统上安装这些，自然就不存在不同应用之间的兼容问题了。\n\n虽然解决了不同应用的兼容问题，但是开发、测试等环境会存在差异，操作系统版本也会有差异，怎么解决这些问题呢？\n\n# 1.1.3.Docker解决操作系统环境差异\n\n要解决不同操作系统环境差异问题，必须先了解操作系统结构。以一个Ubuntu操作系统为例，结构如下：\n\n\n\n结构包括：\n\n * 计算机硬件：例如CPU、内存、磁盘等\n * 系统内核：所有Linux发行版的内核都是Linux，例如CentOS、Ubuntu、Fedora等。内核可以与计算机硬件交互，对外提供内核指令，用于操作计算机硬件。\n * 系统应用：操作系统本身提供的应用、函数库。这些函数库是对内核指令的封装，使用更加方便。\n\n应用于计算机交互的流程如下：\n\n1）应用调用操作系统应用（函数库），实现各种功能\n\n2）系统函数库是对内核指令集的封装，会调用内核指令\n\n3）内核指令操作计算机硬件\n\nUbuntu和CentOSpringBoot都是基于Linux内核，无非是系统应用不同，提供的函数库有差异：\n\n\n\n此时，如果将一个Ubuntu版本的MySQL应用安装到CentOS系统，MySQL在调用Ubuntu函数库时，会发现找不到或者不匹配，就会报错了：\n\n\n\nDocker如何解决不同系统环境的问题？\n\n * Docker将用户程序与所需要调用的系统(比如Ubuntu)函数库一起打包\n * Docker运行到不同操作系统时，直接基于打包的函数库，借助于操作系统的Linux内核来运行\n\n如图：\n\n\n\n# 1.1.4.小结\n\nDocker如何解决大型项目依赖关系复杂，不同组件依赖的兼容性问题？\n\n * Docker允许开发中将应用、依赖、函数库、配置一起打包，形成可移植镜像\n * Docker应用运行在容器中，使用沙箱机制，相互隔离\n\nDocker如何解决开发、测试、生产环境有差异的问题？\n\n * Docker镜像中包含完整运行环境，包括系统函数库，仅依赖系统的Linux内核，因此可以在任意Linux操作系统上运行\n\nDocker是一个快速交付应用、运行应用的技术，具备下列优势：\n\n * 可以将程序及其依赖、运行环境一起打包为一个镜像，可以迁移到任意Linux操作系统\n * 运行时利用沙箱机制形成隔离容器，各个应用互不干扰\n * 启动、移除都可以通过一行命令完成，方便快捷\n\n# Docker和虚拟机的区别\n\nDocker可以让一个应用在任何操作系统中非常方便的运行。而以前我们接触的虚拟机，也能在一个操作系统中，运行另外一个操作系统，保护系统中的任何应用。\n\n两者有什么差异呢？\n\n虚拟机（virtual machine）是在操作系统中模拟硬件设备，然后运行另一个操作系统，比如在 Windows 系统里面运行 Ubuntu 系统，这样就可以运行任意的Ubuntu应用了。\n\nDocker仅仅是封装函数库，并没有模拟完整的操作系统，如图：\n\n\n\n对比来看：\n\n\n\n小结：\n\nDocker和虚拟机的差异：\n\n * docker是一个系统进程；虚拟机是在操作系统中的操作系统\n\n * docker体积小、启动速度快、性能好；虚拟机体积大、启动速度慢、性能一般\n\n# Docker架构\n\n# 镜像和容器\n\nDocker中有几个重要的概念：\n\n镜像（Image）：Docker将应用程序及其所需的依赖、函数库、环境、配置等文件打包在一起，称为镜像。\n\n容器（Container）：镜像中的应用程序运行后形成的进程就是容器，只是Docker会给容器进程做隔离，对外不可见。\n\n一切应用最终都是代码组成，都是硬盘中的一个个的字节形成的文件。只有运行时，才会加载到内存，形成进程。\n\n而镜像，就是把一个应用在硬盘上的文件、及其运行环境、部分系统函数库文件一起打包形成的文件包。这个文件包是只读的。\n\n容器呢，就是将这些文件中编写的程序、函数加载到内存中允许，形成进程，只不过要隔离起来。因此一个镜像可以启动多次，形成多个容器进程。\n\n\n\n例如你下载了一个QQ，如果我们将QQ在磁盘上的运行文件及其运行的操作系统依赖打包，形成QQ镜像。然后你可以启动多次，双开、甚至三开QQ，跟多个妹子聊天。\n\n# DockerHub\n\n开源应用程序非常多，打包这些应用往往是重复的劳动。为了避免这些重复劳动，人们就会将自己打包的应用镜像，例如Redis、MySQL镜像放到网络上，共享使用，就像GitHub的代码共享一样。\n\n * DockerHub：DockerHub是一个官方的Docker镜像的托管平台。这样的平台称为Docker Registry。\n\n * 国内也有类似于DockerHub 的公开服务，比如 网易云镜像服务、阿里云镜像库等。\n\n我们一方面可以将自己的镜像共享到DockerHub，另一方面也可以从DockerHub拉取镜像：\n\n\n\n# Docker架构\n\n我们要使用Docker来操作镜像、容器，就必须要安装Docker。\n\nDocker是一个CS架构的程序，由两部分组成：\n\n * 服务端(server)：Docker守护进程，负责处理Docker指令，管理镜像、容器等\n\n * 客户端(client)：通过命令或RestAPI向Docker服务端发送指令。可以在本地或远程向服务端发送指令。\n\n如图：\n\n\n\n# 小结\n\n镜像：\n\n * 将应用程序及其依赖、环境、配置打包在一起\n\n容器：\n\n * 镜像运行起来就是容器，一个镜像可以运行多个容器\n\nDocker结构：\n\n * 服务端：接收命令或远程请求，操作镜像或容器\n\n * 客户端：发送命令或者请求到Docker服务端\n\nDockerHub：\n\n * 一个镜像托管的服务器，类似的还有阿里云镜像服务，统称为DockerRegistry\n\n# 安装Docker\n\n企业部署一般都是采用Linux操作系统，而其中又数CentOS发行版占比最多，因此我们在CentOS下安装Docker。参考课前资料中的文档：\n\n\n\n\n# Docker的基本操作\n\n# 镜像操作\n\n# 镜像名称\n\n首先来看下镜像的名称组成：\n\n * 镜名称一般分两部分组成：[repository]:[tag]。\n * 在没有指定tag时，默认是latest，代表最新版本的镜像\n\n如图：\n\n\n\n这里的mysql就是repository，5.7就是tag，合一起就是镜像名称，代表5.7版本的MySQL镜像。\n\n# 2.1.2.镜像命令\n\n常见的镜像操作命令如图：\n\n * 命令查看\n\n使用docker COMMAND --help来查看各种命令\n\n例如：docker pull --help\n\n\n\n# 2.1.3.案例1-拉取、查看镜像\n\n需求：从DockerHub中拉取一个nginx镜像并查看\n\n1）首先去镜像仓库搜索nginx镜像，比如DockerHub:\n\n\n\n2）根据查看到的镜像名称，拉取自己需要的镜像，通过命令：docker pull nginx\n\n\n\n3）通过命令：docker images 查看拉取到的镜像\n\n\n\n# 2.1.4.案例2-保存、导入镜像\n\n需求：利用docker save将nginx镜像导出磁盘，然后再通过load加载回来\n\n1）利用docker xx --help命令查看docker save和docker load的语法\n\n例如，查看save命令用法，可以输入命令：\n\ndocker save --help\n\n\n结果：\n\n\n\n命令格式：\n\ndocker save -o [保存的目标文件名称] [镜像名称]\n\n\n2）使用docker save导出镜像到磁盘\n\n运行命令：\n\ndocker save -o nginx.tar nginx:latest\n\n\n结果如图：\n\n\n\n3）使用docker load加载镜像\n\n先删除本地的nginx镜像：\n\ndocker rmi nginx:latest\n\n\n然后运行命令，加载本地文件：\n\ndocker load -i nginx.tar\n\n\n结果：\n\n\n\n# 2.1.5.练习\n\n需求：去DockerHub搜索并拉取一个Redis镜像\n\n目标：\n\n1）去DockerHub搜索Redis镜像\n\n2）查看Redis镜像的名称和版本\n\n3）利用docker pull命令拉取镜像\n\n4）利用docker save命令将 redis:latest打包为一个redis.tar包\n\n5）利用docker rmi 删除本地的redis:latest\n\n6）利用docker load 重新加载 redis.tar文件\n\n# 2.1.6.基本操作-镜像\n\n学会使用 docker commands --help,自己查阅帮助文档\n\n * docker pull——从仓库拉取镜像\n * docker push——将本地镜像推送至仓库\n * docker images——查看镜像\n * docker save——将本地镜像打包成文件\n * docker load——将打包成文件的镜像加载成镜像\n * docker rmi——删除本地镜像\n\n# 容器操作\n\n# 容器相关命令\n\n容器操作的命令如图：\n\n\n\n容器保护三个状态：\n\n * 运行：进程正常运行\n * 暂停：进程暂停，CPU不再运行，并不释放内存\n * 停止：进程终止，回收进程占用的内存、CPU等资源\n\n其中：容器相关命令\n\n * docker run：创建并运行一个容器，处于运行状态\n\n * docker pause：让一个运行的容器暂停\n\n * docker unpause：让一个容器从暂停状态恢复运行\n\n * docker stop：停止一个运行的容器\n\n * docker start：让一个停止的容器再次运行\n\n * docker rm：删除一个容器\n\n * docker ps :查看所有的容器及运行状态\n\n * docker logs: 查看容器运行日志\n\n * docker exec：进入容器内部\n\n# 案例-创建并运行一个容器\n\n创建并运行nginx容器的命令：\n\ndocker run --name containerName -p 80:80 -d nginx\n\n\n命令解读：\n\n * docker run ：创建并运行一个容器\n * --name : 给容器起一个名字，比如叫做mn\n * -p ：将宿主机端口与容器端口映射，冒号左侧是宿主机端口，右侧是容器端口\n   * 一般来说，右侧容器端口是不变的，跟随镜像来的；改变的是宿主机端口\n * -d：后台运行容器\n * nginx：镜像名称，例如nginx，不写标签，表示最新版\n\n这里的-p参数，是将容器端口映射到宿主机端口。\n\n默认情况下，容器是隔离环境，我们直接访问宿主机的80端口，肯定访问不到容器中的nginx。\n\n现在，将容器的80与宿主机的80关联起来，当我们访问宿主机的80端口时，就会被映射到容器的80，这样就能访问到nginx了：\n\n\n\n * 查看容器日志\n   * docker logs\n   * docker logs -f——类似于-tails 持续输出\n * 查看容器状态\n   * docker ps\n\n# 案例-进入容器，修改文件\n\n不推荐在容器内部修改文件\n\n>  1. 不方便修改，没有vim\n>  2. 文件修改没有记录，时间长了，会忘记，丢失信息\n\n需求：进入Nginx容器，修改HTML文件内容，添加“传智教育欢迎您”\n\n提示：进入容器要用到docker exec命令。\n\n步骤：\n\n1）进入容器。进入我们刚刚创建的nginx容器的命令为：\n\ndocker exec -it mn bash\n\n\n命令解读：\n\n * docker exec ：进入容器内部，执行一个命令\n\n * -it : 给当前进入的容器创建一个标准输入、输出终端，允许我们与容器交互\n\n * mn ：要进入的容器的名称\n\n * bash：进入容器后执行的命令，bash是一个linux终端交互命令\n\n2）进入nginx的HTML所在目录 /usr/share/nginx/html\n\n容器内部会模拟一个独立的Linux文件系统，看起来如同一个linux服务器一样：\n\n\n\nnginx的环境、配置、运行文件全部都在这个文件系统中，包括我们要修改的html文件。\n\n查看DockerHub网站中的nginx页面，可以知道nginx的html目录位置在/usr/share/nginx/html\n\n我们执行命令，进入该目录：\n\ncd /usr/share/nginx/html\n\n\n查看目录下文件：\n\n\n\n3）修改index.html的内容\n\n容器内没有vi命令，无法直接修改，我们用下面的命令来修改：\n\nsed -i -e \'s##Welcome to nginx##传智教育欢迎您##g\' -e \'s##<head>##<head><meta charset="utf-8">##g\' index.html\n\n\n在浏览器访问自己的虚拟机地址，例如我的是：http://192.168.150.101，即可看到结果：\n\n\n\n# 小结\n\ndocker run命令的常见参数有哪些？\n\n * --name：指定容器名称\n * -p：指定端口映射\n * -d：让容器后台运行\n\n查看容器日志的命令：\n\n * docker logs\n * 添加 -f 参数可以持续查看日志\n\n查看容器状态：\n\n * docker ps\n * docker ps -a 查看所有容器，包括已经停止的\n\n删除容器\n\n * docker rm （不能删除正在运行的容器）\n * docker rm -f 强制删除\n\n# 案例——redis容器操作\n\n 1. 创建容器\n    \n    docker run --name mredis -p 6379:6379 -d redis——简单创建一个容器\n    \n    docker run --name mredis -p 6379:6379 -d redis redis-server --save 60 1 --loglevel warning——带持久化存储创建一个容器，每60s更新一次，至少有一次写操作，设置日志等级为warning级\n\n 2. 进入容器内部\n    \n    docker exec -it mredis redis-cli\n    \n    redis-cli ——表示可以直接在容器内部运行redis命令\n\n 3. 创建数据\n    \n    set diane angle\n    \n    get diane\n    \n    keys *\n\n# 数据卷（容器数据管理）\n\n在之前的nginx案例中，修改nginx的html页面时，需要进入nginx内部。并且因为没有编辑器，修改文件也很麻烦。\n\n这就是因为容器与数据（容器内文件）耦合带来的后果。\n\n\n\n要解决这个问题，必须将数据与容器解耦，这就要用到数据卷了。\n\n# 什么是数据卷\n\n**数据卷（volume）**是一个虚拟目录，指向宿主机文件系统中的某个目录。\n\n\n\n一旦完成数据卷挂载，对容器的一切操作都会作用在数据卷对应的宿主机目录了。\n\n这样，我们操作宿主机的/var/lib/docker/volumes/html目录，就等于操作容器内的/usr/share/nginx/html目录了\n\n# 数据集操作命令\n\n数据卷操作的基本语法如下：\n\ndocker volume [COMMAND]\n\n\ndocker volume命令是数据卷操作，根据命令后跟随的command来确定下一步的操作：\n\n * create 创建一个volume\n\n * inspect 显示一个或多个volume的信息\n\n * ls 列出所有的volume\n\n * prune 删除未使用的volume\n\n * rm 删除一个或多个指定的volume\n\n# 创建和查看数据卷\n\n需求：创建一个数据卷，并查看数据卷在宿主机的目录位置\n\n① 创建数据卷\n\ndocker volume create html\n\n\n② 查看所有数据\n\ndocker volume ls\n\n\n结果：\n\n\n\n③ 查看数据卷详细信息卷\n\ndocker volume inspect html\n\n\n结果：\n\n\n\n可以看到，我们创建的html这个数据卷关联的宿主机目录为/var/lib/docker/volumes/html/_data目录。\n\n小结：\n\n数据卷的作用：\n\n * 将容器与数据分离，解耦合，方便操作容器内数据，保证数据安全\n\n数据卷操作：\n\n * docker volume create：创建数据卷\n * docker volume ls：查看所有数据卷\n * docker volume inspect：查看数据卷详细信息，包括关联的宿主机目录位置\n * docker volume rm：删除指定数据卷\n * docker volume prune：删除所有未使用的数据卷\n\n# 挂载数据卷\n\n我们在创建容器时，可以通过 -v 参数来挂载一个数据卷到某个容器内目录，命令格式如下：\n\ndocker run \\\n  --name mn \\\n  -v html:/root/html \\\n  -p 8080:80\n  -d\n  nginx \\\n\n\n这里的-v就是挂载数据卷的命令：\n\n * -v html:/root/html ：把html数据卷挂载到容器内的/root/html这个目录中\n\n# 案例-给nginx挂载数据卷\n\n需求：创建一个nginx容器，修改容器内的html目录内的index.html内容\n\n分析：上个案例中，我们进入nginx容器内部，已经知道nginx的html目录所在位置/usr/share/nginx/html ，我们需要把这个目录挂载到html这个数据卷上，方便操作其中的内容。\n\n提示：运行容器时使用 -v 参数挂载数据卷\n\n步骤：\n\n① 创建容器并挂载数据卷到容器内的HTML目录\n\ndocker run --name mn -v html:/usr/share/nginx/html -p 80:80 -d nginx\n\n\n② 进入html数据卷所在位置，并修改HTML内容\n\n## 查看html数据卷的位置\ndocker volume inspect html\n## 进入该目录\ncd /var/lib/docker/volumes/html/_data\n## 修改文件\nvi index.html\n\n\n重要\n\n> 如果容器运行时，volume不存在，会被自动创建出来。\n> \n> 所以一般不需要单独创建数据卷\n\n# 案例-给MySQL挂载本地目录\n\n容器不仅仅可以挂载数据卷，也可以直接挂载到宿主机目录上。关联关系如下：\n\n * 带数据卷模式：宿主机目录 --\x3e 数据卷 ---\x3e 容器内目录\n * 直接挂载模式：宿主机目录 ---\x3e 容器内目录\n\n如图：\n\n\n\n语法：\n\n目录挂载与数据卷挂载的语法是类似的：\n\n * -v [宿主机目录]:[容器内目录]\n * -v [宿主机文件]:[容器内文件]\n\n需求：创建并运行一个MySQL容器，将宿主机目录直接挂载到容器\n\ndocker run --name ms -v /diane/docker/mysql/data:/var/lib/mysql -v /diane/docker/mysql/conf/hmy.cnf:/etc/mysql/conf.d/hmy.cnf  -e MYSQL_ROOT_PASSWORD=123 -p 3305:3306 -d mysql:5.7.25\n\n##使用换行符分隔命令\ndocker run \\\n--name ms \\\n-v /diane/docker/mysql/data:/var/lib/mysql \\  ##左边是主机上的文件夹，自己创建的；右边是容器内文件，docker官方查阅得到\n-v /diane/docker/mysql/conf/hmy.cnf:/etc/mysql/conf.d/hmy.cnf  \\\n-e MYSQL_ROOT_PASSWORD=123 \\\n-p 3305:3306 \\   ##3306被原生的mysql占用着\n-d \\\nmysql:5.7.25\n\n\n\n实现思路如下：\n\n1）在将课前资料中的mysql.tar文件上传到虚拟机，通过load命令加载为镜像\n\n2）创建目录/tmp/mysql/data\n\n3）创建目录/tmp/mysql/conf，将课前资料提供的hmy.cnf文件上传到/tmp/mysql/conf\n\n4）去DockerHub查阅资料，创建并运行MySQL容器，要求：\n\n① 挂载/tmp/mysql/data到mysql容器内数据存储目录\n\n② 挂载/tmp/mysql/conf/hmy.cnf到mysql容器的配置文件\n\n③ 设置MySQL密码\n\n# 小结\n\ndocker run的命令中通过 -v 参数挂载文件或目录到容器中：\n\n * -v volume名称:容器内目录\n * -v 宿主机文件:容器内文\n * -v 宿主机目录:容器内目录\n\n数据卷挂载与目录直接挂载的区别\n\n * 数据卷挂载耦合度低，由docker来管理目录，但是目录较深，不好找\n * 目录挂载耦合度高，需要我们自己管理目录，不过目录容易寻找查看\n\n\n# Dockerfile自定义镜像\n\n常见的镜像在DockerHub就能找到，但是我们自己写的项目就必须自己构建镜像了。\n\n而要自定义镜像，就必须先了解镜像的结构才行。\n\n# 镜像结构\n\n镜像是将应用程序及其需要的系统函数库、环境、配置、依赖打包而成。\n\n我们以MySQL为例，来看看镜像的组成结构：\n\n分层的目的是为了解耦\n\n\n\n简单来说，镜像就是在系统函数库、运行环境基础上，添加应用程序文件、配置文件、依赖文件等组合，然后编写好启动脚本打包在一起形成的文件。\n\n我们要构建镜像，其实就是实现上述打包的过程。\n\n# Dockerfile语法\n\n构建自定义的镜像时，并不需要一个个文件去拷贝，打包。\n\n我们只需要告诉Docker，我们的镜像的组成，需要哪些BaseImage、需要拷贝什么文件、需要安装什么依赖、启动脚本是什么，将来Docker会帮助我们构建镜像。\n\n而描述上述信息的文件就是Dockerfile文件。\n\nDockerfile就是一个文本文件，其中包含一个个的指令(Instruction)，用指令来说明要执行什么操作来构建镜像。每一个指令都会形成一层Layer。\n\n\n\n更新详细语法说明，请参考官网文档： https://docs.docker.com/engine/reference/builder\n\n# 构建Java项目\n\n# 基于Ubuntu构建Java项目\n\n不推荐\n\n需求：基于Ubuntu镜像构建一个新镜像，运行一个java项目\n\n * 步骤1：新建一个空文件夹docker-demo\n   \n   \n\n * 步骤2：拷贝课前资料中的docker-demo.jar文件到docker-demo这个目录\n   \n   \n\n * 步骤3：拷贝课前资料中的jdk8.tar.gz文件到docker-demo这个目录\n   \n   \n\n * 步骤4：拷贝课前资料提供的Dockerfile到docker-demo这个目录\n   \n   \n   \n   其中的内容如下：\n   \n   ## 指定基础镜像\n   FROM ubuntu:16.04\n   ## 配置环境变量，JDK的安装目录\n   ENV JAVA_DIR=/usr/local\n   \n   ## 拷贝jdk和java项目的包\n   COPY ./jdk8.tar.gz $JAVA_DIR/\n   COPY ./docker-demo.jar /tmp/app.jar\n   \n   ## 安装JDK\n   RUN cd $JAVA_DIR \\\n    && tar -xf ./jdk8.tar.gz \\\n    && mv ./jdk1.8.0_144 ./java8\n   \n   ## 配置环境变量\n   ENV JAVA_HOME=$JAVA_DIR/java8\n   ENV PATH=$PATH:$JAVA_HOME/bin\n   \n   ## 暴露端口\n   EXPOSE 8090\n   ## 入口，java项目的启动命令\n   ENTRYPOINT java -jar /tmp/app.jar\n   \n\n * 步骤5：进入docker-demo\n   \n   将准备好的docker-demo上传到虚拟机任意目录，然后进入docker-demo目录下\n\n * 步骤6：运行命令：\n   \n   ##一定记得加点， 代表Dockerfile 所在目录 .表示当前目录\n   docker build -t javaweb:1.0 .\n   \n\n最后访问 http://192.168.159.100:8090/hello/count，其中的ip改成你的虚拟机ip\n\n# 基于java8构建Java项目\n\n虽然我们可以基于Ubuntu基础镜像，添加任意自己需要的安装包，构建镜像，但是却比较麻烦。所以大多数情况下，我们都可以在一些安装了部分软件的基础镜像上做改造。\n\n例如，构建java项目的镜像，可以在已经准备了JDK的基础镜像基础上构建。\n\n需求：基于java:8-alpine镜像，将一个Java项目构建为镜像\n\n实现思路如下：\n\n * ① 新建一个空的目录，然后在目录中新建一个文件，命名为Dockerfile\n\n * ② 拷贝课前资料提供的app.jar到这个目录中\n\n * ③ 编写Dockerfile文件：\n   \n   * a ）基于java:8-alpine作为基础镜像\n   \n   * b ）将app.jar拷贝到镜像中\n   \n   * c ）暴露端口\n   \n   * d ）编写入口ENTRYPOINT\n     \n     内容如下：\n     \n     FROM java:8-alpine\n     COPY ./app.jar /tmp/app.jar\n     EXPOSE 8090\n     ENTRYPOINT java -jar /tmp/app.jar\n     \n\n * ④ 使用docker build命令构建镜像\n   \n   ##一定记得加点， 代表Dockerfile 所在目录 .表示当前目录\n   ## -t 表示 构建镜像的名称\n   docker build -t demo .\n   \n\n * ⑤ 使用docker run创建容器并运行\n   \n   docker run --name web -p 8090:8090 -d demo\n   \n\n# 小结\n\n小结：\n\n 1. Dockerfile的本质是一个文件，通过指令描述镜像的构建过程\n\n 2. Dockerfile的第一行必须是FROM，从一个基础镜像来构建\n\n 3. 基础镜像可以是基本操作系统，如Ubuntu。也可以是其他人制作好的镜像，例如：java:8-alpine\n\n\n# Docker-Compose\n\nDocker Compose可以基于Compose文件帮我们快速的部署分布式应用，而无需手动一个个创建和运行容器！\n\n\n\n# 初识DockerCompose\n\nCompose文件是一个文本文件，通过指令定义集群中的每个容器如何运行。格式如下：\n\nversion: "3.8"\n services:\n  mysql:\n    image: mysql:5.7.25\n    environment:\n     MYSQL_ROOT_PASSWORD: 123 \n    volumes:\n     - "/tmp/mysql/data:/var/lib/mysql"\n     - "/tmp/mysql/conf/hmy.cnf:/etc/mysql/conf.d/hmy.cnf"\n  web:\n    build: .\n    ports:\n     - "8090:8090"\n\n\n\n上面的Compose文件就描述一个项目，其中包含两个容器：\n\n * mysql：一个基于mysql:5.7.25镜像构建的容器，并且挂载了两个目录\n * web：一个基于docker build临时构建的镜像容器，映射端口时8090\n\nDockerCompose的详细语法参考官网：https://docs.docker.com/compose/compose-file/\n\n其实DockerCompose文件可以看做是将多个docker run命令写到一个文件，只是语法稍有差异。\n\n# 部署微服务集群\n\n需求：将之前学习的cloud-demo微服务集群利用DockerCompose部署\n\n实现思路：\n\n① 查看课前资料提供的cloud-demo文件夹，里面已经编写好了docker-compose文件\n\n② 修改自己的cloud-demo项目，将数据库、nacos地址都命名为docker-compose中的服务名\n\n③ 使用maven打包工具，将项目中的每个微服务都打包为app.jar\n\n④ 将打包好的app.jar拷贝到cloud-demo中的每一个对应的子目录中\n\n⑤ 将cloud-demo上传至虚拟机，利用 docker-compose up -d 来部署\n\n# compose文件\n\n查看课前资料提供的cloud-demo文件夹，里面已经编写好了docker-compose文件，而且每个微服务都准备了一个独立的目录：\n\n\n\n内容如下：\n\nversion: "3.2"\n\nservices:\n  nacos:\n    image: nacos/nacos-server\n    environment:\n      MODE: standalone\n    ports:\n      - "8848:8848"\n  mysql:\n    image: mysql:5.7.25\n    environment:\n      MYSQL_ROOT_PASSWORD: 123\n    volumes:\n      - "$PWD/mysql/data:/var/lib/mysql"\n      - "$PWD/mysql/conf:/etc/mysql/conf.d/"\n  userservice:\n    build: ./user-service  ##指定Dockerfile文件\n  orderservice:\n    build: ./order-service\n  gateway:\n    build: ./gateway\n    ports:\n      - "10010:10010"\n\n\n可以看到，其中包含5个service服务：\n\n * nacos：作为注册中心和配置中心\n   * image: nacos/nacos-server： 基于nacos/nacos-server镜像构建\n   * environment：环境变量\n     * MODE: standalone：单点模式启动\n   * ports：端口映射，这里暴露了8848端口\n * mysql：数据库\n   * image: mysql:5.7.25：镜像版本是mysql:5.7.25\n   * environment：环境变量\n     * MYSQL_ROOT_PASSWORD: 123：设置数据库root账户的密码为123\n   * volumes：数据卷挂载，这里挂载了mysql的data、conf目录，其中有我提前准备好的数据\n * userservice、orderservice、gateway：都是基于Dockerfile临时构建的\n\n查看mysql目录，可以看到其中已经准备好了cloud_order、cloud_user表：\n\n\n\n查看微服务目录，可以看到都包含Dockerfile文件：\n\n\n\n内容如下：\n\nFROM java:8-alpine\nCOPY ./app.jar /tmp/app.jar\nENTRYPOINT java -jar /tmp/app.jar\n\n\n# 修改微服务配置\n\n**因为微服务将来要部署为docker容器，而容器之间互联不是通过IP地址，而是通过容器名。**这里我们将order-service、user-service、gateway服务的mysql、nacos地址都修改为基于容器名的访问。\n\n如下所示：\n\nspring:\n  datasource:\n    url: jdbc:mysql://mysql:3306/cloud_order?useSSL=false\n    username: root\n    password: 123\n    driver-class-name: com.mysql.jdbc.Driver\n  application:\n    name: orderservice\n  cloud:\n    nacos:\n      server-addr: nacos:8848 ## nacos服务地址\n\n\n# 打包\n\n接下来需要将我们的每个微服务都打包。因为之前查看到Dockerfile中的jar包名称都是app.jar，因此我们的每个微服务都需要用这个名称。\n\n可以通过修改pom.xml中的打包名称来实现，每个微服务都需要修改：\n\n<build>\n  \x3c!-- 服务打包的最终名称 --\x3e\n  <finalName>app</finalName>\n  <plugins>\n    <plugin>\n      <groupId>org.springframework.boot</groupId>\n      <artifactId>spring-boot-maven-plugin</artifactId>\n    </plugin>\n  </plugins>\n</build>\n\n\n打包后：\n\n\n\n# 拷贝jar包到部署目录\n\n编译打包好的app.jar文件，需要放到Dockerfile的同级目录中。注意：每个微服务的app.jar放到与服务名称对应的目录，别搞错了。\n\nuser-service：\n\n\n\norder-service：\n\n\n\ngateway：\n\n\n\n# 部署\n\n最后，我们需要将文件整个cloud-demo文件夹上传到虚拟机中，理由DockerCompose部署。\n\n上传到任意目录：\n\n\n\n部署：\n\n进入cloud-demo目录，然后运行下面的命令：\n\ndocker-compose up -d\n\n\n# 注意事项\n\n 1. java文件中的sql密码要跟docker-compose.yml文件中的sql密码一致\n\n 2. nacos再和其他微服务一起运行时，会比较慢，导致其他服务启动失败\n    \n    * 改进方法： docker-compose restart gateway userservice orderservice\n      \n      当nacos启动成功后，重新启动其他微服务\n\n 3. 修改文件时，记得删除镜像，如果不删除镜像，会默认用原始的有错误的镜像中构建容器。\n\n\n# Docker镜像仓库\n\n# 推送、拉取镜像\n\n推送镜像到私有镜像服务必须先tag，步骤如下：\n\n① 重新tag本地镜像，名称前缀为私有仓库的地址：192.168.159.100:8080/\n\ndocker tag nginx:latest 192.168.159.100:8080/nginx:1.0 \n\n\n② 推送镜像\n\ndocker push 192.168.159.100:8080/nginx:1.0 \n\n\n③ 拉取镜像\n\ndocker pull 192.168.159.100:8080/nginx:1.0 \n',normalizedContent:'# 初识docker\n\n# 什么是docker\n\n微服务虽然具备各种各样的优势，但服务的拆分通用给部署带来了很大的麻烦。\n\n * 分布式系统中，依赖的组件非常多，不同组件之间部署时往往会产生一些冲突。\n * 在数百上千台服务中重复部署，环境不一定一致，会遇到各种问题\n\n# 1.1.1.应用部署的环境问题\n\n大型项目组件较多，运行环境也较为复杂，部署时会碰到一些问题：\n\n * 依赖关系复杂，容易出现兼容性问题\n\n * 开发、测试、生产环境有差异\n\n\n\n例如一个项目中，部署时需要依赖于node.js、redis、rabbitmq、mysql等，这些服务部署时所需要的函数库、依赖项各不相同，甚至会有冲突。给部署带来了极大的困难。\n\n# 1.1.2.docker解决依赖兼容问题\n\n而docker确巧妙的解决了这些问题，docker是如何实现的呢？\n\ndocker为了解决依赖的兼容问题的，采用了两个手段：\n\n * 将应用的libs（函数库）、deps（依赖）、配置与应用一起打包\n\n * 将每个应用放到一个隔离容器去运行，避免互相干扰\n\n\n\n这样打包好的应用包中，既包含应用本身，也保护应用所需要的libs、deps，无需再操作系统上安装这些，自然就不存在不同应用之间的兼容问题了。\n\n虽然解决了不同应用的兼容问题，但是开发、测试等环境会存在差异，操作系统版本也会有差异，怎么解决这些问题呢？\n\n# 1.1.3.docker解决操作系统环境差异\n\n要解决不同操作系统环境差异问题，必须先了解操作系统结构。以一个ubuntu操作系统为例，结构如下：\n\n\n\n结构包括：\n\n * 计算机硬件：例如cpu、内存、磁盘等\n * 系统内核：所有linux发行版的内核都是linux，例如centos、ubuntu、fedora等。内核可以与计算机硬件交互，对外提供内核指令，用于操作计算机硬件。\n * 系统应用：操作系统本身提供的应用、函数库。这些函数库是对内核指令的封装，使用更加方便。\n\n应用于计算机交互的流程如下：\n\n1）应用调用操作系统应用（函数库），实现各种功能\n\n2）系统函数库是对内核指令集的封装，会调用内核指令\n\n3）内核指令操作计算机硬件\n\nubuntu和centospringboot都是基于linux内核，无非是系统应用不同，提供的函数库有差异：\n\n\n\n此时，如果将一个ubuntu版本的mysql应用安装到centos系统，mysql在调用ubuntu函数库时，会发现找不到或者不匹配，就会报错了：\n\n\n\ndocker如何解决不同系统环境的问题？\n\n * docker将用户程序与所需要调用的系统(比如ubuntu)函数库一起打包\n * docker运行到不同操作系统时，直接基于打包的函数库，借助于操作系统的linux内核来运行\n\n如图：\n\n\n\n# 1.1.4.小结\n\ndocker如何解决大型项目依赖关系复杂，不同组件依赖的兼容性问题？\n\n * docker允许开发中将应用、依赖、函数库、配置一起打包，形成可移植镜像\n * docker应用运行在容器中，使用沙箱机制，相互隔离\n\ndocker如何解决开发、测试、生产环境有差异的问题？\n\n * docker镜像中包含完整运行环境，包括系统函数库，仅依赖系统的linux内核，因此可以在任意linux操作系统上运行\n\ndocker是一个快速交付应用、运行应用的技术，具备下列优势：\n\n * 可以将程序及其依赖、运行环境一起打包为一个镜像，可以迁移到任意linux操作系统\n * 运行时利用沙箱机制形成隔离容器，各个应用互不干扰\n * 启动、移除都可以通过一行命令完成，方便快捷\n\n# docker和虚拟机的区别\n\ndocker可以让一个应用在任何操作系统中非常方便的运行。而以前我们接触的虚拟机，也能在一个操作系统中，运行另外一个操作系统，保护系统中的任何应用。\n\n两者有什么差异呢？\n\n虚拟机（virtual machine）是在操作系统中模拟硬件设备，然后运行另一个操作系统，比如在 windows 系统里面运行 ubuntu 系统，这样就可以运行任意的ubuntu应用了。\n\ndocker仅仅是封装函数库，并没有模拟完整的操作系统，如图：\n\n\n\n对比来看：\n\n\n\n小结：\n\ndocker和虚拟机的差异：\n\n * docker是一个系统进程；虚拟机是在操作系统中的操作系统\n\n * docker体积小、启动速度快、性能好；虚拟机体积大、启动速度慢、性能一般\n\n# docker架构\n\n# 镜像和容器\n\ndocker中有几个重要的概念：\n\n镜像（image）：docker将应用程序及其所需的依赖、函数库、环境、配置等文件打包在一起，称为镜像。\n\n容器（container）：镜像中的应用程序运行后形成的进程就是容器，只是docker会给容器进程做隔离，对外不可见。\n\n一切应用最终都是代码组成，都是硬盘中的一个个的字节形成的文件。只有运行时，才会加载到内存，形成进程。\n\n而镜像，就是把一个应用在硬盘上的文件、及其运行环境、部分系统函数库文件一起打包形成的文件包。这个文件包是只读的。\n\n容器呢，就是将这些文件中编写的程序、函数加载到内存中允许，形成进程，只不过要隔离起来。因此一个镜像可以启动多次，形成多个容器进程。\n\n\n\n例如你下载了一个qq，如果我们将qq在磁盘上的运行文件及其运行的操作系统依赖打包，形成qq镜像。然后你可以启动多次，双开、甚至三开qq，跟多个妹子聊天。\n\n# dockerhub\n\n开源应用程序非常多，打包这些应用往往是重复的劳动。为了避免这些重复劳动，人们就会将自己打包的应用镜像，例如redis、mysql镜像放到网络上，共享使用，就像github的代码共享一样。\n\n * dockerhub：dockerhub是一个官方的docker镜像的托管平台。这样的平台称为docker registry。\n\n * 国内也有类似于dockerhub 的公开服务，比如 网易云镜像服务、阿里云镜像库等。\n\n我们一方面可以将自己的镜像共享到dockerhub，另一方面也可以从dockerhub拉取镜像：\n\n\n\n# docker架构\n\n我们要使用docker来操作镜像、容器，就必须要安装docker。\n\ndocker是一个cs架构的程序，由两部分组成：\n\n * 服务端(server)：docker守护进程，负责处理docker指令，管理镜像、容器等\n\n * 客户端(client)：通过命令或restapi向docker服务端发送指令。可以在本地或远程向服务端发送指令。\n\n如图：\n\n\n\n# 小结\n\n镜像：\n\n * 将应用程序及其依赖、环境、配置打包在一起\n\n容器：\n\n * 镜像运行起来就是容器，一个镜像可以运行多个容器\n\ndocker结构：\n\n * 服务端：接收命令或远程请求，操作镜像或容器\n\n * 客户端：发送命令或者请求到docker服务端\n\ndockerhub：\n\n * 一个镜像托管的服务器，类似的还有阿里云镜像服务，统称为dockerregistry\n\n# 安装docker\n\n企业部署一般都是采用linux操作系统，而其中又数centos发行版占比最多，因此我们在centos下安装docker。参考课前资料中的文档：\n\n\n\n\n# docker的基本操作\n\n# 镜像操作\n\n# 镜像名称\n\n首先来看下镜像的名称组成：\n\n * 镜名称一般分两部分组成：[repository]:[tag]。\n * 在没有指定tag时，默认是latest，代表最新版本的镜像\n\n如图：\n\n\n\n这里的mysql就是repository，5.7就是tag，合一起就是镜像名称，代表5.7版本的mysql镜像。\n\n# 2.1.2.镜像命令\n\n常见的镜像操作命令如图：\n\n * 命令查看\n\n使用docker command --help来查看各种命令\n\n例如：docker pull --help\n\n\n\n# 2.1.3.案例1-拉取、查看镜像\n\n需求：从dockerhub中拉取一个nginx镜像并查看\n\n1）首先去镜像仓库搜索nginx镜像，比如dockerhub:\n\n\n\n2）根据查看到的镜像名称，拉取自己需要的镜像，通过命令：docker pull nginx\n\n\n\n3）通过命令：docker images 查看拉取到的镜像\n\n\n\n# 2.1.4.案例2-保存、导入镜像\n\n需求：利用docker save将nginx镜像导出磁盘，然后再通过load加载回来\n\n1）利用docker xx --help命令查看docker save和docker load的语法\n\n例如，查看save命令用法，可以输入命令：\n\ndocker save --help\n\n\n结果：\n\n\n\n命令格式：\n\ndocker save -o [保存的目标文件名称] [镜像名称]\n\n\n2）使用docker save导出镜像到磁盘\n\n运行命令：\n\ndocker save -o nginx.tar nginx:latest\n\n\n结果如图：\n\n\n\n3）使用docker load加载镜像\n\n先删除本地的nginx镜像：\n\ndocker rmi nginx:latest\n\n\n然后运行命令，加载本地文件：\n\ndocker load -i nginx.tar\n\n\n结果：\n\n\n\n# 2.1.5.练习\n\n需求：去dockerhub搜索并拉取一个redis镜像\n\n目标：\n\n1）去dockerhub搜索redis镜像\n\n2）查看redis镜像的名称和版本\n\n3）利用docker pull命令拉取镜像\n\n4）利用docker save命令将 redis:latest打包为一个redis.tar包\n\n5）利用docker rmi 删除本地的redis:latest\n\n6）利用docker load 重新加载 redis.tar文件\n\n# 2.1.6.基本操作-镜像\n\n学会使用 docker commands --help,自己查阅帮助文档\n\n * docker pull——从仓库拉取镜像\n * docker push——将本地镜像推送至仓库\n * docker images——查看镜像\n * docker save——将本地镜像打包成文件\n * docker load——将打包成文件的镜像加载成镜像\n * docker rmi——删除本地镜像\n\n# 容器操作\n\n# 容器相关命令\n\n容器操作的命令如图：\n\n\n\n容器保护三个状态：\n\n * 运行：进程正常运行\n * 暂停：进程暂停，cpu不再运行，并不释放内存\n * 停止：进程终止，回收进程占用的内存、cpu等资源\n\n其中：容器相关命令\n\n * docker run：创建并运行一个容器，处于运行状态\n\n * docker pause：让一个运行的容器暂停\n\n * docker unpause：让一个容器从暂停状态恢复运行\n\n * docker stop：停止一个运行的容器\n\n * docker start：让一个停止的容器再次运行\n\n * docker rm：删除一个容器\n\n * docker ps :查看所有的容器及运行状态\n\n * docker logs: 查看容器运行日志\n\n * docker exec：进入容器内部\n\n# 案例-创建并运行一个容器\n\n创建并运行nginx容器的命令：\n\ndocker run --name containername -p 80:80 -d nginx\n\n\n命令解读：\n\n * docker run ：创建并运行一个容器\n * --name : 给容器起一个名字，比如叫做mn\n * -p ：将宿主机端口与容器端口映射，冒号左侧是宿主机端口，右侧是容器端口\n   * 一般来说，右侧容器端口是不变的，跟随镜像来的；改变的是宿主机端口\n * -d：后台运行容器\n * nginx：镜像名称，例如nginx，不写标签，表示最新版\n\n这里的-p参数，是将容器端口映射到宿主机端口。\n\n默认情况下，容器是隔离环境，我们直接访问宿主机的80端口，肯定访问不到容器中的nginx。\n\n现在，将容器的80与宿主机的80关联起来，当我们访问宿主机的80端口时，就会被映射到容器的80，这样就能访问到nginx了：\n\n\n\n * 查看容器日志\n   * docker logs\n   * docker logs -f——类似于-tails 持续输出\n * 查看容器状态\n   * docker ps\n\n# 案例-进入容器，修改文件\n\n不推荐在容器内部修改文件\n\n>  1. 不方便修改，没有vim\n>  2. 文件修改没有记录，时间长了，会忘记，丢失信息\n\n需求：进入nginx容器，修改html文件内容，添加“传智教育欢迎您”\n\n提示：进入容器要用到docker exec命令。\n\n步骤：\n\n1）进入容器。进入我们刚刚创建的nginx容器的命令为：\n\ndocker exec -it mn bash\n\n\n命令解读：\n\n * docker exec ：进入容器内部，执行一个命令\n\n * -it : 给当前进入的容器创建一个标准输入、输出终端，允许我们与容器交互\n\n * mn ：要进入的容器的名称\n\n * bash：进入容器后执行的命令，bash是一个linux终端交互命令\n\n2）进入nginx的html所在目录 /usr/share/nginx/html\n\n容器内部会模拟一个独立的linux文件系统，看起来如同一个linux服务器一样：\n\n\n\nnginx的环境、配置、运行文件全部都在这个文件系统中，包括我们要修改的html文件。\n\n查看dockerhub网站中的nginx页面，可以知道nginx的html目录位置在/usr/share/nginx/html\n\n我们执行命令，进入该目录：\n\ncd /usr/share/nginx/html\n\n\n查看目录下文件：\n\n\n\n3）修改index.html的内容\n\n容器内没有vi命令，无法直接修改，我们用下面的命令来修改：\n\nsed -i -e \'s##welcome to nginx##传智教育欢迎您##g\' -e \'s##<head>##<head><meta charset="utf-8">##g\' index.html\n\n\n在浏览器访问自己的虚拟机地址，例如我的是：http://192.168.150.101，即可看到结果：\n\n\n\n# 小结\n\ndocker run命令的常见参数有哪些？\n\n * --name：指定容器名称\n * -p：指定端口映射\n * -d：让容器后台运行\n\n查看容器日志的命令：\n\n * docker logs\n * 添加 -f 参数可以持续查看日志\n\n查看容器状态：\n\n * docker ps\n * docker ps -a 查看所有容器，包括已经停止的\n\n删除容器\n\n * docker rm （不能删除正在运行的容器）\n * docker rm -f 强制删除\n\n# 案例——redis容器操作\n\n 1. 创建容器\n    \n    docker run --name mredis -p 6379:6379 -d redis——简单创建一个容器\n    \n    docker run --name mredis -p 6379:6379 -d redis redis-server --save 60 1 --loglevel warning——带持久化存储创建一个容器，每60s更新一次，至少有一次写操作，设置日志等级为warning级\n\n 2. 进入容器内部\n    \n    docker exec -it mredis redis-cli\n    \n    redis-cli ——表示可以直接在容器内部运行redis命令\n\n 3. 创建数据\n    \n    set diane angle\n    \n    get diane\n    \n    keys *\n\n# 数据卷（容器数据管理）\n\n在之前的nginx案例中，修改nginx的html页面时，需要进入nginx内部。并且因为没有编辑器，修改文件也很麻烦。\n\n这就是因为容器与数据（容器内文件）耦合带来的后果。\n\n\n\n要解决这个问题，必须将数据与容器解耦，这就要用到数据卷了。\n\n# 什么是数据卷\n\n**数据卷（volume）**是一个虚拟目录，指向宿主机文件系统中的某个目录。\n\n\n\n一旦完成数据卷挂载，对容器的一切操作都会作用在数据卷对应的宿主机目录了。\n\n这样，我们操作宿主机的/var/lib/docker/volumes/html目录，就等于操作容器内的/usr/share/nginx/html目录了\n\n# 数据集操作命令\n\n数据卷操作的基本语法如下：\n\ndocker volume [command]\n\n\ndocker volume命令是数据卷操作，根据命令后跟随的command来确定下一步的操作：\n\n * create 创建一个volume\n\n * inspect 显示一个或多个volume的信息\n\n * ls 列出所有的volume\n\n * prune 删除未使用的volume\n\n * rm 删除一个或多个指定的volume\n\n# 创建和查看数据卷\n\n需求：创建一个数据卷，并查看数据卷在宿主机的目录位置\n\n① 创建数据卷\n\ndocker volume create html\n\n\n② 查看所有数据\n\ndocker volume ls\n\n\n结果：\n\n\n\n③ 查看数据卷详细信息卷\n\ndocker volume inspect html\n\n\n结果：\n\n\n\n可以看到，我们创建的html这个数据卷关联的宿主机目录为/var/lib/docker/volumes/html/_data目录。\n\n小结：\n\n数据卷的作用：\n\n * 将容器与数据分离，解耦合，方便操作容器内数据，保证数据安全\n\n数据卷操作：\n\n * docker volume create：创建数据卷\n * docker volume ls：查看所有数据卷\n * docker volume inspect：查看数据卷详细信息，包括关联的宿主机目录位置\n * docker volume rm：删除指定数据卷\n * docker volume prune：删除所有未使用的数据卷\n\n# 挂载数据卷\n\n我们在创建容器时，可以通过 -v 参数来挂载一个数据卷到某个容器内目录，命令格式如下：\n\ndocker run \\\n  --name mn \\\n  -v html:/root/html \\\n  -p 8080:80\n  -d\n  nginx \\\n\n\n这里的-v就是挂载数据卷的命令：\n\n * -v html:/root/html ：把html数据卷挂载到容器内的/root/html这个目录中\n\n# 案例-给nginx挂载数据卷\n\n需求：创建一个nginx容器，修改容器内的html目录内的index.html内容\n\n分析：上个案例中，我们进入nginx容器内部，已经知道nginx的html目录所在位置/usr/share/nginx/html ，我们需要把这个目录挂载到html这个数据卷上，方便操作其中的内容。\n\n提示：运行容器时使用 -v 参数挂载数据卷\n\n步骤：\n\n① 创建容器并挂载数据卷到容器内的html目录\n\ndocker run --name mn -v html:/usr/share/nginx/html -p 80:80 -d nginx\n\n\n② 进入html数据卷所在位置，并修改html内容\n\n## 查看html数据卷的位置\ndocker volume inspect html\n## 进入该目录\ncd /var/lib/docker/volumes/html/_data\n## 修改文件\nvi index.html\n\n\n重要\n\n> 如果容器运行时，volume不存在，会被自动创建出来。\n> \n> 所以一般不需要单独创建数据卷\n\n# 案例-给mysql挂载本地目录\n\n容器不仅仅可以挂载数据卷，也可以直接挂载到宿主机目录上。关联关系如下：\n\n * 带数据卷模式：宿主机目录 --\x3e 数据卷 ---\x3e 容器内目录\n * 直接挂载模式：宿主机目录 ---\x3e 容器内目录\n\n如图：\n\n\n\n语法：\n\n目录挂载与数据卷挂载的语法是类似的：\n\n * -v [宿主机目录]:[容器内目录]\n * -v [宿主机文件]:[容器内文件]\n\n需求：创建并运行一个mysql容器，将宿主机目录直接挂载到容器\n\ndocker run --name ms -v /diane/docker/mysql/data:/var/lib/mysql -v /diane/docker/mysql/conf/hmy.cnf:/etc/mysql/conf.d/hmy.cnf  -e mysql_root_password=123 -p 3305:3306 -d mysql:5.7.25\n\n##使用换行符分隔命令\ndocker run \\\n--name ms \\\n-v /diane/docker/mysql/data:/var/lib/mysql \\  ##左边是主机上的文件夹，自己创建的；右边是容器内文件，docker官方查阅得到\n-v /diane/docker/mysql/conf/hmy.cnf:/etc/mysql/conf.d/hmy.cnf  \\\n-e mysql_root_password=123 \\\n-p 3305:3306 \\   ##3306被原生的mysql占用着\n-d \\\nmysql:5.7.25\n\n\n\n实现思路如下：\n\n1）在将课前资料中的mysql.tar文件上传到虚拟机，通过load命令加载为镜像\n\n2）创建目录/tmp/mysql/data\n\n3）创建目录/tmp/mysql/conf，将课前资料提供的hmy.cnf文件上传到/tmp/mysql/conf\n\n4）去dockerhub查阅资料，创建并运行mysql容器，要求：\n\n① 挂载/tmp/mysql/data到mysql容器内数据存储目录\n\n② 挂载/tmp/mysql/conf/hmy.cnf到mysql容器的配置文件\n\n③ 设置mysql密码\n\n# 小结\n\ndocker run的命令中通过 -v 参数挂载文件或目录到容器中：\n\n * -v volume名称:容器内目录\n * -v 宿主机文件:容器内文\n * -v 宿主机目录:容器内目录\n\n数据卷挂载与目录直接挂载的区别\n\n * 数据卷挂载耦合度低，由docker来管理目录，但是目录较深，不好找\n * 目录挂载耦合度高，需要我们自己管理目录，不过目录容易寻找查看\n\n\n# dockerfile自定义镜像\n\n常见的镜像在dockerhub就能找到，但是我们自己写的项目就必须自己构建镜像了。\n\n而要自定义镜像，就必须先了解镜像的结构才行。\n\n# 镜像结构\n\n镜像是将应用程序及其需要的系统函数库、环境、配置、依赖打包而成。\n\n我们以mysql为例，来看看镜像的组成结构：\n\n分层的目的是为了解耦\n\n\n\n简单来说，镜像就是在系统函数库、运行环境基础上，添加应用程序文件、配置文件、依赖文件等组合，然后编写好启动脚本打包在一起形成的文件。\n\n我们要构建镜像，其实就是实现上述打包的过程。\n\n# dockerfile语法\n\n构建自定义的镜像时，并不需要一个个文件去拷贝，打包。\n\n我们只需要告诉docker，我们的镜像的组成，需要哪些baseimage、需要拷贝什么文件、需要安装什么依赖、启动脚本是什么，将来docker会帮助我们构建镜像。\n\n而描述上述信息的文件就是dockerfile文件。\n\ndockerfile就是一个文本文件，其中包含一个个的指令(instruction)，用指令来说明要执行什么操作来构建镜像。每一个指令都会形成一层layer。\n\n\n\n更新详细语法说明，请参考官网文档： https://docs.docker.com/engine/reference/builder\n\n# 构建java项目\n\n# 基于ubuntu构建java项目\n\n不推荐\n\n需求：基于ubuntu镜像构建一个新镜像，运行一个java项目\n\n * 步骤1：新建一个空文件夹docker-demo\n   \n   \n\n * 步骤2：拷贝课前资料中的docker-demo.jar文件到docker-demo这个目录\n   \n   \n\n * 步骤3：拷贝课前资料中的jdk8.tar.gz文件到docker-demo这个目录\n   \n   \n\n * 步骤4：拷贝课前资料提供的dockerfile到docker-demo这个目录\n   \n   \n   \n   其中的内容如下：\n   \n   ## 指定基础镜像\n   from ubuntu:16.04\n   ## 配置环境变量，jdk的安装目录\n   env java_dir=/usr/local\n   \n   ## 拷贝jdk和java项目的包\n   copy ./jdk8.tar.gz $java_dir/\n   copy ./docker-demo.jar /tmp/app.jar\n   \n   ## 安装jdk\n   run cd $java_dir \\\n    && tar -xf ./jdk8.tar.gz \\\n    && mv ./jdk1.8.0_144 ./java8\n   \n   ## 配置环境变量\n   env java_home=$java_dir/java8\n   env path=$path:$java_home/bin\n   \n   ## 暴露端口\n   expose 8090\n   ## 入口，java项目的启动命令\n   entrypoint java -jar /tmp/app.jar\n   \n\n * 步骤5：进入docker-demo\n   \n   将准备好的docker-demo上传到虚拟机任意目录，然后进入docker-demo目录下\n\n * 步骤6：运行命令：\n   \n   ##一定记得加点， 代表dockerfile 所在目录 .表示当前目录\n   docker build -t javaweb:1.0 .\n   \n\n最后访问 http://192.168.159.100:8090/hello/count，其中的ip改成你的虚拟机ip\n\n# 基于java8构建java项目\n\n虽然我们可以基于ubuntu基础镜像，添加任意自己需要的安装包，构建镜像，但是却比较麻烦。所以大多数情况下，我们都可以在一些安装了部分软件的基础镜像上做改造。\n\n例如，构建java项目的镜像，可以在已经准备了jdk的基础镜像基础上构建。\n\n需求：基于java:8-alpine镜像，将一个java项目构建为镜像\n\n实现思路如下：\n\n * ① 新建一个空的目录，然后在目录中新建一个文件，命名为dockerfile\n\n * ② 拷贝课前资料提供的app.jar到这个目录中\n\n * ③ 编写dockerfile文件：\n   \n   * a ）基于java:8-alpine作为基础镜像\n   \n   * b ）将app.jar拷贝到镜像中\n   \n   * c ）暴露端口\n   \n   * d ）编写入口entrypoint\n     \n     内容如下：\n     \n     from java:8-alpine\n     copy ./app.jar /tmp/app.jar\n     expose 8090\n     entrypoint java -jar /tmp/app.jar\n     \n\n * ④ 使用docker build命令构建镜像\n   \n   ##一定记得加点， 代表dockerfile 所在目录 .表示当前目录\n   ## -t 表示 构建镜像的名称\n   docker build -t demo .\n   \n\n * ⑤ 使用docker run创建容器并运行\n   \n   docker run --name web -p 8090:8090 -d demo\n   \n\n# 小结\n\n小结：\n\n 1. dockerfile的本质是一个文件，通过指令描述镜像的构建过程\n\n 2. dockerfile的第一行必须是from，从一个基础镜像来构建\n\n 3. 基础镜像可以是基本操作系统，如ubuntu。也可以是其他人制作好的镜像，例如：java:8-alpine\n\n\n# docker-compose\n\ndocker compose可以基于compose文件帮我们快速的部署分布式应用，而无需手动一个个创建和运行容器！\n\n\n\n# 初识dockercompose\n\ncompose文件是一个文本文件，通过指令定义集群中的每个容器如何运行。格式如下：\n\nversion: "3.8"\n services:\n  mysql:\n    image: mysql:5.7.25\n    environment:\n     mysql_root_password: 123 \n    volumes:\n     - "/tmp/mysql/data:/var/lib/mysql"\n     - "/tmp/mysql/conf/hmy.cnf:/etc/mysql/conf.d/hmy.cnf"\n  web:\n    build: .\n    ports:\n     - "8090:8090"\n\n\n\n上面的compose文件就描述一个项目，其中包含两个容器：\n\n * mysql：一个基于mysql:5.7.25镜像构建的容器，并且挂载了两个目录\n * web：一个基于docker build临时构建的镜像容器，映射端口时8090\n\ndockercompose的详细语法参考官网：https://docs.docker.com/compose/compose-file/\n\n其实dockercompose文件可以看做是将多个docker run命令写到一个文件，只是语法稍有差异。\n\n# 部署微服务集群\n\n需求：将之前学习的cloud-demo微服务集群利用dockercompose部署\n\n实现思路：\n\n① 查看课前资料提供的cloud-demo文件夹，里面已经编写好了docker-compose文件\n\n② 修改自己的cloud-demo项目，将数据库、nacos地址都命名为docker-compose中的服务名\n\n③ 使用maven打包工具，将项目中的每个微服务都打包为app.jar\n\n④ 将打包好的app.jar拷贝到cloud-demo中的每一个对应的子目录中\n\n⑤ 将cloud-demo上传至虚拟机，利用 docker-compose up -d 来部署\n\n# compose文件\n\n查看课前资料提供的cloud-demo文件夹，里面已经编写好了docker-compose文件，而且每个微服务都准备了一个独立的目录：\n\n\n\n内容如下：\n\nversion: "3.2"\n\nservices:\n  nacos:\n    image: nacos/nacos-server\n    environment:\n      mode: standalone\n    ports:\n      - "8848:8848"\n  mysql:\n    image: mysql:5.7.25\n    environment:\n      mysql_root_password: 123\n    volumes:\n      - "$pwd/mysql/data:/var/lib/mysql"\n      - "$pwd/mysql/conf:/etc/mysql/conf.d/"\n  userservice:\n    build: ./user-service  ##指定dockerfile文件\n  orderservice:\n    build: ./order-service\n  gateway:\n    build: ./gateway\n    ports:\n      - "10010:10010"\n\n\n可以看到，其中包含5个service服务：\n\n * nacos：作为注册中心和配置中心\n   * image: nacos/nacos-server： 基于nacos/nacos-server镜像构建\n   * environment：环境变量\n     * mode: standalone：单点模式启动\n   * ports：端口映射，这里暴露了8848端口\n * mysql：数据库\n   * image: mysql:5.7.25：镜像版本是mysql:5.7.25\n   * environment：环境变量\n     * mysql_root_password: 123：设置数据库root账户的密码为123\n   * volumes：数据卷挂载，这里挂载了mysql的data、conf目录，其中有我提前准备好的数据\n * userservice、orderservice、gateway：都是基于dockerfile临时构建的\n\n查看mysql目录，可以看到其中已经准备好了cloud_order、cloud_user表：\n\n\n\n查看微服务目录，可以看到都包含dockerfile文件：\n\n\n\n内容如下：\n\nfrom java:8-alpine\ncopy ./app.jar /tmp/app.jar\nentrypoint java -jar /tmp/app.jar\n\n\n# 修改微服务配置\n\n**因为微服务将来要部署为docker容器，而容器之间互联不是通过ip地址，而是通过容器名。**这里我们将order-service、user-service、gateway服务的mysql、nacos地址都修改为基于容器名的访问。\n\n如下所示：\n\nspring:\n  datasource:\n    url: jdbc:mysql://mysql:3306/cloud_order?usessl=false\n    username: root\n    password: 123\n    driver-class-name: com.mysql.jdbc.driver\n  application:\n    name: orderservice\n  cloud:\n    nacos:\n      server-addr: nacos:8848 ## nacos服务地址\n\n\n# 打包\n\n接下来需要将我们的每个微服务都打包。因为之前查看到dockerfile中的jar包名称都是app.jar，因此我们的每个微服务都需要用这个名称。\n\n可以通过修改pom.xml中的打包名称来实现，每个微服务都需要修改：\n\n<build>\n  \x3c!-- 服务打包的最终名称 --\x3e\n  <finalname>app</finalname>\n  <plugins>\n    <plugin>\n      <groupid>org.springframework.boot</groupid>\n      <artifactid>spring-boot-maven-plugin</artifactid>\n    </plugin>\n  </plugins>\n</build>\n\n\n打包后：\n\n\n\n# 拷贝jar包到部署目录\n\n编译打包好的app.jar文件，需要放到dockerfile的同级目录中。注意：每个微服务的app.jar放到与服务名称对应的目录，别搞错了。\n\nuser-service：\n\n\n\norder-service：\n\n\n\ngateway：\n\n\n\n# 部署\n\n最后，我们需要将文件整个cloud-demo文件夹上传到虚拟机中，理由dockercompose部署。\n\n上传到任意目录：\n\n\n\n部署：\n\n进入cloud-demo目录，然后运行下面的命令：\n\ndocker-compose up -d\n\n\n# 注意事项\n\n 1. java文件中的sql密码要跟docker-compose.yml文件中的sql密码一致\n\n 2. nacos再和其他微服务一起运行时，会比较慢，导致其他服务启动失败\n    \n    * 改进方法： docker-compose restart gateway userservice orderservice\n      \n      当nacos启动成功后，重新启动其他微服务\n\n 3. 修改文件时，记得删除镜像，如果不删除镜像，会默认用原始的有错误的镜像中构建容器。\n\n\n# docker镜像仓库\n\n# 推送、拉取镜像\n\n推送镜像到私有镜像服务必须先tag，步骤如下：\n\n① 重新tag本地镜像，名称前缀为私有仓库的地址：192.168.159.100:8080/\n\ndocker tag nginx:latest 192.168.159.100:8080/nginx:1.0 \n\n\n② 推送镜像\n\ndocker push 192.168.159.100:8080/nginx:1.0 \n\n\n③ 拉取镜像\n\ndocker pull 192.168.159.100:8080/nginx:1.0 \n',charsets:{cjk:!0}},{title:"索引",frontmatter:{autoSort:99,title:"索引",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/7bb700/",categories:["后端","微服务","ES"],tags:["知识","微服务","ES"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/20.ES/10.%E7%B4%A2%E5%BC%95.html",relativePath:"01.后端/60.微服务/20.ES/10.索引.md",key:"v-8f19b34e",path:"/pages/7bb700/",headers:[{level:2,title:"倒排索引",slug:"倒排索引",normalizedTitle:"倒排索引",charIndex:2},{level:3,title:"正向索引",slug:"正向索引",normalizedTitle:"正向索引",charIndex:26},{level:3,title:"倒排索引",slug:"倒排索引-2",normalizedTitle:"倒排索引",charIndex:2},{level:3,title:"正向和倒排",slug:"正向和倒排",normalizedTitle:"正向和倒排",charIndex:790},{level:2,title:"索引库操作",slug:"索引库操作",normalizedTitle:"索引库操作",charIndex:1178},{level:3,title:"mapping映射属性",slug:"mapping映射属性",normalizedTitle:"mapping映射属性",charIndex:1245},{level:3,title:"索引库的CRUD",slug:"索引库的crud",normalizedTitle:"索引库的crud",charIndex:2295}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"倒排索引 正向索引 倒排索引 正向和倒排 索引库操作 mapping映射属性 索引库的CRUD",content:'# 倒排索引\n\n倒排索引的概念是基于MySQL这样的正向索引而言的。\n\n\n# 正向索引\n\n那么什么是正向索引呢？例如给下表（tb_goods）中的id创建索引：\n\n\n\n如果是根据id查询，那么直接走索引，查询速度非常快。\n\n但如果是基于title做模糊查询，只能是逐行扫描数据，流程如下：\n\n1）用户搜索数据，条件是title符合"%手机%"\n\n2）逐行获取数据，比如id为1的数据\n\n3）判断数据中的title是否符合用户搜索条件\n\n4）如果符合则放入结果集，不符合则丢弃。回到步骤1\n\n逐行扫描，也就是全表扫描，随着数据量增加，其查询效率也会越来越低。当数据量达到数百万时，就是一场灾难。\n\n\n# 倒排索引\n\n倒排索引中有两个非常重要的概念：\n\n * 文档（Document）：用来搜索的数据，其中的每一条数据就是一个文档。例如一个网页、一个商品信息\n * 词条（Term）：对文档数据或用户搜索数据，利用某种算法分词，得到的具备含义的词语就是词条。例如：我是中国人，就可以分为：我、是、中国人、中国、国人这样的几个词条\n\n创建倒排索引是对正向索引的一种特殊处理，流程如下：\n\n * 将每一个文档的数据利用算法分词，得到一个个词条\n * 创建表，每行数据包括词条、词条所在文档id、位置等信息\n * 因为词条唯一性，可以给词条创建索引，例如hash表结构索引\n\n如图：\n\n\n\n倒排索引的搜索流程如下（以搜索"华为手机"为例）：\n\n1）用户输入条件"华为手机"进行搜索。\n\n2）对用户输入内容分词，得到词条：华为、手机。\n\n3）拿着词条在倒排索引中查找，可以得到包含词条的文档id：1、2、3。\n\n4）拿着文档id到正向索引中查找具体文档。\n\n如图：\n\n\n\n虽然要先查询倒排索引，再查询倒排索引，但是无论是词条、还是文档id都建立了索引，查询速度非常快！无需全表扫描。\n\n\n# 正向和倒排\n\n那么为什么一个叫做正向索引，一个叫做倒排索引呢？\n\n * 正向索引是最传统的，根据id索引的方式。但根据词条查询时，必须先逐条获取每个文档，然后判断文档中是否包含所需要的词条，是根据文档找词条的过程。\n\n * 而倒排索引则相反，是先找到用户要搜索的词条，根据词条得到保护词条的文档的id，然后根据id获取文档。是根据词条找文档的过程。\n\n是不是恰好反过来了？\n\n那么两者方式的优缺点是什么呢？\n\n正向索引：\n\n * 优点：\n   * 可以给多个字段创建索引\n   * 根据索引字段搜索、排序速度非常快\n * 缺点：\n   * 根据非索引字段，或者索引字段中的部分词条查找时，只能全表扫描。\n\n倒排索引：\n\n * 优点：\n   * 根据词条搜索、模糊搜索时，速度非常快\n * 缺点：\n   * 只能给词条创建索引，而不是字段\n   * 无法根据字段做排序\n\n\n# 索引库操作\n\n索引库就类似数据库表，mapping映射就类似表的结构。\n\n我们要向es中存储数据，必须先创建“库”和“表”。\n\n\n# mapping映射属性\n\nmapping是对索引库中文档的约束，常见的mapping属性包括：\n\n * type：字段数据类型，常见的简单类型有：\n   * 字符串：text（可分词的文本）、keyword（精确值，例如：品牌、国家、ip地址）——（不需要拆）\n   * 数值：long、integer、short、byte、double、float、\n   * 布尔：boolean\n   * 日期：date\n   * 对象：object\n * index：是否创建索引，默认为true\n * analyzer：使用哪种分词器\n * properties：该字段的子字段\n\n例如下面的json文档：\n\n{\n    "age": 21,\n    "weight": 52.1,\n    "isMarried": false,\n    "info": "黑马程序员Java讲师",\n    "email": "zy@itcast.cn",\n    "score": [99.1, 99.5, 98.9],\n    "name": {\n        "firstName": "云",\n        "lastName": "赵"\n    }\n}\n\n\n对应的每个字段映射（mapping）：\n\n * age：类型为 integer；参与搜索，因此需要index为true；无需分词器\n * weight：类型为float；参与搜索，因此需要index为true；无需分词器\n * isMarried：类型为boolean；参与搜索，因此需要index为true；无需分词器\n * info：类型为字符串，需要分词，因此是text；参与搜索，因此需要index为true；分词器可以用ik_smart\n * email：类型为字符串，但是不需要分词，因此是keyword；不参与搜索，因此需要index为false；无需分词器\n * score：虽然是数组，但是我们只看元素的类型，类型为float；参与搜索，因此需要index为true；无需分词器\n * name：类型为object，需要定义多个子属性\n   * name.firstName；类型为字符串，但是不需要分词，因此是keyword；参与搜索，因此需要index为true；无需分词器\n   * name.lastName；类型为字符串，但是不需要分词，因此是keyword；参与搜索，因此需要index为true；无需分词器\n\n\n# 索引库的CRUD\n\n这里我们统一使用Kibana编写DSL的方式来演示。\n\n# 创建索引库和映射\n\n基本语法：\n\n * 请求方式：PUT\n * 请求路径：/索引库名，可以自定义\n * 请求参数：mapping映射\n\n格式：\n\nPUT /索引库名称\n{\n  "mappings": {\n    "properties": {\n      "字段名":{\n        "type": "text",\n        "analyzer": "ik_smart"\n      },\n      "字段名2":{\n        "type": "keyword",\n        "index": "false"\n      },\n      "字段名3":{\n        "properties": {\n          "子字段": {\n            "type": "keyword"\n          }\n        }\n      },\n      // ...略\n    }\n  }\n}\n\n\n示例：\n\nPUT /diana\n{\n  "mappings": {\n    "properties": {\n      "info": {//text类型，ik分词器，参与索引\n        "type": "text",\n        "analyzer": "ik_smart"\n      },\n      "email": {//keyword类型，不需要分词，不参与索引\n        "type": "keyword",\n        "index": false\n      },\n      "name": {//object类型\n        "type": "object", \n        "properties": {\n          "firstName": {//keyword类型，不需要分词，参与索引\n            "type": "keyword"\n          },\n          "lastName": {\n            "type": "keyword"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n# 查询索引库\n\n基本语法：\n\n * 请求方式：GET\n\n * 请求路径：/索引库名\n\n * 请求参数：无\n\n格式：\n\nGET /索引库名\n\n\n示例：\n\n\n\n# 修改索引库\n\n倒排索引结构虽然不复杂，但是一旦数据结构改变（比如改变了分词器），就需要重新创建倒排索引，这简直是灾难。因此索引库一旦创建，无法修改mapping。\n\n虽然无法修改mapping中已有的字段，但是却允许添加新的字段到mapping中，因为不会对倒排索引产生影响。\n\n语法说明：\n\nPUT /索引库名/_mapping\n{\n  "properties": {\n    "新字段名":{\n      "type": "integer"\n    }\n  }\n}\n\n\n示例：\n\n\n\n# 删除索引库\n\n语法：\n\n * 请求方式：DELETE\n\n * 请求路径：/索引库名\n\n * 请求参数：无\n\n格式：\n\nDELETE /索引库名\n\n\n在kibana中测试：\n\n\n\n# 总结\n\n索引库操作有哪些？\n\n * 创建索引库：PUT /索引库名\n * 查询索引库：GET /索引库名\n * 删除索引库：DELETE /索引库名\n * 添加字段：PUT /索引库名/_mapping\n\n##创建索引库\nPUT /diana\n{\n  "mappings": {\n    "properties": {\n      "info": {\n        "type": "text",\n        "analyzer": "ik_smart"\n      },\n      "email": {\n        "type": "keyword",\n        "index": false\n      },\n      "name": {\n        "type": "object", \n        "properties": {\n          "firstName": {\n            "type": "keyword"\n          },\n          "lastName": {\n            "type": "keyword"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n##查询索引库\nGET /diana\n\n##修改索引库,只允许添加新字段，不允许修改已经存在的字段\nPUT /diana/_mapping\n{\n  "properties": {\n    "age": {\n      "type":"integer"\n    }\n  }\n}\n\n##删除索引库\nDELETE /diana\n',normalizedContent:'# 倒排索引\n\n倒排索引的概念是基于mysql这样的正向索引而言的。\n\n\n# 正向索引\n\n那么什么是正向索引呢？例如给下表（tb_goods）中的id创建索引：\n\n\n\n如果是根据id查询，那么直接走索引，查询速度非常快。\n\n但如果是基于title做模糊查询，只能是逐行扫描数据，流程如下：\n\n1）用户搜索数据，条件是title符合"%手机%"\n\n2）逐行获取数据，比如id为1的数据\n\n3）判断数据中的title是否符合用户搜索条件\n\n4）如果符合则放入结果集，不符合则丢弃。回到步骤1\n\n逐行扫描，也就是全表扫描，随着数据量增加，其查询效率也会越来越低。当数据量达到数百万时，就是一场灾难。\n\n\n# 倒排索引\n\n倒排索引中有两个非常重要的概念：\n\n * 文档（document）：用来搜索的数据，其中的每一条数据就是一个文档。例如一个网页、一个商品信息\n * 词条（term）：对文档数据或用户搜索数据，利用某种算法分词，得到的具备含义的词语就是词条。例如：我是中国人，就可以分为：我、是、中国人、中国、国人这样的几个词条\n\n创建倒排索引是对正向索引的一种特殊处理，流程如下：\n\n * 将每一个文档的数据利用算法分词，得到一个个词条\n * 创建表，每行数据包括词条、词条所在文档id、位置等信息\n * 因为词条唯一性，可以给词条创建索引，例如hash表结构索引\n\n如图：\n\n\n\n倒排索引的搜索流程如下（以搜索"华为手机"为例）：\n\n1）用户输入条件"华为手机"进行搜索。\n\n2）对用户输入内容分词，得到词条：华为、手机。\n\n3）拿着词条在倒排索引中查找，可以得到包含词条的文档id：1、2、3。\n\n4）拿着文档id到正向索引中查找具体文档。\n\n如图：\n\n\n\n虽然要先查询倒排索引，再查询倒排索引，但是无论是词条、还是文档id都建立了索引，查询速度非常快！无需全表扫描。\n\n\n# 正向和倒排\n\n那么为什么一个叫做正向索引，一个叫做倒排索引呢？\n\n * 正向索引是最传统的，根据id索引的方式。但根据词条查询时，必须先逐条获取每个文档，然后判断文档中是否包含所需要的词条，是根据文档找词条的过程。\n\n * 而倒排索引则相反，是先找到用户要搜索的词条，根据词条得到保护词条的文档的id，然后根据id获取文档。是根据词条找文档的过程。\n\n是不是恰好反过来了？\n\n那么两者方式的优缺点是什么呢？\n\n正向索引：\n\n * 优点：\n   * 可以给多个字段创建索引\n   * 根据索引字段搜索、排序速度非常快\n * 缺点：\n   * 根据非索引字段，或者索引字段中的部分词条查找时，只能全表扫描。\n\n倒排索引：\n\n * 优点：\n   * 根据词条搜索、模糊搜索时，速度非常快\n * 缺点：\n   * 只能给词条创建索引，而不是字段\n   * 无法根据字段做排序\n\n\n# 索引库操作\n\n索引库就类似数据库表，mapping映射就类似表的结构。\n\n我们要向es中存储数据，必须先创建“库”和“表”。\n\n\n# mapping映射属性\n\nmapping是对索引库中文档的约束，常见的mapping属性包括：\n\n * type：字段数据类型，常见的简单类型有：\n   * 字符串：text（可分词的文本）、keyword（精确值，例如：品牌、国家、ip地址）——（不需要拆）\n   * 数值：long、integer、short、byte、double、float、\n   * 布尔：boolean\n   * 日期：date\n   * 对象：object\n * index：是否创建索引，默认为true\n * analyzer：使用哪种分词器\n * properties：该字段的子字段\n\n例如下面的json文档：\n\n{\n    "age": 21,\n    "weight": 52.1,\n    "ismarried": false,\n    "info": "黑马程序员java讲师",\n    "email": "zy@itcast.cn",\n    "score": [99.1, 99.5, 98.9],\n    "name": {\n        "firstname": "云",\n        "lastname": "赵"\n    }\n}\n\n\n对应的每个字段映射（mapping）：\n\n * age：类型为 integer；参与搜索，因此需要index为true；无需分词器\n * weight：类型为float；参与搜索，因此需要index为true；无需分词器\n * ismarried：类型为boolean；参与搜索，因此需要index为true；无需分词器\n * info：类型为字符串，需要分词，因此是text；参与搜索，因此需要index为true；分词器可以用ik_smart\n * email：类型为字符串，但是不需要分词，因此是keyword；不参与搜索，因此需要index为false；无需分词器\n * score：虽然是数组，但是我们只看元素的类型，类型为float；参与搜索，因此需要index为true；无需分词器\n * name：类型为object，需要定义多个子属性\n   * name.firstname；类型为字符串，但是不需要分词，因此是keyword；参与搜索，因此需要index为true；无需分词器\n   * name.lastname；类型为字符串，但是不需要分词，因此是keyword；参与搜索，因此需要index为true；无需分词器\n\n\n# 索引库的crud\n\n这里我们统一使用kibana编写dsl的方式来演示。\n\n# 创建索引库和映射\n\n基本语法：\n\n * 请求方式：put\n * 请求路径：/索引库名，可以自定义\n * 请求参数：mapping映射\n\n格式：\n\nput /索引库名称\n{\n  "mappings": {\n    "properties": {\n      "字段名":{\n        "type": "text",\n        "analyzer": "ik_smart"\n      },\n      "字段名2":{\n        "type": "keyword",\n        "index": "false"\n      },\n      "字段名3":{\n        "properties": {\n          "子字段": {\n            "type": "keyword"\n          }\n        }\n      },\n      // ...略\n    }\n  }\n}\n\n\n示例：\n\nput /diana\n{\n  "mappings": {\n    "properties": {\n      "info": {//text类型，ik分词器，参与索引\n        "type": "text",\n        "analyzer": "ik_smart"\n      },\n      "email": {//keyword类型，不需要分词，不参与索引\n        "type": "keyword",\n        "index": false\n      },\n      "name": {//object类型\n        "type": "object", \n        "properties": {\n          "firstname": {//keyword类型，不需要分词，参与索引\n            "type": "keyword"\n          },\n          "lastname": {\n            "type": "keyword"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n# 查询索引库\n\n基本语法：\n\n * 请求方式：get\n\n * 请求路径：/索引库名\n\n * 请求参数：无\n\n格式：\n\nget /索引库名\n\n\n示例：\n\n\n\n# 修改索引库\n\n倒排索引结构虽然不复杂，但是一旦数据结构改变（比如改变了分词器），就需要重新创建倒排索引，这简直是灾难。因此索引库一旦创建，无法修改mapping。\n\n虽然无法修改mapping中已有的字段，但是却允许添加新的字段到mapping中，因为不会对倒排索引产生影响。\n\n语法说明：\n\nput /索引库名/_mapping\n{\n  "properties": {\n    "新字段名":{\n      "type": "integer"\n    }\n  }\n}\n\n\n示例：\n\n\n\n# 删除索引库\n\n语法：\n\n * 请求方式：delete\n\n * 请求路径：/索引库名\n\n * 请求参数：无\n\n格式：\n\ndelete /索引库名\n\n\n在kibana中测试：\n\n\n\n# 总结\n\n索引库操作有哪些？\n\n * 创建索引库：put /索引库名\n * 查询索引库：get /索引库名\n * 删除索引库：delete /索引库名\n * 添加字段：put /索引库名/_mapping\n\n##创建索引库\nput /diana\n{\n  "mappings": {\n    "properties": {\n      "info": {\n        "type": "text",\n        "analyzer": "ik_smart"\n      },\n      "email": {\n        "type": "keyword",\n        "index": false\n      },\n      "name": {\n        "type": "object", \n        "properties": {\n          "firstname": {\n            "type": "keyword"\n          },\n          "lastname": {\n            "type": "keyword"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n##查询索引库\nget /diana\n\n##修改索引库,只允许添加新字段，不允许修改已经存在的字段\nput /diana/_mapping\n{\n  "properties": {\n    "age": {\n      "type":"integer"\n    }\n  }\n}\n\n##删除索引库\ndelete /diana\n',charsets:{cjk:!0}},{title:"操作API",frontmatter:{autoSort:97,title:"操作API",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/dac0d9/",categories:["后端","微服务","ES"],tags:["知识","微服务","ES"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/20.ES/20.%E6%93%8D%E4%BD%9CAPI.html",relativePath:"01.后端/60.微服务/20.ES/20.操作API.md",key:"v-17caaee6",path:"/pages/dac0d9/",headers:[{level:2,title:"RestAPI",slug:"restapi",normalizedTitle:"restapi",charIndex:2},{level:2,title:"RestClient操作文档",slug:"restclient操作文档",normalizedTitle:"restclient操作文档",charIndex:7689}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"RestAPI RestClient操作文档",content:'# RestAPI\n\nES官方提供了各种不同语言的客户端，用来操作ES。这些客户端的本质就是组装DSL语句，通过http请求发送给ES。官方文档地址：https://www.elastic.co/guide/en/elasticsearch/client/index.html\n\n其中的Java Rest Client又包括两种：\n\n * Java Low Level Rest Client\n * Java High Level Rest Client\n\n\n\n我们学习的是Java HighLevel Rest Client客户端API\n\n# 导入Demo工程\n\n# 导入数据\n\n首先导入课前资料提供的数据库数据：\n\n\n\n数据结构如下：\n\nCREATE TABLE `tb_hotel` (\n  `id` bigint(20) NOT NULL COMMENT \'酒店id\',\n  `name` varchar(255) NOT NULL COMMENT \'酒店名称；例：7天酒店\',\n  `address` varchar(255) NOT NULL COMMENT \'酒店地址；例：航头路\',\n  `price` int(10) NOT NULL COMMENT \'酒店价格；例：329\',\n  `score` int(2) NOT NULL COMMENT \'酒店评分；例：45，就是4.5分\',\n  `brand` varchar(32) NOT NULL COMMENT \'酒店品牌；例：如家\',\n  `city` varchar(32) NOT NULL COMMENT \'所在城市；例：上海\',\n  `star_name` varchar(16) DEFAULT NULL COMMENT \'酒店星级，从低到高分别是：1星到5星，1钻到5钻\',\n  `business` varchar(255) DEFAULT NULL COMMENT \'商圈；例：虹桥\',\n  `latitude` varchar(32) NOT NULL COMMENT \'纬度；例：31.2497\',\n  `longitude` varchar(32) NOT NULL COMMENT \'经度；例：120.3925\',\n  `pic` varchar(255) DEFAULT NULL COMMENT \'酒店图片；例:/img/1.jpg\',\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n\n\n# 导入项目\n\n然后导入课前资料提供的项目:\n\n\n\n项目结构如图：\n\n\n\n# mapping映射分析\n\n\n\n创建索引库，最关键的是mapping映射，而mapping映射要考虑的信息包括：\n\n * 字段名\n * 字段数据类型\n * 是否参与搜索\n * 是否需要分词\n * 如果分词，分词器是什么？\n\n其中：\n\n * 字段名、字段数据类型，可以参考数据表结构的名称和类型\n * 是否参与搜索要分析业务来判断，例如图片地址，就无需参与搜索\n * 是否分词呢要看内容，内容如果是一个整体就无需分词，反之则要分词\n * 分词器，我们可以统一使用ik_max_word\n\n来看下酒店数据的索引库结构:\n\n##创建酒店索引\nPUT /hotel\n{\n  "mappings": {\n    "properties": {\n      "id":{//keyword 类型，不是long\n        "type":"keyword"\n      },\n      "name": {\n        "type":"text",\n        "analyzer": "ik_max_word",\n        "copy_to": "all"\n      },\n      "address":{\n        "type":"text",\n        "analyzer": "ik_smart"\n      },\n      "price":{\n        "type":"integer",\n        "copy_to": "all"\n      },\n      "score":{\n        "type":"integer",\n        "copy_to": "all"\n      },\n      "brand":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "city":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "starName":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "business":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "location":{//地理位置特殊字段,包括经度和纬度\n        "type":"geo_point"\n      },\n      "pic":{//不分词，不搜索\n        "type":"keyword",\n        "index":false\n      },\n      "all": {//将许多字段的值聚在一块，方便搜索\n        "type":"text",\n        "analyzer": "ik_max_word"\n      }\n    }\n  }\n}\n\n\n几个特殊字段说明：\n\n * location：地理坐标，里面包含精度、纬度————type类型：geo_point\n * all：一个组合字段，其目的是将多字段的值 利用copy_to合并，提供给用户搜索\n\n地理坐标说明：\n\n\n\ncopy_to说明：\n\n\n\n# 初始化RestClient\n\n在elasticsearch提供的API中，与elasticsearch一切交互都封装在一个名为RestHighLevelClient的类中，必须先完成这个对象的初始化，建立与elasticsearch的连接。\n\n分为三步：\n\n1）引入es的RestHighLevelClient依赖：\n\n<dependency>\n    <groupId>org.elasticsearch.client</groupId>\n    <artifactId>elasticsearch-rest-high-level-client</artifactId>\n</dependency>\n\n\n2）因为SpringBoot默认的ES版本是7.6.2，所以我们需要覆盖默认的ES版本：\n\n<properties>\n    <java.version>1.8</java.version>\n    <elasticsearch.version>7.12.1</elasticsearch.version>\n</properties>\n\n\n3）初始化RestHighLevelClient：\n\n初始化的代码如下：\n\nRestHighLevelClient client = new RestHighLevelClient(RestClient.builder(\n        HttpHost.create("http://192.168.150.101:9200")\n));\n\n\n这里为了单元测试方便，我们创建一个测试类HotelIndexTest，然后将初始化的代码编写在@BeforeEach方法中：\n\npackage cn.itcast.hotel;\n\nimport org.apache.http.HttpHost;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.junit.jupiter.api.AfterEach;\nimport org.junit.jupiter.api.BeforeEach;\nimport org.junit.jupiter.api.Test;\n\nimport java.io.IOException;\n\npublic class HotelIndexTest {\n    private RestHighLevelClient client;\n\n    @BeforeEach\n    void setUp() {\n        this.client = new RestHighLevelClient(RestClient.builder(\n                HttpHost.create("http://192.168.150.101:9200")\n        ));\n    }\n\n    @AfterEach\n    void tearDown() throws IOException {\n        this.client.close();\n    }\n}\n\n\n# 创建索引库\n\n# 代码解读\n\n创建索引库的API如下：\n\n\n\n代码分为三步：\n\n * 1）创建Request对象。因为是创建索引库的操作，因此Request是CreateIndexRequest。\n * 2）添加请求参数，其实就是DSL的JSON参数部分。因为json字符串很长，这里是定义了静态字符串常量MAPPING_TEMPLATE，让代码看起来更加优雅。\n * 3）发送请求，client.indices()方法的返回值是IndicesClient类型，封装了所有与索引库操作有关的方法。\n\n# 完整示例\n\n在hotel-demo的cn.itcast.hotel.constants包下，创建一个类，定义mapping映射的JSON字符串常量：\n\npackage cn.itcast.hotel.constants;\n\npublic class HotelConstants {\n    public static final String MAPPING_TEMPLATE = "{\\n" +\n            "  \\"mappings\\": {\\n" +\n            "    \\"properties\\": {\\n" +\n            "      \\"id\\": {\\n" +\n            "        \\"type\\": \\"keyword\\"\\n" +\n            "      },\\n" +\n            "      \\"name\\":{\\n" +\n            "        \\"type\\": \\"text\\",\\n" +\n            "        \\"analyzer\\": \\"ik_max_word\\",\\n" +\n            "        \\"copy_to\\": \\"all\\"\\n" +\n            "      },\\n" +\n            "      \\"address\\":{\\n" +\n            "        \\"type\\": \\"keyword\\",\\n" +\n            "        \\"index\\": false\\n" +\n            "      },\\n" +\n            "      \\"price\\":{\\n" +\n            "        \\"type\\": \\"integer\\"\\n" +\n            "      },\\n" +\n            "      \\"score\\":{\\n" +\n            "        \\"type\\": \\"integer\\"\\n" +\n            "      },\\n" +\n            "      \\"brand\\":{\\n" +\n            "        \\"type\\": \\"keyword\\",\\n" +\n            "        \\"copy_to\\": \\"all\\"\\n" +\n            "      },\\n" +\n            "      \\"city\\":{\\n" +\n            "        \\"type\\": \\"keyword\\",\\n" +\n            "        \\"copy_to\\": \\"all\\"\\n" +\n            "      },\\n" +\n            "      \\"starName\\":{\\n" +\n            "        \\"type\\": \\"keyword\\"\\n" +\n            "      },\\n" +\n            "      \\"business\\":{\\n" +\n            "        \\"type\\": \\"keyword\\"\\n" +\n            "      },\\n" +\n            "      \\"location\\":{\\n" +\n            "        \\"type\\": \\"geo_point\\"\\n" +\n            "      },\\n" +\n            "      \\"pic\\":{\\n" +\n            "        \\"type\\": \\"keyword\\",\\n" +\n            "        \\"index\\": false\\n" +\n            "      },\\n" +\n            "      \\"all\\":{\\n" +\n            "        \\"type\\": \\"text\\",\\n" +\n            "        \\"analyzer\\": \\"ik_max_word\\"\\n" +\n            "      }\\n" +\n            "    }\\n" +\n            "  }\\n" +\n            "}";\n}\n\n\n在hotel-demo中的HotelIndexTest测试类中，编写单元测试，实现创建索引：\n\n@Test\nvoid createHotelIndex() throws IOException {\n    // 1.创建Request对象\n    CreateIndexRequest request = new CreateIndexRequest("hotel");\n    // 2.准备请求的参数：DSL语句\n    request.source(MAPPING_TEMPLATE, XContentType.JSON);\n    // 3.发送请求\n    client.indices().create(request, RequestOptions.DEFAULT);\n}\n\n\n# 删除索引库\n\n删除索引库的DSL语句非常简单：\n\nDELETE /hotel\n\n\n与创建索引库相比：\n\n * 请求方式从PUT变为DELTE\n * 请求路径不变\n * 无请求参数\n\n所以代码的差异，注意体现在Request对象上。依然是三步走：\n\n * 1）创建Request对象。这次是DeleteIndexRequest对象\n * 2）准备参数。这里是无参\n * 3）发送请求。改用delete方法\n\n在hotel-demo中的HotelIndexTest测试类中，编写单元测试，实现删除索引：\n\n@Test\nvoid testDeleteHotelIndex() throws IOException {\n    // 1.创建Request对象\n    DeleteIndexRequest request = new DeleteIndexRequest("hotel");\n    // 2.发送请求\n    client.indices().delete(request, RequestOptions.DEFAULT);\n}\n\n\n# 判断索引库是否存在\n\n判断索引库是否存在，本质就是查询，对应的DSL是：\n\nGET /hotel\n\n\n因此与删除的Java代码流程是类似的。依然是三步走：\n\n * 1）创建Request对象。这次是GetIndexRequest对象\n * 2）准备参数。这里是无参\n * 3）发送请求。改用exists方法\n\n@Test\nvoid testExistsHotelIndex() throws IOException {\n    // 1.创建Request对象\n    GetIndexRequest request = new GetIndexRequest("hotel");\n    // 2.发送请求\n    boolean exists = client.indices().exists(request, RequestOptions.DEFAULT);\n    // 3.输出\n    System.err.println(exists ? "索引库已经存在！" : "索引库不存在！");\n}\n\n\n# 总结\n\nJavaRestClient操作elasticsearch的流程基本类似。核心是client.indices()方法来获取索引库的操作对象。\n\n索引库操作的基本步骤：\n\n * 初始化RestHighLevelClient\n * 创建XxxIndexRequest。XXX是Create、Get、Delete\n * 准备DSL（ Create时需要，其它是无参）\n * 发送请求。调用RestHighLevelClient##indices().xxx()方法，xxx是create、exists、delete\n\n\n# RestClient操作文档\n\n为了与索引库操作分离，我们再次参加一个测试类，做两件事情：\n\n * 初始化RestHighLevelClient\n * 我们的酒店数据在数据库，需要利用IHotelService去查询，所以注入这个接口\n\npackage cn.itcast.hotel;\n\nimport cn.itcast.hotel.pojo.Hotel;\nimport cn.itcast.hotel.service.IHotelService;\nimport org.junit.jupiter.api.AfterEach;\nimport org.junit.jupiter.api.BeforeEach;\nimport org.junit.jupiter.api.Test;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.test.context.SpringBootTest;\n\nimport java.io.IOException;\nimport java.util.List;\n\n@SpringBootTest\npublic class HotelDocumentTest {\n    @Autowired\n    private IHotelService hotelService;\n\n    private RestHighLevelClient client;\n\n    @BeforeEach\n    void setUp() {\n        this.client = new RestHighLevelClient(RestClient.builder(\n                HttpHost.create("http://192.168.150.101:9200")\n        ));\n    }\n\n    @AfterEach\n    void tearDown() throws IOException {\n        this.client.close();\n    }\n}\n\n\n\n# 新增文档\n\n我们要将数据库的酒店数据查询出来，写入elasticsearch中。\n\n# 索引库实体类\n\n数据库查询后的结果是一个Hotel类型的对象。结构如下：\n\n@Data\n@TableName("tb_hotel")\npublic class Hotel {\n    @TableId(type = IdType.INPUT)\n    private Long id;\n    private String name;\n    private String address;\n    private Integer price;\n    private Integer score;\n    private String brand;\n    private String city;\n    private String starName;\n    private String business;\n    private String longitude;\n    private String latitude;\n    private String pic;\n}\n\n\n与我们的索引库结构存在差异：\n\n * longitude和latitude需要合并为location\n\n因此，我们需要定义一个新的类型，与索引库结构吻合：\n\npackage cn.itcast.hotel.pojo;\n\nimport lombok.Data;\nimport lombok.NoArgsConstructor;\n\n@Data\n@NoArgsConstructor\npublic class HotelDoc {\n    private Long id;\n    private String name;\n    private String address;\n    private Integer price;\n    private Integer score;\n    private String brand;\n    private String city;\n    private String starName;\n    private String business;\n    private String location;\n    private String pic;\n\n    public HotelDoc(Hotel hotel) {\n        this.id = hotel.getId();\n        this.name = hotel.getName();\n        this.address = hotel.getAddress();\n        this.price = hotel.getPrice();\n        this.score = hotel.getScore();\n        this.brand = hotel.getBrand();\n        this.city = hotel.getCity();\n        this.starName = hotel.getStarName();\n        this.business = hotel.getBusiness();\n        this.location = hotel.getLatitude() + ", " + hotel.getLongitude();\n        this.pic = hotel.getPic();\n    }\n}\n\n\n\n# 语法说明\n\n新增文档的DSL语句如下：\n\nPOST /{索引库名}/_doc/1\n{\n    "name": "Jack",\n    "age": 21\n}\n\n\n对应的java代码如图：\n\n\n\n可以看到与创建索引库类似，同样是三步走：\n\n * 1）创建Request对象\n * 2）准备请求参数，也就是DSL中的JSON文档\n * 3）发送请求\n\n变化的地方在于，这里直接使用client.xxx()的API，不再需要client.indices()了。\n\n# 完整代码\n\n我们导入酒店数据，基本流程一致，但是需要考虑几点变化：\n\n * 酒店数据来自于数据库，我们需要先查询出来，得到hotel对象\n * hotel对象需要转为HotelDoc对象\n * HotelDoc需要序列化为json格式\n\n因此，代码整体步骤如下：\n\n * 1）根据id查询酒店数据Hotel\n * 2）将Hotel封装为HotelDoc\n * 3）将HotelDoc序列化为JSON\n * 4）创建IndexRequest，指定索引库名和id\n * 5）准备请求参数，也就是JSON文档\n * 6）发送请求\n\n在hotel-demo的HotelDocumentTest测试类中，编写单元测试：\n\n@Test\nvoid testAddDocument() throws IOException {\n    // 1.根据id查询酒店数据\n    Hotel hotel = hotelService.getById(61083L);\n    // 2.转换为文档类型\n    HotelDoc hotelDoc = new HotelDoc(hotel);\n    // 3.将HotelDoc转json\n    String json = JSON.toJSONString(hotelDoc);\n\n    // 1.准备Request对象\n    IndexRequest request = new IndexRequest("hotel").id(hotelDoc.getId().toString());\n    // 2.准备Json文档\n    request.source(json, XContentType.JSON);\n    // 3.发送请求\n    client.index(request, RequestOptions.DEFAULT);\n}\n\n\n# 查询文档\n\n# 语法说明\n\n查询的DSL语句如下：\n\nGET /hotel/_doc/{id}\n\n\n非常简单，因此代码大概分两步：\n\n * 准备Request对象\n * 发送请求\n\n不过查询的目的是得到结果，解析为HotelDoc，因此难点是结果的解析。完整代码如下：\n\n\n\n可以看到，结果是一个JSON，其中文档放在一个_source属性中，因此解析就是拿到_source，反序列化为Java对象即可。\n\n与之前类似，也是三步走：\n\n * 1）准备Request对象。这次是查询，所以是GetRequest\n * 2）发送请求，得到结果。因为是查询，这里调用client.get()方法\n * 3）解析结果，就是对JSON做反序列化\n\n# 完整代码\n\n在hotel-demo的HotelDocumentTest测试类中，编写单元测试：\n\n@Test\nvoid testGetDocumentById() throws IOException {\n    // 1.准备Request\n    GetRequest request = new GetRequest("hotel", "61082");\n    // 2.发送请求，得到响应\n    GetResponse response = client.get(request, RequestOptions.DEFAULT);\n    // 3.解析响应结果\n    String json = response.getSourceAsString();\n\n    HotelDoc hotelDoc = JSON.parseObject(json, HotelDoc.class);\n    System.out.println(hotelDoc);\n}\n\n\n# 删除文档\n\n删除的DSL为是这样的：\n\nDELETE /hotel/_doc/{id}\n\n\n与查询相比，仅仅是请求方式从DELETE变成GET，可以想象Java代码应该依然是三步走：\n\n * 1）准备Request对象，因为是删除，这次是DeleteRequest对象。要指定索引库名和id\n * 2）准备参数，无参\n * 3）发送请求。因为是删除，所以是client.delete()方法\n\n在hotel-demo的HotelDocumentTest测试类中，编写单元测试：\n\n@Test\nvoid testDeleteDocument() throws IOException {\n    // 1.准备Request\n    DeleteRequest request = new DeleteRequest("hotel", "61083");\n    // 2.发送请求\n    client.delete(request, RequestOptions.DEFAULT);\n}\n\n\n# 修改文档\n\n# 语法说明\n\n修改我们讲过两种方式：\n\n * 全量修改：本质是先根据id删除，再新增\n * 增量修改：修改文档中的指定字段值\n\n在RestClient的API中，全量修改与新增的API完全一致，判断依据是ID：\n\n * 如果新增时，ID已经存在，则修改\n * 如果新增时，ID不存在，则新增\n\n这里不再赘述，我们主要关注增量修改。\n\n代码示例如图：\n\n\n\n与之前类似，也是三步走：\n\n * 1）准备Request对象。这次是修改，所以是UpdateRequest\n * 2）准备参数。也就是JSON文档，里面包含要修改的字段\n * 3）更新文档。这里调用client.update()方法\n\n# 完整代码\n\n在hotel-demo的HotelDocumentTest测试类中，编写单元测试：\n\n@Test\nvoid testUpdateDocument() throws IOException {\n    // 1.准备Request\n    UpdateRequest request = new UpdateRequest("hotel", "61083");\n    // 2.准备请求参数\n    request.doc(\n        "price", "952",\n        "starName", "四钻"\n    );\n    // 3.发送请求\n    client.update(request, RequestOptions.DEFAULT);\n}\n\n\n# 批量导入文档\n\n案例需求：利用BulkRequest批量将数据库数据导入到索引库中。\n\n步骤如下：\n\n * 利用mybatis-plus查询酒店数据\n\n * 将查询到的酒店数据（Hotel）转换为文档类型数据（HotelDoc）\n\n * 利用JavaRestClient中的BulkRequest批处理，实现批量新增文档\n\n# 语法说明\n\n批量处理BulkRequest，其本质就是将多个普通的CRUD请求组合在一起发送。\n\n其中提供了一个add方法，用来添加其他请求：\n\n\n\n可以看到，能添加的请求包括：\n\n * IndexRequest，也就是新增\n * UpdateRequest，也就是修改\n * DeleteRequest，也就是删除\n\n因此Bulk中添加了多个IndexRequest，就是批量新增功能了。示例：\n\n\n\n其实还是三步走：\n\n * 1）创建Request对象。这里是BulkRequest\n * 2）准备参数。批处理的参数，就是其它Request对象，这里就是多个IndexRequest\n * 3）发起请求。这里是批处理，调用的方法为client.bulk()方法\n\n我们在导入酒店数据时，将上述代码改造成for循环处理即可。\n\n# 完整代码\n\n在hotel-demo的HotelDocumentTest测试类中，编写单元测试：\n\n@Test\nvoid testBulkRequest() throws IOException {\n    // 批量查询酒店数据\n    List<Hotel> hotels = hotelService.list();\n\n    // 1.创建Request\n    BulkRequest request = new BulkRequest();\n    // 2.准备参数，添加多个新增的Request\n    for (Hotel hotel : hotels) {\n        // 2.1.转换为文档类型HotelDoc\n        HotelDoc hotelDoc = new HotelDoc(hotel);\n        // 2.2.创建新增文档的Request对象\n        request.add(new IndexRequest("hotel")\n                    .id(hotelDoc.getId().toString())\n                    .source(JSON.toJSONString(hotelDoc), XContentType.JSON));\n    }\n    // 3.发送请求\n    client.bulk(request, RequestOptions.DEFAULT);\n}\n\n\n# 小结\n\n文档操作的基本步骤：\n\n * 初始化RestHighLevelClient\n * 创建XxxRequest。XXX是Index、Get、Update、Delete、Bulk\n * 准备参数（Index、Update、Bulk时需要）\n * 发送请求。调用RestHighLevelClient##.xxx()方法，xxx是index、get、update、delete、bulk\n * 解析结果（Get时需要）',normalizedContent:'# restapi\n\nes官方提供了各种不同语言的客户端，用来操作es。这些客户端的本质就是组装dsl语句，通过http请求发送给es。官方文档地址：https://www.elastic.co/guide/en/elasticsearch/client/index.html\n\n其中的java rest client又包括两种：\n\n * java low level rest client\n * java high level rest client\n\n\n\n我们学习的是java highlevel rest client客户端api\n\n# 导入demo工程\n\n# 导入数据\n\n首先导入课前资料提供的数据库数据：\n\n\n\n数据结构如下：\n\ncreate table `tb_hotel` (\n  `id` bigint(20) not null comment \'酒店id\',\n  `name` varchar(255) not null comment \'酒店名称；例：7天酒店\',\n  `address` varchar(255) not null comment \'酒店地址；例：航头路\',\n  `price` int(10) not null comment \'酒店价格；例：329\',\n  `score` int(2) not null comment \'酒店评分；例：45，就是4.5分\',\n  `brand` varchar(32) not null comment \'酒店品牌；例：如家\',\n  `city` varchar(32) not null comment \'所在城市；例：上海\',\n  `star_name` varchar(16) default null comment \'酒店星级，从低到高分别是：1星到5星，1钻到5钻\',\n  `business` varchar(255) default null comment \'商圈；例：虹桥\',\n  `latitude` varchar(32) not null comment \'纬度；例：31.2497\',\n  `longitude` varchar(32) not null comment \'经度；例：120.3925\',\n  `pic` varchar(255) default null comment \'酒店图片；例:/img/1.jpg\',\n  primary key (`id`)\n) engine=innodb default charset=utf8mb4;\n\n\n# 导入项目\n\n然后导入课前资料提供的项目:\n\n\n\n项目结构如图：\n\n\n\n# mapping映射分析\n\n\n\n创建索引库，最关键的是mapping映射，而mapping映射要考虑的信息包括：\n\n * 字段名\n * 字段数据类型\n * 是否参与搜索\n * 是否需要分词\n * 如果分词，分词器是什么？\n\n其中：\n\n * 字段名、字段数据类型，可以参考数据表结构的名称和类型\n * 是否参与搜索要分析业务来判断，例如图片地址，就无需参与搜索\n * 是否分词呢要看内容，内容如果是一个整体就无需分词，反之则要分词\n * 分词器，我们可以统一使用ik_max_word\n\n来看下酒店数据的索引库结构:\n\n##创建酒店索引\nput /hotel\n{\n  "mappings": {\n    "properties": {\n      "id":{//keyword 类型，不是long\n        "type":"keyword"\n      },\n      "name": {\n        "type":"text",\n        "analyzer": "ik_max_word",\n        "copy_to": "all"\n      },\n      "address":{\n        "type":"text",\n        "analyzer": "ik_smart"\n      },\n      "price":{\n        "type":"integer",\n        "copy_to": "all"\n      },\n      "score":{\n        "type":"integer",\n        "copy_to": "all"\n      },\n      "brand":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "city":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "starname":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "business":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "location":{//地理位置特殊字段,包括经度和纬度\n        "type":"geo_point"\n      },\n      "pic":{//不分词，不搜索\n        "type":"keyword",\n        "index":false\n      },\n      "all": {//将许多字段的值聚在一块，方便搜索\n        "type":"text",\n        "analyzer": "ik_max_word"\n      }\n    }\n  }\n}\n\n\n几个特殊字段说明：\n\n * location：地理坐标，里面包含精度、纬度————type类型：geo_point\n * all：一个组合字段，其目的是将多字段的值 利用copy_to合并，提供给用户搜索\n\n地理坐标说明：\n\n\n\ncopy_to说明：\n\n\n\n# 初始化restclient\n\n在elasticsearch提供的api中，与elasticsearch一切交互都封装在一个名为resthighlevelclient的类中，必须先完成这个对象的初始化，建立与elasticsearch的连接。\n\n分为三步：\n\n1）引入es的resthighlevelclient依赖：\n\n<dependency>\n    <groupid>org.elasticsearch.client</groupid>\n    <artifactid>elasticsearch-rest-high-level-client</artifactid>\n</dependency>\n\n\n2）因为springboot默认的es版本是7.6.2，所以我们需要覆盖默认的es版本：\n\n<properties>\n    <java.version>1.8</java.version>\n    <elasticsearch.version>7.12.1</elasticsearch.version>\n</properties>\n\n\n3）初始化resthighlevelclient：\n\n初始化的代码如下：\n\nresthighlevelclient client = new resthighlevelclient(restclient.builder(\n        httphost.create("http://192.168.150.101:9200")\n));\n\n\n这里为了单元测试方便，我们创建一个测试类hotelindextest，然后将初始化的代码编写在@beforeeach方法中：\n\npackage cn.itcast.hotel;\n\nimport org.apache.http.httphost;\nimport org.elasticsearch.client.resthighlevelclient;\nimport org.junit.jupiter.api.aftereach;\nimport org.junit.jupiter.api.beforeeach;\nimport org.junit.jupiter.api.test;\n\nimport java.io.ioexception;\n\npublic class hotelindextest {\n    private resthighlevelclient client;\n\n    @beforeeach\n    void setup() {\n        this.client = new resthighlevelclient(restclient.builder(\n                httphost.create("http://192.168.150.101:9200")\n        ));\n    }\n\n    @aftereach\n    void teardown() throws ioexception {\n        this.client.close();\n    }\n}\n\n\n# 创建索引库\n\n# 代码解读\n\n创建索引库的api如下：\n\n\n\n代码分为三步：\n\n * 1）创建request对象。因为是创建索引库的操作，因此request是createindexrequest。\n * 2）添加请求参数，其实就是dsl的json参数部分。因为json字符串很长，这里是定义了静态字符串常量mapping_template，让代码看起来更加优雅。\n * 3）发送请求，client.indices()方法的返回值是indicesclient类型，封装了所有与索引库操作有关的方法。\n\n# 完整示例\n\n在hotel-demo的cn.itcast.hotel.constants包下，创建一个类，定义mapping映射的json字符串常量：\n\npackage cn.itcast.hotel.constants;\n\npublic class hotelconstants {\n    public static final string mapping_template = "{\\n" +\n            "  \\"mappings\\": {\\n" +\n            "    \\"properties\\": {\\n" +\n            "      \\"id\\": {\\n" +\n            "        \\"type\\": \\"keyword\\"\\n" +\n            "      },\\n" +\n            "      \\"name\\":{\\n" +\n            "        \\"type\\": \\"text\\",\\n" +\n            "        \\"analyzer\\": \\"ik_max_word\\",\\n" +\n            "        \\"copy_to\\": \\"all\\"\\n" +\n            "      },\\n" +\n            "      \\"address\\":{\\n" +\n            "        \\"type\\": \\"keyword\\",\\n" +\n            "        \\"index\\": false\\n" +\n            "      },\\n" +\n            "      \\"price\\":{\\n" +\n            "        \\"type\\": \\"integer\\"\\n" +\n            "      },\\n" +\n            "      \\"score\\":{\\n" +\n            "        \\"type\\": \\"integer\\"\\n" +\n            "      },\\n" +\n            "      \\"brand\\":{\\n" +\n            "        \\"type\\": \\"keyword\\",\\n" +\n            "        \\"copy_to\\": \\"all\\"\\n" +\n            "      },\\n" +\n            "      \\"city\\":{\\n" +\n            "        \\"type\\": \\"keyword\\",\\n" +\n            "        \\"copy_to\\": \\"all\\"\\n" +\n            "      },\\n" +\n            "      \\"starname\\":{\\n" +\n            "        \\"type\\": \\"keyword\\"\\n" +\n            "      },\\n" +\n            "      \\"business\\":{\\n" +\n            "        \\"type\\": \\"keyword\\"\\n" +\n            "      },\\n" +\n            "      \\"location\\":{\\n" +\n            "        \\"type\\": \\"geo_point\\"\\n" +\n            "      },\\n" +\n            "      \\"pic\\":{\\n" +\n            "        \\"type\\": \\"keyword\\",\\n" +\n            "        \\"index\\": false\\n" +\n            "      },\\n" +\n            "      \\"all\\":{\\n" +\n            "        \\"type\\": \\"text\\",\\n" +\n            "        \\"analyzer\\": \\"ik_max_word\\"\\n" +\n            "      }\\n" +\n            "    }\\n" +\n            "  }\\n" +\n            "}";\n}\n\n\n在hotel-demo中的hotelindextest测试类中，编写单元测试，实现创建索引：\n\n@test\nvoid createhotelindex() throws ioexception {\n    // 1.创建request对象\n    createindexrequest request = new createindexrequest("hotel");\n    // 2.准备请求的参数：dsl语句\n    request.source(mapping_template, xcontenttype.json);\n    // 3.发送请求\n    client.indices().create(request, requestoptions.default);\n}\n\n\n# 删除索引库\n\n删除索引库的dsl语句非常简单：\n\ndelete /hotel\n\n\n与创建索引库相比：\n\n * 请求方式从put变为delte\n * 请求路径不变\n * 无请求参数\n\n所以代码的差异，注意体现在request对象上。依然是三步走：\n\n * 1）创建request对象。这次是deleteindexrequest对象\n * 2）准备参数。这里是无参\n * 3）发送请求。改用delete方法\n\n在hotel-demo中的hotelindextest测试类中，编写单元测试，实现删除索引：\n\n@test\nvoid testdeletehotelindex() throws ioexception {\n    // 1.创建request对象\n    deleteindexrequest request = new deleteindexrequest("hotel");\n    // 2.发送请求\n    client.indices().delete(request, requestoptions.default);\n}\n\n\n# 判断索引库是否存在\n\n判断索引库是否存在，本质就是查询，对应的dsl是：\n\nget /hotel\n\n\n因此与删除的java代码流程是类似的。依然是三步走：\n\n * 1）创建request对象。这次是getindexrequest对象\n * 2）准备参数。这里是无参\n * 3）发送请求。改用exists方法\n\n@test\nvoid testexistshotelindex() throws ioexception {\n    // 1.创建request对象\n    getindexrequest request = new getindexrequest("hotel");\n    // 2.发送请求\n    boolean exists = client.indices().exists(request, requestoptions.default);\n    // 3.输出\n    system.err.println(exists ? "索引库已经存在！" : "索引库不存在！");\n}\n\n\n# 总结\n\njavarestclient操作elasticsearch的流程基本类似。核心是client.indices()方法来获取索引库的操作对象。\n\n索引库操作的基本步骤：\n\n * 初始化resthighlevelclient\n * 创建xxxindexrequest。xxx是create、get、delete\n * 准备dsl（ create时需要，其它是无参）\n * 发送请求。调用resthighlevelclient##indices().xxx()方法，xxx是create、exists、delete\n\n\n# restclient操作文档\n\n为了与索引库操作分离，我们再次参加一个测试类，做两件事情：\n\n * 初始化resthighlevelclient\n * 我们的酒店数据在数据库，需要利用ihotelservice去查询，所以注入这个接口\n\npackage cn.itcast.hotel;\n\nimport cn.itcast.hotel.pojo.hotel;\nimport cn.itcast.hotel.service.ihotelservice;\nimport org.junit.jupiter.api.aftereach;\nimport org.junit.jupiter.api.beforeeach;\nimport org.junit.jupiter.api.test;\nimport org.springframework.beans.factory.annotation.autowired;\nimport org.springframework.boot.test.context.springboottest;\n\nimport java.io.ioexception;\nimport java.util.list;\n\n@springboottest\npublic class hoteldocumenttest {\n    @autowired\n    private ihotelservice hotelservice;\n\n    private resthighlevelclient client;\n\n    @beforeeach\n    void setup() {\n        this.client = new resthighlevelclient(restclient.builder(\n                httphost.create("http://192.168.150.101:9200")\n        ));\n    }\n\n    @aftereach\n    void teardown() throws ioexception {\n        this.client.close();\n    }\n}\n\n\n\n# 新增文档\n\n我们要将数据库的酒店数据查询出来，写入elasticsearch中。\n\n# 索引库实体类\n\n数据库查询后的结果是一个hotel类型的对象。结构如下：\n\n@data\n@tablename("tb_hotel")\npublic class hotel {\n    @tableid(type = idtype.input)\n    private long id;\n    private string name;\n    private string address;\n    private integer price;\n    private integer score;\n    private string brand;\n    private string city;\n    private string starname;\n    private string business;\n    private string longitude;\n    private string latitude;\n    private string pic;\n}\n\n\n与我们的索引库结构存在差异：\n\n * longitude和latitude需要合并为location\n\n因此，我们需要定义一个新的类型，与索引库结构吻合：\n\npackage cn.itcast.hotel.pojo;\n\nimport lombok.data;\nimport lombok.noargsconstructor;\n\n@data\n@noargsconstructor\npublic class hoteldoc {\n    private long id;\n    private string name;\n    private string address;\n    private integer price;\n    private integer score;\n    private string brand;\n    private string city;\n    private string starname;\n    private string business;\n    private string location;\n    private string pic;\n\n    public hoteldoc(hotel hotel) {\n        this.id = hotel.getid();\n        this.name = hotel.getname();\n        this.address = hotel.getaddress();\n        this.price = hotel.getprice();\n        this.score = hotel.getscore();\n        this.brand = hotel.getbrand();\n        this.city = hotel.getcity();\n        this.starname = hotel.getstarname();\n        this.business = hotel.getbusiness();\n        this.location = hotel.getlatitude() + ", " + hotel.getlongitude();\n        this.pic = hotel.getpic();\n    }\n}\n\n\n\n# 语法说明\n\n新增文档的dsl语句如下：\n\npost /{索引库名}/_doc/1\n{\n    "name": "jack",\n    "age": 21\n}\n\n\n对应的java代码如图：\n\n\n\n可以看到与创建索引库类似，同样是三步走：\n\n * 1）创建request对象\n * 2）准备请求参数，也就是dsl中的json文档\n * 3）发送请求\n\n变化的地方在于，这里直接使用client.xxx()的api，不再需要client.indices()了。\n\n# 完整代码\n\n我们导入酒店数据，基本流程一致，但是需要考虑几点变化：\n\n * 酒店数据来自于数据库，我们需要先查询出来，得到hotel对象\n * hotel对象需要转为hoteldoc对象\n * hoteldoc需要序列化为json格式\n\n因此，代码整体步骤如下：\n\n * 1）根据id查询酒店数据hotel\n * 2）将hotel封装为hoteldoc\n * 3）将hoteldoc序列化为json\n * 4）创建indexrequest，指定索引库名和id\n * 5）准备请求参数，也就是json文档\n * 6）发送请求\n\n在hotel-demo的hoteldocumenttest测试类中，编写单元测试：\n\n@test\nvoid testadddocument() throws ioexception {\n    // 1.根据id查询酒店数据\n    hotel hotel = hotelservice.getbyid(61083l);\n    // 2.转换为文档类型\n    hoteldoc hoteldoc = new hoteldoc(hotel);\n    // 3.将hoteldoc转json\n    string json = json.tojsonstring(hoteldoc);\n\n    // 1.准备request对象\n    indexrequest request = new indexrequest("hotel").id(hoteldoc.getid().tostring());\n    // 2.准备json文档\n    request.source(json, xcontenttype.json);\n    // 3.发送请求\n    client.index(request, requestoptions.default);\n}\n\n\n# 查询文档\n\n# 语法说明\n\n查询的dsl语句如下：\n\nget /hotel/_doc/{id}\n\n\n非常简单，因此代码大概分两步：\n\n * 准备request对象\n * 发送请求\n\n不过查询的目的是得到结果，解析为hoteldoc，因此难点是结果的解析。完整代码如下：\n\n\n\n可以看到，结果是一个json，其中文档放在一个_source属性中，因此解析就是拿到_source，反序列化为java对象即可。\n\n与之前类似，也是三步走：\n\n * 1）准备request对象。这次是查询，所以是getrequest\n * 2）发送请求，得到结果。因为是查询，这里调用client.get()方法\n * 3）解析结果，就是对json做反序列化\n\n# 完整代码\n\n在hotel-demo的hoteldocumenttest测试类中，编写单元测试：\n\n@test\nvoid testgetdocumentbyid() throws ioexception {\n    // 1.准备request\n    getrequest request = new getrequest("hotel", "61082");\n    // 2.发送请求，得到响应\n    getresponse response = client.get(request, requestoptions.default);\n    // 3.解析响应结果\n    string json = response.getsourceasstring();\n\n    hoteldoc hoteldoc = json.parseobject(json, hoteldoc.class);\n    system.out.println(hoteldoc);\n}\n\n\n# 删除文档\n\n删除的dsl为是这样的：\n\ndelete /hotel/_doc/{id}\n\n\n与查询相比，仅仅是请求方式从delete变成get，可以想象java代码应该依然是三步走：\n\n * 1）准备request对象，因为是删除，这次是deleterequest对象。要指定索引库名和id\n * 2）准备参数，无参\n * 3）发送请求。因为是删除，所以是client.delete()方法\n\n在hotel-demo的hoteldocumenttest测试类中，编写单元测试：\n\n@test\nvoid testdeletedocument() throws ioexception {\n    // 1.准备request\n    deleterequest request = new deleterequest("hotel", "61083");\n    // 2.发送请求\n    client.delete(request, requestoptions.default);\n}\n\n\n# 修改文档\n\n# 语法说明\n\n修改我们讲过两种方式：\n\n * 全量修改：本质是先根据id删除，再新增\n * 增量修改：修改文档中的指定字段值\n\n在restclient的api中，全量修改与新增的api完全一致，判断依据是id：\n\n * 如果新增时，id已经存在，则修改\n * 如果新增时，id不存在，则新增\n\n这里不再赘述，我们主要关注增量修改。\n\n代码示例如图：\n\n\n\n与之前类似，也是三步走：\n\n * 1）准备request对象。这次是修改，所以是updaterequest\n * 2）准备参数。也就是json文档，里面包含要修改的字段\n * 3）更新文档。这里调用client.update()方法\n\n# 完整代码\n\n在hotel-demo的hoteldocumenttest测试类中，编写单元测试：\n\n@test\nvoid testupdatedocument() throws ioexception {\n    // 1.准备request\n    updaterequest request = new updaterequest("hotel", "61083");\n    // 2.准备请求参数\n    request.doc(\n        "price", "952",\n        "starname", "四钻"\n    );\n    // 3.发送请求\n    client.update(request, requestoptions.default);\n}\n\n\n# 批量导入文档\n\n案例需求：利用bulkrequest批量将数据库数据导入到索引库中。\n\n步骤如下：\n\n * 利用mybatis-plus查询酒店数据\n\n * 将查询到的酒店数据（hotel）转换为文档类型数据（hoteldoc）\n\n * 利用javarestclient中的bulkrequest批处理，实现批量新增文档\n\n# 语法说明\n\n批量处理bulkrequest，其本质就是将多个普通的crud请求组合在一起发送。\n\n其中提供了一个add方法，用来添加其他请求：\n\n\n\n可以看到，能添加的请求包括：\n\n * indexrequest，也就是新增\n * updaterequest，也就是修改\n * deleterequest，也就是删除\n\n因此bulk中添加了多个indexrequest，就是批量新增功能了。示例：\n\n\n\n其实还是三步走：\n\n * 1）创建request对象。这里是bulkrequest\n * 2）准备参数。批处理的参数，就是其它request对象，这里就是多个indexrequest\n * 3）发起请求。这里是批处理，调用的方法为client.bulk()方法\n\n我们在导入酒店数据时，将上述代码改造成for循环处理即可。\n\n# 完整代码\n\n在hotel-demo的hoteldocumenttest测试类中，编写单元测试：\n\n@test\nvoid testbulkrequest() throws ioexception {\n    // 批量查询酒店数据\n    list<hotel> hotels = hotelservice.list();\n\n    // 1.创建request\n    bulkrequest request = new bulkrequest();\n    // 2.准备参数，添加多个新增的request\n    for (hotel hotel : hotels) {\n        // 2.1.转换为文档类型hoteldoc\n        hoteldoc hoteldoc = new hoteldoc(hotel);\n        // 2.2.创建新增文档的request对象\n        request.add(new indexrequest("hotel")\n                    .id(hoteldoc.getid().tostring())\n                    .source(json.tojsonstring(hoteldoc), xcontenttype.json));\n    }\n    // 3.发送请求\n    client.bulk(request, requestoptions.default);\n}\n\n\n# 小结\n\n文档操作的基本步骤：\n\n * 初始化resthighlevelclient\n * 创建xxxrequest。xxx是index、get、update、delete、bulk\n * 准备参数（index、update、bulk时需要）\n * 发送请求。调用resthighlevelclient##.xxx()方法，xxx是index、get、update、delete、bulk\n * 解析结果（get时需要）',charsets:{cjk:!0}},{title:"案例练习",frontmatter:{autoSort:95,title:"案例练习",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/5b74f8/",categories:["后端","微服务","ES"],tags:["知识","微服务","ES"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/20.ES/30.%E6%A1%88%E4%BE%8B%E7%BB%83%E4%B9%A0.html",relativePath:"01.后端/60.微服务/20.ES/30.案例练习.md",key:"v-243fb89a",path:"/pages/5b74f8/",headers:[{level:2,title:"酒店搜索和分页",slug:"酒店搜索和分页",normalizedTitle:"酒店搜索和分页",charIndex:45},{level:2,title:"酒店结果过滤",slug:"酒店结果过滤",normalizedTitle:"酒店结果过滤",charIndex:56},{level:2,title:"我周边的酒店",slug:"我周边的酒店",normalizedTitle:"我周边的酒店",charIndex:66},{level:2,title:"酒店竞价排名",slug:"酒店竞价排名",normalizedTitle:"酒店竞价排名",charIndex:76}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"酒店搜索和分页 酒店结果过滤 我周边的酒店 酒店竞价排名",content:'下面，我们通过黑马旅游的案例来实战演练下之前学习的知识。\n\n我们实现四部分功能：\n\n * 酒店搜索和分页\n * 酒店结果过滤\n * 我周边的酒店\n * 酒店竞价排名\n\n启动我们提供的hotel-demo项目，其默认端口是8090，访问http://localhost:8090，就能看到项目页面了：\n\n\n\n\n# 酒店搜索和分页\n\n案例需求：实现黑马旅游的酒店搜索功能，完成关键字搜索和分页\n\n# 需求分析\n\n在项目的首页，有一个大大的搜索框，还有分页按钮：\n\n\n\n点击搜索按钮，可以看到浏览器控制台发出了请求：\n\n\n\n请求参数如下：\n\n\n\n由此可以知道，我们这个请求的信息如下：\n\n * 请求方式：POST\n * 请求路径：/hotel/list\n * 请求参数：JSON对象，包含4个字段：\n   * key：搜索关键字\n   * page：页码\n   * size：每页大小\n   * sortBy：排序，目前暂不实现\n * 返回值：分页查询，需要返回分页结果PageResult，包含两个属性：\n   * total：总条数\n   * List<HotelDoc>：当前页的数据\n\n因此，我们实现业务的流程如下：\n\n * 步骤一：定义实体类，接收请求参数的JSON对象\n * 步骤二：编写controller，接收页面的请求\n * 步骤三：编写业务实现，利用RestHighLevelClient实现搜索、分页\n\n# 定义实体类\n\n实体类有两个，一个是前端的请求参数实体，一个是服务端应该返回的响应结果实体。\n\n1）请求参数\n\n前端请求的json结构如下：\n\n{\n    "key": "搜索关键字",\n    "page": 1,\n    "size": 3,\n    "sortBy": "default"\n}\n\n\n因此，我们在cn.itcast.hotel.pojo包下定义一个实体类：\n\npackage cn.itcast.hotel.pojo;\n\nimport lombok.Data;\n\n@Data\npublic class RequestParams {\n    private String key;\n    private Integer page;\n    private Integer size;\n    private String sortBy;\n}\n\n\n2）返回值\n\n分页查询，需要返回分页结果PageResult，包含两个属性：\n\n * total：总条数\n * List<HotelDoc>：当前页的数据\n\n因此，我们在cn.itcast.hotel.pojo中定义返回结果：\n\npackage cn.itcast.hotel.pojo;\n\nimport lombok.Data;\n\nimport java.util.List;\n\n@Data\npublic class PageResult {\n    private Long total;\n    private List<HotelDoc> hotels;\n\n    public PageResult() {\n    }\n\n    public PageResult(Long total, List<HotelDoc> hotels) {\n        this.total = total;\n        this.hotels = hotels;\n    }\n}\n\n\n# 定义controller\n\n定义一个HotelController，声明查询接口，满足下列要求：\n\n * 请求方式：Post\n * 请求路径：/hotel/list\n * 请求参数：对象，类型为RequestParam\n * 返回值：PageResult，包含两个属性\n   * Long total：总条数\n   * List<HotelDoc> hotels：酒店数据\n\n因此，我们在cn.itcast.hotel.web中定义HotelController：\n\n@RestController\n@RequestMapping("/hotel")\npublic class HotelController {\n\n    @Autowired\n    private IHotelService hotelService;\n\t// 搜索酒店数据\n    @PostMapping("/list")\n    public PageResult search(@RequestBody RequestParams params){\n        return hotelService.search(params);\n    }\n}\n\n\n# 实现搜索业务\n\n我们在controller调用了IHotelService，并没有实现该方法，因此下面我们就在IHotelService中定义方法，并且去实现业务逻辑。\n\n1）在cn.itcast.hotel.service中的IHotelService接口中定义一个方法：\n\n/**\n * 根据关键字搜索酒店信息\n * @param params 请求参数对象，包含用户输入的关键字 \n * @return 酒店文档列表\n */\nPageResult search(RequestParams params);\n\n\n2）实现搜索业务，肯定离不开RestHighLevelClient，我们需要把它注册到Spring中作为一个Bean。在cn.itcast.hotel中的HotelDemoApplication中声明这个Bean：\n\n@Bean\npublic RestHighLevelClient client(){\n    return  new RestHighLevelClient(RestClient.builder(\n        HttpHost.create("http://192.168.150.101:9200")\n    ));\n}\n\n\n3）在cn.itcast.hotel.service.impl中的HotelService中实现search方法：\n\n@Override\npublic PageResult search(RequestParams params) {\n    try {\n        // 1.准备Request\n        SearchRequest request = new SearchRequest("hotel");\n        // 2.准备DSL\n        // 2.1.query\n        String key = params.getKey();\n        if (key ** null || "".equals(key)) {\n            boolQuery.must(QueryBuilders.matchAllQuery());\n        } else {\n            boolQuery.must(QueryBuilders.matchQuery("all", key));\n        }\n\n        // 2.2.分页\n        int page = params.getPage();\n        int size = params.getSize();\n        request.source().from((page - 1) * size).size(size);\n\n        // 3.发送请求\n        SearchResponse response = client.search(request, RequestOptions.DEFAULT);\n        // 4.解析响应\n        return handleResponse(response);\n    } catch (IOException e) {\n        throw new RuntimeException(e);\n    }\n}\n\n// 结果解析\nprivate PageResult handleResponse(SearchResponse response) {\n    // 4.解析响应\n    SearchHits searchHits = response.getHits();\n    // 4.1.获取总条数\n    long total = searchHits.getTotalHits().value;\n    // 4.2.文档数组\n    SearchHit[] hits = searchHits.getHits();\n    // 4.3.遍历\n    List<HotelDoc> hotels = new ArrayList<>();\n    for (SearchHit hit : hits) {\n        // 获取文档source\n        String json = hit.getSourceAsString();\n        // 反序列化\n        HotelDoc hotelDoc = JSON.parseObject(json, HotelDoc.class);\n\t\t// 放入集合\n        hotels.add(hotelDoc);\n    }\n    // 4.4.封装返回\n    return new PageResult(total, hotels);\n}\n\n\n# 问题总结\n\n * 注入bean 的形式注册客户端\n   \n   //TODO 注入bean 的形式注册客户端\n   @Bean\n   public RestHighLevelClient client() {\n       return new RestHighLevelClient(RestClient.builder(\n           HttpHost.create("http://192.168.159.100:9200")\n       ));\n   }\n   \n\n * 将复杂的查询操作 放到service来实现,\n   \n   * 具体的业务实现代码 放到service层\n   \n   @PostMapping("/list")\n   public PageResult search(@RequestBody RequestParams requestParams) throws IOException {\n       System.out.println(requestParams);\n       return hotelService.search(requestParams);\n   }\n   \n\n * 查询 TODO 健壮性处理 考虑key为空的情况\n   \n   //查询 TODO 健壮性处理 考虑key为空的情况\n   if ("".equals(key) || key ** null) {//key 为空 查询全部\n       request.source().query(QueryBuilders.matchAllQuery());\n   } else {//不为空，从聚合字段all中 查询key值\n       request.source().query(QueryBuilders.matchQuery("all", key));\n   }\n   \n\n * 使用构造函数封装\n   \n   * 也可以使用set方法来单独封装数据，但是还是推荐使用这种方法\n   \n   // 构造函数\n   public PageResult(Long total, List<HotelDoc> hotels) {\n       this.total = total;\n       this.hotels = hotels;\n   }\n   \n   \n   //使用构造函数封装 TODO\n   return new PageResult(value, hotelDocs);\n   \n\n * 业务中的异常不能直接抛出，要捕获它，然后统一处理\n\n\n# 酒店结果过滤\n\n需求：添加品牌、城市、星级、价格等过滤功能\n\n# 需求分析\n\n在页面搜索框下面，会有一些过滤项：\n\n\n\n传递的参数如图：\n\n\n\n包含的过滤条件有：\n\n * brand：品牌值\n * city：城市\n * minPrice~maxPrice：价格范围\n * starName：星级\n\n我们需要做两件事情：\n\n * 修改请求参数的对象RequestParams，接收上述参数\n * 修改业务逻辑，在搜索条件之外，添加一些过滤条件\n\n# 修改实体类\n\n修改在cn.itcast.hotel.pojo包下的实体类RequestParams：\n\n@Data\npublic class RequestParams {\n    private String key;\n    private Integer page;\n    private Integer size;\n    private String sortBy;\n    // 下面是新增的过滤条件参数\n    private String city;\n    private String brand;\n    private String starName;\n    private Integer minPrice;\n    private Integer maxPrice;\n}\n\n\n# 修改搜索业务\n\n在HotelService的search方法中，只有一个地方需要修改：requet.source().query( ... )其中的查询条件。\n\n在之前的业务中，只有match查询，根据关键字搜索，现在要添加条件过滤，包括：\n\n * 品牌过滤：是keyword类型，用term查询\n * 星级过滤：是keyword类型，用term查询\n * 价格过滤：是数值类型，用range查询\n * 城市过滤：是keyword类型，用term查询\n\n多个查询条件组合，肯定是boolean查询来组合：\n\n * 关键字搜索放到must中，参与算分\n * 其它过滤条件放到filter中，不参与算分\n\n因为条件构建的逻辑比较复杂，这里先封装为一个函数：\n\n\n\nbuildBasicQuery的代码如下：\n\nprivate void buildBasicQuery(RequestParams params, SearchRequest request) {\n    // 1.构建BooleanQuery\n    BoolQueryBuilder boolQuery = QueryBuilders.boolQuery();\n    // 2.关键字搜索\n    String key = params.getKey();\n    if (key ** null || "".equals(key)) {\n        boolQuery.must(QueryBuilders.matchAllQuery());\n    } else {\n        boolQuery.must(QueryBuilders.matchQuery("all", key));\n    }\n    // 3.城市条件\n    if (params.getCity() != null && !params.getCity().equals("")) {\n        boolQuery.filter(QueryBuilders.termQuery("city", params.getCity()));\n    }\n    // 4.品牌条件\n    if (params.getBrand() != null && !params.getBrand().equals("")) {\n        boolQuery.filter(QueryBuilders.termQuery("brand", params.getBrand()));\n    }\n    // 5.星级条件\n    if (params.getStarName() != null && !params.getStarName().equals("")) {\n        boolQuery.filter(QueryBuilders.termQuery("starName", params.getStarName()));\n    }\n\t// 6.价格\n    if (params.getMinPrice() != null && params.getMaxPrice() != null) {\n        boolQuery.filter(QueryBuilders\n                         .rangeQuery("price")\n                         .gte(params.getMinPrice())\n                         .lte(params.getMaxPrice())\n                        );\n    }\n\t// 7.放入source\n    request.source().query(boolQuery);\n}\n\n\n\n# 我周边的酒店\n\n需求：我附近的酒店\n\n# 需求分析\n\n在酒店列表页的右侧，有一个小地图，点击地图的定位按钮，地图会找到你所在的位置：\n\n\n\n并且，在前端会发起查询请求，将你的坐标发送到服务端：\n\n\n\n我们要做的事情就是基于这个location坐标，然后按照距离对周围酒店排序。实现思路如下：\n\n * 修改RequestParams参数，接收location字段\n * 修改search方法业务逻辑，如果location有值，添加根据geo_distance排序的功能\n\n# 修改实体类\n\n修改在cn.itcast.hotel.pojo包下的实体类RequestParams：\n\npackage cn.itcast.hotel.pojo;\n\nimport lombok.Data;\n\n@Data\npublic class RequestParams {\n    private String key;\n    private Integer page;\n    private Integer size;\n    private String sortBy;\n    private String city;\n    private String brand;\n    private String starName;\n    private Integer minPrice;\n    private Integer maxPrice;\n    // 我当前的地理坐标\n    private String location;\n}\n\n\n\n# 距离排序API\n\n我们以前学习过排序功能，包括两种：\n\n * 普通字段排序\n * 地理坐标排序\n\n我们只讲了普通字段排序对应的java写法。地理坐标排序只学过DSL语法，如下：\n\nGET /indexName/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "sort": [\n    {\n      "price": "asc"  \n    },\n    {\n      "_geo_distance" : {\n          "FIELD" : "纬度，经度",\n          "order" : "asc",\n          "unit" : "km"\n      }\n    }\n  ]\n}\n\n\n对应的java代码示例：\n\n\n\n# 添加距离排序\n\n在cn.itcast.hotel.service.impl的HotelService的search方法中，添加一个排序功能：\n\n\n\n完整代码：\n\n@Override\npublic PageResult search(RequestParams params) {\n    try {\n        // 1.准备Request\n        SearchRequest request = new SearchRequest("hotel");\n        // 2.准备DSL\n        // 2.1.query\n        buildBasicQuery(params, request);\n\n        // 2.2.分页\n        int page = params.getPage();\n        int size = params.getSize();\n        request.source().from((page - 1) * size).size(size);\n\n        // 2.3.排序\n        String location = params.getLocation();\n        if (location != null && !location.equals("")) {\n            request.source().sort(SortBuilders\n                                  .geoDistanceSort("location", new GeoPoint(location))\n                                  .order(SortOrder.ASC)\n                                  .unit(DistanceUnit.KILOMETERS)\n                                 );\n        }\n\n        // 3.发送请求\n        SearchResponse response = client.search(request, RequestOptions.DEFAULT);\n        // 4.解析响应\n        return handleResponse(response);\n    } catch (IOException e) {\n        throw new RuntimeException(e);\n    }\n}\n\n\n# 排序距离显示\n\n重启服务后，测试我的酒店功能：\n\n\n\n发现确实可以实现对我附近酒店的排序，不过并没有看到酒店到底距离我多远，这该怎么办？\n\n排序完成后，页面还要获取我附近每个酒店的具体距离值，这个值在响应结果中是独立的：\n\n\n\n因此，我们在结果解析阶段，除了解析source部分以外，还要得到sort部分，也就是排序的距离，然后放到响应结果中。\n\n我们要做两件事：\n\n * 修改HotelDoc，添加排序距离字段，用于页面显示\n * 修改HotelService类中的handleResponse方法，添加对sort值的获取\n\n1）修改HotelDoc类，添加距离字段\n\npackage cn.itcast.hotel.pojo;\n\nimport lombok.Data;\nimport lombok.NoArgsConstructor;\n\n\n@Data\n@NoArgsConstructor\npublic class HotelDoc {\n    private Long id;\n    private String name;\n    private String address;\n    private Integer price;\n    private Integer score;\n    private String brand;\n    private String city;\n    private String starName;\n    private String business;\n    private String location;\n    private String pic;\n    // 排序时的 距离值\n    private Object distance;\n\n    public HotelDoc(Hotel hotel) {\n        this.id = hotel.getId();\n        this.name = hotel.getName();\n        this.address = hotel.getAddress();\n        this.price = hotel.getPrice();\n        this.score = hotel.getScore();\n        this.brand = hotel.getBrand();\n        this.city = hotel.getCity();\n        this.starName = hotel.getStarName();\n        this.business = hotel.getBusiness();\n        this.location = hotel.getLatitude() + ", " + hotel.getLongitude();\n        this.pic = hotel.getPic();\n    }\n}\n\n\n\n2）修改HotelService中的handleResponse方法\n\n\n\n重启后测试，发现页面能成功显示距离了：\n\n\n\n\n# 酒店竞价排名\n\n需求：让指定的酒店在搜索结果中排名置顶\n\n# 需求分析\n\n要让指定酒店在搜索结果中排名置顶，效果如图：\n\n\n\n页面会给指定的酒店添加广告标记。\n\n那怎样才能让指定的酒店排名置顶呢？\n\n我们之前学习过的function_score查询可以影响算分，算分高了，自然排名也就高了。而function_score包含3个要素：\n\n * 过滤条件：哪些文档要加分\n * 算分函数：如何计算function score\n * 加权方式：function score 与 query score如何运算\n\n这里的需求是：让指定酒店排名靠前。因此我们需要给这些酒店添加一个标记，这样在过滤条件中就可以根据这个标记来判断，是否要提高算分。\n\n比如，我们给酒店添加一个字段：isAD，Boolean类型：\n\n * true：是广告\n * false：不是广告\n\n这样function_score包含3个要素就很好确定了：\n\n * 过滤条件：判断isAD 是否为true\n * 算分函数：我们可以用最简单暴力的weight，固定加权值\n * 加权方式：可以用默认的相乘，大大提高算分\n\n因此，业务的实现步骤包括：\n\n 1. 给HotelDoc类添加isAD字段，Boolean类型\n\n 2. 挑选几个你喜欢的酒店，给它的文档数据添加isAD字段，值为true\n\n 3. 修改search方法，添加function score功能，给isAD值为true的酒店增加权重\n\n# 修改HotelDoc实体\n\n给cn.itcast.hotel.pojo包下的HotelDoc类添加isAD字段：\n\n\n\n# 添加广告标记\n\n接下来，我们挑几个酒店，添加isAD字段，设置为true：\n\nPOST /hotel/_update/1902197537\n{\n    "doc": {\n        "isAD": true\n    }\n}\nPOST /hotel/_update/2056126831\n{\n    "doc": {\n        "isAD": true\n    }\n}\nPOST /hotel/_update/1989806195\n{\n    "doc": {\n        "isAD": true\n    }\n}\nPOST /hotel/_update/2056105938\n{\n    "doc": {\n        "isAD": true\n    }\n}\n\n\n# 添加算分函数查询\n\n接下来我们就要修改查询条件了。之前是用的boolean 查询，现在要改成function_socre查询。\n\nfunction_score查询结构如下：\n\n\n\n对应的JavaAPI如下：\n\n\n\n我们可以将之前写的boolean查询作为原始查询条件放到query中，接下来就是添加过滤条件、算分函数、加权模式了。所以原来的代码依然可以沿用。\n\n修改cn.itcast.hotel.service.impl包下的HotelService类中的buildBasicQuery方法，添加算分函数查询：\n\nprivate void buildBasicQuery(RequestParams params, SearchRequest request) {\n    // 1.构建BooleanQuery\n    BoolQueryBuilder boolQuery = QueryBuilders.boolQuery();\n    // 关键字搜索\n    String key = params.getKey();\n    if (key ** null || "".equals(key)) {\n        boolQuery.must(QueryBuilders.matchAllQuery());\n    } else {\n        boolQuery.must(QueryBuilders.matchQuery("all", key));\n    }\n    // 城市条件\n    if (params.getCity() != null && !params.getCity().equals("")) {\n        boolQuery.filter(QueryBuilders.termQuery("city", params.getCity()));\n    }\n    // 品牌条件\n    if (params.getBrand() != null && !params.getBrand().equals("")) {\n        boolQuery.filter(QueryBuilders.termQuery("brand", params.getBrand()));\n    }\n    // 星级条件\n    if (params.getStarName() != null && !params.getStarName().equals("")) {\n        boolQuery.filter(QueryBuilders.termQuery("starName", params.getStarName()));\n    }\n    // 价格\n    if (params.getMinPrice() != null && params.getMaxPrice() != null) {\n        boolQuery.filter(QueryBuilders\n                         .rangeQuery("price")\n                         .gte(params.getMinPrice())\n                         .lte(params.getMaxPrice())\n                        );\n    }\n\n    // TODO 2.算分控制\n    FunctionScoreQueryBuilder functionScoreQuery =\n        QueryBuilders.functionScoreQuery(\n        // 原始查询，相关性算分的查询\n        boolQuery,\n        // function score的数组\n        new FunctionScoreQueryBuilder.FilterFunctionBuilder[]{\n            // 其中的一个function score 元素\n            new FunctionScoreQueryBuilder.FilterFunctionBuilder(\n                // 过滤条件\n                QueryBuilders.termQuery("isAD", true),\n                // 算分函数\n                ScoreFunctionBuilders.weightFactorFunction(10)\n            )\n        });\n    request.source().query(functionScoreQuery);\n}\n',normalizedContent:'下面，我们通过黑马旅游的案例来实战演练下之前学习的知识。\n\n我们实现四部分功能：\n\n * 酒店搜索和分页\n * 酒店结果过滤\n * 我周边的酒店\n * 酒店竞价排名\n\n启动我们提供的hotel-demo项目，其默认端口是8090，访问http://localhost:8090，就能看到项目页面了：\n\n\n\n\n# 酒店搜索和分页\n\n案例需求：实现黑马旅游的酒店搜索功能，完成关键字搜索和分页\n\n# 需求分析\n\n在项目的首页，有一个大大的搜索框，还有分页按钮：\n\n\n\n点击搜索按钮，可以看到浏览器控制台发出了请求：\n\n\n\n请求参数如下：\n\n\n\n由此可以知道，我们这个请求的信息如下：\n\n * 请求方式：post\n * 请求路径：/hotel/list\n * 请求参数：json对象，包含4个字段：\n   * key：搜索关键字\n   * page：页码\n   * size：每页大小\n   * sortby：排序，目前暂不实现\n * 返回值：分页查询，需要返回分页结果pageresult，包含两个属性：\n   * total：总条数\n   * list<hoteldoc>：当前页的数据\n\n因此，我们实现业务的流程如下：\n\n * 步骤一：定义实体类，接收请求参数的json对象\n * 步骤二：编写controller，接收页面的请求\n * 步骤三：编写业务实现，利用resthighlevelclient实现搜索、分页\n\n# 定义实体类\n\n实体类有两个，一个是前端的请求参数实体，一个是服务端应该返回的响应结果实体。\n\n1）请求参数\n\n前端请求的json结构如下：\n\n{\n    "key": "搜索关键字",\n    "page": 1,\n    "size": 3,\n    "sortby": "default"\n}\n\n\n因此，我们在cn.itcast.hotel.pojo包下定义一个实体类：\n\npackage cn.itcast.hotel.pojo;\n\nimport lombok.data;\n\n@data\npublic class requestparams {\n    private string key;\n    private integer page;\n    private integer size;\n    private string sortby;\n}\n\n\n2）返回值\n\n分页查询，需要返回分页结果pageresult，包含两个属性：\n\n * total：总条数\n * list<hoteldoc>：当前页的数据\n\n因此，我们在cn.itcast.hotel.pojo中定义返回结果：\n\npackage cn.itcast.hotel.pojo;\n\nimport lombok.data;\n\nimport java.util.list;\n\n@data\npublic class pageresult {\n    private long total;\n    private list<hoteldoc> hotels;\n\n    public pageresult() {\n    }\n\n    public pageresult(long total, list<hoteldoc> hotels) {\n        this.total = total;\n        this.hotels = hotels;\n    }\n}\n\n\n# 定义controller\n\n定义一个hotelcontroller，声明查询接口，满足下列要求：\n\n * 请求方式：post\n * 请求路径：/hotel/list\n * 请求参数：对象，类型为requestparam\n * 返回值：pageresult，包含两个属性\n   * long total：总条数\n   * list<hoteldoc> hotels：酒店数据\n\n因此，我们在cn.itcast.hotel.web中定义hotelcontroller：\n\n@restcontroller\n@requestmapping("/hotel")\npublic class hotelcontroller {\n\n    @autowired\n    private ihotelservice hotelservice;\n\t// 搜索酒店数据\n    @postmapping("/list")\n    public pageresult search(@requestbody requestparams params){\n        return hotelservice.search(params);\n    }\n}\n\n\n# 实现搜索业务\n\n我们在controller调用了ihotelservice，并没有实现该方法，因此下面我们就在ihotelservice中定义方法，并且去实现业务逻辑。\n\n1）在cn.itcast.hotel.service中的ihotelservice接口中定义一个方法：\n\n/**\n * 根据关键字搜索酒店信息\n * @param params 请求参数对象，包含用户输入的关键字 \n * @return 酒店文档列表\n */\npageresult search(requestparams params);\n\n\n2）实现搜索业务，肯定离不开resthighlevelclient，我们需要把它注册到spring中作为一个bean。在cn.itcast.hotel中的hoteldemoapplication中声明这个bean：\n\n@bean\npublic resthighlevelclient client(){\n    return  new resthighlevelclient(restclient.builder(\n        httphost.create("http://192.168.150.101:9200")\n    ));\n}\n\n\n3）在cn.itcast.hotel.service.impl中的hotelservice中实现search方法：\n\n@override\npublic pageresult search(requestparams params) {\n    try {\n        // 1.准备request\n        searchrequest request = new searchrequest("hotel");\n        // 2.准备dsl\n        // 2.1.query\n        string key = params.getkey();\n        if (key ** null || "".equals(key)) {\n            boolquery.must(querybuilders.matchallquery());\n        } else {\n            boolquery.must(querybuilders.matchquery("all", key));\n        }\n\n        // 2.2.分页\n        int page = params.getpage();\n        int size = params.getsize();\n        request.source().from((page - 1) * size).size(size);\n\n        // 3.发送请求\n        searchresponse response = client.search(request, requestoptions.default);\n        // 4.解析响应\n        return handleresponse(response);\n    } catch (ioexception e) {\n        throw new runtimeexception(e);\n    }\n}\n\n// 结果解析\nprivate pageresult handleresponse(searchresponse response) {\n    // 4.解析响应\n    searchhits searchhits = response.gethits();\n    // 4.1.获取总条数\n    long total = searchhits.gettotalhits().value;\n    // 4.2.文档数组\n    searchhit[] hits = searchhits.gethits();\n    // 4.3.遍历\n    list<hoteldoc> hotels = new arraylist<>();\n    for (searchhit hit : hits) {\n        // 获取文档source\n        string json = hit.getsourceasstring();\n        // 反序列化\n        hoteldoc hoteldoc = json.parseobject(json, hoteldoc.class);\n\t\t// 放入集合\n        hotels.add(hoteldoc);\n    }\n    // 4.4.封装返回\n    return new pageresult(total, hotels);\n}\n\n\n# 问题总结\n\n * 注入bean 的形式注册客户端\n   \n   //todo 注入bean 的形式注册客户端\n   @bean\n   public resthighlevelclient client() {\n       return new resthighlevelclient(restclient.builder(\n           httphost.create("http://192.168.159.100:9200")\n       ));\n   }\n   \n\n * 将复杂的查询操作 放到service来实现,\n   \n   * 具体的业务实现代码 放到service层\n   \n   @postmapping("/list")\n   public pageresult search(@requestbody requestparams requestparams) throws ioexception {\n       system.out.println(requestparams);\n       return hotelservice.search(requestparams);\n   }\n   \n\n * 查询 todo 健壮性处理 考虑key为空的情况\n   \n   //查询 todo 健壮性处理 考虑key为空的情况\n   if ("".equals(key) || key ** null) {//key 为空 查询全部\n       request.source().query(querybuilders.matchallquery());\n   } else {//不为空，从聚合字段all中 查询key值\n       request.source().query(querybuilders.matchquery("all", key));\n   }\n   \n\n * 使用构造函数封装\n   \n   * 也可以使用set方法来单独封装数据，但是还是推荐使用这种方法\n   \n   // 构造函数\n   public pageresult(long total, list<hoteldoc> hotels) {\n       this.total = total;\n       this.hotels = hotels;\n   }\n   \n   \n   //使用构造函数封装 todo\n   return new pageresult(value, hoteldocs);\n   \n\n * 业务中的异常不能直接抛出，要捕获它，然后统一处理\n\n\n# 酒店结果过滤\n\n需求：添加品牌、城市、星级、价格等过滤功能\n\n# 需求分析\n\n在页面搜索框下面，会有一些过滤项：\n\n\n\n传递的参数如图：\n\n\n\n包含的过滤条件有：\n\n * brand：品牌值\n * city：城市\n * minprice~maxprice：价格范围\n * starname：星级\n\n我们需要做两件事情：\n\n * 修改请求参数的对象requestparams，接收上述参数\n * 修改业务逻辑，在搜索条件之外，添加一些过滤条件\n\n# 修改实体类\n\n修改在cn.itcast.hotel.pojo包下的实体类requestparams：\n\n@data\npublic class requestparams {\n    private string key;\n    private integer page;\n    private integer size;\n    private string sortby;\n    // 下面是新增的过滤条件参数\n    private string city;\n    private string brand;\n    private string starname;\n    private integer minprice;\n    private integer maxprice;\n}\n\n\n# 修改搜索业务\n\n在hotelservice的search方法中，只有一个地方需要修改：requet.source().query( ... )其中的查询条件。\n\n在之前的业务中，只有match查询，根据关键字搜索，现在要添加条件过滤，包括：\n\n * 品牌过滤：是keyword类型，用term查询\n * 星级过滤：是keyword类型，用term查询\n * 价格过滤：是数值类型，用range查询\n * 城市过滤：是keyword类型，用term查询\n\n多个查询条件组合，肯定是boolean查询来组合：\n\n * 关键字搜索放到must中，参与算分\n * 其它过滤条件放到filter中，不参与算分\n\n因为条件构建的逻辑比较复杂，这里先封装为一个函数：\n\n\n\nbuildbasicquery的代码如下：\n\nprivate void buildbasicquery(requestparams params, searchrequest request) {\n    // 1.构建booleanquery\n    boolquerybuilder boolquery = querybuilders.boolquery();\n    // 2.关键字搜索\n    string key = params.getkey();\n    if (key ** null || "".equals(key)) {\n        boolquery.must(querybuilders.matchallquery());\n    } else {\n        boolquery.must(querybuilders.matchquery("all", key));\n    }\n    // 3.城市条件\n    if (params.getcity() != null && !params.getcity().equals("")) {\n        boolquery.filter(querybuilders.termquery("city", params.getcity()));\n    }\n    // 4.品牌条件\n    if (params.getbrand() != null && !params.getbrand().equals("")) {\n        boolquery.filter(querybuilders.termquery("brand", params.getbrand()));\n    }\n    // 5.星级条件\n    if (params.getstarname() != null && !params.getstarname().equals("")) {\n        boolquery.filter(querybuilders.termquery("starname", params.getstarname()));\n    }\n\t// 6.价格\n    if (params.getminprice() != null && params.getmaxprice() != null) {\n        boolquery.filter(querybuilders\n                         .rangequery("price")\n                         .gte(params.getminprice())\n                         .lte(params.getmaxprice())\n                        );\n    }\n\t// 7.放入source\n    request.source().query(boolquery);\n}\n\n\n\n# 我周边的酒店\n\n需求：我附近的酒店\n\n# 需求分析\n\n在酒店列表页的右侧，有一个小地图，点击地图的定位按钮，地图会找到你所在的位置：\n\n\n\n并且，在前端会发起查询请求，将你的坐标发送到服务端：\n\n\n\n我们要做的事情就是基于这个location坐标，然后按照距离对周围酒店排序。实现思路如下：\n\n * 修改requestparams参数，接收location字段\n * 修改search方法业务逻辑，如果location有值，添加根据geo_distance排序的功能\n\n# 修改实体类\n\n修改在cn.itcast.hotel.pojo包下的实体类requestparams：\n\npackage cn.itcast.hotel.pojo;\n\nimport lombok.data;\n\n@data\npublic class requestparams {\n    private string key;\n    private integer page;\n    private integer size;\n    private string sortby;\n    private string city;\n    private string brand;\n    private string starname;\n    private integer minprice;\n    private integer maxprice;\n    // 我当前的地理坐标\n    private string location;\n}\n\n\n\n# 距离排序api\n\n我们以前学习过排序功能，包括两种：\n\n * 普通字段排序\n * 地理坐标排序\n\n我们只讲了普通字段排序对应的java写法。地理坐标排序只学过dsl语法，如下：\n\nget /indexname/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "sort": [\n    {\n      "price": "asc"  \n    },\n    {\n      "_geo_distance" : {\n          "field" : "纬度，经度",\n          "order" : "asc",\n          "unit" : "km"\n      }\n    }\n  ]\n}\n\n\n对应的java代码示例：\n\n\n\n# 添加距离排序\n\n在cn.itcast.hotel.service.impl的hotelservice的search方法中，添加一个排序功能：\n\n\n\n完整代码：\n\n@override\npublic pageresult search(requestparams params) {\n    try {\n        // 1.准备request\n        searchrequest request = new searchrequest("hotel");\n        // 2.准备dsl\n        // 2.1.query\n        buildbasicquery(params, request);\n\n        // 2.2.分页\n        int page = params.getpage();\n        int size = params.getsize();\n        request.source().from((page - 1) * size).size(size);\n\n        // 2.3.排序\n        string location = params.getlocation();\n        if (location != null && !location.equals("")) {\n            request.source().sort(sortbuilders\n                                  .geodistancesort("location", new geopoint(location))\n                                  .order(sortorder.asc)\n                                  .unit(distanceunit.kilometers)\n                                 );\n        }\n\n        // 3.发送请求\n        searchresponse response = client.search(request, requestoptions.default);\n        // 4.解析响应\n        return handleresponse(response);\n    } catch (ioexception e) {\n        throw new runtimeexception(e);\n    }\n}\n\n\n# 排序距离显示\n\n重启服务后，测试我的酒店功能：\n\n\n\n发现确实可以实现对我附近酒店的排序，不过并没有看到酒店到底距离我多远，这该怎么办？\n\n排序完成后，页面还要获取我附近每个酒店的具体距离值，这个值在响应结果中是独立的：\n\n\n\n因此，我们在结果解析阶段，除了解析source部分以外，还要得到sort部分，也就是排序的距离，然后放到响应结果中。\n\n我们要做两件事：\n\n * 修改hoteldoc，添加排序距离字段，用于页面显示\n * 修改hotelservice类中的handleresponse方法，添加对sort值的获取\n\n1）修改hoteldoc类，添加距离字段\n\npackage cn.itcast.hotel.pojo;\n\nimport lombok.data;\nimport lombok.noargsconstructor;\n\n\n@data\n@noargsconstructor\npublic class hoteldoc {\n    private long id;\n    private string name;\n    private string address;\n    private integer price;\n    private integer score;\n    private string brand;\n    private string city;\n    private string starname;\n    private string business;\n    private string location;\n    private string pic;\n    // 排序时的 距离值\n    private object distance;\n\n    public hoteldoc(hotel hotel) {\n        this.id = hotel.getid();\n        this.name = hotel.getname();\n        this.address = hotel.getaddress();\n        this.price = hotel.getprice();\n        this.score = hotel.getscore();\n        this.brand = hotel.getbrand();\n        this.city = hotel.getcity();\n        this.starname = hotel.getstarname();\n        this.business = hotel.getbusiness();\n        this.location = hotel.getlatitude() + ", " + hotel.getlongitude();\n        this.pic = hotel.getpic();\n    }\n}\n\n\n\n2）修改hotelservice中的handleresponse方法\n\n\n\n重启后测试，发现页面能成功显示距离了：\n\n\n\n\n# 酒店竞价排名\n\n需求：让指定的酒店在搜索结果中排名置顶\n\n# 需求分析\n\n要让指定酒店在搜索结果中排名置顶，效果如图：\n\n\n\n页面会给指定的酒店添加广告标记。\n\n那怎样才能让指定的酒店排名置顶呢？\n\n我们之前学习过的function_score查询可以影响算分，算分高了，自然排名也就高了。而function_score包含3个要素：\n\n * 过滤条件：哪些文档要加分\n * 算分函数：如何计算function score\n * 加权方式：function score 与 query score如何运算\n\n这里的需求是：让指定酒店排名靠前。因此我们需要给这些酒店添加一个标记，这样在过滤条件中就可以根据这个标记来判断，是否要提高算分。\n\n比如，我们给酒店添加一个字段：isad，boolean类型：\n\n * true：是广告\n * false：不是广告\n\n这样function_score包含3个要素就很好确定了：\n\n * 过滤条件：判断isad 是否为true\n * 算分函数：我们可以用最简单暴力的weight，固定加权值\n * 加权方式：可以用默认的相乘，大大提高算分\n\n因此，业务的实现步骤包括：\n\n 1. 给hoteldoc类添加isad字段，boolean类型\n\n 2. 挑选几个你喜欢的酒店，给它的文档数据添加isad字段，值为true\n\n 3. 修改search方法，添加function score功能，给isad值为true的酒店增加权重\n\n# 修改hoteldoc实体\n\n给cn.itcast.hotel.pojo包下的hoteldoc类添加isad字段：\n\n\n\n# 添加广告标记\n\n接下来，我们挑几个酒店，添加isad字段，设置为true：\n\npost /hotel/_update/1902197537\n{\n    "doc": {\n        "isad": true\n    }\n}\npost /hotel/_update/2056126831\n{\n    "doc": {\n        "isad": true\n    }\n}\npost /hotel/_update/1989806195\n{\n    "doc": {\n        "isad": true\n    }\n}\npost /hotel/_update/2056105938\n{\n    "doc": {\n        "isad": true\n    }\n}\n\n\n# 添加算分函数查询\n\n接下来我们就要修改查询条件了。之前是用的boolean 查询，现在要改成function_socre查询。\n\nfunction_score查询结构如下：\n\n\n\n对应的javaapi如下：\n\n\n\n我们可以将之前写的boolean查询作为原始查询条件放到query中，接下来就是添加过滤条件、算分函数、加权模式了。所以原来的代码依然可以沿用。\n\n修改cn.itcast.hotel.service.impl包下的hotelservice类中的buildbasicquery方法，添加算分函数查询：\n\nprivate void buildbasicquery(requestparams params, searchrequest request) {\n    // 1.构建booleanquery\n    boolquerybuilder boolquery = querybuilders.boolquery();\n    // 关键字搜索\n    string key = params.getkey();\n    if (key ** null || "".equals(key)) {\n        boolquery.must(querybuilders.matchallquery());\n    } else {\n        boolquery.must(querybuilders.matchquery("all", key));\n    }\n    // 城市条件\n    if (params.getcity() != null && !params.getcity().equals("")) {\n        boolquery.filter(querybuilders.termquery("city", params.getcity()));\n    }\n    // 品牌条件\n    if (params.getbrand() != null && !params.getbrand().equals("")) {\n        boolquery.filter(querybuilders.termquery("brand", params.getbrand()));\n    }\n    // 星级条件\n    if (params.getstarname() != null && !params.getstarname().equals("")) {\n        boolquery.filter(querybuilders.termquery("starname", params.getstarname()));\n    }\n    // 价格\n    if (params.getminprice() != null && params.getmaxprice() != null) {\n        boolquery.filter(querybuilders\n                         .rangequery("price")\n                         .gte(params.getminprice())\n                         .lte(params.getmaxprice())\n                        );\n    }\n\n    // todo 2.算分控制\n    functionscorequerybuilder functionscorequery =\n        querybuilders.functionscorequery(\n        // 原始查询，相关性算分的查询\n        boolquery,\n        // function score的数组\n        new functionscorequerybuilder.filterfunctionbuilder[]{\n            // 其中的一个function score 元素\n            new functionscorequerybuilder.filterfunctionbuilder(\n                // 过滤条件\n                querybuilders.termquery("isad", true),\n                // 算分函数\n                scorefunctionbuilders.weightfactorfunction(10)\n            )\n        });\n    request.source().query(functionscorequery);\n}\n',charsets:{cjk:!0}},{title:"RabbitMQ入门",frontmatter:{autoSort:99,title:"RabbitMQ入门",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/bdbadd/",categories:["后端","微服务","RabbitMQ"],tags:["知识","微服务","MQ"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/30.RabbitMQ/10.RabbitMQ%E5%85%A5%E9%97%A8.html",relativePath:"01.后端/60.微服务/30.RabbitMQ/10.RabbitMQ入门.md",key:"v-d2252dc2",path:"/pages/bdbadd/",headers:[{level:2,title:"RabbitMQ基本结构",slug:"rabbitmq基本结构",normalizedTitle:"rabbitmq基本结构",charIndex:2},{level:2,title:"RabbitMQ消息模型",slug:"rabbitmq消息模型",normalizedTitle:"rabbitmq消息模型",charIndex:171},{level:2,title:"入门案例",slug:"入门案例",normalizedTitle:"入门案例",charIndex:228},{level:3,title:"publisher实现",slug:"publisher实现",normalizedTitle:"publisher实现",charIndex:377},{level:3,title:"consumer实现",slug:"consumer实现",normalizedTitle:"consumer实现",charIndex:1704},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:3143}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"RabbitMQ基本结构 RabbitMQ消息模型 入门案例 publisher实现 consumer实现 总结",content:'# RabbitMQ基本结构\n\nMQ的基本结构：\n\n\n\nRabbitMQ中的一些角色：\n\n * publisher：生产者\n * consumer：消费者\n * exchange个：交换机，负责消息路由\n * queue：队列，存储消息\n * virtualHost：虚拟主机，隔离不同租户的exchange、queue、消息的隔离\n\n\n# RabbitMQ消息模型\n\nRabbitMQ官方提供了5个不同的Demo示例，对应了不同的消息模型：\n\n\n\n\n# 入门案例\n\n简单队列模式的模型图：\n\n\n\n官方的HelloWorld是基于最基础的消息队列模型来实现的，只包括三个角色：\n\n * publisher：消息发布者，将消息发送到队列queue\n * queue：消息队列，负责接受并缓存消息\n * consumer：订阅队列，处理队列中的消息\n\n\n# publisher实现\n\n思路：\n\n * 建立连接\n * 创建Channel\n * 声明队列\n * 发送消息\n * 关闭连接和channel\n\n代码实现：\n\npackage cn.itcast.mq.helloworld;\n\nimport com.rabbitmq.client.Channel;\nimport com.rabbitmq.client.Connection;\nimport com.rabbitmq.client.ConnectionFactory;\nimport org.junit.Test;\n\nimport java.io.IOException;\nimport java.util.concurrent.TimeoutException;\n\npublic class PublisherTest {\n    @Test\n    public void testSendMessage() throws IOException, TimeoutException {\n        // 1.建立连接\n        ConnectionFactory factory = new ConnectionFactory();\n        // 1.1.设置连接参数，分别是：主机名、端口号、vhost、用户名、密码\n        factory.setHost("192.168.159.100");\n        factory.setPort(5672);\n        factory.setVirtualHost("/");\n        factory.setUsername("diana");\n        factory.setPassword("123321");\n        // 1.2.建立连接\n        Connection connection = factory.newConnection();\n\n        // 2.创建通道Channel\n        Channel channel = connection.createChannel();\n\n        // 3.创建队列\n        String queueName = "simple.queue";\n        channel.queueDeclare(queueName, false, false, false, null);\n\n        // 4.发送消息\n        String message = "hello, rabbitmq!";\n        channel.basicPublish("", queueName, null, message.getBytes());\n        System.out.println("发送消息成功：【" + message + "】");\n\n        // 5.关闭通道和连接\n        channel.close();\n        connection.close();\n\n    }\n}\n\n\n\n# consumer实现\n\n代码思路：\n\n * 建立连接\n * 创建Channel\n * 声明队列\n * 订阅消息\n\n代码实现：\n\npackage cn.itcast.mq.helloworld;\n\nimport com.rabbitmq.client.*;\n\nimport java.io.IOException;\nimport java.util.concurrent.TimeoutException;\n\npublic class ConsumerTest {\n\n    public static void main(String[] args) throws IOException, TimeoutException {\n        // 1.建立连接\n        ConnectionFactory factory = new ConnectionFactory();\n        // 1.1.设置连接参数，分别是：主机名、端口号、vhost、用户名、密码\n        factory.setHost("192.168.159.100");\n        factory.setPort(5672);\n        factory.setVirtualHost("/");\n        factory.setUsername("diana");\n        factory.setPassword("123321");\n        // 1.2.建立连接\n        Connection connection = factory.newConnection();\n\n        // 2.创建通道Channel\n        Channel channel = connection.createChannel();\n\n        // 3.创建队列\n        String queueName = "simple.queue";\n        channel.queueDeclare(queueName, false, false, false, null);\n\n        // 4.订阅消息\n        channel.basicConsume(queueName, true, new DefaultConsumer(channel){\n            @Override\n            public void handleDelivery(String consumerTag, Envelope envelope,\n                                       AMQP.BasicProperties properties, byte[] body) throws IOException {\n                // 5.处理消息\n                String message = new String(body);\n                System.out.println("接收到消息：【" + message + "】");\n            }\n        });\n        System.out.println("等待接收消息。。。。");\n    }\n}\n\n\n\n# 总结\n\n基本消息队列的消息发送流程：\n\n 1. 建立connection\n\n 2. 创建channel\n\n 3. 利用channel声明队列\n\n 4. 利用channel向队列发送消息\n\n基本消息队列的消息接收流程：\n\n 1. 建立connection\n\n 2. 创建channel\n\n 3. 利用channel声明队列————再次声明队列是为了保险，如果名字一样的话，不会创建新的队列\n\n 4. 定义consumer的消费行为handleDelivery()\n\n 5. 利用channel将消费者与队列绑定',normalizedContent:'# rabbitmq基本结构\n\nmq的基本结构：\n\n\n\nrabbitmq中的一些角色：\n\n * publisher：生产者\n * consumer：消费者\n * exchange个：交换机，负责消息路由\n * queue：队列，存储消息\n * virtualhost：虚拟主机，隔离不同租户的exchange、queue、消息的隔离\n\n\n# rabbitmq消息模型\n\nrabbitmq官方提供了5个不同的demo示例，对应了不同的消息模型：\n\n\n\n\n# 入门案例\n\n简单队列模式的模型图：\n\n\n\n官方的helloworld是基于最基础的消息队列模型来实现的，只包括三个角色：\n\n * publisher：消息发布者，将消息发送到队列queue\n * queue：消息队列，负责接受并缓存消息\n * consumer：订阅队列，处理队列中的消息\n\n\n# publisher实现\n\n思路：\n\n * 建立连接\n * 创建channel\n * 声明队列\n * 发送消息\n * 关闭连接和channel\n\n代码实现：\n\npackage cn.itcast.mq.helloworld;\n\nimport com.rabbitmq.client.channel;\nimport com.rabbitmq.client.connection;\nimport com.rabbitmq.client.connectionfactory;\nimport org.junit.test;\n\nimport java.io.ioexception;\nimport java.util.concurrent.timeoutexception;\n\npublic class publishertest {\n    @test\n    public void testsendmessage() throws ioexception, timeoutexception {\n        // 1.建立连接\n        connectionfactory factory = new connectionfactory();\n        // 1.1.设置连接参数，分别是：主机名、端口号、vhost、用户名、密码\n        factory.sethost("192.168.159.100");\n        factory.setport(5672);\n        factory.setvirtualhost("/");\n        factory.setusername("diana");\n        factory.setpassword("123321");\n        // 1.2.建立连接\n        connection connection = factory.newconnection();\n\n        // 2.创建通道channel\n        channel channel = connection.createchannel();\n\n        // 3.创建队列\n        string queuename = "simple.queue";\n        channel.queuedeclare(queuename, false, false, false, null);\n\n        // 4.发送消息\n        string message = "hello, rabbitmq!";\n        channel.basicpublish("", queuename, null, message.getbytes());\n        system.out.println("发送消息成功：【" + message + "】");\n\n        // 5.关闭通道和连接\n        channel.close();\n        connection.close();\n\n    }\n}\n\n\n\n# consumer实现\n\n代码思路：\n\n * 建立连接\n * 创建channel\n * 声明队列\n * 订阅消息\n\n代码实现：\n\npackage cn.itcast.mq.helloworld;\n\nimport com.rabbitmq.client.*;\n\nimport java.io.ioexception;\nimport java.util.concurrent.timeoutexception;\n\npublic class consumertest {\n\n    public static void main(string[] args) throws ioexception, timeoutexception {\n        // 1.建立连接\n        connectionfactory factory = new connectionfactory();\n        // 1.1.设置连接参数，分别是：主机名、端口号、vhost、用户名、密码\n        factory.sethost("192.168.159.100");\n        factory.setport(5672);\n        factory.setvirtualhost("/");\n        factory.setusername("diana");\n        factory.setpassword("123321");\n        // 1.2.建立连接\n        connection connection = factory.newconnection();\n\n        // 2.创建通道channel\n        channel channel = connection.createchannel();\n\n        // 3.创建队列\n        string queuename = "simple.queue";\n        channel.queuedeclare(queuename, false, false, false, null);\n\n        // 4.订阅消息\n        channel.basicconsume(queuename, true, new defaultconsumer(channel){\n            @override\n            public void handledelivery(string consumertag, envelope envelope,\n                                       amqp.basicproperties properties, byte[] body) throws ioexception {\n                // 5.处理消息\n                string message = new string(body);\n                system.out.println("接收到消息：【" + message + "】");\n            }\n        });\n        system.out.println("等待接收消息。。。。");\n    }\n}\n\n\n\n# 总结\n\n基本消息队列的消息发送流程：\n\n 1. 建立connection\n\n 2. 创建channel\n\n 3. 利用channel声明队列\n\n 4. 利用channel向队列发送消息\n\n基本消息队列的消息接收流程：\n\n 1. 建立connection\n\n 2. 创建channel\n\n 3. 利用channel声明队列————再次声明队列是为了保险，如果名字一样的话，不会创建新的队列\n\n 4. 定义consumer的消费行为handledelivery()\n\n 5. 利用channel将消费者与队列绑定',charsets:{cjk:!0}},{title:"RabbitMQ",frontmatter:{autoSort:100,title:"RabbitMQ",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/0af5eb/",categories:["后端","微服务","RabbitMQ"],tags:["知识","微服务","MQ"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/30.RabbitMQ/05.RabbitMQ%E4%BB%8B%E7%BB%8D.html",relativePath:"01.后端/60.微服务/30.RabbitMQ/05.RabbitMQ介绍.md",key:"v-508e8e26",path:"/pages/0af5eb/",headers:[{level:2,title:"同步和异步通讯",slug:"同步和异步通讯",normalizedTitle:"同步和异步通讯",charIndex:2},{level:3,title:"同步通讯",slug:"同步通讯",normalizedTitle:"同步通讯",charIndex:30},{level:3,title:"异步通讯",slug:"异步通讯",normalizedTitle:"异步通讯",charIndex:5},{level:3,title:"同步与异步的选用",slug:"同步与异步的选用",normalizedTitle:"同步与异步的选用",charIndex:938},{level:2,title:"技术对比：",slug:"技术对比",normalizedTitle:"技术对比：",charIndex:1039}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"同步和异步通讯 同步通讯 异步通讯 同步与异步的选用 技术对比：",content:"# 同步和异步通讯\n\n微服务间通讯有同步和异步两种方式：\n\n同步通讯：就像打电话，需要实时响应。\n\n异步通讯：就像发邮件，不需要马上回复。\n\n\n\n两种方式各有优劣，打电话可以立即得到响应，但是你却不能跟多个人同时通话。发送邮件可以同时与多个人收发邮件，但是往往响应会有延迟。\n\n\n# 同步通讯\n\n我们之前学习的Feign调用就属于同步方式，虽然调用可以实时得到结果，但存在下面的问题：\n\n\n\n总结：\n\n同步调用的优点：\n\n * 时效性较强，可以立即得到结果\n\n同步调用的问题：\n\n * 耦合度高\n * 性能和吞吐能力下降\n * 有额外的资源消耗\n * 有级联失败问题\n\n\n# 异步通讯\n\n异步调用则可以避免上述问题：\n\n我们以购买商品为例，用户支付后需要调用订单服务完成订单状态修改，调用物流服务，从仓库分配响应的库存并准备发货。\n\n在事件模式中，支付服务是事件发布者（publisher），在支付完成后只需要发布一个支付成功的事件（event），事件中带上订单id。\n\n订单服务和物流服务是事件订阅者（Consumer），订阅支付成功的事件，监听到事件后完成自己业务即可。\n\n为了解除事件发布者与订阅者之间的耦合，两者并不是直接通信，而是有一个中间人（Broker）。发布者发布事件到Broker，不关心谁来订阅事件。订阅者从Broker订阅事件，不关心谁发来的消息。\n\n\n\nBroker 是一个像数据总线一样的东西，所有的服务要接收数据和发送数据都发到这个总线上，这个总线就像协议一样，让服务间的通讯变得标准和可控。\n\n好处：\n\n * 吞吐量提升：无需等待订阅者处理完成，响应更快速\n\n * 故障隔离：服务没有直接调用，不存在级联失败问题\n\n * 调用间没有阻塞，不会造成无效的资源占用\n\n * 耦合度极低，每个服务都可以灵活插拔，可替换\n\n * 流量削峰：不管发布事件的流量波动多大，都由Broker接收，订阅者可以按照自己的速度去处理事件\n\n缺点：\n\n * 架构复杂了，业务没有明显的流程线，不好管理\n * 需要依赖于Broker的可靠、安全、性能\n\n好在现在开源软件或云平台上 Broker 的软件是非常成熟的，比较常见的一种就是我们今天要学习的MQ技术。\n\n\n# 同步与异步的选用\n\n * 并发性要求不高，优先考虑同步，时效性较强，可以立即得到结果，可以拿到结果去做一些事。\n * 如果并发性要求较高，希望解除耦合关系，且不需要知道服务的结果，选用异步调用\n\n\n# 技术对比：\n\nMQ，中文是消息队列（MessageQueue），字面来看就是存放消息的队列。也就是事件驱动架构中的Broker。\n\n比较常见的MQ实现：\n\n * ActiveMQ\n * RabbitMQ\n * RocketMQ\n * Kafka\n\n几种常见MQ的对比：\n\n        RABBITMQ               ACTIVEMQ                        ROCKETMQ   KAFKA\n公司/社区   Rabbit                 Apache                          阿里         Apache\n开发语言    Erlang                 Java                            Java       Scala&Java\n协议支持    AMQP，XMPP，SMTP，STOMP   OpenWire,STOMP，REST,XMPP,AMQP   自定义协议      自定义协议\n可用性     高                      一般                              高          高\n单机吞吐量   一般                     差                               高          非常高\n消息延迟    微秒级                    毫秒级                             毫秒级        毫秒以内\n消息可靠性   高                      一般                              高          一般\n\n追求可用性：Kafka、 RocketMQ 、RabbitMQ\n\n追求可靠性：RabbitMQ、RocketMQ\n\n追求吞吐能力：RocketMQ、Kafka\n\n追求消息低延迟：RabbitMQ、Kafka",normalizedContent:"# 同步和异步通讯\n\n微服务间通讯有同步和异步两种方式：\n\n同步通讯：就像打电话，需要实时响应。\n\n异步通讯：就像发邮件，不需要马上回复。\n\n\n\n两种方式各有优劣，打电话可以立即得到响应，但是你却不能跟多个人同时通话。发送邮件可以同时与多个人收发邮件，但是往往响应会有延迟。\n\n\n# 同步通讯\n\n我们之前学习的feign调用就属于同步方式，虽然调用可以实时得到结果，但存在下面的问题：\n\n\n\n总结：\n\n同步调用的优点：\n\n * 时效性较强，可以立即得到结果\n\n同步调用的问题：\n\n * 耦合度高\n * 性能和吞吐能力下降\n * 有额外的资源消耗\n * 有级联失败问题\n\n\n# 异步通讯\n\n异步调用则可以避免上述问题：\n\n我们以购买商品为例，用户支付后需要调用订单服务完成订单状态修改，调用物流服务，从仓库分配响应的库存并准备发货。\n\n在事件模式中，支付服务是事件发布者（publisher），在支付完成后只需要发布一个支付成功的事件（event），事件中带上订单id。\n\n订单服务和物流服务是事件订阅者（consumer），订阅支付成功的事件，监听到事件后完成自己业务即可。\n\n为了解除事件发布者与订阅者之间的耦合，两者并不是直接通信，而是有一个中间人（broker）。发布者发布事件到broker，不关心谁来订阅事件。订阅者从broker订阅事件，不关心谁发来的消息。\n\n\n\nbroker 是一个像数据总线一样的东西，所有的服务要接收数据和发送数据都发到这个总线上，这个总线就像协议一样，让服务间的通讯变得标准和可控。\n\n好处：\n\n * 吞吐量提升：无需等待订阅者处理完成，响应更快速\n\n * 故障隔离：服务没有直接调用，不存在级联失败问题\n\n * 调用间没有阻塞，不会造成无效的资源占用\n\n * 耦合度极低，每个服务都可以灵活插拔，可替换\n\n * 流量削峰：不管发布事件的流量波动多大，都由broker接收，订阅者可以按照自己的速度去处理事件\n\n缺点：\n\n * 架构复杂了，业务没有明显的流程线，不好管理\n * 需要依赖于broker的可靠、安全、性能\n\n好在现在开源软件或云平台上 broker 的软件是非常成熟的，比较常见的一种就是我们今天要学习的mq技术。\n\n\n# 同步与异步的选用\n\n * 并发性要求不高，优先考虑同步，时效性较强，可以立即得到结果，可以拿到结果去做一些事。\n * 如果并发性要求较高，希望解除耦合关系，且不需要知道服务的结果，选用异步调用\n\n\n# 技术对比：\n\nmq，中文是消息队列（messagequeue），字面来看就是存放消息的队列。也就是事件驱动架构中的broker。\n\n比较常见的mq实现：\n\n * activemq\n * rabbitmq\n * rocketmq\n * kafka\n\n几种常见mq的对比：\n\n        rabbitmq               activemq                        rocketmq   kafka\n公司/社区   rabbit                 apache                          阿里         apache\n开发语言    erlang                 java                            java       scala&java\n协议支持    amqp，xmpp，smtp，stomp   openwire,stomp，rest,xmpp,amqp   自定义协议      自定义协议\n可用性     高                      一般                              高          高\n单机吞吐量   一般                     差                               高          非常高\n消息延迟    微秒级                    毫秒级                             毫秒级        毫秒以内\n消息可靠性   高                      一般                              高          一般\n\n追求可用性：kafka、 rocketmq 、rabbitmq\n\n追求可靠性：rabbitmq、rocketmq\n\n追求吞吐能力：rocketmq、kafka\n\n追求消息低延迟：rabbitmq、kafka",charsets:{cjk:!0}},{title:"ES进阶",frontmatter:{autoSort:94,title:"ES进阶",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/18dc29/",categories:["后端","微服务","ES"],tags:["知识","微服务","ES"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/20.ES/35.ES%E8%BF%9B%E9%98%B6.html",relativePath:"01.后端/60.微服务/20.ES/35.ES进阶.md",key:"v-06f84544",path:"/pages/18dc29/",headers:[{level:2,title:'数据聚合——"aggs"',slug:"数据聚合-aggs",normalizedTitle:"数据聚合——&quot;aggs&quot;",charIndex:null},{level:3,title:"聚合的种类",slug:"聚合的种类",normalizedTitle:"聚合的种类",charIndex:175},{level:3,title:"DSL实现聚合",slug:"dsl实现聚合",normalizedTitle:"dsl实现聚合",charIndex:525},{level:3,title:"RestAPI实现聚合",slug:"restapi实现聚合",normalizedTitle:"restapi实现聚合",charIndex:3128},{level:2,title:"自动补全",slug:"自动补全",normalizedTitle:"自动补全",charIndex:6665},{level:3,title:"拼音分词器",slug:"拼音分词器",normalizedTitle:"拼音分词器",charIndex:6772},{level:3,title:"自定义分词器",slug:"自定义分词器",normalizedTitle:"自定义分词器",charIndex:7106},{level:3,title:"自动补全查询",slug:"自动补全查询",normalizedTitle:"自动补全查询",charIndex:8443},{level:3,title:"实现酒店搜索框自动补全",slug:"实现酒店搜索框自动补全",normalizedTitle:"实现酒店搜索框自动补全",charIndex:9272},{level:2,title:"数据同步",slug:"数据同步",normalizedTitle:"数据同步",charIndex:14983},{level:3,title:"思路分析",slug:"思路分析",normalizedTitle:"思路分析",charIndex:15094},{level:3,title:"实现数据同步",slug:"实现数据同步",normalizedTitle:"实现数据同步",charIndex:15658},{level:2,title:"集群",slug:"集群",normalizedTitle:"集群",charIndex:21413},{level:3,title:"搭建ES集群",slug:"搭建es集群",normalizedTitle:"搭建es集群",charIndex:22121},{level:3,title:"集群脑裂问题",slug:"集群脑裂问题",normalizedTitle:"集群脑裂问题",charIndex:22158},{level:3,title:"集群分布式存储",slug:"集群分布式存储",normalizedTitle:"集群分布式存储",charIndex:23040},{level:3,title:"集群分布式查询",slug:"集群分布式查询",normalizedTitle:"集群分布式查询",charIndex:23555},{level:3,title:"集群故障转移",slug:"集群故障转移",normalizedTitle:"集群故障转移",charIndex:23760}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:'数据聚合——"aggs" 聚合的种类 DSL实现聚合 RestAPI实现聚合 自动补全 拼音分词器 自定义分词器 自动补全查询 实现酒店搜索框自动补全 数据同步 思路分析 实现数据同步 集群 搭建ES集群 集群脑裂问题 集群分布式存储 集群分布式查询 集群故障转移',content:'# 数据聚合——"aggs"\n\n**聚合（aggregations）**可以让我们极其方便的实现对数据的统计、分析、运算。例如：\n\n * 什么品牌的手机最受欢迎？\n * 这些手机的平均价格、最高价格、最低价格？\n * 这些手机每月的销售情况如何？\n\n实现这些统计功能的比数据库的sql要方便的多，而且查询速度非常快，可以实现近实时搜索效果。\n\n\n# 聚合的种类\n\n聚合常见的有三类：\n\n * **桶（Bucket）**聚合：用来对文档做分组\n   \n   * TermAggregation：按照文档字段值分组，例如按照品牌值分组、按照国家分组\n   * Date Histogram：按照日期阶梯分组，例如一周为一组，或者一月为一组\n\n * **度量（Metric）**聚合：用以计算一些值，比如：最大值、最小值、平均值等\n   \n   * Avg：求平均值\n   * Max：求最大值\n   * Min：求最小值\n   * Stats：同时求max、min、avg、sum等\n\n * **管道（pipeline）**聚合：其它聚合的结果为基础做聚合\n\n****注意：参加聚合的字段必须是keyword、日期、数值、布尔类型\n\n不可以分词\n\n\n# DSL实现聚合\n\n现在，我们要统计所有数据中的酒店品牌有几种，其实就是按照品牌对数据分组。此时可以根据酒店品牌的名称做聚合，也就是Bucket聚合。\n\n# Bucket聚合语法\n\n语法如下：\n\nGET /hotel/_search\n{\n  "size": 0,  // 设置size为0，结果中不包含文档，只包含聚合结果\n  "aggs": { // 定义聚合\n    "brandAgg": { //给聚合起个名字\n      "terms": { // 聚合的类型，按照品牌值聚合，所以选择term\n        "field": "brand", // 参与聚合的字段\n        "size": 20 // 希望获取的聚合结果数量\n      }\n    }\n  }\n}\n\n\n结果如图：\n\n\n\n# 多字段聚合\n\n同时对 品牌和城市 进行 分别聚合\n\n## 多字段聚合\nGET /hotel/_search\n{\n  "size": 0,\n  "aggs": {\n    "brandAggs": {\n      "terms": {\n        "field": "brand",\n        "size": 3\n      }\n    },\n    "cityAgg": {\n      "terms": {\n        "field": "city",\n        "size": 3\n      }\n    }\n  }\n}\n\n\n聚合效果如下\n\n\n\n# 聚合结果排序\n\n默认情况下，Bucket聚合会统计Bucket内的文档数量，记为_count，并且按照_count降序排序。\n\n我们可以指定order属性，自定义聚合的排序方式：\n\nGET /hotel/_search\n{\n  "size": 0, \n  "aggs": {\n    "brandAgg": {\n      "terms": {\n        "field": "brand",\n        "order": {\n          "_count": "asc" // 按照_count升序排列\n        },\n        "size": 20\n      }\n    }\n  }\n}\n\n\n# 限定聚合范围\n\n默认情况下，Bucket聚合是对索引库的所有文档做聚合，但真实场景下，用户会输入搜索条件，因此聚合必须是对搜索结果聚合。那么聚合必须添加限定条件。\n\n我们可以限定要聚合的文档范围，只要添加query条件即可：\n\nGET /hotel/_search\n{\n  "query": {\n    "range": {\n      "price": {\n        "lte": 200 // 只对200元以下的文档聚合\n      }\n    }\n  }, \n  "size": 0, \n  "aggs": {\n    "brandAgg": {\n      "terms": {\n        "field": "brand",\n        "size": 20\n      }\n    }\n  }\n}\n\n\n这次，聚合得到的品牌明显变少了：\n\n\n\n# Metric聚合语法\n\n上节课，我们对酒店按照品牌分组，形成了一个个桶。现在我们需要对桶内的酒店做运算，获取每个品牌的用户评分的min、max、avg等值。\n\n这就要用到Metric聚合了，例如stats聚合：就可以获取min、max、avg等结果。\n\n语法如下：\n\nGET /hotel/_search\n{\n  "size": 0, \n  "aggs": {\n    "brandAgg": { \n      "terms": { \n        "field": "brand", \n        "size": 20,\n         "order": {\n          "scoreAgg.avg": "asc" //按照下面的聚合名称的平均值进行排序\n        }\n      },\n      "aggs": { // 是brands聚合的子聚合，也就是分组后对每组分别计算\n        "scoreAgg": { // 聚合名称 自定义\n          "stats": { // 聚合类型，这里stats可以计算min、max、avg等\n            "field": "score" // 聚合字段，这里是score\n          }\n        }\n      }\n    }\n  }\n}\n\n\n这次的score_stats聚合是在brandAgg的聚合内部嵌套的子聚合。因为我们需要在每个桶分别计算。\n\n另外，我们还可以给聚合结果做个排序，例如按照每个桶的酒店平均分做排序：\n\n\n\n# 聚合小结\n\n##对品牌排序 限定聚合范围-限定范围小于200\nGET /hotel/_search\n{\n  "query": {\n    "range": {\n      "price": {\n        "lte": 200\n      }\n    }\n  }, \n  "size": 0,\n  "aggs": {\n    "brandAggs": {\n      "terms": {\n        "field": "brand",\n        "size": 10,\n        "order": {\n          "_count": "asc" \n        }\n      }\n    }\n  }\n}\n\n\naggs代表聚合，与query同级，此时query的作用是？\n\n * 限定聚合的的文档范围\n   * ——限定价格位于200以下\n\n聚合必须的三要素：\n\n * 聚合名称\n   * ——brandAggs --自定义\n   * ——scoreAgg\n * 聚合类型\n   * —— terms\n   * ——stats\n * 聚合字段\n   * —— brand\n   * ——score\n\n聚合可配置属性有：\n\n * size：指定聚合结果数量\n * order：指定聚合结果排序方式\n * field：指定聚合字段\n\n\n# RestAPI实现聚合\n\n# API语法\n\n聚合条件与query条件同级别，因此需要使用request.source()来指定聚合条件。\n\n聚合条件的语法：\n\n\n\n聚合的结果也与查询结果不同，API也比较特殊。不过同样是JSON逐层解析：\n\n\n\n# 业务需求\n\n需求：搜索页面的品牌、城市等信息不应该是在页面写死，而是通过聚合索引库中的酒店数据得来的：\n\n\n\n分析：\n\n目前，页面的城市列表、星级列表、品牌列表都是写死的，并不会随着搜索结果的变化而变化。但是用户搜索条件改变时，搜索结果会跟着变化。\n\n例如：用户搜索“东方明珠”，那搜索的酒店肯定是在上海东方明珠附近，因此，城市只能是上海，此时城市列表中就不应该显示北京、深圳、杭州这些信息了。\n\n也就是说，搜索结果中包含哪些城市，页面就应该列出哪些城市；搜索结果中包含哪些品牌，页面就应该列出哪些品牌。\n\n如何得知搜索结果中包含哪些品牌？如何得知搜索结果中包含哪些城市？\n\n使用聚合功能，利用Bucket聚合，对搜索结果中的文档基于品牌分组、基于城市分组，就能得知包含哪些品牌、哪些城市了。\n\n因为是对搜索结果聚合，因此聚合是限定范围的聚合，也就是说聚合的限定条件跟搜索文档的条件一致。\n\n查看浏览器可以发现，前端其实已经发出了这样的一个请求：\n\n\n\n请求参数与搜索文档的参数完全一致。\n\n返回值类型就是页面要展示的最终结果：\n\n\n\n结果是一个Map结构：\n\n * key是字符串，城市、星级、品牌、价格\n * value是集合，例如多个城市的名称\n\n# 业务实现\n\n在cn.itcast.hotel.web包的HotelController中添加一个方法，遵循下面的要求：\n\n * 请求方式：POST\n * 请求路径：/hotel/filters\n * 请求参数：RequestParams，与搜索文档的参数一致\n * 返回值类型：Map<String, List<String>>\n\n代码：\n\n    @PostMapping("filters")\n    public Map<String, List<String>> getFilters(@RequestBody RequestParams params){\n        return hotelService.getFilters(params);\n    }\n\n\n这里调用了IHotelService中的getFilters方法，尚未实现。\n\n在cn.itcast.hotel.service.IHotelService中定义新方法：\n\nMap<String, List<String>> filters(RequestParams params);\n\n\n在cn.itcast.hotel.service.impl.HotelService中实现该方法：\n\n@Override\npublic Map<String, List<String>> filters(RequestParams params) {\n    try {\n        // 1.准备Request\n        SearchRequest request = new SearchRequest("hotel");\n        // 2.准备DSL\n        // 2.1.query\n        buildBasicQuery(params, request);\n        // 2.2.设置size\n        request.source().size(0);\n        // 2.3.聚合\n        buildAggregation(request);\n        // 3.发出请求\n        SearchResponse response = client.search(request, RequestOptions.DEFAULT);\n        // 4.解析结果\n        Map<String, List<String>> result = new HashMap<>();\n        Aggregations aggregations = response.getAggregations();\n        // 4.1.根据品牌名称，获取品牌结果\n        List<String> brandList = getAggByName(aggregations, "brandAgg");\n        result.put("品牌", brandList);\n        // 4.2.根据品牌名称，获取品牌结果\n        List<String> cityList = getAggByName(aggregations, "cityAgg");\n        result.put("城市", cityList);\n        // 4.3.根据品牌名称，获取品牌结果\n        List<String> starList = getAggByName(aggregations, "starAgg");\n        result.put("星级", starList);\n\n        return result;\n    } catch (IOException e) {\n        throw new RuntimeException(e);\n    }\n}\n\nprivate void buildAggregation(SearchRequest request) {\n    request.source().aggregation(AggregationBuilders\n                                 .terms("brandAgg")\n                                 .field("brand")\n                                 .size(100)\n                                );\n    request.source().aggregation(AggregationBuilders\n                                 .terms("cityAgg")\n                                 .field("city")\n                                 .size(100)\n                                );\n    request.source().aggregation(AggregationBuilders\n                                 .terms("starAgg")\n                                 .field("starName")\n                                 .size(100)\n                                );\n}\n\nprivate List<String> getAggByName(Aggregations aggregations, String aggName) {\n    // 4.1.根据聚合名称获取聚合结果\n    Terms brandTerms = aggregations.get(aggName);\n    // 4.2.获取buckets\n    List<? extends Terms.Bucket> buckets = brandTerms.getBuckets();\n    // 4.3.遍历\n    List<String> brandList = new ArrayList<>();\n    for (Terms.Bucket bucket : buckets) {\n        // 4.4.获取key\n        String key = bucket.getKeyAsString();\n        brandList.add(key);\n    }\n    return brandList;\n}\n\n\n\n# 自动补全\n\n当用户在搜索框输入字符时，我们应该提示出与该字符有关的搜索项，如图：\n\n\n\n这种根据用户输入的字母，提示完整词条的功能，就是自动补全了。\n\n因为需要根据拼音字母来推断，因此要用到拼音分词功能。\n\n\n# 拼音分词器\n\n要实现根据字母做补全，就必须对文档按照拼音分词。在GitHub上恰好有elasticsearch的拼音分词插件。地址：https://github.com/medcl/elasticsearch-analysis-pinyin\n\n\n\n课前资料中也提供了拼音分词器的安装包：\n\n\n\n安装方式与IK分词器一样，分三步：\n\n①解压\n\n②上传到虚拟机中，elasticsearch的plugin目录\n\n③重启elasticsearch\n\n④测试\n\n详细安装步骤可以参考IK分词器的安装过程。\n\n测试用法如下：\n\nPOST /_analyze\n{\n  "text": "如家酒店还不错",\n  "analyzer": "pinyin"\n}\n\n\n结果：\n\n\n\n\n# 自定义分词器\n\n默认的拼音分词器会将每个汉字单独分为拼音，而我们希望的是每个词条形成一组拼音，需要对拼音分词器做个性化定制，形成自定义分词器。\n\nelasticsearch中分词器（analyzer）的组成包含三部分：\n\n * character filters：在tokenizer之前对文本进行处理。例如删除字符、替换字符\n * tokenizer：将文本按照一定的规则切割成词条（term）。例如keyword，就是不分词；还有ik_smart\n * tokenizer filter：将tokenizer输出的词条做进一步处理。例如大小写转换、同义词处理、拼音处理等\n\n文档分词时会依次由这三部分来处理文档：\n\n\n\n声明自定义分词器的语法如下：\n\nPUT /test\n{\n  "settings": {\n    "analysis": {\n      "analyzer": { // 自定义分词器\n        "my_analyzer": {  // 分词器名称\n          "tokenizer": "ik_max_word",\n          "filter": "py"\n        }\n      },\n      "filter": { // 自定义tokenizer filter\n        "py": { // 过滤器名称\n          "type": "pinyin", // 过滤器类型，这里是pinyin\n\t\t  "keep_full_pinyin": false,\n          "keep_joined_full_pinyin": true,\n          "keep_original": true, //保留中文\n          "limit_first_letter_length": 16,\n          "remove_duplicated_term": true,\n          "none_chinese_pinyin_tokenize": false\n        }\n      }\n    }\n  },\n  "mappings": {\n    "properties": {\n      "name": {\n        "type": "text",\n        "analyzer": "my_analyzer",//创建索引时使用的分词器--带拼音\n        "search_analyzer": "ik_smart"//搜索时使用的分词器-- 不带拼音\n      }\n    }\n  }\n}\n\n\n测试：\n\n\n\n总结：\n\n如何使用拼音分词器？\n\n * ①下载pinyin分词器\n\n * ②解压并放到elasticsearch的plugin目录\n\n * ③重启即可\n\n如何自定义分词器？\n\n * ①创建索引库时，在settings中配置，可以包含三部分\n\n * ②character filter\n\n * ③tokenizer\n\n * ④filter\n\n拼音分词器注意事项？\n\n * 为了避免搜索到同音字，搜索时不要使用拼音分词器\n\n\n# 自动补全查询\n\nelasticsearch提供了Completion Suggester查询来实现自动补全功能。这个查询会匹配以用户输入内容开头的词条并返回。为了提高补全查询的效率，对于文档中字段的类型有一些约束：\n\n * 参与补全查询的字段必须是completion类型。\n\n * 字段的内容一般是用来补全的多个词条形成的数组。\n   \n   * 这样，对于["Sony", "WH-1000XM3"]，搜索输入 s 和 w 都可以补全\n\n比如，一个这样的索引库：\n\n// 创建索引库\nPUT test\n{\n  "mappings": {\n    "properties": {\n      "title":{\n        "type": "completion"//这里一定是completion类型\n      }\n    }\n  }\n}\n\n\n然后插入下面的数据：\n\n// 示例数据\nPOST test/_doc\n{\n  "title": ["Sony", "WH-1000XM3"]\n}\nPOST test/_doc\n{\n  "title": ["SK-II", "PITERA"]\n}\nPOST test/_doc\n{\n  "title": ["Nintendo", "switch"]\n}\n\n\n查询的DSL语句如下：\n\n// 自动补全查询\nGET /test/_search\n{\n  "suggest": {\n    "title_suggest": {//查询名称，自定义\n      "text": "s", // 关键字\n      "completion": {\n        "field": "title", // 补全查询的字段\n        "skip_duplicates": true, // 跳过重复的\n        "size": 10 // 获取前10条结果\n      }\n    }\n  }\n}\n\n\n\n# 实现酒店搜索框自动补全\n\n现在，我们的hotel索引库还没有设置拼音分词器，需要修改索引库中的配置。但是我们知道索引库是无法修改的，只能删除然后重新创建。\n\n另外，我们需要添加一个字段，用来做自动补全，将brand、suggestion、city等都放进去，作为自动补全的提示。\n\n因此，总结一下，我们需要做的事情包括：\n\n 1. 修改hotel索引库结构，设置自定义拼音分词器\n\n 2. 修改索引库的name、all字段，使用自定义分词器\n\n 3. 索引库添加一个新字段suggestion，类型为completion类型，使用自定义的分词器\n\n 4. 给HotelDoc类添加suggestion字段，内容包含brand、business\n\n 5. 重新导入数据到hotel库\n\n# 修改酒店映射结构\n\n代码如下：\n\n// 酒店数据索引库\nPUT /hotel\n{\n  "settings": {\n    "analysis": {\n      "analyzer": {\n        "text_anlyzer": {//自定义分词器名称\n          "tokenizer": "ik_max_word",//中间用ik分割词语\n          "filter": "py"//最后用拼音分词器处理\n        },\n        "completion_analyzer": {\n          "tokenizer": "keyword",//中间 不用词语分割\n          "filter": "py" //最后用拼音分词器处理\n        }\n      },\n      "filter": {\n        "py": {\n          "type": "pinyin",\n          "keep_full_pinyin": false,// 刘德华> [liu,de,hua],默认为true\n          "keep_joined_full_pinyin": true,//刘德华> [liudehua],默认为false\n          "keep_original": true,//保留原始的汉字输入\n          "limit_first_letter_length": 16,\n          "remove_duplicated_term": true,//移除重复，[de的]>de\n          "none_chinese_pinyin_tokenize": false//保持拼音连续，不拆开\n        }\n      }\n    }\n  },\n  "mappings": {\n    "properties": {\n      "id":{\n        "type": "keyword"\n      },\n      "name":{\n        "type": "text",\n        "analyzer": "text_anlyzer",//创建索引时 使用自定义混合分词器\n        "search_analyzer": "ik_smart",//搜索时 使用ik分词器\n        "copy_to": "all"\n      },\n      "address":{\n        "type": "keyword",\n        "index": false\n      },\n      "price":{\n        "type": "integer"\n      },\n      "score":{\n        "type": "integer"\n      },\n      "brand":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "city":{\n        "type": "keyword"\n      },\n      "starName":{\n        "type": "keyword"\n      },\n      "business":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "location":{\n        "type": "geo_point"\n      },\n      "pic":{\n        "type": "keyword",\n        "index": false\n      },\n      "all":{\n        "type": "text",\n        "analyzer": "text_anlyzer",\n        "search_analyzer": "ik_smart"\n      },\n      "suggestion":{//关键词，自动补全字段\n          "type": "completion",//必须是completion类型\n          "analyzer": "completion_analyzer"//不分词，直接转拼音\n      }\n    }\n  }\n}\n\n\n# 修改HotelDoc实体\n\nHotelDoc中要添加一个字段，用来做自动补全，内容可以是酒店品牌、城市、商圈等信息。按照自动补全字段的要求，最好是这些字段的数组。\n\n因此我们在HotelDoc中添加一个suggestion字段，类型为List<String>，然后将brand、city、business等信息放到里面。\n\n代码如下：\n\npackage cn.itcast.hotel.pojo;\n\nimport lombok.Data;\nimport lombok.NoArgsConstructor;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.List;\n\n@Data\n@NoArgsConstructor\npublic class HotelDoc {\n    private Long id;\n    private String name;\n    private String address;\n    private Integer price;\n    private Integer score;\n    private String brand;\n    private String city;\n    private String starName;\n    private String business;\n    private String location;\n    private String pic;\n    private Object distance;\n    private Boolean isAD;\n    private List<String> suggestion;\n\n    public HotelDoc(Hotel hotel) {\n        this.id = hotel.getId();\n        this.name = hotel.getName();\n        this.address = hotel.getAddress();\n        this.price = hotel.getPrice();\n        this.score = hotel.getScore();\n        this.brand = hotel.getBrand();\n        this.city = hotel.getCity();\n        this.starName = hotel.getStarName();\n        this.business = hotel.getBusiness();\n        this.location = hotel.getLatitude() + ", " + hotel.getLongitude();\n        this.pic = hotel.getPic();\n        // 组装suggestion\n        if(this.business.contains("/")){\n            // business有多个值，需要切割\n            String[] arr = this.business.split("/");\n            // 添加元素\n            this.suggestion = new ArrayList<>();\n            this.suggestion.add(this.brand);\n            Collections.addAll(this.suggestion, arr);\n        }else {\n            this.suggestion = Arrays.asList(this.brand, this.business);\n        }\n    }\n}\n\n\n# 重新导入\n\n重新执行之前编写的导入数据功能，可以看到新的酒店数据中包含了suggestion：\n\n\n\n# 自动补全查询的JavaAPI\n\n之前我们学习了自动补全查询的DSL，而没有学习对应的JavaAPI，这里给出一个示例：\n\n\n\n而自动补全的结果也比较特殊，解析的代码如下：\n\n\n\n# 实现搜索框自动补全\n\n查看前端页面，可以发现当我们在输入框键入时，前端会发起ajax请求：\n\n\n\n返回值是补全词条的集合，类型为List<String>\n\n1）在cn.itcast.hotel.web包下的HotelController中添加新接口，接收新的请求：\n\n@GetMapping("suggestion")\npublic List<String> getSuggestions(@RequestParam("key") String prefix) {\n    return hotelService.getSuggestions(prefix);\n}\n\n\n2）在cn.itcast.hotel.service包下的IhotelService中添加方法：\n\nList<String> getSuggestions(String prefix);\n\n\n3）在cn.itcast.hotel.service.impl.HotelService中实现该方法：\n\n@Override\npublic List<String> getSuggestions(String prefix) {\n    try {\n        // 1.准备Request\n        SearchRequest request = new SearchRequest("hotel");\n        // 2.准备DSL\n        request.source().suggest(new SuggestBuilder().addSuggestion(\n            "suggestions",\n            SuggestBuilders.completionSuggestion("suggestion")\n            .prefix(prefix)\n            .skipDuplicates(true)\n            .size(10)\n        ));\n        // 3.发起请求\n        SearchResponse response = client.search(request, RequestOptions.DEFAULT);\n        // 4.解析结果\n        Suggest suggest = response.getSuggest();\n        // 4.1.根据补全查询名称，获取补全结果\n        CompletionSuggestion suggestions = suggest.getSuggestion("suggestions");\n        // 4.2.获取options\n        List<CompletionSuggestion.Entry.Option> options = suggestions.getOptions();\n        // 4.3.遍历\n        List<String> list = new ArrayList<>(options.size());\n        for (CompletionSuggestion.Entry.Option option : options) {\n            String text = option.getText().toString();\n            list.add(text);\n        }\n        return list;\n    } catch (IOException e) {\n        throw new RuntimeException(e);\n    }\n}\n\n\n\n# 数据同步\n\nelasticsearch中的酒店数据来自于mysql数据库，因此mysql数据发生改变时，elasticsearch也必须跟着改变，这个就是elasticsearch与mysql之间的数据同步。\n\n\n\n\n# 思路分析\n\n常见的数据同步方案有三种：\n\n * 同步调用\n * 异步通知\n * 监听binlog\n\n# 同步调用\n\n方案一：同步调用\n\n\n\n基本步骤如下：\n\n * hotel-demo对外提供接口，用来修改elasticsearch中的数据\n * 酒店管理服务在完成数据库操作后，直接调用hotel-demo提供的接口，\n\n# 异步通知\n\n方案二：异步通知\n\n\n\n流程如下：\n\n * hotel-admin对mysql数据库数据完成增、删、改后，发送MQ消息\n * hotel-demo监听MQ，接收到消息后完成elasticsearch数据修改\n\n# 监听binlog\n\n方案三：监听binlog\n\n\n\n流程如下：\n\n * 给mysql开启binlog功能\n * mysql完成增、删、改操作都会记录在binlog中\n * hotel-demo基于canal监听binlog变化，实时更新elasticsearch中的内容\n\n# 选择\n\n方式一：同步调用\n\n * 优点：实现简单，粗暴\n * 缺点：业务耦合度高\n\n方式二：异步通知\n\n * 优点：低耦合，实现难度一般\n * 缺点：依赖mq的可靠性\n\n方式三：监听binlog\n\n * 优点：完全解除服务间耦合\n * 缺点：开启binlog增加数据库负担、实现复杂度高\n\n\n# 实现数据同步\n\n# 思路\n\n利用课前资料提供的hotel-admin项目作为酒店管理的微服务。当酒店数据发生增、删、改时，要求对elasticsearch中数据也要完成相同操作。\n\n步骤：\n\n * 导入课前资料提供的hotel-admin项目，启动并测试酒店数据的CRUD\n\n * 声明exchange、queue、RoutingKey\n\n * 在hotel-admin中的增、删、改业务中完成消息发送\n\n * 在hotel-demo中完成消息监听，并更新elasticsearch中数据\n\n * 启动并测试数据同步功能\n\n# 导入demo\n\n导入课前资料提供的hotel-admin项目：\n\n\n\n运行后，访问 http://localhost:8099\n\n\n\n其中包含了酒店的CRUD功能：\n\n\n\n# 声明交换机、队列\n\nMQ结构如图：\n\n\n\n# 1）引入依赖\n\n在hotel-admin、hotel-demo中引入rabbitmq的依赖：\n\n\x3c!--amqp--\x3e\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-amqp</artifactId>\n</dependency>\n\n\n# 2）声明队列交换机名称\n\n在hotel-admin和hotel-demo中的cn.itcast.hotel.constatnts包下新建一个类MqConstants：\n\npackage cn.itcast.hotel.constatnts;\n\n    public class MqConstants {\n    /**\n     * 交换机\n     */\n    public final static String HOTEL_EXCHANGE = "hotel.topic";\n    /**\n     * 监听新增和修改的队列\n     */\n    public final static String HOTEL_INSERT_QUEUE = "hotel.insert.queue";\n    /**\n     * 监听删除的队列\n     */\n    public final static String HOTEL_DELETE_QUEUE = "hotel.delete.queue";\n    /**\n     * 新增或修改的RoutingKey\n     */\n    public final static String HOTEL_INSERT_KEY = "hotel.insert";\n    /**\n     * 删除的RoutingKey\n     */\n    public final static String HOTEL_DELETE_KEY = "hotel.delete";\n}\n\n\n# 3）声明队列交换机\n\n在hotel-demo中，定义配置类，声明队列、交换机：\n\npackage cn.itcast.hotel.config;\n\nimport cn.itcast.hotel.constants.MqConstants;\nimport org.springframework.amqp.core.Binding;\nimport org.springframework.amqp.core.BindingBuilder;\nimport org.springframework.amqp.core.Queue;\nimport org.springframework.amqp.core.TopicExchange;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\n@Configuration\npublic class MqConfig {\n    @Bean\n    public TopicExchange topicExchange(){\n        return new TopicExchange(MqConstants.HOTEL_EXCHANGE, true, false);\n    }\n\n    @Bean\n    public Queue insertQueue(){\n        return new Queue(MqConstants.HOTEL_INSERT_QUEUE, true);\n    }\n\n    @Bean\n    public Queue deleteQueue(){\n        return new Queue(MqConstants.HOTEL_DELETE_QUEUE, true);\n    }\n\n    @Bean\n    public Binding insertQueueBinding(){\n        return BindingBuilder.bind(insertQueue()).to(topicExchange()).with(MqConstants.HOTEL_INSERT_KEY);\n    }\n\n    @Bean\n    public Binding deleteQueueBinding(){\n        return BindingBuilder.bind(deleteQueue()).to(topicExchange()).with(MqConstants.HOTEL_DELETE_KEY);\n    }\n}\n\n\n# 发送MQ消息\n\n在hotel-admin中的增、删、改业务中分别发送MQ消息：\n\n\n\n# 接收MQ消息\n\nhotel-demo接收到MQ消息要做的事情包括：\n\n * 新增消息：根据传递的hotel的id查询hotel信息，然后新增一条数据到索引库\n * 删除消息：根据传递的hotel的id删除索引库中的一条数据\n\n1）首先在hotel-demo的cn.itcast.hotel.service包下的IHotelService中新增新增、删除业务\n\nvoid deleteById(Long id);\n\nvoid insertById(Long id);\n\n\n2）给hotel-demo中的cn.itcast.hotel.service.impl包下的HotelService中实现业务：\n\n@Override\npublic void deleteById(Long id) {\n    try {\n        // 1.准备Request\n        DeleteRequest request = new DeleteRequest("hotel", id.toString());\n        // 2.发送请求\n        client.delete(request, RequestOptions.DEFAULT);\n    } catch (IOException e) {\n        throw new RuntimeException(e);\n    }\n}\n\n@Override\npublic void insertById(Long id) {\n    try {\n        // 0.根据id查询酒店数据\n        Hotel hotel = getById(id);\n        // 转换为文档类型\n        HotelDoc hotelDoc = new HotelDoc(hotel);\n\n        // 1.准备Request对象\n        IndexRequest request = new IndexRequest("hotel").id(hotel.getId().toString());\n        // 2.准备Json文档\n        request.source(JSON.toJSONString(hotelDoc), XContentType.JSON);\n        // 3.发送请求\n        client.index(request, RequestOptions.DEFAULT);\n    } catch (IOException e) {\n        throw new RuntimeException(e);\n    }\n}\n\n\n3）编写监听器\n\n在hotel-demo中的cn.itcast.hotel.mq包新增一个类：\n\npackage cn.itcast.hotel.mq;\n\nimport cn.itcast.hotel.constants.MqConstants;\nimport cn.itcast.hotel.service.IHotelService;\nimport org.springframework.amqp.rabbit.annotation.RabbitListener;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.stereotype.Component;\n\n@Component\npublic class HotelListener {\n\n    @Autowired\n    private IHotelService hotelService;\n\n    /**\n     * 监听酒店新增或修改的业务\n     * @param id 酒店id\n     */\n    @RabbitListener(queues = MqConstants.HOTEL_INSERT_QUEUE)\n    public void listenHotelInsertOrUpdate(Long id){\n        hotelService.insertById(id);\n    }\n\n    /**\n     * 监听酒店删除的业务\n     * @param id 酒店id\n     */\n    @RabbitListener(queues = MqConstants.HOTEL_DELETE_QUEUE)\n    public void listenHotelDelete(Long id){\n        hotelService.deleteById(id);\n    }\n}\n\n\n# 问题\n\n 1. 常量命名一定要规范\n    \n    * 大写，下划线，带有文字注释\n      \n      /**\n      * 监听删除的队列\n      */\n      public final static String HOTEL_DELETE_QUEUE = "hotel.delete.queue";\n      \n\n 2. topic类型的交换机，在进行队列和交换机进行绑定的时候，注意带上 RoutingKey\n    \n    @Bean\n    public Binding hotelDeleteBinding(Queue hotelDeleteQueue, TopicExchange hotelTopicExchange) {\n        //TODO 这里要记得带上RoutingKey， 用with\n        return BindingBuilder.bind(hotelDeleteQueue).to(hotelTopicExchange).with(HOTEL_DELETE_KEY);\n    }\n    \n\n 3. 往交换机中发送信息，信息为 新增对象的id， 体积小，占内存空间小\n    \n    * 不要直接发送java 的hotel 对象， 容易把mq占满\n    \n    //TODO 往交换机中发送信息，信息为 新增对象的id  体积小，\n    // 不要直接发送java 的hotel 对象， 容易把mq占满\n    \n    // 交换机名称，Routingkey, java类型对象\n    rabbitTemplate.convertAndSend(HOTEL_EXCHANGE, HOTEL_INSERT_KEY, hotel.getId());\n    \n\n 4. mybatisPlus的save方法，可以返回新增数据的id**,但是id的生成策略 一定要设置成为自动增长**\n    \n    //TODO 设置id自增，mybatisplus的添加方法才可以返回id\n    @TableId(type = IdType.AUTO)\n    private Long id;\n    \n\n 5. 新增信息和修改信息，设置为同一个队列即可，因为在es中修改信息的全量修改方法和新增信息是一样的\n    \n    \n\n 6. id生成时，尽量用跟随数据库自动生成，即@TableId(type = IdType.AUTO)\n    \n    * 不要用雪花算法生成id，因为位数过长，如果前端不将id转换成字符串处理的话，会将id精度丢失。\n    \n    * 即java后端 生成id = 1586232717316648962 ,JS接收信息为id=1586232717316649000\n    \n    * **这样后端在接收前端传来的id信息时，在数据库中无法找到这个id对应的信息。**会导致一系列错误。\n    \n    * 控制id自动生成的选项\n      \n      \n\n\n# 集群\n\n单机的elasticsearch做数据存储，必然面临两个问题：海量数据存储问题、单点故障问题。\n\n * 海量数据存储问题：将索引库从逻辑上拆分为N个分片（shard），存储到多个节点\n * 单点故障问题：将分片数据在不同节点备份（replica ）\n\nES集群相关概念:\n\n * 集群（cluster）：一组拥有共同的 cluster name 的 节点。\n\n * 节点（node) ：集群中的一个 Elasticearch 实例\n\n * 分片（shard）：索引可以被拆分为不同的部分进行存储，称为分片。在集群环境下，一个索引的不同分片可以拆分到不同的节点中\n   \n   解决问题：数据量太大，单点存储量有限的问题。\n   \n   \n   \n   > 此处，我们把数据分成3片：shard0、shard1、shard2\n\n * 主分片（Primary shard）：相对于副本分片的定义。\n\n * 副本分片（Replica shard）每个主分片可以有一个或者多个副本，数据和主分片一样。\n   \n   \n\n数据备份可以保证高可用，但是每个分片备份一份，所需要的节点数量就会翻一倍，成本实在是太高了！\n\n为了在高可用和成本间寻求平衡，我们可以这样做：\n\n * 首先对数据分片，存储到不同节点\n * 然后对每个分片进行备份，放到对方节点，完成互相备份\n\n这样可以大大减少所需要的服务节点数量，如图，我们以3分片，每个分片备份一份为例：\n\n\n\n现在，每个分片都有1个备份，存储在3个节点：\n\n * node0：保存了分片0和1\n * node1：保存了分片0和2\n * node2：保存了分片1和2\n\n\n# 搭建ES集群\n\n参考课前资料的文档：\n\n\n\n其中的第四章节：\n\n\n\n\n# 集群脑裂问题\n\n# 集群职责划分\n\nelasticsearch中集群节点有不同的职责划分：\n\n\n\n默认情况下，集群中的任何一个节点都同时具备上述四种角色。\n\n但是真实的集群一定要将集群职责分离：\n\n * master节点：对CPU要求高，但是内存要求第\n\n * data节点：对CPU和内存要求都高\n\n * coordinating节点：对网络带宽、CPU要求高\n\n * ingest节点，一般不用，数据预处理在后端进行\n\n职责分离可以让我们根据不同节点的需求分配不同的硬件去部署。而且避免业务之间的互相干扰。\n\n一个典型的es集群职责划分如图：\n\n\n\n# 脑裂问题\n\n脑裂是因为集群中的节点失联导致的。\n\n例如一个集群中，主节点与其它节点失联：\n\n\n\n此时，node2和node3认为node1宕机，就会重新选主：\n\n\n\n当node3当选后，集群继续对外提供服务，node2和node3自成集群，node1自成集群，两个集群数据不同步，出现数据差异。\n\n当网络恢复后，因为集群中有两个master节点，集群状态的不一致，出现脑裂的情况：\n\n\n\n解决脑裂的方案是，要求选票超过 ( eligible节点数量 + 1 ）/ 2 才能当选为主，因此eligible节点数量最好是奇数。对应配置项是discovery.zen.minimum_master_nodes，在es7.0以后，已经成为默认配置，因此一般不会发生脑裂问题\n\n例如：3个节点形成的集群，选票必须超过 （3 + 1） / 2 ，也就是2票。node3得到node2和node3的选票，当选为主。node1只有自己1票，没有当选。集群中依然只有1个主节点，没有出现脑裂。\n\n# 小结\n\nmaster eligible节点的作用是什么？\n\n * 参与集群选主\n * 主节点可以管理集群状态、管理分片信息、处理创建和删除索引库的请求\n\ndata节点的作用是什么？\n\n * 数据的CRUD\n\ncoordinator节点的作用是什么？\n\n * 路由请求到其它节点\n\n * 合并查询到的结果，返回给用户\n\n\n# 集群分布式存储\n\n当新增文档时，应该保存到不同分片，保证数据均衡，那么coordinating node如何确定数据该存储到哪个分片呢？\n\n# 分片存储测试\n\n插入三条数据：\n\n\n\n\n\n\n\n测试可以看到，三条数据分别在不同分片：\n\n查看数据具体位置--explain\n\n{\n    "explain":true,\n    "query":{\n        "match_all":{}\n    }\n}\n\n\n\n\n结果：\n\n\n\n# 分片存储原理\n\nelasticsearch会通过hash算法来计算文档应该存储到哪个分片：\n\n\n\n说明：\n\n * _routing默认是文档的id\n * 算法与分片数量有关，因此索引库一旦创建，分片数量不能修改！\n\n新增文档的流程如下：\n\n\n\n解读：\n\n * 1）新增一个id=1的文档\n * 2）对id做hash运算，假如得到的是2，则应该存储到shard-2\n * 3）shard-2的主分片在node3节点，将数据路由到node3\n * 4）保存文档\n * 5）同步给shard-2的副本replica-2，在node2节点\n * 6）返回结果给coordinating-node节点\n\n\n# 集群分布式查询\n\nelasticsearch的查询分成两个阶段：\n\n**coordinating node 协调节点进行请求的分发和结果的收集 **\n\n * scatter phase：分散阶段，coordinating node会把请求分发到每一个分片\n\n * gather phase：聚集阶段，coordinating node汇总data node的搜索结果，并处理为最终结果集返回给用户\n\n\n\n\n# 集群故障转移\n\n集群的master节点会监控集群中的节点状态，如果发现有节点宕机，会立即将宕机节点的分片数据迁移到其它节点，确保数据安全，这个叫做故障转移。\n\n1）例如一个集群结构如图：\n\n\n\n现在，node1是主节点，其它两个节点是从节点。\n\n2）突然，node1发生了故障：\n\n\n\n宕机后的第一件事，需要重新选主，例如选中了node2：\n\n\n\nnode2成为主节点后，会检测集群监控状态，发现：shard-1没有副本节点，shard-0没有主分片节点。因此需要将node1上的数据迁移到node2、node3：\n\n\n\n节点刚刚宕机(原先节点3是主节点，宕机后节点1被选为主节点)\n\n\n\n宕机后，数据转移\n\n\n\n宕机后数据查询\n\n数据还是分的3片，虽然节点少了，但是数据片数量没有改变，查询时即不受影响\n\n{\n    "took": 11,\n    "timed_out": false,\n    "_shards": {\n        "total": 3,\n        "successful": 3,\n        "skipped": 0,\n        "failed": 0\n    },\n    "hits": {\n        "total": {\n            "value": 3,\n            "relation": "eq"\n        },\n        "max_score": 1.0,\n        "hits": [\n            {\n                "_shard": "[diana][0]",\n                "_node": "6bYo3oLMQ2Or9yqaWv7GRg",\n                "_index": "diana",\n                "_type": "_doc",\n                "_id": "5",\n                "_score": 1.0,\n                "_source": {\n                    "title": "试着插入一条 id = 5"\n                },\n                "_explanation": {\n                    "value": 1.0,\n                    "description": "*:*",\n                    "details": []\n                }\n            },\n            {\n                "_shard": "[diana][1]",\n                "_node": "0HRL3vFmSCygb_Uvf-dmxw",\n                "_index": "diana",\n                "_type": "_doc",\n                "_id": "3",\n                "_score": 1.0,\n                "_source": {\n                    "title": "试着插入一条 id = 3"\n                },\n                "_explanation": {\n                    "value": 1.0,\n                    "description": "*:*",\n                    "details": []\n                }\n            },\n            {\n                "_shard": "[diana][2]",\n                "_node": "0HRL3vFmSCygb_Uvf-dmxw",\n                "_index": "diana",\n                "_type": "_doc",\n                "_id": "1",\n                "_score": 1.0,\n                "_source": {\n                    "title": "试着插入一条 id = 1"\n                },\n                "_explanation": {\n                    "value": 1.0,\n                    "description": "*:*",\n                    "details": []\n                }\n            }\n        ]\n    }\n}\n\n\n当node1重新上线后，主节点node2 会将数据重新分配给node1，但是node1不再是主节点。\n\n\n\n宕机恢复后（节点1依然是主节点，节点3的辉煌已不再）\n\n\n\n故障转移总结\n\n * master 宕机后，选举一个新的主节点\n * 新主节点监控分片、节点状态，将故障节点上的分片转移到正常节点，确保数据安全。\n * 当宕机节点重新上线后，主节点会将数据重新分配给它，让它正常工作。',normalizedContent:'# 数据聚合——"aggs"\n\n**聚合（aggregations）**可以让我们极其方便的实现对数据的统计、分析、运算。例如：\n\n * 什么品牌的手机最受欢迎？\n * 这些手机的平均价格、最高价格、最低价格？\n * 这些手机每月的销售情况如何？\n\n实现这些统计功能的比数据库的sql要方便的多，而且查询速度非常快，可以实现近实时搜索效果。\n\n\n# 聚合的种类\n\n聚合常见的有三类：\n\n * **桶（bucket）**聚合：用来对文档做分组\n   \n   * termaggregation：按照文档字段值分组，例如按照品牌值分组、按照国家分组\n   * date histogram：按照日期阶梯分组，例如一周为一组，或者一月为一组\n\n * **度量（metric）**聚合：用以计算一些值，比如：最大值、最小值、平均值等\n   \n   * avg：求平均值\n   * max：求最大值\n   * min：求最小值\n   * stats：同时求max、min、avg、sum等\n\n * **管道（pipeline）**聚合：其它聚合的结果为基础做聚合\n\n****注意：参加聚合的字段必须是keyword、日期、数值、布尔类型\n\n不可以分词\n\n\n# dsl实现聚合\n\n现在，我们要统计所有数据中的酒店品牌有几种，其实就是按照品牌对数据分组。此时可以根据酒店品牌的名称做聚合，也就是bucket聚合。\n\n# bucket聚合语法\n\n语法如下：\n\nget /hotel/_search\n{\n  "size": 0,  // 设置size为0，结果中不包含文档，只包含聚合结果\n  "aggs": { // 定义聚合\n    "brandagg": { //给聚合起个名字\n      "terms": { // 聚合的类型，按照品牌值聚合，所以选择term\n        "field": "brand", // 参与聚合的字段\n        "size": 20 // 希望获取的聚合结果数量\n      }\n    }\n  }\n}\n\n\n结果如图：\n\n\n\n# 多字段聚合\n\n同时对 品牌和城市 进行 分别聚合\n\n## 多字段聚合\nget /hotel/_search\n{\n  "size": 0,\n  "aggs": {\n    "brandaggs": {\n      "terms": {\n        "field": "brand",\n        "size": 3\n      }\n    },\n    "cityagg": {\n      "terms": {\n        "field": "city",\n        "size": 3\n      }\n    }\n  }\n}\n\n\n聚合效果如下\n\n\n\n# 聚合结果排序\n\n默认情况下，bucket聚合会统计bucket内的文档数量，记为_count，并且按照_count降序排序。\n\n我们可以指定order属性，自定义聚合的排序方式：\n\nget /hotel/_search\n{\n  "size": 0, \n  "aggs": {\n    "brandagg": {\n      "terms": {\n        "field": "brand",\n        "order": {\n          "_count": "asc" // 按照_count升序排列\n        },\n        "size": 20\n      }\n    }\n  }\n}\n\n\n# 限定聚合范围\n\n默认情况下，bucket聚合是对索引库的所有文档做聚合，但真实场景下，用户会输入搜索条件，因此聚合必须是对搜索结果聚合。那么聚合必须添加限定条件。\n\n我们可以限定要聚合的文档范围，只要添加query条件即可：\n\nget /hotel/_search\n{\n  "query": {\n    "range": {\n      "price": {\n        "lte": 200 // 只对200元以下的文档聚合\n      }\n    }\n  }, \n  "size": 0, \n  "aggs": {\n    "brandagg": {\n      "terms": {\n        "field": "brand",\n        "size": 20\n      }\n    }\n  }\n}\n\n\n这次，聚合得到的品牌明显变少了：\n\n\n\n# metric聚合语法\n\n上节课，我们对酒店按照品牌分组，形成了一个个桶。现在我们需要对桶内的酒店做运算，获取每个品牌的用户评分的min、max、avg等值。\n\n这就要用到metric聚合了，例如stats聚合：就可以获取min、max、avg等结果。\n\n语法如下：\n\nget /hotel/_search\n{\n  "size": 0, \n  "aggs": {\n    "brandagg": { \n      "terms": { \n        "field": "brand", \n        "size": 20,\n         "order": {\n          "scoreagg.avg": "asc" //按照下面的聚合名称的平均值进行排序\n        }\n      },\n      "aggs": { // 是brands聚合的子聚合，也就是分组后对每组分别计算\n        "scoreagg": { // 聚合名称 自定义\n          "stats": { // 聚合类型，这里stats可以计算min、max、avg等\n            "field": "score" // 聚合字段，这里是score\n          }\n        }\n      }\n    }\n  }\n}\n\n\n这次的score_stats聚合是在brandagg的聚合内部嵌套的子聚合。因为我们需要在每个桶分别计算。\n\n另外，我们还可以给聚合结果做个排序，例如按照每个桶的酒店平均分做排序：\n\n\n\n# 聚合小结\n\n##对品牌排序 限定聚合范围-限定范围小于200\nget /hotel/_search\n{\n  "query": {\n    "range": {\n      "price": {\n        "lte": 200\n      }\n    }\n  }, \n  "size": 0,\n  "aggs": {\n    "brandaggs": {\n      "terms": {\n        "field": "brand",\n        "size": 10,\n        "order": {\n          "_count": "asc" \n        }\n      }\n    }\n  }\n}\n\n\naggs代表聚合，与query同级，此时query的作用是？\n\n * 限定聚合的的文档范围\n   * ——限定价格位于200以下\n\n聚合必须的三要素：\n\n * 聚合名称\n   * ——brandaggs --自定义\n   * ——scoreagg\n * 聚合类型\n   * —— terms\n   * ——stats\n * 聚合字段\n   * —— brand\n   * ——score\n\n聚合可配置属性有：\n\n * size：指定聚合结果数量\n * order：指定聚合结果排序方式\n * field：指定聚合字段\n\n\n# restapi实现聚合\n\n# api语法\n\n聚合条件与query条件同级别，因此需要使用request.source()来指定聚合条件。\n\n聚合条件的语法：\n\n\n\n聚合的结果也与查询结果不同，api也比较特殊。不过同样是json逐层解析：\n\n\n\n# 业务需求\n\n需求：搜索页面的品牌、城市等信息不应该是在页面写死，而是通过聚合索引库中的酒店数据得来的：\n\n\n\n分析：\n\n目前，页面的城市列表、星级列表、品牌列表都是写死的，并不会随着搜索结果的变化而变化。但是用户搜索条件改变时，搜索结果会跟着变化。\n\n例如：用户搜索“东方明珠”，那搜索的酒店肯定是在上海东方明珠附近，因此，城市只能是上海，此时城市列表中就不应该显示北京、深圳、杭州这些信息了。\n\n也就是说，搜索结果中包含哪些城市，页面就应该列出哪些城市；搜索结果中包含哪些品牌，页面就应该列出哪些品牌。\n\n如何得知搜索结果中包含哪些品牌？如何得知搜索结果中包含哪些城市？\n\n使用聚合功能，利用bucket聚合，对搜索结果中的文档基于品牌分组、基于城市分组，就能得知包含哪些品牌、哪些城市了。\n\n因为是对搜索结果聚合，因此聚合是限定范围的聚合，也就是说聚合的限定条件跟搜索文档的条件一致。\n\n查看浏览器可以发现，前端其实已经发出了这样的一个请求：\n\n\n\n请求参数与搜索文档的参数完全一致。\n\n返回值类型就是页面要展示的最终结果：\n\n\n\n结果是一个map结构：\n\n * key是字符串，城市、星级、品牌、价格\n * value是集合，例如多个城市的名称\n\n# 业务实现\n\n在cn.itcast.hotel.web包的hotelcontroller中添加一个方法，遵循下面的要求：\n\n * 请求方式：post\n * 请求路径：/hotel/filters\n * 请求参数：requestparams，与搜索文档的参数一致\n * 返回值类型：map<string, list<string>>\n\n代码：\n\n    @postmapping("filters")\n    public map<string, list<string>> getfilters(@requestbody requestparams params){\n        return hotelservice.getfilters(params);\n    }\n\n\n这里调用了ihotelservice中的getfilters方法，尚未实现。\n\n在cn.itcast.hotel.service.ihotelservice中定义新方法：\n\nmap<string, list<string>> filters(requestparams params);\n\n\n在cn.itcast.hotel.service.impl.hotelservice中实现该方法：\n\n@override\npublic map<string, list<string>> filters(requestparams params) {\n    try {\n        // 1.准备request\n        searchrequest request = new searchrequest("hotel");\n        // 2.准备dsl\n        // 2.1.query\n        buildbasicquery(params, request);\n        // 2.2.设置size\n        request.source().size(0);\n        // 2.3.聚合\n        buildaggregation(request);\n        // 3.发出请求\n        searchresponse response = client.search(request, requestoptions.default);\n        // 4.解析结果\n        map<string, list<string>> result = new hashmap<>();\n        aggregations aggregations = response.getaggregations();\n        // 4.1.根据品牌名称，获取品牌结果\n        list<string> brandlist = getaggbyname(aggregations, "brandagg");\n        result.put("品牌", brandlist);\n        // 4.2.根据品牌名称，获取品牌结果\n        list<string> citylist = getaggbyname(aggregations, "cityagg");\n        result.put("城市", citylist);\n        // 4.3.根据品牌名称，获取品牌结果\n        list<string> starlist = getaggbyname(aggregations, "staragg");\n        result.put("星级", starlist);\n\n        return result;\n    } catch (ioexception e) {\n        throw new runtimeexception(e);\n    }\n}\n\nprivate void buildaggregation(searchrequest request) {\n    request.source().aggregation(aggregationbuilders\n                                 .terms("brandagg")\n                                 .field("brand")\n                                 .size(100)\n                                );\n    request.source().aggregation(aggregationbuilders\n                                 .terms("cityagg")\n                                 .field("city")\n                                 .size(100)\n                                );\n    request.source().aggregation(aggregationbuilders\n                                 .terms("staragg")\n                                 .field("starname")\n                                 .size(100)\n                                );\n}\n\nprivate list<string> getaggbyname(aggregations aggregations, string aggname) {\n    // 4.1.根据聚合名称获取聚合结果\n    terms brandterms = aggregations.get(aggname);\n    // 4.2.获取buckets\n    list<? extends terms.bucket> buckets = brandterms.getbuckets();\n    // 4.3.遍历\n    list<string> brandlist = new arraylist<>();\n    for (terms.bucket bucket : buckets) {\n        // 4.4.获取key\n        string key = bucket.getkeyasstring();\n        brandlist.add(key);\n    }\n    return brandlist;\n}\n\n\n\n# 自动补全\n\n当用户在搜索框输入字符时，我们应该提示出与该字符有关的搜索项，如图：\n\n\n\n这种根据用户输入的字母，提示完整词条的功能，就是自动补全了。\n\n因为需要根据拼音字母来推断，因此要用到拼音分词功能。\n\n\n# 拼音分词器\n\n要实现根据字母做补全，就必须对文档按照拼音分词。在github上恰好有elasticsearch的拼音分词插件。地址：https://github.com/medcl/elasticsearch-analysis-pinyin\n\n\n\n课前资料中也提供了拼音分词器的安装包：\n\n\n\n安装方式与ik分词器一样，分三步：\n\n①解压\n\n②上传到虚拟机中，elasticsearch的plugin目录\n\n③重启elasticsearch\n\n④测试\n\n详细安装步骤可以参考ik分词器的安装过程。\n\n测试用法如下：\n\npost /_analyze\n{\n  "text": "如家酒店还不错",\n  "analyzer": "pinyin"\n}\n\n\n结果：\n\n\n\n\n# 自定义分词器\n\n默认的拼音分词器会将每个汉字单独分为拼音，而我们希望的是每个词条形成一组拼音，需要对拼音分词器做个性化定制，形成自定义分词器。\n\nelasticsearch中分词器（analyzer）的组成包含三部分：\n\n * character filters：在tokenizer之前对文本进行处理。例如删除字符、替换字符\n * tokenizer：将文本按照一定的规则切割成词条（term）。例如keyword，就是不分词；还有ik_smart\n * tokenizer filter：将tokenizer输出的词条做进一步处理。例如大小写转换、同义词处理、拼音处理等\n\n文档分词时会依次由这三部分来处理文档：\n\n\n\n声明自定义分词器的语法如下：\n\nput /test\n{\n  "settings": {\n    "analysis": {\n      "analyzer": { // 自定义分词器\n        "my_analyzer": {  // 分词器名称\n          "tokenizer": "ik_max_word",\n          "filter": "py"\n        }\n      },\n      "filter": { // 自定义tokenizer filter\n        "py": { // 过滤器名称\n          "type": "pinyin", // 过滤器类型，这里是pinyin\n\t\t  "keep_full_pinyin": false,\n          "keep_joined_full_pinyin": true,\n          "keep_original": true, //保留中文\n          "limit_first_letter_length": 16,\n          "remove_duplicated_term": true,\n          "none_chinese_pinyin_tokenize": false\n        }\n      }\n    }\n  },\n  "mappings": {\n    "properties": {\n      "name": {\n        "type": "text",\n        "analyzer": "my_analyzer",//创建索引时使用的分词器--带拼音\n        "search_analyzer": "ik_smart"//搜索时使用的分词器-- 不带拼音\n      }\n    }\n  }\n}\n\n\n测试：\n\n\n\n总结：\n\n如何使用拼音分词器？\n\n * ①下载pinyin分词器\n\n * ②解压并放到elasticsearch的plugin目录\n\n * ③重启即可\n\n如何自定义分词器？\n\n * ①创建索引库时，在settings中配置，可以包含三部分\n\n * ②character filter\n\n * ③tokenizer\n\n * ④filter\n\n拼音分词器注意事项？\n\n * 为了避免搜索到同音字，搜索时不要使用拼音分词器\n\n\n# 自动补全查询\n\nelasticsearch提供了completion suggester查询来实现自动补全功能。这个查询会匹配以用户输入内容开头的词条并返回。为了提高补全查询的效率，对于文档中字段的类型有一些约束：\n\n * 参与补全查询的字段必须是completion类型。\n\n * 字段的内容一般是用来补全的多个词条形成的数组。\n   \n   * 这样，对于["sony", "wh-1000xm3"]，搜索输入 s 和 w 都可以补全\n\n比如，一个这样的索引库：\n\n// 创建索引库\nput test\n{\n  "mappings": {\n    "properties": {\n      "title":{\n        "type": "completion"//这里一定是completion类型\n      }\n    }\n  }\n}\n\n\n然后插入下面的数据：\n\n// 示例数据\npost test/_doc\n{\n  "title": ["sony", "wh-1000xm3"]\n}\npost test/_doc\n{\n  "title": ["sk-ii", "pitera"]\n}\npost test/_doc\n{\n  "title": ["nintendo", "switch"]\n}\n\n\n查询的dsl语句如下：\n\n// 自动补全查询\nget /test/_search\n{\n  "suggest": {\n    "title_suggest": {//查询名称，自定义\n      "text": "s", // 关键字\n      "completion": {\n        "field": "title", // 补全查询的字段\n        "skip_duplicates": true, // 跳过重复的\n        "size": 10 // 获取前10条结果\n      }\n    }\n  }\n}\n\n\n\n# 实现酒店搜索框自动补全\n\n现在，我们的hotel索引库还没有设置拼音分词器，需要修改索引库中的配置。但是我们知道索引库是无法修改的，只能删除然后重新创建。\n\n另外，我们需要添加一个字段，用来做自动补全，将brand、suggestion、city等都放进去，作为自动补全的提示。\n\n因此，总结一下，我们需要做的事情包括：\n\n 1. 修改hotel索引库结构，设置自定义拼音分词器\n\n 2. 修改索引库的name、all字段，使用自定义分词器\n\n 3. 索引库添加一个新字段suggestion，类型为completion类型，使用自定义的分词器\n\n 4. 给hoteldoc类添加suggestion字段，内容包含brand、business\n\n 5. 重新导入数据到hotel库\n\n# 修改酒店映射结构\n\n代码如下：\n\n// 酒店数据索引库\nput /hotel\n{\n  "settings": {\n    "analysis": {\n      "analyzer": {\n        "text_anlyzer": {//自定义分词器名称\n          "tokenizer": "ik_max_word",//中间用ik分割词语\n          "filter": "py"//最后用拼音分词器处理\n        },\n        "completion_analyzer": {\n          "tokenizer": "keyword",//中间 不用词语分割\n          "filter": "py" //最后用拼音分词器处理\n        }\n      },\n      "filter": {\n        "py": {\n          "type": "pinyin",\n          "keep_full_pinyin": false,// 刘德华> [liu,de,hua],默认为true\n          "keep_joined_full_pinyin": true,//刘德华> [liudehua],默认为false\n          "keep_original": true,//保留原始的汉字输入\n          "limit_first_letter_length": 16,\n          "remove_duplicated_term": true,//移除重复，[de的]>de\n          "none_chinese_pinyin_tokenize": false//保持拼音连续，不拆开\n        }\n      }\n    }\n  },\n  "mappings": {\n    "properties": {\n      "id":{\n        "type": "keyword"\n      },\n      "name":{\n        "type": "text",\n        "analyzer": "text_anlyzer",//创建索引时 使用自定义混合分词器\n        "search_analyzer": "ik_smart",//搜索时 使用ik分词器\n        "copy_to": "all"\n      },\n      "address":{\n        "type": "keyword",\n        "index": false\n      },\n      "price":{\n        "type": "integer"\n      },\n      "score":{\n        "type": "integer"\n      },\n      "brand":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "city":{\n        "type": "keyword"\n      },\n      "starname":{\n        "type": "keyword"\n      },\n      "business":{\n        "type": "keyword",\n        "copy_to": "all"\n      },\n      "location":{\n        "type": "geo_point"\n      },\n      "pic":{\n        "type": "keyword",\n        "index": false\n      },\n      "all":{\n        "type": "text",\n        "analyzer": "text_anlyzer",\n        "search_analyzer": "ik_smart"\n      },\n      "suggestion":{//关键词，自动补全字段\n          "type": "completion",//必须是completion类型\n          "analyzer": "completion_analyzer"//不分词，直接转拼音\n      }\n    }\n  }\n}\n\n\n# 修改hoteldoc实体\n\nhoteldoc中要添加一个字段，用来做自动补全，内容可以是酒店品牌、城市、商圈等信息。按照自动补全字段的要求，最好是这些字段的数组。\n\n因此我们在hoteldoc中添加一个suggestion字段，类型为list<string>，然后将brand、city、business等信息放到里面。\n\n代码如下：\n\npackage cn.itcast.hotel.pojo;\n\nimport lombok.data;\nimport lombok.noargsconstructor;\n\nimport java.util.arraylist;\nimport java.util.arrays;\nimport java.util.collections;\nimport java.util.list;\n\n@data\n@noargsconstructor\npublic class hoteldoc {\n    private long id;\n    private string name;\n    private string address;\n    private integer price;\n    private integer score;\n    private string brand;\n    private string city;\n    private string starname;\n    private string business;\n    private string location;\n    private string pic;\n    private object distance;\n    private boolean isad;\n    private list<string> suggestion;\n\n    public hoteldoc(hotel hotel) {\n        this.id = hotel.getid();\n        this.name = hotel.getname();\n        this.address = hotel.getaddress();\n        this.price = hotel.getprice();\n        this.score = hotel.getscore();\n        this.brand = hotel.getbrand();\n        this.city = hotel.getcity();\n        this.starname = hotel.getstarname();\n        this.business = hotel.getbusiness();\n        this.location = hotel.getlatitude() + ", " + hotel.getlongitude();\n        this.pic = hotel.getpic();\n        // 组装suggestion\n        if(this.business.contains("/")){\n            // business有多个值，需要切割\n            string[] arr = this.business.split("/");\n            // 添加元素\n            this.suggestion = new arraylist<>();\n            this.suggestion.add(this.brand);\n            collections.addall(this.suggestion, arr);\n        }else {\n            this.suggestion = arrays.aslist(this.brand, this.business);\n        }\n    }\n}\n\n\n# 重新导入\n\n重新执行之前编写的导入数据功能，可以看到新的酒店数据中包含了suggestion：\n\n\n\n# 自动补全查询的javaapi\n\n之前我们学习了自动补全查询的dsl，而没有学习对应的javaapi，这里给出一个示例：\n\n\n\n而自动补全的结果也比较特殊，解析的代码如下：\n\n\n\n# 实现搜索框自动补全\n\n查看前端页面，可以发现当我们在输入框键入时，前端会发起ajax请求：\n\n\n\n返回值是补全词条的集合，类型为list<string>\n\n1）在cn.itcast.hotel.web包下的hotelcontroller中添加新接口，接收新的请求：\n\n@getmapping("suggestion")\npublic list<string> getsuggestions(@requestparam("key") string prefix) {\n    return hotelservice.getsuggestions(prefix);\n}\n\n\n2）在cn.itcast.hotel.service包下的ihotelservice中添加方法：\n\nlist<string> getsuggestions(string prefix);\n\n\n3）在cn.itcast.hotel.service.impl.hotelservice中实现该方法：\n\n@override\npublic list<string> getsuggestions(string prefix) {\n    try {\n        // 1.准备request\n        searchrequest request = new searchrequest("hotel");\n        // 2.准备dsl\n        request.source().suggest(new suggestbuilder().addsuggestion(\n            "suggestions",\n            suggestbuilders.completionsuggestion("suggestion")\n            .prefix(prefix)\n            .skipduplicates(true)\n            .size(10)\n        ));\n        // 3.发起请求\n        searchresponse response = client.search(request, requestoptions.default);\n        // 4.解析结果\n        suggest suggest = response.getsuggest();\n        // 4.1.根据补全查询名称，获取补全结果\n        completionsuggestion suggestions = suggest.getsuggestion("suggestions");\n        // 4.2.获取options\n        list<completionsuggestion.entry.option> options = suggestions.getoptions();\n        // 4.3.遍历\n        list<string> list = new arraylist<>(options.size());\n        for (completionsuggestion.entry.option option : options) {\n            string text = option.gettext().tostring();\n            list.add(text);\n        }\n        return list;\n    } catch (ioexception e) {\n        throw new runtimeexception(e);\n    }\n}\n\n\n\n# 数据同步\n\nelasticsearch中的酒店数据来自于mysql数据库，因此mysql数据发生改变时，elasticsearch也必须跟着改变，这个就是elasticsearch与mysql之间的数据同步。\n\n\n\n\n# 思路分析\n\n常见的数据同步方案有三种：\n\n * 同步调用\n * 异步通知\n * 监听binlog\n\n# 同步调用\n\n方案一：同步调用\n\n\n\n基本步骤如下：\n\n * hotel-demo对外提供接口，用来修改elasticsearch中的数据\n * 酒店管理服务在完成数据库操作后，直接调用hotel-demo提供的接口，\n\n# 异步通知\n\n方案二：异步通知\n\n\n\n流程如下：\n\n * hotel-admin对mysql数据库数据完成增、删、改后，发送mq消息\n * hotel-demo监听mq，接收到消息后完成elasticsearch数据修改\n\n# 监听binlog\n\n方案三：监听binlog\n\n\n\n流程如下：\n\n * 给mysql开启binlog功能\n * mysql完成增、删、改操作都会记录在binlog中\n * hotel-demo基于canal监听binlog变化，实时更新elasticsearch中的内容\n\n# 选择\n\n方式一：同步调用\n\n * 优点：实现简单，粗暴\n * 缺点：业务耦合度高\n\n方式二：异步通知\n\n * 优点：低耦合，实现难度一般\n * 缺点：依赖mq的可靠性\n\n方式三：监听binlog\n\n * 优点：完全解除服务间耦合\n * 缺点：开启binlog增加数据库负担、实现复杂度高\n\n\n# 实现数据同步\n\n# 思路\n\n利用课前资料提供的hotel-admin项目作为酒店管理的微服务。当酒店数据发生增、删、改时，要求对elasticsearch中数据也要完成相同操作。\n\n步骤：\n\n * 导入课前资料提供的hotel-admin项目，启动并测试酒店数据的crud\n\n * 声明exchange、queue、routingkey\n\n * 在hotel-admin中的增、删、改业务中完成消息发送\n\n * 在hotel-demo中完成消息监听，并更新elasticsearch中数据\n\n * 启动并测试数据同步功能\n\n# 导入demo\n\n导入课前资料提供的hotel-admin项目：\n\n\n\n运行后，访问 http://localhost:8099\n\n\n\n其中包含了酒店的crud功能：\n\n\n\n# 声明交换机、队列\n\nmq结构如图：\n\n\n\n# 1）引入依赖\n\n在hotel-admin、hotel-demo中引入rabbitmq的依赖：\n\n\x3c!--amqp--\x3e\n<dependency>\n    <groupid>org.springframework.boot</groupid>\n    <artifactid>spring-boot-starter-amqp</artifactid>\n</dependency>\n\n\n# 2）声明队列交换机名称\n\n在hotel-admin和hotel-demo中的cn.itcast.hotel.constatnts包下新建一个类mqconstants：\n\npackage cn.itcast.hotel.constatnts;\n\n    public class mqconstants {\n    /**\n     * 交换机\n     */\n    public final static string hotel_exchange = "hotel.topic";\n    /**\n     * 监听新增和修改的队列\n     */\n    public final static string hotel_insert_queue = "hotel.insert.queue";\n    /**\n     * 监听删除的队列\n     */\n    public final static string hotel_delete_queue = "hotel.delete.queue";\n    /**\n     * 新增或修改的routingkey\n     */\n    public final static string hotel_insert_key = "hotel.insert";\n    /**\n     * 删除的routingkey\n     */\n    public final static string hotel_delete_key = "hotel.delete";\n}\n\n\n# 3）声明队列交换机\n\n在hotel-demo中，定义配置类，声明队列、交换机：\n\npackage cn.itcast.hotel.config;\n\nimport cn.itcast.hotel.constants.mqconstants;\nimport org.springframework.amqp.core.binding;\nimport org.springframework.amqp.core.bindingbuilder;\nimport org.springframework.amqp.core.queue;\nimport org.springframework.amqp.core.topicexchange;\nimport org.springframework.context.annotation.bean;\nimport org.springframework.context.annotation.configuration;\n\n@configuration\npublic class mqconfig {\n    @bean\n    public topicexchange topicexchange(){\n        return new topicexchange(mqconstants.hotel_exchange, true, false);\n    }\n\n    @bean\n    public queue insertqueue(){\n        return new queue(mqconstants.hotel_insert_queue, true);\n    }\n\n    @bean\n    public queue deletequeue(){\n        return new queue(mqconstants.hotel_delete_queue, true);\n    }\n\n    @bean\n    public binding insertqueuebinding(){\n        return bindingbuilder.bind(insertqueue()).to(topicexchange()).with(mqconstants.hotel_insert_key);\n    }\n\n    @bean\n    public binding deletequeuebinding(){\n        return bindingbuilder.bind(deletequeue()).to(topicexchange()).with(mqconstants.hotel_delete_key);\n    }\n}\n\n\n# 发送mq消息\n\n在hotel-admin中的增、删、改业务中分别发送mq消息：\n\n\n\n# 接收mq消息\n\nhotel-demo接收到mq消息要做的事情包括：\n\n * 新增消息：根据传递的hotel的id查询hotel信息，然后新增一条数据到索引库\n * 删除消息：根据传递的hotel的id删除索引库中的一条数据\n\n1）首先在hotel-demo的cn.itcast.hotel.service包下的ihotelservice中新增新增、删除业务\n\nvoid deletebyid(long id);\n\nvoid insertbyid(long id);\n\n\n2）给hotel-demo中的cn.itcast.hotel.service.impl包下的hotelservice中实现业务：\n\n@override\npublic void deletebyid(long id) {\n    try {\n        // 1.准备request\n        deleterequest request = new deleterequest("hotel", id.tostring());\n        // 2.发送请求\n        client.delete(request, requestoptions.default);\n    } catch (ioexception e) {\n        throw new runtimeexception(e);\n    }\n}\n\n@override\npublic void insertbyid(long id) {\n    try {\n        // 0.根据id查询酒店数据\n        hotel hotel = getbyid(id);\n        // 转换为文档类型\n        hoteldoc hoteldoc = new hoteldoc(hotel);\n\n        // 1.准备request对象\n        indexrequest request = new indexrequest("hotel").id(hotel.getid().tostring());\n        // 2.准备json文档\n        request.source(json.tojsonstring(hoteldoc), xcontenttype.json);\n        // 3.发送请求\n        client.index(request, requestoptions.default);\n    } catch (ioexception e) {\n        throw new runtimeexception(e);\n    }\n}\n\n\n3）编写监听器\n\n在hotel-demo中的cn.itcast.hotel.mq包新增一个类：\n\npackage cn.itcast.hotel.mq;\n\nimport cn.itcast.hotel.constants.mqconstants;\nimport cn.itcast.hotel.service.ihotelservice;\nimport org.springframework.amqp.rabbit.annotation.rabbitlistener;\nimport org.springframework.beans.factory.annotation.autowired;\nimport org.springframework.stereotype.component;\n\n@component\npublic class hotellistener {\n\n    @autowired\n    private ihotelservice hotelservice;\n\n    /**\n     * 监听酒店新增或修改的业务\n     * @param id 酒店id\n     */\n    @rabbitlistener(queues = mqconstants.hotel_insert_queue)\n    public void listenhotelinsertorupdate(long id){\n        hotelservice.insertbyid(id);\n    }\n\n    /**\n     * 监听酒店删除的业务\n     * @param id 酒店id\n     */\n    @rabbitlistener(queues = mqconstants.hotel_delete_queue)\n    public void listenhoteldelete(long id){\n        hotelservice.deletebyid(id);\n    }\n}\n\n\n# 问题\n\n 1. 常量命名一定要规范\n    \n    * 大写，下划线，带有文字注释\n      \n      /**\n      * 监听删除的队列\n      */\n      public final static string hotel_delete_queue = "hotel.delete.queue";\n      \n\n 2. topic类型的交换机，在进行队列和交换机进行绑定的时候，注意带上 routingkey\n    \n    @bean\n    public binding hoteldeletebinding(queue hoteldeletequeue, topicexchange hoteltopicexchange) {\n        //todo 这里要记得带上routingkey， 用with\n        return bindingbuilder.bind(hoteldeletequeue).to(hoteltopicexchange).with(hotel_delete_key);\n    }\n    \n\n 3. 往交换机中发送信息，信息为 新增对象的id， 体积小，占内存空间小\n    \n    * 不要直接发送java 的hotel 对象， 容易把mq占满\n    \n    //todo 往交换机中发送信息，信息为 新增对象的id  体积小，\n    // 不要直接发送java 的hotel 对象， 容易把mq占满\n    \n    // 交换机名称，routingkey, java类型对象\n    rabbittemplate.convertandsend(hotel_exchange, hotel_insert_key, hotel.getid());\n    \n\n 4. mybatisplus的save方法，可以返回新增数据的id**,但是id的生成策略 一定要设置成为自动增长**\n    \n    //todo 设置id自增，mybatisplus的添加方法才可以返回id\n    @tableid(type = idtype.auto)\n    private long id;\n    \n\n 5. 新增信息和修改信息，设置为同一个队列即可，因为在es中修改信息的全量修改方法和新增信息是一样的\n    \n    \n\n 6. id生成时，尽量用跟随数据库自动生成，即@tableid(type = idtype.auto)\n    \n    * 不要用雪花算法生成id，因为位数过长，如果前端不将id转换成字符串处理的话，会将id精度丢失。\n    \n    * 即java后端 生成id = 1586232717316648962 ,js接收信息为id=1586232717316649000\n    \n    * **这样后端在接收前端传来的id信息时，在数据库中无法找到这个id对应的信息。**会导致一系列错误。\n    \n    * 控制id自动生成的选项\n      \n      \n\n\n# 集群\n\n单机的elasticsearch做数据存储，必然面临两个问题：海量数据存储问题、单点故障问题。\n\n * 海量数据存储问题：将索引库从逻辑上拆分为n个分片（shard），存储到多个节点\n * 单点故障问题：将分片数据在不同节点备份（replica ）\n\nes集群相关概念:\n\n * 集群（cluster）：一组拥有共同的 cluster name 的 节点。\n\n * 节点（node) ：集群中的一个 elasticearch 实例\n\n * 分片（shard）：索引可以被拆分为不同的部分进行存储，称为分片。在集群环境下，一个索引的不同分片可以拆分到不同的节点中\n   \n   解决问题：数据量太大，单点存储量有限的问题。\n   \n   \n   \n   > 此处，我们把数据分成3片：shard0、shard1、shard2\n\n * 主分片（primary shard）：相对于副本分片的定义。\n\n * 副本分片（replica shard）每个主分片可以有一个或者多个副本，数据和主分片一样。\n   \n   \n\n数据备份可以保证高可用，但是每个分片备份一份，所需要的节点数量就会翻一倍，成本实在是太高了！\n\n为了在高可用和成本间寻求平衡，我们可以这样做：\n\n * 首先对数据分片，存储到不同节点\n * 然后对每个分片进行备份，放到对方节点，完成互相备份\n\n这样可以大大减少所需要的服务节点数量，如图，我们以3分片，每个分片备份一份为例：\n\n\n\n现在，每个分片都有1个备份，存储在3个节点：\n\n * node0：保存了分片0和1\n * node1：保存了分片0和2\n * node2：保存了分片1和2\n\n\n# 搭建es集群\n\n参考课前资料的文档：\n\n\n\n其中的第四章节：\n\n\n\n\n# 集群脑裂问题\n\n# 集群职责划分\n\nelasticsearch中集群节点有不同的职责划分：\n\n\n\n默认情况下，集群中的任何一个节点都同时具备上述四种角色。\n\n但是真实的集群一定要将集群职责分离：\n\n * master节点：对cpu要求高，但是内存要求第\n\n * data节点：对cpu和内存要求都高\n\n * coordinating节点：对网络带宽、cpu要求高\n\n * ingest节点，一般不用，数据预处理在后端进行\n\n职责分离可以让我们根据不同节点的需求分配不同的硬件去部署。而且避免业务之间的互相干扰。\n\n一个典型的es集群职责划分如图：\n\n\n\n# 脑裂问题\n\n脑裂是因为集群中的节点失联导致的。\n\n例如一个集群中，主节点与其它节点失联：\n\n\n\n此时，node2和node3认为node1宕机，就会重新选主：\n\n\n\n当node3当选后，集群继续对外提供服务，node2和node3自成集群，node1自成集群，两个集群数据不同步，出现数据差异。\n\n当网络恢复后，因为集群中有两个master节点，集群状态的不一致，出现脑裂的情况：\n\n\n\n解决脑裂的方案是，要求选票超过 ( eligible节点数量 + 1 ）/ 2 才能当选为主，因此eligible节点数量最好是奇数。对应配置项是discovery.zen.minimum_master_nodes，在es7.0以后，已经成为默认配置，因此一般不会发生脑裂问题\n\n例如：3个节点形成的集群，选票必须超过 （3 + 1） / 2 ，也就是2票。node3得到node2和node3的选票，当选为主。node1只有自己1票，没有当选。集群中依然只有1个主节点，没有出现脑裂。\n\n# 小结\n\nmaster eligible节点的作用是什么？\n\n * 参与集群选主\n * 主节点可以管理集群状态、管理分片信息、处理创建和删除索引库的请求\n\ndata节点的作用是什么？\n\n * 数据的crud\n\ncoordinator节点的作用是什么？\n\n * 路由请求到其它节点\n\n * 合并查询到的结果，返回给用户\n\n\n# 集群分布式存储\n\n当新增文档时，应该保存到不同分片，保证数据均衡，那么coordinating node如何确定数据该存储到哪个分片呢？\n\n# 分片存储测试\n\n插入三条数据：\n\n\n\n\n\n\n\n测试可以看到，三条数据分别在不同分片：\n\n查看数据具体位置--explain\n\n{\n    "explain":true,\n    "query":{\n        "match_all":{}\n    }\n}\n\n\n\n\n结果：\n\n\n\n# 分片存储原理\n\nelasticsearch会通过hash算法来计算文档应该存储到哪个分片：\n\n\n\n说明：\n\n * _routing默认是文档的id\n * 算法与分片数量有关，因此索引库一旦创建，分片数量不能修改！\n\n新增文档的流程如下：\n\n\n\n解读：\n\n * 1）新增一个id=1的文档\n * 2）对id做hash运算，假如得到的是2，则应该存储到shard-2\n * 3）shard-2的主分片在node3节点，将数据路由到node3\n * 4）保存文档\n * 5）同步给shard-2的副本replica-2，在node2节点\n * 6）返回结果给coordinating-node节点\n\n\n# 集群分布式查询\n\nelasticsearch的查询分成两个阶段：\n\n**coordinating node 协调节点进行请求的分发和结果的收集 **\n\n * scatter phase：分散阶段，coordinating node会把请求分发到每一个分片\n\n * gather phase：聚集阶段，coordinating node汇总data node的搜索结果，并处理为最终结果集返回给用户\n\n\n\n\n# 集群故障转移\n\n集群的master节点会监控集群中的节点状态，如果发现有节点宕机，会立即将宕机节点的分片数据迁移到其它节点，确保数据安全，这个叫做故障转移。\n\n1）例如一个集群结构如图：\n\n\n\n现在，node1是主节点，其它两个节点是从节点。\n\n2）突然，node1发生了故障：\n\n\n\n宕机后的第一件事，需要重新选主，例如选中了node2：\n\n\n\nnode2成为主节点后，会检测集群监控状态，发现：shard-1没有副本节点，shard-0没有主分片节点。因此需要将node1上的数据迁移到node2、node3：\n\n\n\n节点刚刚宕机(原先节点3是主节点，宕机后节点1被选为主节点)\n\n\n\n宕机后，数据转移\n\n\n\n宕机后数据查询\n\n数据还是分的3片，虽然节点少了，但是数据片数量没有改变，查询时即不受影响\n\n{\n    "took": 11,\n    "timed_out": false,\n    "_shards": {\n        "total": 3,\n        "successful": 3,\n        "skipped": 0,\n        "failed": 0\n    },\n    "hits": {\n        "total": {\n            "value": 3,\n            "relation": "eq"\n        },\n        "max_score": 1.0,\n        "hits": [\n            {\n                "_shard": "[diana][0]",\n                "_node": "6byo3olmq2or9yqawv7grg",\n                "_index": "diana",\n                "_type": "_doc",\n                "_id": "5",\n                "_score": 1.0,\n                "_source": {\n                    "title": "试着插入一条 id = 5"\n                },\n                "_explanation": {\n                    "value": 1.0,\n                    "description": "*:*",\n                    "details": []\n                }\n            },\n            {\n                "_shard": "[diana][1]",\n                "_node": "0hrl3vfmscygb_uvf-dmxw",\n                "_index": "diana",\n                "_type": "_doc",\n                "_id": "3",\n                "_score": 1.0,\n                "_source": {\n                    "title": "试着插入一条 id = 3"\n                },\n                "_explanation": {\n                    "value": 1.0,\n                    "description": "*:*",\n                    "details": []\n                }\n            },\n            {\n                "_shard": "[diana][2]",\n                "_node": "0hrl3vfmscygb_uvf-dmxw",\n                "_index": "diana",\n                "_type": "_doc",\n                "_id": "1",\n                "_score": 1.0,\n                "_source": {\n                    "title": "试着插入一条 id = 1"\n                },\n                "_explanation": {\n                    "value": 1.0,\n                    "description": "*:*",\n                    "details": []\n                }\n            }\n        ]\n    }\n}\n\n\n当node1重新上线后，主节点node2 会将数据重新分配给node1，但是node1不再是主节点。\n\n\n\n宕机恢复后（节点1依然是主节点，节点3的辉煌已不再）\n\n\n\n故障转移总结\n\n * master 宕机后，选举一个新的主节点\n * 新主节点监控分片、节点状态，将故障节点上的分片转移到正常节点，确保数据安全。\n * 当宕机节点重新上线后，主节点会将数据重新分配给它，让它正常工作。',charsets:{cjk:!0}},{title:"文档搜索",frontmatter:{autoSort:96,title:"文档搜索",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/a93137/",categories:["后端","微服务","ES"],tags:["知识","微服务","ES"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/20.ES/25.%E6%96%87%E6%A1%A3%E6%90%9C%E7%B4%A2.html",relativePath:"01.后端/60.微服务/20.ES/25.文档搜索.md",key:"v-29cee25f",path:"/pages/a93137/",headers:[{level:2,title:"DSL查询文档",slug:"dsl查询文档",normalizedTitle:"dsl查询文档",charIndex:2},{level:3,title:"DSL查询分类",slug:"dsl查询分类",normalizedTitle:"dsl查询分类",charIndex:52},{level:3,title:"全文检索查询",slug:"全文检索查询",normalizedTitle:"全文检索查询",charIndex:756},{level:3,title:"精准查询",slug:"精准查询",normalizedTitle:"精准查询",charIndex:1834},{level:3,title:"地理坐标查询",slug:"地理坐标查询",normalizedTitle:"地理坐标查询",charIndex:2885},{level:3,title:"复合查询",slug:"复合查询",normalizedTitle:"复合查询",charIndex:443},{level:2,title:"搜索结果处理",slug:"搜索结果处理",normalizedTitle:"搜索结果处理",charIndex:8622},{level:3,title:"排序",slug:"排序",normalizedTitle:"排序",charIndex:8658},{level:3,title:"分页",slug:"分页",normalizedTitle:"分页",charIndex:10039},{level:3,title:"高亮",slug:"高亮",normalizedTitle:"高亮",charIndex:11511},{level:3,title:"总结",slug:"总结-5",normalizedTitle:"总结",charIndex:1465},{level:2,title:"RestClient查询文档",slug:"restclient查询文档",normalizedTitle:"restclient查询文档",charIndex:12421},{level:3,title:"快速入门",slug:"快速入门",normalizedTitle:"快速入门",charIndex:12536},{level:3,title:"match查询",slug:"match查询",normalizedTitle:"match查询",charIndex:963},{level:3,title:"精确查询",slug:"精确查询",normalizedTitle:"精确查询",charIndex:267},{level:3,title:"布尔查询",slug:"布尔查询-2",normalizedTitle:"布尔查询",charIndex:4581},{level:3,title:"排序、分页",slug:"排序、分页",normalizedTitle:"排序、分页",charIndex:12815},{level:3,title:"高亮",slug:"高亮-2",normalizedTitle:"高亮",charIndex:11511},{level:3,title:"总结",slug:"总结-6",normalizedTitle:"总结",charIndex:1465}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"DSL查询文档 DSL查询分类 全文检索查询 精准查询 地理坐标查询 复合查询 搜索结果处理 排序 分页 高亮 总结 RestClient查询文档 快速入门 match查询 精确查询 布尔查询 排序、分页 高亮 总结",content:'# DSL查询文档\n\nelasticsearch的查询依然是基于JSON风格的DSL来实现的。\n\n\n# DSL查询分类\n\nElasticsearch提供了基于JSON的DSL（Domain Specific Language）来定义查询。常见的查询类型包括：\n\n * 查询所有：查询出所有数据，一般测试用。例如：match_all\n\n * 全文检索（full text）查询：利用分词器对用户输入内容分词，然后去倒排索引库中匹配。例如：\n   \n   * match_query\n   * multi_match_query\n\n * 精确查询：根据精确词条值查找数据，一般是查找keyword、数值、日期、boolean等类型字段。例如：\n   \n   * ids\n   * range\n   * term\n\n * 地理（geo）查询：根据经纬度查询。例如：\n   \n   * geo_distance\n   * geo_bounding_box\n\n * 复合（compound）查询：复合查询可以将上述各种查询条件组合起来，合并查询条件。例如：\n   \n   * bool\n   * function_score\n\n查询的语法基本一致：\n\nGET /indexName/_search\n{\n  "query": {\n    "查询类型": {\n      "查询条件": "条件值"\n    }\n  }\n}\n\n\n我们以查询所有为例，其中：\n\n * 查询类型为match_all\n * 没有查询条件\n\n// 查询所有\nGET /indexName/_search\n{\n  "query": {\n    "match_all": {\n    }\n  }\n}\n\n\n其它查询无非就是查询类型、查询条件的变化。\n\n\n# 全文检索查询\n\n# 使用场景\n\n全文检索查询的基本流程如下：\n\n * 对用户搜索的内容做分词，得到词条\n * 根据词条去倒排索引库中匹配，得到文档id\n * 根据文档id找到文档，返回给用户\n\n比较常用的场景包括：\n\n * 商城的输入框搜索\n * 百度输入框搜索\n\n例如京东：\n\n\n\n因为是拿着词条去匹配，因此参与搜索的字段也必须是可分词的text类型的字段。\n\n# 基本语法\n\n常见的全文检索查询包括：\n\n * match查询：单字段查询\n * multi_match查询：多字段查询，任意一个字段符合条件就算符合查询条件\n\nmatch查询语法如下：\n\nGET /indexName/_search\n{\n  "query": {\n    "match": {\n      "FIELD": "TEXT"\n    }\n  }\n}\n\n\nmulit_match语法如下：\n\nGET /indexName/_search\n{\n  "query": {\n    "multi_match": {\n      "query": "TEXT",\n      "fields": ["FIELD1", " FIELD12"]\n    }\n  }\n}\n\n\n# 示例\n\nmatch查询示例：\n\n\n\nmulti_match查询示例：\n\n\n\n可以看到，两种查询结果是一样的，为什么？\n\n因为我们将brand、name、business值都利用copy_to复制到了all字段中。因此你根据三个字段搜索，和根据all字段搜索效果当然一样了。\n\n但是，搜索字段越多，对查询性能影响越大，因此建议采用copy_to，然后单字段查询的方式。\n\n# 总结\n\nmatch和multi_match的区别是什么？\n\n######全文检索\n##match  单个字段查询  --性能高，推荐\nGET /hotel/_search\n{\n  "query": {\n    "match": {\n      "all": "外滩如家"\n    }\n  }\n}\n\n##multi_match  查询多个字段\nGET /hotel/_search\n{\n  "query": {\n    "multi_match": {\n      "query": "外滩如家",\n      "fields":["brand","name", "business"]\n    }\n  }\n}\n\n\n * match：根据一个字段查询\n * multi_match：根据多个字段查询，参与查询字段越多，查询性能越差\n\n\n# 精准查询\n\n精确查询一般是查找keyword、数值、日期、boolean等类型字段。所以不会对搜索条件分词。常见的有：\n\n * term：根据词条精确值查询\n * range：根据值的范围查询\n\n# term查询\n\n因为精确查询的字段搜是不分词的字段，因此查询的条件也必须是不分词的词条。查询时，用户输入的内容跟自动值完全匹配时才认为符合条件。如果用户输入的内容过多，反而搜索不到数据。\n\n语法说明：\n\n// term查询\nGET /indexName/_search\n{\n  "query": {\n    "term": {\n      "FIELD": {\n        "value": "VALUE"\n      }\n    }\n  }\n}\n\n\n示例：\n\n当我搜索的是精确词条时，能正确查询出结果：\n\n\n\n但是，当我搜索的内容不是词条，而是多个词语形成的短语时，反而搜索不到：\n\n\n\n# range查询\n\n范围查询，一般应用在对数值类型做范围过滤的时候。比如做价格范围过滤。\n\n基本语法：\n\n// range查询\nGET /indexName/_search\n{\n  "query": {\n    "range": {\n      "FIELD": {\n        "gte": 10, // 这里的gte代表大于等于，gt则代表大于\n        "lte": 20 // lte代表小于等于，lt则代表小于\n      }\n    }\n  }\n}\n\n\n示例：\n\n\n\n# 总结\n\n######精确查询\n##term  根据词条精确值查询\nGET /hotel/_search\n{\n  "query": {\n    "term": {\n      "city": {\n        "value": "上海"\n      }\n    }\n  }\n}\n\n##range 根据值的范围查询\nGET /hotel/_search\n{\n  "query": {\n    "range": {\n      "price": {\n        "gte": 100, //带e是等于  大于等于-gte；大于gt\n        "lte": 300\n      }\n    }\n  }\n}\n\n\n精确查询常见的有哪些？\n\n * term查询：根据词条精确匹配，一般搜索keyword类型、数值类型、布尔类型、日期类型字段\n * range查询：根据数值范围查询，可以是数值、日期的范围\n\n\n# 地理坐标查询\n\n所谓的地理坐标查询，其实就是根据经纬度查询，官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/geo-queries.html\n\n常见的使用场景包括：\n\n * 携程：搜索我附近的酒店\n * 滴滴：搜索我附近的出租车\n * 微信：搜索我附近的人\n\n附近的酒店：\n\n\n\n附近的车：\n\n\n\n# 矩形范围查询\n\n矩形范围查询，也就是geo_bounding_box查询，查询坐标落在某个矩形范围的所有文档：\n\n\n\n查询时，需要指定矩形的左上、右下两个点的坐标，然后画出一个矩形，落在该矩形内的都是符合条件的点。\n\n语法如下：\n\n// geo_bounding_box查询\nGET /indexName/_search\n{\n  "query": {\n    "geo_bounding_box": {\n      "FIELD": {\n        "top_left": { // 左上点\n          "lat": 31.1,\n          "lon": 121.5\n        },\n        "bottom_right": { // 右下点\n          "lat": 30.9,\n          "lon": 121.7\n        }\n      }\n    }\n  }\n}\n\n\n这种并不符合“附近的人”这样的需求，所以我们就不做了。\n\n# 附近查询\n\n附近查询，也叫做距离查询（geo_distance）：查询到指定中心点小于某个距离值的所有文档。\n\n换句话来说，在地图上找一个点作为圆心，以指定距离为半径，画一个圆，落在圆内的坐标都算符合条件：\n\n\n\n语法说明：\n\n// geo_distance 查询\nGET /indexName/_search\n{\n  "query": {\n    "geo_distance": {\n      "distance": "15km", // 半径\n      "FIELD": "31.21,121.5" // 圆心\n    }\n  }\n}\n\n\n示例：\n\n我们先搜索陆家嘴附近15km的酒店：\n\n\n\n发现共有47家酒店。\n\n然后把半径缩短到3公里：\n\n\n\n可以发现，搜索到的酒店数量减少到了5家。\n\n# 总结\n\n######地理查询\n##geo_bounding_box 矩形范围查询，左上，右下坐标\nGET /hotel/_search\n{\n  "query": {\n    "geo_bounding_box": {\n      "location": {\n        "top_left": {//左上\n          "lat": 31.1,\n          "lon": 121.5\n        },\n        "bottom_right": {//右下\n          "lat": 30.9,\n          "lon": 121.7\n        }\n      }\n     \n    }\n  }\n}\n\n\n##geo_distance 距离查询  以坐标为中心画一个圆  --常用\nGET /hotel/_search\n{\n  "query": {\n    "geo_distance": {\n      "distance": "5km",\n      "location": "31.21, 121.5"\n    }\n  }\n}\n\n\n 1. geo_bounding_box查询——查询坐标落在某个矩形范围的所有文档\n 2. 距离查询（geo_distance)——查询到指定中心点小于某个距离值的所有文档。\n\n\n# 复合查询\n\n复合（compound）查询：复合查询可以将其它简单查询组合起来，实现更复杂的搜索逻辑。常见的有两种：\n\n * fuction score：算分函数查询，可以控制文档相关性算分，控制文档排名\n * bool query：布尔查询，利用逻辑关系组合多个其它的查询，实现复杂搜索\n\n# 相关性算分\n\n当我们利用match查询时，文档结果会根据与搜索词条的关联度打分（_score），返回结果时按照分值降序排列。\n\n例如，我们搜索 "虹桥如家"，结果如下：\n\n[\n  {\n    "_score" : 17.850193,\n    "_source" : {\n      "name" : "虹桥如家酒店真不错",\n    }\n  },\n  {\n    "_score" : 12.259849,\n    "_source" : {\n      "name" : "外滩如家酒店真不错",\n    }\n  },\n  {\n    "_score" : 11.91091,\n    "_source" : {\n      "name" : "迪士尼如家酒店真不错",\n    }\n  }\n]\n\n\n在elasticsearch中，早期使用的打分算法是TF-IDF算法，公式如下：\n\n\n\n在后来的5.1版本升级中，elasticsearch将算法改进为BM25算法，公式如下：\n\n\n\nTF-IDF算法有一各缺陷，就是词条频率越高，文档得分也会越高，单个词条对文档影响较大。而BM25则会让单个词条的算分有一个上限，曲线更加平滑：\n\n\n\n小结：elasticsearch会根据词条和文档的相关度做打分，算法由两种：\n\n * TF-IDF算法\n * BM25算法，elasticsearch5.1版本后采用的算法\n\n# 算分函数查询\n\n根据相关度打分是比较合理的需求，但合理的不一定是产品经理需要的。\n\n以百度为例，你搜索的结果中，并不是相关度越高排名越靠前，而是谁掏的钱多排名就越靠前。如图：\n\n\n\n要想认为控制相关性算分，就需要利用elasticsearch中的function score 查询了。\n\n# 1）语法说明\n\n\n\nfunction score 查询中包含四部分内容：\n\n * 原始查询条件：query部分，基于这个条件搜索文档，并且基于BM25算法给文档打分，原始算分（query score)\n * 过滤条件：filter部分，符合该条件的文档才会重新算分\n * 算分函数：符合filter条件的文档要根据这个函数做运算，得到的函数算分（function score），有四种函数\n   * weight：函数结果是常量\n   * field_value_factor：以文档中的某个字段值作为函数结果\n   * random_score：以随机数作为函数结果\n   * script_score：自定义算分函数算法\n * 运算模式：算分函数的结果、原始查询的相关性算分，两者之间的运算方式，包括：\n   * multiply：相乘\n   * replace：用function score替换query score\n   * 其它，例如：sum、avg、max、min\n\nfunction score的运行流程如下：\n\n * 1）根据原始条件查询搜索文档，并且计算相关性算分，称为原始算分（query score）\n * 2）根据过滤条件，过滤文档\n * 3）符合过滤条件的文档，基于算分函数运算，得到函数算分（function score）\n * 4）将原始算分（query score）和函数算分（function score）基于运算模式做运算，得到最终结果，作为相关性算分。\n\n因此，其中的关键点是：\n\n * 过滤条件：决定哪些文档的算分被修改\n * 算分函数：决定函数算分的算法\n * 运算模式：决定最终算分结果\n\n# 2）示例\n\n需求：给“如家”这个品牌的酒店排名靠前一些\n\n翻译一下这个需求，转换为之前说的四个要点：\n\n * 原始条件：不确定，可以任意变化\n * 过滤条件：brand = "如家"\n * 算分函数：可以简单粗暴，直接给固定的算分结果，weight\n * 运算模式：比如求和\n\n因此最终的DSL语句如下：\n\n######复合查询\n##默认按相关度打分，现在要修改打分数 function_score\nGET /hotel/_search\n{\n  "query": {\n    "function_score": {\n      "query": {//正常查询\n        "match": {\n          "all": "外滩"\n        }\n      },\n      "functions": [\n        {\n          "filter": {//筛选条件\n            "term": {\n              "brand": "如家"\n            }\n          },\n          "weight": 10 //算分权值  默认是乘算\n        }\n      ],\n      "boost_mode": "sum"//乘法运算\n    }\n  }\n}\n\n\n测试，在未添加算分函数时，如家得分如下：\n\n\n\n添加了算分函数后，如家得分就提升了：\n\n\n\n# 3）小结\n\nfunction score query定义的三要素是什么？\n\n * 过滤条件：哪些文档要加分\n * 算分函数：如何计算function score\n * 加权方式：function score 与 query score如何运算,默认是乘算\n\n# 布尔查询\n\n布尔查询是一个或多个查询子句的组合，每一个子句就是一个子查询。子查询的组合方式有：\n\n * 算分查询\n   \n   * must：必须匹配每个子查询，类似“与”\n   \n   * should：选择性匹配子查询，类似“或”\n\n * 不算分查询\n   * 不算分，效率高；还可以将其放入缓存，得到更高的效率; 应该尽可能将所有不算分的项都放到这里面\n   * must_not：必须不匹配，不参与算分，类似“非”\n   * filter：必须匹配，不参与算分\n\n比如在搜索酒店时，除了关键字搜索外，我们还可能根据品牌、价格、城市等字段做过滤：\n\n\n\n每一个不同的字段，其查询的条件、方式都不一样，必须是多个不同的查询，而要组合这些查询，就必须用bool查询了。\n\n需要注意的是，搜索时，参与打分的字段越多，查询的性能也越差。因此这种多条件查询时，建议这样做：\n\n * 搜索框的关键字搜索，是全文检索查询，使用must查询，参与算分\n * 其它过滤条件，采用filter查询。不参与算分\n\n# 1）语法示例：\n\nGET /hotel/_search\n{\n  "query": {\n    "bool": {\n      "must": [//城市一定是上海\n        {"term": {"city": "上海" }}\n      ],\n      "should": [//品牌 两者中的一个\n        {"term": {"brand": "皇冠假日" }},\n        {"term": {"brand": "华美达" }}\n      ],\n      "must_not": [//不能小于500\n        { "range": { "price": { "lte": 500 } }}\n      ],\n      "filter": [//大于等于45分\n        { "range": {"score": { "gte": 45 } }}\n      ]\n    }\n  }\n}\n\n\n# 2）示例\n\n需求：搜索名字包含“如家”，价格不高于400，在坐标31.21,121.5周围10km范围内的酒店。\n\n分析：\n\n * 名称搜索，属于全文检索查询，应该参与算分。放到must中\n * 价格不高于400，用range查询，属于过滤条件，不参与算分。放到must_not中\n * 周围10km范围内，用geo_distance查询，属于过滤条件，不参与算分。放到filter中\n\n\n\nGET /hotel/_search\n{\n  "query": {\n    "bool": {\n      "must": [//必须是如家品牌\n        {\n          "term": {\n            "brand": {\n              "value": "如家"\n            }\n          }\n        }\n      ],\n      "must_not": [//必须不大于400 -- 小于等于400\n        {\n          "range": {\n            "price": {//大于400\n              "gt": 400\n            }\n          }\n        }\n      ],\n      "filter": [//筛选范围\n        {\n          "geo_distance": {\n            "distance": "10km",\n            "location": "31.21, 121.5"\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n# 3）小结\n\nbool查询有几种逻辑关系？ —————— 关键字搜索一般放到must，其他基本都不算分\n\n * must：必须匹配的条件，可以理解为“与”\n * should：选择性匹配的条件，可以理解为“或”\n * must_not：必须不匹配的条件，不参与打分\n * filter：必须匹配的条件，不参与打分\n\n\n# 搜索结果处理\n\n搜索的结果可以按照用户指定的方式去处理或展示。\n\n\n# 排序\n\nelasticsearch默认是根据相关度算分（_score）来排序，但是也支持自定义方式对搜索结果排序。可以排序字段类型有：keyword类型、数值类型、地理坐标类型、日期类型等。\n\n# 普通字段排序\n\nkeyword、数值、日期类型排序的语法基本一致。\n\n语法：\n\nGET /indexName/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "sort": [\n    {\n      "FIELD": "desc"  // 排序字段、排序方式ASC、DESC\n    }\n  ]\n}\n\n\n排序条件是一个数组，也就是可以写多个排序条件。按照声明的顺序，当第一个条件相等时，再按照第二个条件排序，以此类推\n\n示例：\n\n需求描述：酒店数据按照用户评价（score)降序排序，评价相同的按照价格(price)升序排序\n\n\n\n# 地理坐标排序\n\n地理坐标排序略有不同。\n\n语法说明：\n\nGET /indexName/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "sort": [\n    {\n      "_geo_distance" : {\n          "FIELD" : "纬度，经度", // 文档中geo_point类型的字段名、目标坐标点\n          "order" : "asc", // 排序方式\n          "unit" : "km" // 排序的距离单位\n      }\n    }\n  ]\n}\n\n\n这个查询的含义是：\n\n * 指定一个坐标，作为目标点\n * 计算每一个文档中，指定字段（必须是geo_point类型）的坐标 到目标点的距离是多少\n * 根据距离排序\n\n示例：\n\n需求描述：实现对酒店数据按照到你的位置坐标的距离升序排序\n\n提示：获取你的位置的经纬度的方式：https://lbs.amap.com/demo/jsapi-v2/example/map/click-to-get-lnglat/\n\n假设我的位置是：31.034661，121.612282，寻找我周围距离最近的酒店。\n\n\n\n# 总结\n\n######排序\n##普通字段排序\nGET /hotel/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "sort": [//先按评分降序，在按价格升序\n    {\n      "score": "desc"\n    },\n    {\n      "price": "asc"\n    }\n  ]\n}\n##地理坐标排序\nGET /hotel/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "sort": [\n    {\n      "_geo_distance": {\n        "location": {\n          "lat": 40, //纬度\n          "lon": -70 //经度\n        },\n        "order": "desc",\n        "unit": "km"\n      }\n    }\n  ]\n}\n\n\n\n# 分页\n\nelasticsearch 默认情况下只返回top10的数据。而如果要查询更多数据就需要修改分页参数了。elasticsearch中通过修改from、size参数来控制要返回的分页结果：\n\n * from：从第几个文档开始\n * size：总共查询几个文档\n\n类似于mysql中的limit ?, ?\n\n# 基本的分页\n\n分页的基本语法如下：\n\nGET /hotel/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "from": 0, // 分页开始的位置，默认为0\n  "size": 10, // 期望获取的文档总数\n  "sort": [\n    {"price": "asc"}\n  ]\n}\n\n\n# 深度分页问题\n\n现在，我要查询990~1000的数据，查询逻辑要这么写：\n\nGET /hotel/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "from": 990, // 分页开始的位置，默认为0\n  "size": 10, // 期望获取的文档总数\n  "sort": [\n    {"price": "asc"}\n  ]\n}\n\n\n这里是查询990开始的数据，也就是 第990~第1000条 数据。\n\n不过，elasticsearch内部分页时，必须先查询 0~1000条，然后截取其中的990 ~ 1000的这10条：\n\n\n\n查询TOP1000，如果es是单点模式，这并无太大影响。\n\n但是elasticsearch将来一定是集群，例如我集群有5个节点，我要查询TOP1000的数据，并不是每个节点查询200条就可以了。\n\n因为节点A的TOP200，在另一个节点可能排到10000名以外了。\n\n因此要想获取整个集群的TOP1000，必须先查询出每个节点的TOP1000，汇总结果后，重新排名，重新截取TOP1000。\n\n\n\n那如果我要查询9900~10000的数据呢？是不是要先查询TOP10000呢？那每个节点都要查询10000条？汇总到内存中？\n\n当查询分页深度较大时，汇总数据过多，对内存和CPU会产生非常大的压力，因此elasticsearch会禁止from+ size 超过10000的请求。\n\n针对深度分页，ES提供了两种解决方案，官方文档：\n\n * search after：分页时需要排序，原理是从上一次的排序值开始，查询下一页数据。官方推荐使用的方式。\n * scroll：原理将排序后的文档id形成快照，保存在内存。官方已经不推荐使用。\n\n# 小结\n\n分页查询的常见实现方案以及优缺点：\n\n * from + size：\n   \n   * 优点：支持随机翻页\n   * 缺点：深度分页问题，默认查询上限（from + size）是10000\n   * 场景：百度、京东、谷歌、淘宝这样的随机翻页搜索\n\n * after search：\n   \n   * 优点：没有查询上限（单次查询的size不超过10000）\n   * 缺点：只能向后逐页查询，不支持随机翻页\n   * 场景：没有随机翻页需求的搜索，例如手机向下滚动翻页\n\n * scroll：\n   \n   * 优点：没有查询上限（单次查询的size不超过10000）\n   * 缺点：会有额外内存消耗，并且搜索结果是非实时的\n   * 场景：海量数据的获取和迁移。从ES7.1开始不推荐，建议用 after search方案。\n\n\n# 高亮\n\n# 高亮原理\n\n什么是高亮显示呢？\n\n我们在百度，京东搜索时，关键字会变成红色，比较醒目，这叫高亮显示：\n\n\n\n高亮显示的实现分为两步：\n\n * 1）给文档中的所有关键字都添加一个标签，例如<em>标签\n * 2）页面给<em>标签编写CSS样式\n\n# 实现高亮\n\n高亮的语法：\n\nGET /hotel/_search\n{\n  "query": {\n    "match": {\n      "FIELD": "TEXT" // 查询条件，高亮一定要使用全文检索查询\n    }\n  },\n  "highlight": {\n    "fields": { // 指定要高亮的字段\n      "FIELD": {\n        "pre_tags": "<em>",  // 用来标记高亮字段的前置标签\n        "post_tags": "</em>" // 用来标记高亮字段的后置标签\n      }\n    }\n  }\n}\n\n\n注意：\n\n * 高亮是对关键字高亮，因此搜索条件必须带有关键字，而不能是范围这样的查询。\n * 默认情况下，高亮的字段，必须与搜索指定的字段一致，否则无法高亮\n * 如果要对非搜索字段高亮，则需要添加一个属性：required_field_match=false\n\n示例：\n\n\n\nGET /hotel/_search\n{\n  "query": {\n    "match": {\n      "all": "如家"\n    }\n  },\n  "highlight": {\n    "fields": {// 指定要高亮的字段\n      "name": {//默认就是加 <em></em>\n        "require_field_match": "false"//默认是要字段匹配，这是设置成不需要匹配\n      }\n    }\n  }\n}\n\n\n\n# 总结\n\n查询的DSL是一个大的JSON对象，包含下列属性：\n\n * query：查询条件\n * from和size：分页条件\n * sort：排序条件\n * highlight：高亮条件\n\n示例：\n\n\n\n\n# RestClient查询文档\n\n文档的查询同样适用昨天学习的 RestHighLevelClient对象，基本步骤包括：\n\n * 1）准备Request对象\n * 2）准备请求参数\n * 3）发起请求\n * 4）解析响应\n\n\n# 快速入门\n\n我们以match_all查询为例\n\n# 发起查询请求\n\n\n\n代码解读：\n\n * 第一步，创建SearchRequest对象，指定索引库名\n\n * 第二步，利用request.source()构建DSL，DSL中可以包含查询、分页、排序、高亮等\n   \n   * query()：代表查询条件，利用QueryBuilders.matchAllQuery()构建一个match_all查询的DSL\n\n * 第三步，利用client.search()发送请求，得到响应\n\n这里关键的API有两个，一个是request.source()，其中包含了查询、排序、分页、高亮等所有功能：\n\n\n\n另一个是QueryBuilders，其中包含match、term、function_score、bool等各种查询：\n\n\n\n# 解析响应\n\n响应结果的解析：\n\n\n\nelasticsearch返回的结果是一个JSON字符串，结构包含：\n\n * hits：命中的结果\n   * total：总条数，其中的value是具体的总条数值\n   * max_score：所有结果中得分最高的文档的相关性算分\n   * hits：搜索结果的文档数组，其中的每个文档都是一个json对象\n     * _source：文档中的原始数据，也是json对象\n\n因此，我们解析响应结果，就是逐层解析JSON字符串，流程如下：\n\n * SearchHits：通过response.getHits()获取，就是JSON中的最外层的hits，代表命中的结果\n   * SearchHits##getTotalHits().value：获取总条数信息\n   * SearchHits##getHits()：获取SearchHit数组，也就是文档数组\n     * SearchHit##getSourceAsString()：获取文档结果中的_source，也就是原始的json文档数据\n\n# 完整代码\n\n完整代码如下：\n\n@Test\nvoid testMatchAll() throws IOException {\n    // 1.准备Request\n    SearchRequest request = new SearchRequest("hotel");\n    // 2.准备DSL\n    request.source()\n        .query(QueryBuilders.matchAllQuery());\n    // 3.发送请求\n    SearchResponse response = client.search(request, RequestOptions.DEFAULT);\n\n    // 4.解析响应\n    handleResponse(response);\n}\n\nprivate void handleResponse(SearchResponse response) {\n    // 4.解析响应\n    SearchHits searchHits = response.getHits();\n    // 4.1.获取总条数\n    long total = searchHits.getTotalHits().value;\n    System.out.println("共搜索到" + total + "条数据");\n    // 4.2.文档数组\n    SearchHit[] hits = searchHits.getHits();\n    // 4.3.遍历\n    for (SearchHit hit : hits) {\n        // 获取文档source\n        String json = hit.getSourceAsString();\n        // 反序列化\n        HotelDoc hotelDoc = JSON.parseObject(json, HotelDoc.class);\n        System.out.println("hotelDoc = " + hotelDoc);\n    }\n}\n\n\n# 小结\n\n查询的基本步骤是：\n\n 1. 创建SearchRequest对象\n\n 2. 准备Request.source()，也就是DSL。\n    \n    ① QueryBuilders来构建查询条件\n    \n    ② 传入Request.source() 的 query() 方法\n\n 3. 发送请求，得到结果\n\n 4. 解析结果（参考JSON结果，从外到内，逐层解析）\n\n\n# match查询\n\n全文检索的match和multi_match查询与match_all的API基本一致。差别是查询条件，也就是query的部分。\n\n\n\n因此，Java代码上的差异主要是request.source().query()中的参数了。同样是利用QueryBuilders提供的方法：\n\n\n\n而结果解析代码则完全一致，可以抽取并共享。\n\nctrl+alt+M\n\n完整代码如下：\n\n@Test\nvoid testMatch() throws IOException {\n    // 1.准备Request\n    SearchRequest request = new SearchRequest("hotel");\n    // 2.准备DSL\n    request.source()\n        .query(QueryBuilders.matchQuery("all", "如家"));\n    // 3.发送请求\n    SearchResponse response = client.search(request, RequestOptions.DEFAULT);\n    // 4.解析响应\n    handleResponse(response);\n\n}\n\n\n\n# 精确查询\n\n精确查询主要是两者：\n\n * term：词条精确匹配\n * range：范围查询\n\n与之前的查询相比，差异同样在查询条件，其它都一样。\n\n查询条件构造的API如下：\n\n\n\n\n# 布尔查询\n\n布尔查询是用must、must_not、filter等方式组合其它查询，代码示例如下：\n\n\n\n可以看到，API与其它查询的差别同样是在查询条件的构建，QueryBuilders，结果解析等其他代码完全不变。\n\n完整代码如下：\n\n@Test\nvoid testBool() throws IOException {\n    // 1.准备Request\n    SearchRequest request = new SearchRequest("hotel");\n    // 2.准备DSL\n    // 2.1.准备BooleanQuery\n    BoolQueryBuilder boolQuery = QueryBuilders.boolQuery();\n    // 2.2.添加term\n    boolQuery.must(QueryBuilders.termQuery("city", "杭州"));\n    // 2.3.添加range\n    boolQuery.filter(QueryBuilders.rangeQuery("price").lte(250));\n\n    request.source().query(boolQuery);\n    // 3.发送请求\n    SearchResponse response = client.search(request, RequestOptions.DEFAULT);\n    // 4.解析响应\n    handleResponse(response);\n\n}\n\n\n\n# 排序、分页\n\n搜索结果的排序和分页是与query同级的参数，因此同样是使用request.source()来设置。\n\n对应的API如下：\n\n\n\n完整代码示例：\n\n@Test\nvoid testPageAndSort() throws IOException {\n    // 页码，每页大小\n    int page = 1, size = 5;\n\n    // 1.准备Request\n    SearchRequest request = new SearchRequest("hotel");\n    // 2.准备DSL\n    // 2.1.query\n    request.source().query(QueryBuilders.matchAllQuery());\n    // 2.2.排序 sort\n    request.source().sort("price", SortOrder.ASC);\n    // 2.3.分页 from、size\n    request.source().from((page - 1) * size).size(5);\n    // 3.发送请求\n    SearchResponse response = client.search(request, RequestOptions.DEFAULT);\n    // 4.解析响应\n    handleResponse(response);\n\n}\n\n\n\n# 高亮\n\n高亮的代码与之前代码差异较大，有两点：\n\n * 查询的DSL：其中除了查询条件，还需要添加高亮条件，同样是与query同级。\n * 结果解析：结果除了要解析_source文档数据，还要解析高亮结果\n\n# 高亮请求构建\n\n高亮请求的构建API如下：\n\n\n\n上述代码省略了查询条件部分，但是大家不要忘了：高亮查询必须使用全文检索查询，并且要有搜索关键字，将来才可以对关键字高亮。\n\n完整代码如下：\n\n@Test\nvoid testHighlight() throws IOException {\n    // 1.准备Request\n    SearchRequest request = new SearchRequest("hotel");\n    // 2.准备DSL\n    // 2.1.query\n    request.source().query(QueryBuilders.matchQuery("all", "如家"));\n    // 2.2.高亮\n    request.source().highlighter(new HighlightBuilder().field("name").requireFieldMatch(false));\n    // 3.发送请求\n    SearchResponse response = client.search(request, RequestOptions.DEFAULT);\n    // 4.解析响应\n    handleResponse(response);\n\n}\n\n\n# 高亮结果解析\n\n高亮的结果与查询的文档结果默认是分离的，并不在一起。\n\n因此解析高亮的代码需要额外处理：\n\n\n\n代码解读：\n\n * 第一步：从结果中获取source。hit.getSourceAsString()，这部分是非高亮结果，json字符串。还需要反序列为HotelDoc对象\n * 第二步：获取高亮结果。hit.getHighlightFields()，返回值是一个Map，key是高亮字段名称，值是HighlightField对象，代表高亮值\n * 第三步：从map中根据高亮字段名称，获取高亮字段值对象HighlightField\n * 第四步：从HighlightField中获取Fragments，并且转为字符串。这部分就是真正的高亮字符串了\n * 第五步：用高亮的结果替换HotelDoc中的非高亮结果\n\n完整代码如下：\n\nprivate void handleResponse(SearchResponse response) {\n    // 4.解析响应\n    SearchHits searchHits = response.getHits();\n    // 4.1.获取总条数\n    long total = searchHits.getTotalHits().value;\n    System.out.println("共搜索到" + total + "条数据");\n    // 4.2.文档数组\n    SearchHit[] hits = searchHits.getHits();\n    // 4.3.遍历\n    for (SearchHit hit : hits) {\n        // 获取文档source\n        String json = hit.getSourceAsString();\n        // 反序列化\n        HotelDoc hotelDoc = JSON.parseObject(json, HotelDoc.class);\n        // 获取高亮结果\n        Map<String, HighlightField> highlightFields = hit.getHighlightFields();\n        if (!CollectionUtils.isEmpty(highlightFields)) {\n            // 根据字段名获取高亮结果\n            HighlightField highlightField = highlightFields.get("name");\n            if (highlightField != null) {\n                // 获取高亮值\n                String name = highlightField.getFragments()[0].string();\n                // 覆盖非高亮结果\n                hotelDoc.setName(name);\n            }\n        }\n        System.out.println("hotelDoc = " + hotelDoc);\n    }\n}\n\n\n\n# 总结\n\n * 所有的DSL的构建，记住一个API：\n   * SearchRequest的source()方法\n * 高亮结果解析是参考JSON结果，逐层解析',normalizedContent:'# dsl查询文档\n\nelasticsearch的查询依然是基于json风格的dsl来实现的。\n\n\n# dsl查询分类\n\nelasticsearch提供了基于json的dsl（domain specific language）来定义查询。常见的查询类型包括：\n\n * 查询所有：查询出所有数据，一般测试用。例如：match_all\n\n * 全文检索（full text）查询：利用分词器对用户输入内容分词，然后去倒排索引库中匹配。例如：\n   \n   * match_query\n   * multi_match_query\n\n * 精确查询：根据精确词条值查找数据，一般是查找keyword、数值、日期、boolean等类型字段。例如：\n   \n   * ids\n   * range\n   * term\n\n * 地理（geo）查询：根据经纬度查询。例如：\n   \n   * geo_distance\n   * geo_bounding_box\n\n * 复合（compound）查询：复合查询可以将上述各种查询条件组合起来，合并查询条件。例如：\n   \n   * bool\n   * function_score\n\n查询的语法基本一致：\n\nget /indexname/_search\n{\n  "query": {\n    "查询类型": {\n      "查询条件": "条件值"\n    }\n  }\n}\n\n\n我们以查询所有为例，其中：\n\n * 查询类型为match_all\n * 没有查询条件\n\n// 查询所有\nget /indexname/_search\n{\n  "query": {\n    "match_all": {\n    }\n  }\n}\n\n\n其它查询无非就是查询类型、查询条件的变化。\n\n\n# 全文检索查询\n\n# 使用场景\n\n全文检索查询的基本流程如下：\n\n * 对用户搜索的内容做分词，得到词条\n * 根据词条去倒排索引库中匹配，得到文档id\n * 根据文档id找到文档，返回给用户\n\n比较常用的场景包括：\n\n * 商城的输入框搜索\n * 百度输入框搜索\n\n例如京东：\n\n\n\n因为是拿着词条去匹配，因此参与搜索的字段也必须是可分词的text类型的字段。\n\n# 基本语法\n\n常见的全文检索查询包括：\n\n * match查询：单字段查询\n * multi_match查询：多字段查询，任意一个字段符合条件就算符合查询条件\n\nmatch查询语法如下：\n\nget /indexname/_search\n{\n  "query": {\n    "match": {\n      "field": "text"\n    }\n  }\n}\n\n\nmulit_match语法如下：\n\nget /indexname/_search\n{\n  "query": {\n    "multi_match": {\n      "query": "text",\n      "fields": ["field1", " field12"]\n    }\n  }\n}\n\n\n# 示例\n\nmatch查询示例：\n\n\n\nmulti_match查询示例：\n\n\n\n可以看到，两种查询结果是一样的，为什么？\n\n因为我们将brand、name、business值都利用copy_to复制到了all字段中。因此你根据三个字段搜索，和根据all字段搜索效果当然一样了。\n\n但是，搜索字段越多，对查询性能影响越大，因此建议采用copy_to，然后单字段查询的方式。\n\n# 总结\n\nmatch和multi_match的区别是什么？\n\n######全文检索\n##match  单个字段查询  --性能高，推荐\nget /hotel/_search\n{\n  "query": {\n    "match": {\n      "all": "外滩如家"\n    }\n  }\n}\n\n##multi_match  查询多个字段\nget /hotel/_search\n{\n  "query": {\n    "multi_match": {\n      "query": "外滩如家",\n      "fields":["brand","name", "business"]\n    }\n  }\n}\n\n\n * match：根据一个字段查询\n * multi_match：根据多个字段查询，参与查询字段越多，查询性能越差\n\n\n# 精准查询\n\n精确查询一般是查找keyword、数值、日期、boolean等类型字段。所以不会对搜索条件分词。常见的有：\n\n * term：根据词条精确值查询\n * range：根据值的范围查询\n\n# term查询\n\n因为精确查询的字段搜是不分词的字段，因此查询的条件也必须是不分词的词条。查询时，用户输入的内容跟自动值完全匹配时才认为符合条件。如果用户输入的内容过多，反而搜索不到数据。\n\n语法说明：\n\n// term查询\nget /indexname/_search\n{\n  "query": {\n    "term": {\n      "field": {\n        "value": "value"\n      }\n    }\n  }\n}\n\n\n示例：\n\n当我搜索的是精确词条时，能正确查询出结果：\n\n\n\n但是，当我搜索的内容不是词条，而是多个词语形成的短语时，反而搜索不到：\n\n\n\n# range查询\n\n范围查询，一般应用在对数值类型做范围过滤的时候。比如做价格范围过滤。\n\n基本语法：\n\n// range查询\nget /indexname/_search\n{\n  "query": {\n    "range": {\n      "field": {\n        "gte": 10, // 这里的gte代表大于等于，gt则代表大于\n        "lte": 20 // lte代表小于等于，lt则代表小于\n      }\n    }\n  }\n}\n\n\n示例：\n\n\n\n# 总结\n\n######精确查询\n##term  根据词条精确值查询\nget /hotel/_search\n{\n  "query": {\n    "term": {\n      "city": {\n        "value": "上海"\n      }\n    }\n  }\n}\n\n##range 根据值的范围查询\nget /hotel/_search\n{\n  "query": {\n    "range": {\n      "price": {\n        "gte": 100, //带e是等于  大于等于-gte；大于gt\n        "lte": 300\n      }\n    }\n  }\n}\n\n\n精确查询常见的有哪些？\n\n * term查询：根据词条精确匹配，一般搜索keyword类型、数值类型、布尔类型、日期类型字段\n * range查询：根据数值范围查询，可以是数值、日期的范围\n\n\n# 地理坐标查询\n\n所谓的地理坐标查询，其实就是根据经纬度查询，官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/geo-queries.html\n\n常见的使用场景包括：\n\n * 携程：搜索我附近的酒店\n * 滴滴：搜索我附近的出租车\n * 微信：搜索我附近的人\n\n附近的酒店：\n\n\n\n附近的车：\n\n\n\n# 矩形范围查询\n\n矩形范围查询，也就是geo_bounding_box查询，查询坐标落在某个矩形范围的所有文档：\n\n\n\n查询时，需要指定矩形的左上、右下两个点的坐标，然后画出一个矩形，落在该矩形内的都是符合条件的点。\n\n语法如下：\n\n// geo_bounding_box查询\nget /indexname/_search\n{\n  "query": {\n    "geo_bounding_box": {\n      "field": {\n        "top_left": { // 左上点\n          "lat": 31.1,\n          "lon": 121.5\n        },\n        "bottom_right": { // 右下点\n          "lat": 30.9,\n          "lon": 121.7\n        }\n      }\n    }\n  }\n}\n\n\n这种并不符合“附近的人”这样的需求，所以我们就不做了。\n\n# 附近查询\n\n附近查询，也叫做距离查询（geo_distance）：查询到指定中心点小于某个距离值的所有文档。\n\n换句话来说，在地图上找一个点作为圆心，以指定距离为半径，画一个圆，落在圆内的坐标都算符合条件：\n\n\n\n语法说明：\n\n// geo_distance 查询\nget /indexname/_search\n{\n  "query": {\n    "geo_distance": {\n      "distance": "15km", // 半径\n      "field": "31.21,121.5" // 圆心\n    }\n  }\n}\n\n\n示例：\n\n我们先搜索陆家嘴附近15km的酒店：\n\n\n\n发现共有47家酒店。\n\n然后把半径缩短到3公里：\n\n\n\n可以发现，搜索到的酒店数量减少到了5家。\n\n# 总结\n\n######地理查询\n##geo_bounding_box 矩形范围查询，左上，右下坐标\nget /hotel/_search\n{\n  "query": {\n    "geo_bounding_box": {\n      "location": {\n        "top_left": {//左上\n          "lat": 31.1,\n          "lon": 121.5\n        },\n        "bottom_right": {//右下\n          "lat": 30.9,\n          "lon": 121.7\n        }\n      }\n     \n    }\n  }\n}\n\n\n##geo_distance 距离查询  以坐标为中心画一个圆  --常用\nget /hotel/_search\n{\n  "query": {\n    "geo_distance": {\n      "distance": "5km",\n      "location": "31.21, 121.5"\n    }\n  }\n}\n\n\n 1. geo_bounding_box查询——查询坐标落在某个矩形范围的所有文档\n 2. 距离查询（geo_distance)——查询到指定中心点小于某个距离值的所有文档。\n\n\n# 复合查询\n\n复合（compound）查询：复合查询可以将其它简单查询组合起来，实现更复杂的搜索逻辑。常见的有两种：\n\n * fuction score：算分函数查询，可以控制文档相关性算分，控制文档排名\n * bool query：布尔查询，利用逻辑关系组合多个其它的查询，实现复杂搜索\n\n# 相关性算分\n\n当我们利用match查询时，文档结果会根据与搜索词条的关联度打分（_score），返回结果时按照分值降序排列。\n\n例如，我们搜索 "虹桥如家"，结果如下：\n\n[\n  {\n    "_score" : 17.850193,\n    "_source" : {\n      "name" : "虹桥如家酒店真不错",\n    }\n  },\n  {\n    "_score" : 12.259849,\n    "_source" : {\n      "name" : "外滩如家酒店真不错",\n    }\n  },\n  {\n    "_score" : 11.91091,\n    "_source" : {\n      "name" : "迪士尼如家酒店真不错",\n    }\n  }\n]\n\n\n在elasticsearch中，早期使用的打分算法是tf-idf算法，公式如下：\n\n\n\n在后来的5.1版本升级中，elasticsearch将算法改进为bm25算法，公式如下：\n\n\n\ntf-idf算法有一各缺陷，就是词条频率越高，文档得分也会越高，单个词条对文档影响较大。而bm25则会让单个词条的算分有一个上限，曲线更加平滑：\n\n\n\n小结：elasticsearch会根据词条和文档的相关度做打分，算法由两种：\n\n * tf-idf算法\n * bm25算法，elasticsearch5.1版本后采用的算法\n\n# 算分函数查询\n\n根据相关度打分是比较合理的需求，但合理的不一定是产品经理需要的。\n\n以百度为例，你搜索的结果中，并不是相关度越高排名越靠前，而是谁掏的钱多排名就越靠前。如图：\n\n\n\n要想认为控制相关性算分，就需要利用elasticsearch中的function score 查询了。\n\n# 1）语法说明\n\n\n\nfunction score 查询中包含四部分内容：\n\n * 原始查询条件：query部分，基于这个条件搜索文档，并且基于bm25算法给文档打分，原始算分（query score)\n * 过滤条件：filter部分，符合该条件的文档才会重新算分\n * 算分函数：符合filter条件的文档要根据这个函数做运算，得到的函数算分（function score），有四种函数\n   * weight：函数结果是常量\n   * field_value_factor：以文档中的某个字段值作为函数结果\n   * random_score：以随机数作为函数结果\n   * script_score：自定义算分函数算法\n * 运算模式：算分函数的结果、原始查询的相关性算分，两者之间的运算方式，包括：\n   * multiply：相乘\n   * replace：用function score替换query score\n   * 其它，例如：sum、avg、max、min\n\nfunction score的运行流程如下：\n\n * 1）根据原始条件查询搜索文档，并且计算相关性算分，称为原始算分（query score）\n * 2）根据过滤条件，过滤文档\n * 3）符合过滤条件的文档，基于算分函数运算，得到函数算分（function score）\n * 4）将原始算分（query score）和函数算分（function score）基于运算模式做运算，得到最终结果，作为相关性算分。\n\n因此，其中的关键点是：\n\n * 过滤条件：决定哪些文档的算分被修改\n * 算分函数：决定函数算分的算法\n * 运算模式：决定最终算分结果\n\n# 2）示例\n\n需求：给“如家”这个品牌的酒店排名靠前一些\n\n翻译一下这个需求，转换为之前说的四个要点：\n\n * 原始条件：不确定，可以任意变化\n * 过滤条件：brand = "如家"\n * 算分函数：可以简单粗暴，直接给固定的算分结果，weight\n * 运算模式：比如求和\n\n因此最终的dsl语句如下：\n\n######复合查询\n##默认按相关度打分，现在要修改打分数 function_score\nget /hotel/_search\n{\n  "query": {\n    "function_score": {\n      "query": {//正常查询\n        "match": {\n          "all": "外滩"\n        }\n      },\n      "functions": [\n        {\n          "filter": {//筛选条件\n            "term": {\n              "brand": "如家"\n            }\n          },\n          "weight": 10 //算分权值  默认是乘算\n        }\n      ],\n      "boost_mode": "sum"//乘法运算\n    }\n  }\n}\n\n\n测试，在未添加算分函数时，如家得分如下：\n\n\n\n添加了算分函数后，如家得分就提升了：\n\n\n\n# 3）小结\n\nfunction score query定义的三要素是什么？\n\n * 过滤条件：哪些文档要加分\n * 算分函数：如何计算function score\n * 加权方式：function score 与 query score如何运算,默认是乘算\n\n# 布尔查询\n\n布尔查询是一个或多个查询子句的组合，每一个子句就是一个子查询。子查询的组合方式有：\n\n * 算分查询\n   \n   * must：必须匹配每个子查询，类似“与”\n   \n   * should：选择性匹配子查询，类似“或”\n\n * 不算分查询\n   * 不算分，效率高；还可以将其放入缓存，得到更高的效率; 应该尽可能将所有不算分的项都放到这里面\n   * must_not：必须不匹配，不参与算分，类似“非”\n   * filter：必须匹配，不参与算分\n\n比如在搜索酒店时，除了关键字搜索外，我们还可能根据品牌、价格、城市等字段做过滤：\n\n\n\n每一个不同的字段，其查询的条件、方式都不一样，必须是多个不同的查询，而要组合这些查询，就必须用bool查询了。\n\n需要注意的是，搜索时，参与打分的字段越多，查询的性能也越差。因此这种多条件查询时，建议这样做：\n\n * 搜索框的关键字搜索，是全文检索查询，使用must查询，参与算分\n * 其它过滤条件，采用filter查询。不参与算分\n\n# 1）语法示例：\n\nget /hotel/_search\n{\n  "query": {\n    "bool": {\n      "must": [//城市一定是上海\n        {"term": {"city": "上海" }}\n      ],\n      "should": [//品牌 两者中的一个\n        {"term": {"brand": "皇冠假日" }},\n        {"term": {"brand": "华美达" }}\n      ],\n      "must_not": [//不能小于500\n        { "range": { "price": { "lte": 500 } }}\n      ],\n      "filter": [//大于等于45分\n        { "range": {"score": { "gte": 45 } }}\n      ]\n    }\n  }\n}\n\n\n# 2）示例\n\n需求：搜索名字包含“如家”，价格不高于400，在坐标31.21,121.5周围10km范围内的酒店。\n\n分析：\n\n * 名称搜索，属于全文检索查询，应该参与算分。放到must中\n * 价格不高于400，用range查询，属于过滤条件，不参与算分。放到must_not中\n * 周围10km范围内，用geo_distance查询，属于过滤条件，不参与算分。放到filter中\n\n\n\nget /hotel/_search\n{\n  "query": {\n    "bool": {\n      "must": [//必须是如家品牌\n        {\n          "term": {\n            "brand": {\n              "value": "如家"\n            }\n          }\n        }\n      ],\n      "must_not": [//必须不大于400 -- 小于等于400\n        {\n          "range": {\n            "price": {//大于400\n              "gt": 400\n            }\n          }\n        }\n      ],\n      "filter": [//筛选范围\n        {\n          "geo_distance": {\n            "distance": "10km",\n            "location": "31.21, 121.5"\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n# 3）小结\n\nbool查询有几种逻辑关系？ —————— 关键字搜索一般放到must，其他基本都不算分\n\n * must：必须匹配的条件，可以理解为“与”\n * should：选择性匹配的条件，可以理解为“或”\n * must_not：必须不匹配的条件，不参与打分\n * filter：必须匹配的条件，不参与打分\n\n\n# 搜索结果处理\n\n搜索的结果可以按照用户指定的方式去处理或展示。\n\n\n# 排序\n\nelasticsearch默认是根据相关度算分（_score）来排序，但是也支持自定义方式对搜索结果排序。可以排序字段类型有：keyword类型、数值类型、地理坐标类型、日期类型等。\n\n# 普通字段排序\n\nkeyword、数值、日期类型排序的语法基本一致。\n\n语法：\n\nget /indexname/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "sort": [\n    {\n      "field": "desc"  // 排序字段、排序方式asc、desc\n    }\n  ]\n}\n\n\n排序条件是一个数组，也就是可以写多个排序条件。按照声明的顺序，当第一个条件相等时，再按照第二个条件排序，以此类推\n\n示例：\n\n需求描述：酒店数据按照用户评价（score)降序排序，评价相同的按照价格(price)升序排序\n\n\n\n# 地理坐标排序\n\n地理坐标排序略有不同。\n\n语法说明：\n\nget /indexname/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "sort": [\n    {\n      "_geo_distance" : {\n          "field" : "纬度，经度", // 文档中geo_point类型的字段名、目标坐标点\n          "order" : "asc", // 排序方式\n          "unit" : "km" // 排序的距离单位\n      }\n    }\n  ]\n}\n\n\n这个查询的含义是：\n\n * 指定一个坐标，作为目标点\n * 计算每一个文档中，指定字段（必须是geo_point类型）的坐标 到目标点的距离是多少\n * 根据距离排序\n\n示例：\n\n需求描述：实现对酒店数据按照到你的位置坐标的距离升序排序\n\n提示：获取你的位置的经纬度的方式：https://lbs.amap.com/demo/jsapi-v2/example/map/click-to-get-lnglat/\n\n假设我的位置是：31.034661，121.612282，寻找我周围距离最近的酒店。\n\n\n\n# 总结\n\n######排序\n##普通字段排序\nget /hotel/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "sort": [//先按评分降序，在按价格升序\n    {\n      "score": "desc"\n    },\n    {\n      "price": "asc"\n    }\n  ]\n}\n##地理坐标排序\nget /hotel/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "sort": [\n    {\n      "_geo_distance": {\n        "location": {\n          "lat": 40, //纬度\n          "lon": -70 //经度\n        },\n        "order": "desc",\n        "unit": "km"\n      }\n    }\n  ]\n}\n\n\n\n# 分页\n\nelasticsearch 默认情况下只返回top10的数据。而如果要查询更多数据就需要修改分页参数了。elasticsearch中通过修改from、size参数来控制要返回的分页结果：\n\n * from：从第几个文档开始\n * size：总共查询几个文档\n\n类似于mysql中的limit ?, ?\n\n# 基本的分页\n\n分页的基本语法如下：\n\nget /hotel/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "from": 0, // 分页开始的位置，默认为0\n  "size": 10, // 期望获取的文档总数\n  "sort": [\n    {"price": "asc"}\n  ]\n}\n\n\n# 深度分页问题\n\n现在，我要查询990~1000的数据，查询逻辑要这么写：\n\nget /hotel/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "from": 990, // 分页开始的位置，默认为0\n  "size": 10, // 期望获取的文档总数\n  "sort": [\n    {"price": "asc"}\n  ]\n}\n\n\n这里是查询990开始的数据，也就是 第990~第1000条 数据。\n\n不过，elasticsearch内部分页时，必须先查询 0~1000条，然后截取其中的990 ~ 1000的这10条：\n\n\n\n查询top1000，如果es是单点模式，这并无太大影响。\n\n但是elasticsearch将来一定是集群，例如我集群有5个节点，我要查询top1000的数据，并不是每个节点查询200条就可以了。\n\n因为节点a的top200，在另一个节点可能排到10000名以外了。\n\n因此要想获取整个集群的top1000，必须先查询出每个节点的top1000，汇总结果后，重新排名，重新截取top1000。\n\n\n\n那如果我要查询9900~10000的数据呢？是不是要先查询top10000呢？那每个节点都要查询10000条？汇总到内存中？\n\n当查询分页深度较大时，汇总数据过多，对内存和cpu会产生非常大的压力，因此elasticsearch会禁止from+ size 超过10000的请求。\n\n针对深度分页，es提供了两种解决方案，官方文档：\n\n * search after：分页时需要排序，原理是从上一次的排序值开始，查询下一页数据。官方推荐使用的方式。\n * scroll：原理将排序后的文档id形成快照，保存在内存。官方已经不推荐使用。\n\n# 小结\n\n分页查询的常见实现方案以及优缺点：\n\n * from + size：\n   \n   * 优点：支持随机翻页\n   * 缺点：深度分页问题，默认查询上限（from + size）是10000\n   * 场景：百度、京东、谷歌、淘宝这样的随机翻页搜索\n\n * after search：\n   \n   * 优点：没有查询上限（单次查询的size不超过10000）\n   * 缺点：只能向后逐页查询，不支持随机翻页\n   * 场景：没有随机翻页需求的搜索，例如手机向下滚动翻页\n\n * scroll：\n   \n   * 优点：没有查询上限（单次查询的size不超过10000）\n   * 缺点：会有额外内存消耗，并且搜索结果是非实时的\n   * 场景：海量数据的获取和迁移。从es7.1开始不推荐，建议用 after search方案。\n\n\n# 高亮\n\n# 高亮原理\n\n什么是高亮显示呢？\n\n我们在百度，京东搜索时，关键字会变成红色，比较醒目，这叫高亮显示：\n\n\n\n高亮显示的实现分为两步：\n\n * 1）给文档中的所有关键字都添加一个标签，例如<em>标签\n * 2）页面给<em>标签编写css样式\n\n# 实现高亮\n\n高亮的语法：\n\nget /hotel/_search\n{\n  "query": {\n    "match": {\n      "field": "text" // 查询条件，高亮一定要使用全文检索查询\n    }\n  },\n  "highlight": {\n    "fields": { // 指定要高亮的字段\n      "field": {\n        "pre_tags": "<em>",  // 用来标记高亮字段的前置标签\n        "post_tags": "</em>" // 用来标记高亮字段的后置标签\n      }\n    }\n  }\n}\n\n\n注意：\n\n * 高亮是对关键字高亮，因此搜索条件必须带有关键字，而不能是范围这样的查询。\n * 默认情况下，高亮的字段，必须与搜索指定的字段一致，否则无法高亮\n * 如果要对非搜索字段高亮，则需要添加一个属性：required_field_match=false\n\n示例：\n\n\n\nget /hotel/_search\n{\n  "query": {\n    "match": {\n      "all": "如家"\n    }\n  },\n  "highlight": {\n    "fields": {// 指定要高亮的字段\n      "name": {//默认就是加 <em></em>\n        "require_field_match": "false"//默认是要字段匹配，这是设置成不需要匹配\n      }\n    }\n  }\n}\n\n\n\n# 总结\n\n查询的dsl是一个大的json对象，包含下列属性：\n\n * query：查询条件\n * from和size：分页条件\n * sort：排序条件\n * highlight：高亮条件\n\n示例：\n\n\n\n\n# restclient查询文档\n\n文档的查询同样适用昨天学习的 resthighlevelclient对象，基本步骤包括：\n\n * 1）准备request对象\n * 2）准备请求参数\n * 3）发起请求\n * 4）解析响应\n\n\n# 快速入门\n\n我们以match_all查询为例\n\n# 发起查询请求\n\n\n\n代码解读：\n\n * 第一步，创建searchrequest对象，指定索引库名\n\n * 第二步，利用request.source()构建dsl，dsl中可以包含查询、分页、排序、高亮等\n   \n   * query()：代表查询条件，利用querybuilders.matchallquery()构建一个match_all查询的dsl\n\n * 第三步，利用client.search()发送请求，得到响应\n\n这里关键的api有两个，一个是request.source()，其中包含了查询、排序、分页、高亮等所有功能：\n\n\n\n另一个是querybuilders，其中包含match、term、function_score、bool等各种查询：\n\n\n\n# 解析响应\n\n响应结果的解析：\n\n\n\nelasticsearch返回的结果是一个json字符串，结构包含：\n\n * hits：命中的结果\n   * total：总条数，其中的value是具体的总条数值\n   * max_score：所有结果中得分最高的文档的相关性算分\n   * hits：搜索结果的文档数组，其中的每个文档都是一个json对象\n     * _source：文档中的原始数据，也是json对象\n\n因此，我们解析响应结果，就是逐层解析json字符串，流程如下：\n\n * searchhits：通过response.gethits()获取，就是json中的最外层的hits，代表命中的结果\n   * searchhits##gettotalhits().value：获取总条数信息\n   * searchhits##gethits()：获取searchhit数组，也就是文档数组\n     * searchhit##getsourceasstring()：获取文档结果中的_source，也就是原始的json文档数据\n\n# 完整代码\n\n完整代码如下：\n\n@test\nvoid testmatchall() throws ioexception {\n    // 1.准备request\n    searchrequest request = new searchrequest("hotel");\n    // 2.准备dsl\n    request.source()\n        .query(querybuilders.matchallquery());\n    // 3.发送请求\n    searchresponse response = client.search(request, requestoptions.default);\n\n    // 4.解析响应\n    handleresponse(response);\n}\n\nprivate void handleresponse(searchresponse response) {\n    // 4.解析响应\n    searchhits searchhits = response.gethits();\n    // 4.1.获取总条数\n    long total = searchhits.gettotalhits().value;\n    system.out.println("共搜索到" + total + "条数据");\n    // 4.2.文档数组\n    searchhit[] hits = searchhits.gethits();\n    // 4.3.遍历\n    for (searchhit hit : hits) {\n        // 获取文档source\n        string json = hit.getsourceasstring();\n        // 反序列化\n        hoteldoc hoteldoc = json.parseobject(json, hoteldoc.class);\n        system.out.println("hoteldoc = " + hoteldoc);\n    }\n}\n\n\n# 小结\n\n查询的基本步骤是：\n\n 1. 创建searchrequest对象\n\n 2. 准备request.source()，也就是dsl。\n    \n    ① querybuilders来构建查询条件\n    \n    ② 传入request.source() 的 query() 方法\n\n 3. 发送请求，得到结果\n\n 4. 解析结果（参考json结果，从外到内，逐层解析）\n\n\n# match查询\n\n全文检索的match和multi_match查询与match_all的api基本一致。差别是查询条件，也就是query的部分。\n\n\n\n因此，java代码上的差异主要是request.source().query()中的参数了。同样是利用querybuilders提供的方法：\n\n\n\n而结果解析代码则完全一致，可以抽取并共享。\n\nctrl+alt+m\n\n完整代码如下：\n\n@test\nvoid testmatch() throws ioexception {\n    // 1.准备request\n    searchrequest request = new searchrequest("hotel");\n    // 2.准备dsl\n    request.source()\n        .query(querybuilders.matchquery("all", "如家"));\n    // 3.发送请求\n    searchresponse response = client.search(request, requestoptions.default);\n    // 4.解析响应\n    handleresponse(response);\n\n}\n\n\n\n# 精确查询\n\n精确查询主要是两者：\n\n * term：词条精确匹配\n * range：范围查询\n\n与之前的查询相比，差异同样在查询条件，其它都一样。\n\n查询条件构造的api如下：\n\n\n\n\n# 布尔查询\n\n布尔查询是用must、must_not、filter等方式组合其它查询，代码示例如下：\n\n\n\n可以看到，api与其它查询的差别同样是在查询条件的构建，querybuilders，结果解析等其他代码完全不变。\n\n完整代码如下：\n\n@test\nvoid testbool() throws ioexception {\n    // 1.准备request\n    searchrequest request = new searchrequest("hotel");\n    // 2.准备dsl\n    // 2.1.准备booleanquery\n    boolquerybuilder boolquery = querybuilders.boolquery();\n    // 2.2.添加term\n    boolquery.must(querybuilders.termquery("city", "杭州"));\n    // 2.3.添加range\n    boolquery.filter(querybuilders.rangequery("price").lte(250));\n\n    request.source().query(boolquery);\n    // 3.发送请求\n    searchresponse response = client.search(request, requestoptions.default);\n    // 4.解析响应\n    handleresponse(response);\n\n}\n\n\n\n# 排序、分页\n\n搜索结果的排序和分页是与query同级的参数，因此同样是使用request.source()来设置。\n\n对应的api如下：\n\n\n\n完整代码示例：\n\n@test\nvoid testpageandsort() throws ioexception {\n    // 页码，每页大小\n    int page = 1, size = 5;\n\n    // 1.准备request\n    searchrequest request = new searchrequest("hotel");\n    // 2.准备dsl\n    // 2.1.query\n    request.source().query(querybuilders.matchallquery());\n    // 2.2.排序 sort\n    request.source().sort("price", sortorder.asc);\n    // 2.3.分页 from、size\n    request.source().from((page - 1) * size).size(5);\n    // 3.发送请求\n    searchresponse response = client.search(request, requestoptions.default);\n    // 4.解析响应\n    handleresponse(response);\n\n}\n\n\n\n# 高亮\n\n高亮的代码与之前代码差异较大，有两点：\n\n * 查询的dsl：其中除了查询条件，还需要添加高亮条件，同样是与query同级。\n * 结果解析：结果除了要解析_source文档数据，还要解析高亮结果\n\n# 高亮请求构建\n\n高亮请求的构建api如下：\n\n\n\n上述代码省略了查询条件部分，但是大家不要忘了：高亮查询必须使用全文检索查询，并且要有搜索关键字，将来才可以对关键字高亮。\n\n完整代码如下：\n\n@test\nvoid testhighlight() throws ioexception {\n    // 1.准备request\n    searchrequest request = new searchrequest("hotel");\n    // 2.准备dsl\n    // 2.1.query\n    request.source().query(querybuilders.matchquery("all", "如家"));\n    // 2.2.高亮\n    request.source().highlighter(new highlightbuilder().field("name").requirefieldmatch(false));\n    // 3.发送请求\n    searchresponse response = client.search(request, requestoptions.default);\n    // 4.解析响应\n    handleresponse(response);\n\n}\n\n\n# 高亮结果解析\n\n高亮的结果与查询的文档结果默认是分离的，并不在一起。\n\n因此解析高亮的代码需要额外处理：\n\n\n\n代码解读：\n\n * 第一步：从结果中获取source。hit.getsourceasstring()，这部分是非高亮结果，json字符串。还需要反序列为hoteldoc对象\n * 第二步：获取高亮结果。hit.gethighlightfields()，返回值是一个map，key是高亮字段名称，值是highlightfield对象，代表高亮值\n * 第三步：从map中根据高亮字段名称，获取高亮字段值对象highlightfield\n * 第四步：从highlightfield中获取fragments，并且转为字符串。这部分就是真正的高亮字符串了\n * 第五步：用高亮的结果替换hoteldoc中的非高亮结果\n\n完整代码如下：\n\nprivate void handleresponse(searchresponse response) {\n    // 4.解析响应\n    searchhits searchhits = response.gethits();\n    // 4.1.获取总条数\n    long total = searchhits.gettotalhits().value;\n    system.out.println("共搜索到" + total + "条数据");\n    // 4.2.文档数组\n    searchhit[] hits = searchhits.gethits();\n    // 4.3.遍历\n    for (searchhit hit : hits) {\n        // 获取文档source\n        string json = hit.getsourceasstring();\n        // 反序列化\n        hoteldoc hoteldoc = json.parseobject(json, hoteldoc.class);\n        // 获取高亮结果\n        map<string, highlightfield> highlightfields = hit.gethighlightfields();\n        if (!collectionutils.isempty(highlightfields)) {\n            // 根据字段名获取高亮结果\n            highlightfield highlightfield = highlightfields.get("name");\n            if (highlightfield != null) {\n                // 获取高亮值\n                string name = highlightfield.getfragments()[0].string();\n                // 覆盖非高亮结果\n                hoteldoc.setname(name);\n            }\n        }\n        system.out.println("hoteldoc = " + hoteldoc);\n    }\n}\n\n\n\n# 总结\n\n * 所有的dsl的构建，记住一个api：\n   * searchrequest的source()方法\n * 高亮结果解析是参考json结果，逐层解析',charsets:{cjk:!0}},{title:"SpringAMQP",frontmatter:{autoSort:98,title:"SpringAMQP",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/4b3e75/",categories:["后端","微服务","RabbitMQ"],tags:["知识","数据库","MQ"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/30.RabbitMQ/15.SpringAMQP.html",relativePath:"01.后端/60.微服务/30.RabbitMQ/15.SpringAMQP.md",key:"v-413ff23e",path:"/pages/4b3e75/",headers:[{level:2,title:"Basic Queue 简单队列模型",slug:"basic-queue-简单队列模型",normalizedTitle:"basic queue 简单队列模型",charIndex:220},{level:3,title:"消息发送",slug:"消息发送",normalizedTitle:"消息发送",charIndex:417},{level:3,title:"消息接收",slug:"消息接收",normalizedTitle:"消息接收",charIndex:1495},{level:3,title:"测试",slug:"测试",normalizedTitle:"测试",charIndex:729},{level:2,title:"WorkQueue",slug:"workqueue",normalizedTitle:"workqueue",charIndex:2364},{level:3,title:"消息发送",slug:"消息发送-2",normalizedTitle:"消息发送",charIndex:417},{level:3,title:"消息接收",slug:"消息接收-2",normalizedTitle:"消息接收",charIndex:1495},{level:3,title:"测试",slug:"测试-2",normalizedTitle:"测试",charIndex:729},{level:3,title:"能者多劳",slug:"能者多劳",normalizedTitle:"能者多劳",charIndex:3684},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:3894},{level:2,title:"发布/订阅",slug:"发布-订阅",normalizedTitle:"发布/订阅",charIndex:3975},{level:2,title:"Fanout-广播模式",slug:"fanout-广播模式",normalizedTitle:"fanout-广播模式",charIndex:4471},{level:3,title:"声明队列和交换机",slug:"声明队列和交换机",normalizedTitle:"声明队列和交换机",charIndex:4798},{level:3,title:"消息发送",slug:"消息发送-3",normalizedTitle:"消息发送",charIndex:417},{level:3,title:"消息接收",slug:"消息接收-3",normalizedTitle:"消息接收",charIndex:1495},{level:3,title:"总结",slug:"总结-2",normalizedTitle:"总结",charIndex:3894},{level:2,title:"Direct-路由模式",slug:"direct-路由模式",normalizedTitle:"direct-路由模式",charIndex:6872},{level:3,title:"基于注解声明队列和交换机",slug:"基于注解声明队列和交换机",normalizedTitle:"基于注解声明队列和交换机",charIndex:7340},{level:3,title:"消息发送",slug:"消息发送-4",normalizedTitle:"消息发送",charIndex:417},{level:3,title:"总结",slug:"总结-3",normalizedTitle:"总结",charIndex:3894},{level:2,title:"Topic-主题模式",slug:"topic-主题模式",normalizedTitle:"topic-主题模式",charIndex:8801},{level:3,title:"说明",slug:"说明",normalizedTitle:"说明",charIndex:8816},{level:3,title:"消息发送",slug:"消息发送-5",normalizedTitle:"消息发送",charIndex:417},{level:3,title:"消息接收",slug:"消息接收-4",normalizedTitle:"消息接收",charIndex:1495},{level:3,title:"总结",slug:"总结-4",normalizedTitle:"总结",charIndex:3894},{level:2,title:"消息转换器",slug:"消息转换器",normalizedTitle:"消息转换器",charIndex:10520},{level:3,title:"测试默认转换器",slug:"测试默认转换器",normalizedTitle:"测试默认转换器",charIndex:10679},{level:3,title:"配置JSON转换器",slug:"配置json转换器",normalizedTitle:"配置json转换器",charIndex:10992},{level:3,title:"接收消息",slug:"接收消息",normalizedTitle:"接收消息",charIndex:181}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"Basic Queue 简单队列模型 消息发送 消息接收 测试 WorkQueue 消息发送 消息接收 测试 能者多劳 总结 发布/订阅 Fanout-广播模式 声明队列和交换机 消息发送 消息接收 总结 Direct-路由模式 基于注解声明队列和交换机 消息发送 总结 Topic-主题模式 说明 消息发送 消息接收 总结 消息转换器 测试默认转换器 配置JSON转换器 接收消息",content:'SpringAMQP是基于RabbitMQ封装的一套模板，并且还利用SpringBoot对其实现了自动装配，使用起来非常方便。\n\nSpringAmqp的官方地址：https://spring.io/projects/spring-amqp\n\n\n\n\n\nSpringAMQP提供了三个功能：\n\n * 自动声明队列、交换机及其绑定关系\n * 基于注解的监听器模式，异步接收消息\n * 封装了RabbitTemplate工具，用于发送消息\n\n\n# Basic Queue 简单队列模型\n\n在父工程mq-demo中引入依赖\n\n\x3c!--AMQP依赖，包含RabbitMQ--\x3e\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-amqp</artifactId>\n</dependency>\n\n\n\n# 消息发送\n\n步骤\n\n>  1. 引入amqp的starter依赖\n>  2. 配置RabbitMQ地址\n>  3. 利用RabbitTemplate的convertAndSend方法发送消息\n\n首先配置MQ地址，在publisher服务的application.yml中添加配置：\n\nspring:\n  rabbitmq:\n    host: 192.168.159.100 # 主机名\n    port: 5672 # 端口\n    virtual-host: / # 虚拟主机\n    username: diana # 用户名\n    password: 123321 # 密码\n\n\n然后在publisher服务中编写测试类SpringAmqpTest，并利用RabbitTemplate实现消息发送：\n\npackage cn.itcast.mq.spring;\n\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.springframework.amqp.rabbit.core.RabbitTemplate;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.test.context.junit4.SpringRunner;\n\n@RunWith(SpringRunner.class)\n@SpringBootTest\npublic class SpringAmqpTest {\n\n    @Autowired\n    private RabbitTemplate rabbitTemplate;\n\n    @Test\n    public void testSimpleQueue() {\n        // 队列名称\n        String queueName = "simple.queue";\n        // 消息\n        String message = "hello, spring amqp!";\n        // 发送消息\n        rabbitTemplate.convertAndSend(queueName, message);\n    }\n}\n\n\n\n# 消息接收\n\n * 步骤\n   \n   >  1. 引入amqp的starter依赖\n   >  2. 配置RabbitMQ地址\n   >  3. 定义类，添加@Comment注解\n   >  4. 类中声明方法，添加@RabbitListener注解，方法参数就是消息\n\n首先配置MQ地址，在consumer服务的application.yml中添加配置：\n\nspring:\n  rabbitmq:\n    host: 192.168.159.100 # 主机名\n    port: 5672 # 端口\n    virtual-host: / # 虚拟主机\n    username: diana # 用户名\n    password: 123321 # 密码\n\n\n然后在consumer服务的cn.itcast.mq.listener包中新建一个类SpringRabbitListener，代码如下：\n\npackage cn.itcast.mq.listener;\n\nimport org.springframework.amqp.rabbit.annotation.RabbitListener;\nimport org.springframework.stereotype.Component;\n\n@Component\npublic class SpringRabbitListener {\n\n    @RabbitListener(queues = "simple.queue")//监听队列simple.queue\n    public void listenSimpleQueueMessage(String msg) throws InterruptedException {\n        System.out.println("spring 消费者接收到消息：【" + msg + "】");\n    }\n}\n\n\n\n# 测试\n\n启动consumer服务，然后在publisher服务中运行测试代码，发送MQ消息\n\n\n# WorkQueue\n\nWork queues，也被称为（Task queues），任务模型。简单来说就是让多个消费者绑定到一个队列，共同消费队列中的消息。\n\n\n\n当消息处理比较耗时的时候，可能生产消息的速度会远远大于消息的消费速度。长此以往，消息就会堆积越来越多，无法及时处理。\n\n此时就可以使用work 模型，多个消费者共同处理消息处理，速度就能大大提高了。\n\n\n# 消息发送\n\n这次我们循环发送，模拟大量消息堆积现象。\n\n在publisher服务中的SpringAmqpTest类中添加一个测试方法：\n\n/**\n     * workQueue\n     * 向队列中不停发送消息，模拟消息堆积。\n     */\n@Test\npublic void testWorkQueue() throws InterruptedException {\n    // 队列名称\n    String queueName = "simple.queue";\n    // 消息\n    String message = "hello, message_";\n    for (int i = 0; i < 50; i++) {\n        // 发送消息\n        rabbitTemplate.convertAndSend(queueName, message + i);\n        Thread.sleep(20);\n    }\n}\n\n\n\n# 消息接收\n\n要模拟多个消费者绑定同一个队列，我们在consumer服务的SpringRabbitListener中添加2个新的方法：\n\n@RabbitListener(queues = "simple.queue")\npublic void listenWorkQueue1(String msg) throws InterruptedException {\n    System.out.println("消费者1接收到消息：【" + msg + "】" + LocalTime.now());\n    Thread.sleep(20);\n}\n\n@RabbitListener(queues = "simple.queue")\npublic void listenWorkQueue2(String msg) throws InterruptedException {\n    System.err.println("消费者2........接收到消息：【" + msg + "】" + LocalTime.now());\n    Thread.sleep(200);\n}\n\n\n注意到这个消费者sleep了1000秒，模拟任务耗时。\n\n\n# 测试\n\n启动ConsumerApplication后，在执行publisher服务中刚刚编写的发送测试方法testWorkQueue。\n\n可以看到消费者1很快完成了自己的25条消息。消费者2却在缓慢的处理自己的25条消息。\n\n也就是说消息是平均分配给每个消费者，并没有考虑到消费者的处理能力。这样显然是有问题的。默认是轮询的接收消息\n\n\n# 能者多劳\n\n如果不配做预取消息的数量的话，那么默认多个消费者是以轮询的方式接收消息的\n\n在spring中有一个简单的配置，可以解决这个问题。我们修改consumer服务的application.yml文件，添加配置：\n\nspring:\n  rabbitmq:\n    listener:\n      simple:\n        prefetch: 1 # 每次只能获取一条消息，处理完成才能获取下一个消息\n\n\n\n# 总结\n\nWork模型的使用：\n\n * 多个消费者绑定到一个队列，同一条消息只会被一个消费者处理\n * 通过设置prefetch来控制消费者预取的消息数量\n\n\n# 发布/订阅\n\n发布订阅的模型如图：\n\n\n\n可以看到，在订阅模型中，多了一个exchange角色，而且过程略有变化：\n\n * Publisher：生产者，也就是要发送消息的程序，但是不再发送到队列中，而是发给X（交换机）\n * **Exchange：**交换机，图中的X。一方面，接收生产者发送的消息。另一方面，知道如何处理消息，例如递交给某个特别队列、递交给所有队列、或是将消息丢弃。到底如何操作，取决于Exchange的类型。Exchange有以下3种类型：\n   * Fanout：广播，将消息交给所有绑定到交换机的队列\n   * Direct：定向，把消息交给符合指定routing key 的队列\n   * Topic：通配符，把消息交给符合routing pattern（路由模式） 的队列\n * Consumer：消费者，与以前一样，订阅队列，没有变化\n * Queue：消息队列也与以前一样，接收消息、缓存消息。\n\nExchange（交换机）只负责转发消息，不具备存储消息的能力，因此如果没有任何队列与Exchange绑定，或者没有符合路由规则的队列，那么消息会丢失！\n\n\n# Fanout-广播模式\n\n订阅队列的消费者都能拿到消息\n\nFanout，英文翻译是扇出，我觉得在MQ中叫广播更合适。\n\n\n\n在广播模式下，消息发送流程是这样的：\n\n * 1） 可以有多个队列\n * 2） 每个队列都要绑定到Exchange（交换机）\n * 3） 生产者发送的消息，只能发送到交换机，交换机来决定要发给哪个队列，生产者无法决定\n * 4） 交换机把消息发送给绑定过的所有队列\n * 5） 订阅队列的消费者都能拿到消息\n\n我们的计划是这样的：\n\n * 创建一个交换机 itcast.fanout，类型是Fanout\n * 创建两个队列fanout.queue1和fanout.queue2，绑定到交换机itcast.fanout\n\n\n\n\n# 声明队列和交换机\n\nSpring提供了一个接口Exchange，来表示所有不同类型的交换机：\n\n\n\n在consumer中创建一个类，声明队列和交换机：\n\npackage cn.itcast.mq.config;\n\nimport org.springframework.amqp.core.Binding;\nimport org.springframework.amqp.core.BindingBuilder;\nimport org.springframework.amqp.core.FanoutExchange;\nimport org.springframework.amqp.core.Queue;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\n@Configuration\npublic class FanoutConfig {\n    /**\n     * 声明交换机\n     * @return Fanout类型交换机\n     */\n    @Bean\n    public FanoutExchange fanoutExchange(){\n        return new FanoutExchange("itcast.fanout");\n    }\n\n    /**\n     * 第1个队列\n     */\n    @Bean\n    public Queue fanoutQueue1(){\n        return new Queue("fanout.queue1");\n    }\n\n    /**\n     * 绑定队列和交换机\n     */\n    @Bean\n    public Binding bindingQueue1(Queue fanoutQueue1, FanoutExchange fanoutExchange){\n        return BindingBuilder.bind(fanoutQueue1).to(fanoutExchange);\n    }\n\n    /**\n     * 第2个队列\n     */\n    @Bean\n    public Queue fanoutQueue2(){\n        return new Queue("fanout.queue2");\n    }\n\n    /**\n     * 绑定队列和交换机\n     */\n    @Bean\n    public Binding bindingQueue2(Queue fanoutQueue2, FanoutExchange fanoutExchange){\n        return BindingBuilder.bind(fanoutQueue2).to(fanoutExchange);\n    }\n}\n\n\n\n# 消息发送\n\n在publisher服务的SpringAmqpTest类中添加测试方法：\n\n@Test\npublic void testFanoutExchange() {\n    // 队列名称\n    String exchangeName = "itcast.fanout";\n    // 消息\n    String message = "hello, everyone!";\n    rabbitTemplate.convertAndSend(exchangeName, "", message);\n}\n\n\n\n# 消息接收\n\n在consumer服务的SpringRabbitListener中添加两个方法，作为消费者：\n\n@RabbitListener(queues = "fanout.queue1")\npublic void listenFanoutQueue1(String msg) {\n    System.out.println("消费者1接收到Fanout消息：【" + msg + "】");\n}\n\n@RabbitListener(queues = "fanout.queue2")\npublic void listenFanoutQueue2(String msg) {\n    System.out.println("消费者2接收到Fanout消息：【" + msg + "】");\n}\n\n\n\n# 总结\n\n交换机的作用是什么？\n\n * 接收publisher发送的消息\n * 将消息按照规则路由到与之绑定的队列\n * 不能缓存消息，路由失败，消息丢失\n * FanoutExchange的会将消息路由到每个绑定的队列\n\n声明队列、交换机、绑定关系的Bean是什么？\n\n * Queue\n * FanoutExchange\n * Binding\n\n\n# Direct-路由模式\n\n在Fanout模式中，一条消息，会被所有订阅的队列都消费。但是，在某些场景下，我们希望不同的消息被不同的队列消费。这时就要用到Direct类型的Exchange。\n\n\n\n在Direct模型下：\n\n * 队列与交换机的绑定，不能是任意绑定了，而是要指定一个RoutingKey（路由key）\n * 消息的发送方在 向 Exchange发送消息时，也必须指定消息的 RoutingKey。\n * Exchange不再把消息交给每一个绑定的队列，而是根据消息的Routing Key进行判断，只有队列的Routingkey与消息的 Routing key完全一致，才会接收到消息\n\n案例需求如下：\n\n 1. 利用@RabbitListener声明Exchange、Queue、RoutingKey\n\n 2. 在consumer服务中，编写两个消费者方法，分别监听direct.queue1和direct.queue2\n\n 3. 在publisher中编写测试方法，向itcast. direct发送消息\n\n\n\n\n# 基于注解声明队列和交换机\n\n基于@Bean的方式声明队列和交换机比较麻烦，Spring还提供了基于注解方式来声明。\n\n在consumer的SpringRabbitListener中添加两个消费者，同时基于注解来声明队列和交换机：\n\n@RabbitListener(bindings = @QueueBinding(\n    value = @Queue(name = "direct.queue1"),\n    exchange = @Exchange(name = "itcast.direct", type = ExchangeTypes.DIRECT),\n    key = {"red", "blue"}\n))\npublic void listenDirectQueue1(String msg){\n    System.out.println("消费者接收到direct.queue1的消息：【" + msg + "】");\n}\n\n@RabbitListener(bindings = @QueueBinding(\n    value = @Queue(name = "direct.queue2"),\n    exchange = @Exchange(name = "itcast.direct", type = ExchangeTypes.DIRECT),\n    key = {"red", "yellow"}\n))\npublic void listenDirectQueue2(String msg){\n    System.out.println("消费者接收到direct.queue2的消息：【" + msg + "】");\n}\n\n\n\n# 消息发送\n\n在publisher服务的SpringAmqpTest类中添加测试方法：\n\n@Test\npublic void testSendDirectExchange() {\n    // 交换机名称\n    String exchangeName = "itcast.direct";\n    // 消息\n    String message = "红色警报！日本乱排核废水，导致海洋生物变异，惊现哥斯拉！";\n    // 发送消息\n    rabbitTemplate.convertAndSend(exchangeName, "red", message);\n}\n\n\n\n# 总结\n\n描述下Direct交换机与Fanout交换机的差异？\n\n * Fanout交换机将消息路由给每一个与之绑定的队列\n * Direct交换机根据RoutingKey判断路由给哪个队列\n * 如果多个队列具有相同的RoutingKey，则与Fanout功能类似\n\n基于@RabbitListener注解声明队列和交换机有哪些常见注解？\n\n * @Queue\n * @Exchange\n * 使用ctrl+P提示\n\n * @RabbitListener(bindings = @QueueBinding(\n           value = @Queue(name = "direct.queue1"),\n           exchange = @Exchange(name = "diana.direct", type = ExchangeTypes.DIRECT),\n           key = {"red", "blue"}\n   ))\n   \n\n\n# Topic-主题模式\n\n\n# 说明\n\nTopic类型的Exchange与Direct相比，都是可以根据RoutingKey把消息路由到不同的队列。只不过Topic类型Exchange可以让队列在绑定Routing key 的时候使用通配符！\n\nRoutingkey 一般都是有一个或多个单词组成，多个单词之间以”.”分割，例如： item.insert\n\n通配符规则：\n\n#：匹配一个或多个词\n\n*：匹配不多不少恰好1个词\n\n举例：\n\nitem.#：能够匹配item.spu.insert 或者 item.spu\n\nitem.*：只能匹配item.spu\n\n\n\n图示：\n\n\n\n解释：\n\n * Queue1：绑定的是china.# ，因此凡是以 china.开头的routing key 都会被匹配到。包括china.news和china.weather\n * Queue2：绑定的是#.news ，因此凡是以 .news结尾的 routing key 都会被匹配。包括china.news和japan.news\n\n案例需求：\n\n实现思路如下：\n\n 1. 并利用@RabbitListener声明Exchange、Queue、RoutingKey\n\n 2. 在consumer服务中，编写两个消费者方法，分别监听topic.queue1和topic.queue2\n\n 3. 在publisher中编写测试方法，向itcast. topic发送消息\n\n\n\n\n# 消息发送\n\n在publisher服务的SpringAmqpTest类中添加测试方法：\n\n/**\n     * topicExchange\n     */\n@Test\npublic void testSendTopicExchange() {\n    // 交换机名称\n    String exchangeName = "itcast.topic";\n    // 消息\n    String message = "喜报！孙悟空大战哥斯拉，胜!";\n    // 发送消息\n    rabbitTemplate.convertAndSend(exchangeName, "china.news", message);\n}\n\n\n\n# 消息接收\n\n在consumer服务的SpringRabbitListener中添加方法：\n\n@RabbitListener(bindings = @QueueBinding(\n    value = @Queue(name = "topic.queue1"),\n    exchange = @Exchange(name = "itcast.topic", type = ExchangeTypes.TOPIC),\n    key = "china.#"\n))\npublic void listenTopicQueue1(String msg){\n    System.out.println("消费者接收到topic.queue1的消息：【" + msg + "】");\n}\n\n@RabbitListener(bindings = @QueueBinding(\n    value = @Queue(name = "topic.queue2"),\n    exchange = @Exchange(name = "itcast.topic", type = ExchangeTypes.TOPIC),\n    key = "#.news"\n))\npublic void listenTopicQueue2(String msg){\n    System.out.println("消费者接收到topic.queue2的消息：【" + msg + "】");\n}\n\n\n\n# 总结\n\n描述下Direct交换机与Topic交换机的差异？\n\n * Topic交换机接收的消息RoutingKey必须是多个单词，以 . 分割\n * Topic交换机与队列绑定时的bindingKey可以指定通配符\n * #：代表0个或多个词\n * *：代表1个词\n\n\n# 消息转换器\n\n之前说过，Spring会把你发送的消息序列化为字节发送给MQ，接收消息的时候，还会把字节反序列化为Java对象。\n\n\n\n只不过，默认情况下Spring采用的序列化方式是JDK序列化。众所周知，JDK序列化存在下列问题：\n\n * 数据体积过大\n * 有安全漏洞\n * 可读性差\n\n我们来测试一下。\n\n\n# 测试默认转换器\n\n我们修改消息发送的代码，发送一个Map对象：\n\n@Test\npublic void testSendMap() throws InterruptedException {\n    // 准备消息\n    Map<String,Object> msg = new HashMap<>();\n    msg.put("name", "Jack");\n    msg.put("age", 21);\n    // 发送消息\n    rabbitTemplate.convertAndSend("simple.queue","", msg);\n}\n\n\n停止consumer服务\n\n发送消息后查看控制台：\n\n\n\n\n# 配置JSON转换器\n\n显然，JDK序列化方式并不合适。我们希望消息体的体积更小、可读性更高，因此可以使用JSON方式来做序列化和反序列化。\n\n在publisher和consumer两个服务中都引入依赖：\n\n<dependency>\n    <groupId>com.fasterxml.jackson.dataformat</groupId>\n    <artifactId>jackson-dataformat-xml</artifactId>\n    <version>2.9.10</version>\n</dependency>\n\n\n配置消息转换器。\n\n在启动类中添加一个Bean即可：\n\n@Bean\npublic MessageConverter jsonMessageConverter(){\n    return new Jackson2JsonMessageConverter();\n}\n\n\n发送消息后查看控制台：\n\n\n\n\n# 接收消息\n\n 1. 接收方和发送方要使用一样的MessageConverter\n 2. 接收消息类型，java对象格式要一致\n\n//监听队列object.queue\n@RabbitListener(queues = "object.queue")\n//类型要是map类型\npublic void listenSimpleQueueMessage(Map<String, String> msg) {\n    System.out.println("Spring 消费者接收到的消息是：[" + msg + " ]");\n}\n',normalizedContent:'springamqp是基于rabbitmq封装的一套模板，并且还利用springboot对其实现了自动装配，使用起来非常方便。\n\nspringamqp的官方地址：https://spring.io/projects/spring-amqp\n\n\n\n\n\nspringamqp提供了三个功能：\n\n * 自动声明队列、交换机及其绑定关系\n * 基于注解的监听器模式，异步接收消息\n * 封装了rabbittemplate工具，用于发送消息\n\n\n# basic queue 简单队列模型\n\n在父工程mq-demo中引入依赖\n\n\x3c!--amqp依赖，包含rabbitmq--\x3e\n<dependency>\n    <groupid>org.springframework.boot</groupid>\n    <artifactid>spring-boot-starter-amqp</artifactid>\n</dependency>\n\n\n\n# 消息发送\n\n步骤\n\n>  1. 引入amqp的starter依赖\n>  2. 配置rabbitmq地址\n>  3. 利用rabbittemplate的convertandsend方法发送消息\n\n首先配置mq地址，在publisher服务的application.yml中添加配置：\n\nspring:\n  rabbitmq:\n    host: 192.168.159.100 # 主机名\n    port: 5672 # 端口\n    virtual-host: / # 虚拟主机\n    username: diana # 用户名\n    password: 123321 # 密码\n\n\n然后在publisher服务中编写测试类springamqptest，并利用rabbittemplate实现消息发送：\n\npackage cn.itcast.mq.spring;\n\nimport org.junit.test;\nimport org.junit.runner.runwith;\nimport org.springframework.amqp.rabbit.core.rabbittemplate;\nimport org.springframework.beans.factory.annotation.autowired;\nimport org.springframework.boot.test.context.springboottest;\nimport org.springframework.test.context.junit4.springrunner;\n\n@runwith(springrunner.class)\n@springboottest\npublic class springamqptest {\n\n    @autowired\n    private rabbittemplate rabbittemplate;\n\n    @test\n    public void testsimplequeue() {\n        // 队列名称\n        string queuename = "simple.queue";\n        // 消息\n        string message = "hello, spring amqp!";\n        // 发送消息\n        rabbittemplate.convertandsend(queuename, message);\n    }\n}\n\n\n\n# 消息接收\n\n * 步骤\n   \n   >  1. 引入amqp的starter依赖\n   >  2. 配置rabbitmq地址\n   >  3. 定义类，添加@comment注解\n   >  4. 类中声明方法，添加@rabbitlistener注解，方法参数就是消息\n\n首先配置mq地址，在consumer服务的application.yml中添加配置：\n\nspring:\n  rabbitmq:\n    host: 192.168.159.100 # 主机名\n    port: 5672 # 端口\n    virtual-host: / # 虚拟主机\n    username: diana # 用户名\n    password: 123321 # 密码\n\n\n然后在consumer服务的cn.itcast.mq.listener包中新建一个类springrabbitlistener，代码如下：\n\npackage cn.itcast.mq.listener;\n\nimport org.springframework.amqp.rabbit.annotation.rabbitlistener;\nimport org.springframework.stereotype.component;\n\n@component\npublic class springrabbitlistener {\n\n    @rabbitlistener(queues = "simple.queue")//监听队列simple.queue\n    public void listensimplequeuemessage(string msg) throws interruptedexception {\n        system.out.println("spring 消费者接收到消息：【" + msg + "】");\n    }\n}\n\n\n\n# 测试\n\n启动consumer服务，然后在publisher服务中运行测试代码，发送mq消息\n\n\n# workqueue\n\nwork queues，也被称为（task queues），任务模型。简单来说就是让多个消费者绑定到一个队列，共同消费队列中的消息。\n\n\n\n当消息处理比较耗时的时候，可能生产消息的速度会远远大于消息的消费速度。长此以往，消息就会堆积越来越多，无法及时处理。\n\n此时就可以使用work 模型，多个消费者共同处理消息处理，速度就能大大提高了。\n\n\n# 消息发送\n\n这次我们循环发送，模拟大量消息堆积现象。\n\n在publisher服务中的springamqptest类中添加一个测试方法：\n\n/**\n     * workqueue\n     * 向队列中不停发送消息，模拟消息堆积。\n     */\n@test\npublic void testworkqueue() throws interruptedexception {\n    // 队列名称\n    string queuename = "simple.queue";\n    // 消息\n    string message = "hello, message_";\n    for (int i = 0; i < 50; i++) {\n        // 发送消息\n        rabbittemplate.convertandsend(queuename, message + i);\n        thread.sleep(20);\n    }\n}\n\n\n\n# 消息接收\n\n要模拟多个消费者绑定同一个队列，我们在consumer服务的springrabbitlistener中添加2个新的方法：\n\n@rabbitlistener(queues = "simple.queue")\npublic void listenworkqueue1(string msg) throws interruptedexception {\n    system.out.println("消费者1接收到消息：【" + msg + "】" + localtime.now());\n    thread.sleep(20);\n}\n\n@rabbitlistener(queues = "simple.queue")\npublic void listenworkqueue2(string msg) throws interruptedexception {\n    system.err.println("消费者2........接收到消息：【" + msg + "】" + localtime.now());\n    thread.sleep(200);\n}\n\n\n注意到这个消费者sleep了1000秒，模拟任务耗时。\n\n\n# 测试\n\n启动consumerapplication后，在执行publisher服务中刚刚编写的发送测试方法testworkqueue。\n\n可以看到消费者1很快完成了自己的25条消息。消费者2却在缓慢的处理自己的25条消息。\n\n也就是说消息是平均分配给每个消费者，并没有考虑到消费者的处理能力。这样显然是有问题的。默认是轮询的接收消息\n\n\n# 能者多劳\n\n如果不配做预取消息的数量的话，那么默认多个消费者是以轮询的方式接收消息的\n\n在spring中有一个简单的配置，可以解决这个问题。我们修改consumer服务的application.yml文件，添加配置：\n\nspring:\n  rabbitmq:\n    listener:\n      simple:\n        prefetch: 1 # 每次只能获取一条消息，处理完成才能获取下一个消息\n\n\n\n# 总结\n\nwork模型的使用：\n\n * 多个消费者绑定到一个队列，同一条消息只会被一个消费者处理\n * 通过设置prefetch来控制消费者预取的消息数量\n\n\n# 发布/订阅\n\n发布订阅的模型如图：\n\n\n\n可以看到，在订阅模型中，多了一个exchange角色，而且过程略有变化：\n\n * publisher：生产者，也就是要发送消息的程序，但是不再发送到队列中，而是发给x（交换机）\n * **exchange：**交换机，图中的x。一方面，接收生产者发送的消息。另一方面，知道如何处理消息，例如递交给某个特别队列、递交给所有队列、或是将消息丢弃。到底如何操作，取决于exchange的类型。exchange有以下3种类型：\n   * fanout：广播，将消息交给所有绑定到交换机的队列\n   * direct：定向，把消息交给符合指定routing key 的队列\n   * topic：通配符，把消息交给符合routing pattern（路由模式） 的队列\n * consumer：消费者，与以前一样，订阅队列，没有变化\n * queue：消息队列也与以前一样，接收消息、缓存消息。\n\nexchange（交换机）只负责转发消息，不具备存储消息的能力，因此如果没有任何队列与exchange绑定，或者没有符合路由规则的队列，那么消息会丢失！\n\n\n# fanout-广播模式\n\n订阅队列的消费者都能拿到消息\n\nfanout，英文翻译是扇出，我觉得在mq中叫广播更合适。\n\n\n\n在广播模式下，消息发送流程是这样的：\n\n * 1） 可以有多个队列\n * 2） 每个队列都要绑定到exchange（交换机）\n * 3） 生产者发送的消息，只能发送到交换机，交换机来决定要发给哪个队列，生产者无法决定\n * 4） 交换机把消息发送给绑定过的所有队列\n * 5） 订阅队列的消费者都能拿到消息\n\n我们的计划是这样的：\n\n * 创建一个交换机 itcast.fanout，类型是fanout\n * 创建两个队列fanout.queue1和fanout.queue2，绑定到交换机itcast.fanout\n\n\n\n\n# 声明队列和交换机\n\nspring提供了一个接口exchange，来表示所有不同类型的交换机：\n\n\n\n在consumer中创建一个类，声明队列和交换机：\n\npackage cn.itcast.mq.config;\n\nimport org.springframework.amqp.core.binding;\nimport org.springframework.amqp.core.bindingbuilder;\nimport org.springframework.amqp.core.fanoutexchange;\nimport org.springframework.amqp.core.queue;\nimport org.springframework.context.annotation.bean;\nimport org.springframework.context.annotation.configuration;\n\n@configuration\npublic class fanoutconfig {\n    /**\n     * 声明交换机\n     * @return fanout类型交换机\n     */\n    @bean\n    public fanoutexchange fanoutexchange(){\n        return new fanoutexchange("itcast.fanout");\n    }\n\n    /**\n     * 第1个队列\n     */\n    @bean\n    public queue fanoutqueue1(){\n        return new queue("fanout.queue1");\n    }\n\n    /**\n     * 绑定队列和交换机\n     */\n    @bean\n    public binding bindingqueue1(queue fanoutqueue1, fanoutexchange fanoutexchange){\n        return bindingbuilder.bind(fanoutqueue1).to(fanoutexchange);\n    }\n\n    /**\n     * 第2个队列\n     */\n    @bean\n    public queue fanoutqueue2(){\n        return new queue("fanout.queue2");\n    }\n\n    /**\n     * 绑定队列和交换机\n     */\n    @bean\n    public binding bindingqueue2(queue fanoutqueue2, fanoutexchange fanoutexchange){\n        return bindingbuilder.bind(fanoutqueue2).to(fanoutexchange);\n    }\n}\n\n\n\n# 消息发送\n\n在publisher服务的springamqptest类中添加测试方法：\n\n@test\npublic void testfanoutexchange() {\n    // 队列名称\n    string exchangename = "itcast.fanout";\n    // 消息\n    string message = "hello, everyone!";\n    rabbittemplate.convertandsend(exchangename, "", message);\n}\n\n\n\n# 消息接收\n\n在consumer服务的springrabbitlistener中添加两个方法，作为消费者：\n\n@rabbitlistener(queues = "fanout.queue1")\npublic void listenfanoutqueue1(string msg) {\n    system.out.println("消费者1接收到fanout消息：【" + msg + "】");\n}\n\n@rabbitlistener(queues = "fanout.queue2")\npublic void listenfanoutqueue2(string msg) {\n    system.out.println("消费者2接收到fanout消息：【" + msg + "】");\n}\n\n\n\n# 总结\n\n交换机的作用是什么？\n\n * 接收publisher发送的消息\n * 将消息按照规则路由到与之绑定的队列\n * 不能缓存消息，路由失败，消息丢失\n * fanoutexchange的会将消息路由到每个绑定的队列\n\n声明队列、交换机、绑定关系的bean是什么？\n\n * queue\n * fanoutexchange\n * binding\n\n\n# direct-路由模式\n\n在fanout模式中，一条消息，会被所有订阅的队列都消费。但是，在某些场景下，我们希望不同的消息被不同的队列消费。这时就要用到direct类型的exchange。\n\n\n\n在direct模型下：\n\n * 队列与交换机的绑定，不能是任意绑定了，而是要指定一个routingkey（路由key）\n * 消息的发送方在 向 exchange发送消息时，也必须指定消息的 routingkey。\n * exchange不再把消息交给每一个绑定的队列，而是根据消息的routing key进行判断，只有队列的routingkey与消息的 routing key完全一致，才会接收到消息\n\n案例需求如下：\n\n 1. 利用@rabbitlistener声明exchange、queue、routingkey\n\n 2. 在consumer服务中，编写两个消费者方法，分别监听direct.queue1和direct.queue2\n\n 3. 在publisher中编写测试方法，向itcast. direct发送消息\n\n\n\n\n# 基于注解声明队列和交换机\n\n基于@bean的方式声明队列和交换机比较麻烦，spring还提供了基于注解方式来声明。\n\n在consumer的springrabbitlistener中添加两个消费者，同时基于注解来声明队列和交换机：\n\n@rabbitlistener(bindings = @queuebinding(\n    value = @queue(name = "direct.queue1"),\n    exchange = @exchange(name = "itcast.direct", type = exchangetypes.direct),\n    key = {"red", "blue"}\n))\npublic void listendirectqueue1(string msg){\n    system.out.println("消费者接收到direct.queue1的消息：【" + msg + "】");\n}\n\n@rabbitlistener(bindings = @queuebinding(\n    value = @queue(name = "direct.queue2"),\n    exchange = @exchange(name = "itcast.direct", type = exchangetypes.direct),\n    key = {"red", "yellow"}\n))\npublic void listendirectqueue2(string msg){\n    system.out.println("消费者接收到direct.queue2的消息：【" + msg + "】");\n}\n\n\n\n# 消息发送\n\n在publisher服务的springamqptest类中添加测试方法：\n\n@test\npublic void testsenddirectexchange() {\n    // 交换机名称\n    string exchangename = "itcast.direct";\n    // 消息\n    string message = "红色警报！日本乱排核废水，导致海洋生物变异，惊现哥斯拉！";\n    // 发送消息\n    rabbittemplate.convertandsend(exchangename, "red", message);\n}\n\n\n\n# 总结\n\n描述下direct交换机与fanout交换机的差异？\n\n * fanout交换机将消息路由给每一个与之绑定的队列\n * direct交换机根据routingkey判断路由给哪个队列\n * 如果多个队列具有相同的routingkey，则与fanout功能类似\n\n基于@rabbitlistener注解声明队列和交换机有哪些常见注解？\n\n * @queue\n * @exchange\n * 使用ctrl+p提示\n\n * @rabbitlistener(bindings = @queuebinding(\n           value = @queue(name = "direct.queue1"),\n           exchange = @exchange(name = "diana.direct", type = exchangetypes.direct),\n           key = {"red", "blue"}\n   ))\n   \n\n\n# topic-主题模式\n\n\n# 说明\n\ntopic类型的exchange与direct相比，都是可以根据routingkey把消息路由到不同的队列。只不过topic类型exchange可以让队列在绑定routing key 的时候使用通配符！\n\nroutingkey 一般都是有一个或多个单词组成，多个单词之间以”.”分割，例如： item.insert\n\n通配符规则：\n\n#：匹配一个或多个词\n\n*：匹配不多不少恰好1个词\n\n举例：\n\nitem.#：能够匹配item.spu.insert 或者 item.spu\n\nitem.*：只能匹配item.spu\n\n\n\n图示：\n\n\n\n解释：\n\n * queue1：绑定的是china.# ，因此凡是以 china.开头的routing key 都会被匹配到。包括china.news和china.weather\n * queue2：绑定的是#.news ，因此凡是以 .news结尾的 routing key 都会被匹配。包括china.news和japan.news\n\n案例需求：\n\n实现思路如下：\n\n 1. 并利用@rabbitlistener声明exchange、queue、routingkey\n\n 2. 在consumer服务中，编写两个消费者方法，分别监听topic.queue1和topic.queue2\n\n 3. 在publisher中编写测试方法，向itcast. topic发送消息\n\n\n\n\n# 消息发送\n\n在publisher服务的springamqptest类中添加测试方法：\n\n/**\n     * topicexchange\n     */\n@test\npublic void testsendtopicexchange() {\n    // 交换机名称\n    string exchangename = "itcast.topic";\n    // 消息\n    string message = "喜报！孙悟空大战哥斯拉，胜!";\n    // 发送消息\n    rabbittemplate.convertandsend(exchangename, "china.news", message);\n}\n\n\n\n# 消息接收\n\n在consumer服务的springrabbitlistener中添加方法：\n\n@rabbitlistener(bindings = @queuebinding(\n    value = @queue(name = "topic.queue1"),\n    exchange = @exchange(name = "itcast.topic", type = exchangetypes.topic),\n    key = "china.#"\n))\npublic void listentopicqueue1(string msg){\n    system.out.println("消费者接收到topic.queue1的消息：【" + msg + "】");\n}\n\n@rabbitlistener(bindings = @queuebinding(\n    value = @queue(name = "topic.queue2"),\n    exchange = @exchange(name = "itcast.topic", type = exchangetypes.topic),\n    key = "#.news"\n))\npublic void listentopicqueue2(string msg){\n    system.out.println("消费者接收到topic.queue2的消息：【" + msg + "】");\n}\n\n\n\n# 总结\n\n描述下direct交换机与topic交换机的差异？\n\n * topic交换机接收的消息routingkey必须是多个单词，以 . 分割\n * topic交换机与队列绑定时的bindingkey可以指定通配符\n * #：代表0个或多个词\n * *：代表1个词\n\n\n# 消息转换器\n\n之前说过，spring会把你发送的消息序列化为字节发送给mq，接收消息的时候，还会把字节反序列化为java对象。\n\n\n\n只不过，默认情况下spring采用的序列化方式是jdk序列化。众所周知，jdk序列化存在下列问题：\n\n * 数据体积过大\n * 有安全漏洞\n * 可读性差\n\n我们来测试一下。\n\n\n# 测试默认转换器\n\n我们修改消息发送的代码，发送一个map对象：\n\n@test\npublic void testsendmap() throws interruptedexception {\n    // 准备消息\n    map<string,object> msg = new hashmap<>();\n    msg.put("name", "jack");\n    msg.put("age", 21);\n    // 发送消息\n    rabbittemplate.convertandsend("simple.queue","", msg);\n}\n\n\n停止consumer服务\n\n发送消息后查看控制台：\n\n\n\n\n# 配置json转换器\n\n显然，jdk序列化方式并不合适。我们希望消息体的体积更小、可读性更高，因此可以使用json方式来做序列化和反序列化。\n\n在publisher和consumer两个服务中都引入依赖：\n\n<dependency>\n    <groupid>com.fasterxml.jackson.dataformat</groupid>\n    <artifactid>jackson-dataformat-xml</artifactid>\n    <version>2.9.10</version>\n</dependency>\n\n\n配置消息转换器。\n\n在启动类中添加一个bean即可：\n\n@bean\npublic messageconverter jsonmessageconverter(){\n    return new jackson2jsonmessageconverter();\n}\n\n\n发送消息后查看控制台：\n\n\n\n\n# 接收消息\n\n 1. 接收方和发送方要使用一样的messageconverter\n 2. 接收消息类型，java对象格式要一致\n\n//监听队列object.queue\n@rabbitlistener(queues = "object.queue")\n//类型要是map类型\npublic void listensimplequeuemessage(map<string, string> msg) {\n    system.out.println("spring 消费者接收到的消息是：[" + msg + " ]");\n}\n',charsets:{cjk:!0}},{title:"认识微服务",frontmatter:{autoSort:100,title:"认识微服务",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/49c1ac/",categories:["后端","微服务","SpringCloud"],tags:["知识","微服务","SpringCloud"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/40.SpringCloud/05.%E8%AE%A4%E8%AF%86%E5%BE%AE%E6%9C%8D%E5%8A%A1.html",relativePath:"01.后端/60.微服务/40.SpringCloud/05.认识微服务.md",key:"v-41d84bb8",path:"/pages/49c1ac/",headers:[{level:2,title:"单体架构",slug:"单体架构",normalizedTitle:"单体架构",charIndex:29},{level:2,title:"分布式架构",slug:"分布式架构",normalizedTitle:"分布式架构",charIndex:174},{level:2,title:"微服务",slug:"微服务",normalizedTitle:"微服务",charIndex:43},{level:2,title:"SpringCloud",slug:"springcloud",normalizedTitle:"springcloud",charIndex:710},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:1006}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"单体架构 分布式架构 微服务 SpringCloud 总结",content:"随着互联网行业的发展，对服务的要求也越来越高，服务架构也从单体架构逐渐演变为现在流行的微服务架构。这些架构之间有怎样的差别呢？\n\n\n# 单体架构\n\n单体架构：将业务的所有功能集中在一个项目中开发，打成一个包部署。\n\n\n\n单体架构的优缺点如下：\n\n优点：\n\n * 架构简单\n * 部署成本低\n\n缺点：\n\n * 耦合度高（维护困难、升级困难）\n\n\n# 分布式架构\n\n分布式架构：根据业务功能对系统做拆分，每个业务功能模块作为独立项目开发，称为一个服务。\n\n\n\n分布式架构的优缺点：\n\n优点：\n\n * 降低服务耦合\n * 有利于服务升级和拓展\n\n缺点：\n\n * 服务调用关系错综复杂\n\n分布式架构虽然降低了服务耦合，但是服务拆分时也有很多问题需要思考：\n\n * 服务拆分的粒度如何界定？\n * 服务之间如何调用？\n * 服务的调用关系如何管理？\n\n人们需要制定一套行之有效的标准来约束分布式架构。\n\n\n# 微服务\n\n微服务的架构特征：\n\n * 单一职责：微服务拆分粒度更小，每一个服务都对应唯一的业务能力，做到单一职责\n * 自治：团队独立、技术独立、数据独立，独立部署和交付\n * 面向服务：服务提供统一标准的接口，与语言和技术无关\n * 隔离性强：服务调用做好隔离、容错、降级，避免出现级联问题\n\n\n\n微服务的上述特性其实是在给分布式架构制定一个标准，进一步降低服务之间的耦合度，提供服务的独立性和灵活性。做到高内聚，低耦合。\n\n因此，可以认为微服务是一种经过良好架构设计的分布式架构方案 。\n\n但方案该怎么落地？选用什么样的技术栈？全球的互联网公司都在积极尝试自己的微服务落地方案。\n\n其中在Java领域最引人注目的就是SpringCloud提供的方案了。\n\n\n# SpringCloud\n\nSpringCloud是目前国内使用最广泛的微服务框架。官网地址：https://spring.io/projects/spring-cloud。\n\nSpringCloud集成了各种微服务功能组件，并基于SpringBoot实现了这些组件的自动装配，从而提供了良好的开箱即用体验。\n\n其中常见的组件包括：\n\n\n\n另外，SpringCloud底层是依赖于SpringBoot的，并且有版本的兼容关系，如下：\n\n\n\n我们课堂学习的版本是 Hoxton.SR10，因此对应的SpringBoot版本是2.3.x版本。\n\n\n# 总结\n\n * 单体架构：简单方便，高度耦合，扩展性差，适合小型项目。例如：学生管理系统\n\n * 分布式架构：松耦合，扩展性好，但架构复杂，难度大。适合大型互联网项目，例如：京东、淘宝\n\n * 微服务：一种良好的分布式架构方案\n   \n   ①优点：拆分粒度更小、服务更独立、耦合度更低\n   \n   ②缺点：架构非常复杂，运维、监控、部署难度提高\n\n * SpringCloud是微服务架构的一站式解决方案，集成了各种优秀微服务功能组件",normalizedContent:"随着互联网行业的发展，对服务的要求也越来越高，服务架构也从单体架构逐渐演变为现在流行的微服务架构。这些架构之间有怎样的差别呢？\n\n\n# 单体架构\n\n单体架构：将业务的所有功能集中在一个项目中开发，打成一个包部署。\n\n\n\n单体架构的优缺点如下：\n\n优点：\n\n * 架构简单\n * 部署成本低\n\n缺点：\n\n * 耦合度高（维护困难、升级困难）\n\n\n# 分布式架构\n\n分布式架构：根据业务功能对系统做拆分，每个业务功能模块作为独立项目开发，称为一个服务。\n\n\n\n分布式架构的优缺点：\n\n优点：\n\n * 降低服务耦合\n * 有利于服务升级和拓展\n\n缺点：\n\n * 服务调用关系错综复杂\n\n分布式架构虽然降低了服务耦合，但是服务拆分时也有很多问题需要思考：\n\n * 服务拆分的粒度如何界定？\n * 服务之间如何调用？\n * 服务的调用关系如何管理？\n\n人们需要制定一套行之有效的标准来约束分布式架构。\n\n\n# 微服务\n\n微服务的架构特征：\n\n * 单一职责：微服务拆分粒度更小，每一个服务都对应唯一的业务能力，做到单一职责\n * 自治：团队独立、技术独立、数据独立，独立部署和交付\n * 面向服务：服务提供统一标准的接口，与语言和技术无关\n * 隔离性强：服务调用做好隔离、容错、降级，避免出现级联问题\n\n\n\n微服务的上述特性其实是在给分布式架构制定一个标准，进一步降低服务之间的耦合度，提供服务的独立性和灵活性。做到高内聚，低耦合。\n\n因此，可以认为微服务是一种经过良好架构设计的分布式架构方案 。\n\n但方案该怎么落地？选用什么样的技术栈？全球的互联网公司都在积极尝试自己的微服务落地方案。\n\n其中在java领域最引人注目的就是springcloud提供的方案了。\n\n\n# springcloud\n\nspringcloud是目前国内使用最广泛的微服务框架。官网地址：https://spring.io/projects/spring-cloud。\n\nspringcloud集成了各种微服务功能组件，并基于springboot实现了这些组件的自动装配，从而提供了良好的开箱即用体验。\n\n其中常见的组件包括：\n\n\n\n另外，springcloud底层是依赖于springboot的，并且有版本的兼容关系，如下：\n\n\n\n我们课堂学习的版本是 hoxton.sr10，因此对应的springboot版本是2.3.x版本。\n\n\n# 总结\n\n * 单体架构：简单方便，高度耦合，扩展性差，适合小型项目。例如：学生管理系统\n\n * 分布式架构：松耦合，扩展性好，但架构复杂，难度大。适合大型互联网项目，例如：京东、淘宝\n\n * 微服务：一种良好的分布式架构方案\n   \n   ①优点：拆分粒度更小、服务更独立、耦合度更低\n   \n   ②缺点：架构非常复杂，运维、监控、部署难度提高\n\n * springcloud是微服务架构的一站式解决方案，集成了各种优秀微服务功能组件",charsets:{cjk:!0}},{title:"服务拆分和远程调用",frontmatter:{autoSort:99,title:"服务拆分和远程调用",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/1b3d4d/",categories:["后端","微服务","SpringCloud"],tags:["知识","微服务","SpringCloud"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/40.SpringCloud/10.%E6%9C%8D%E5%8A%A1%E6%8B%86%E5%88%86%E5%92%8C%E8%BF%9C%E7%A8%8B%E8%B0%83%E7%94%A8.html",relativePath:"01.后端/60.微服务/40.SpringCloud/10.服务拆分和远程调用.md",key:"v-3d750c14",path:"/pages/1b3d4d/",headers:[{level:2,title:"服务拆分原则",slug:"服务拆分原则",normalizedTitle:"服务拆分原则",charIndex:30},{level:2,title:"服务拆分示例",slug:"服务拆分示例",normalizedTitle:"服务拆分示例",charIndex:138},{level:3,title:"导入Sql语句",slug:"导入sql语句",normalizedTitle:"导入sql语句",charIndex:378},{level:3,title:"导入demo工程",slug:"导入demo工程",normalizedTitle:"导入demo工程",charIndex:527},{level:2,title:"实现远程调用案例",slug:"实现远程调用案例",normalizedTitle:"实现远程调用案例",charIndex:642},{level:3,title:"案例需求：",slug:"案例需求",normalizedTitle:"案例需求：",charIndex:779},{level:3,title:"注册RestTemplate",slug:"注册resttemplate",normalizedTitle:"注册resttemplate",charIndex:1101},{level:3,title:"实现远程调用",slug:"实现远程调用",normalizedTitle:"实现远程调用",charIndex:642},{level:2,title:"提供者与消费者",slug:"提供者与消费者",normalizedTitle:"提供者与消费者",charIndex:1878}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"服务拆分原则 服务拆分示例 导入Sql语句 导入demo工程 实现远程调用案例 案例需求： 注册RestTemplate 实现远程调用 提供者与消费者",content:'任何分布式架构都离不开服务的拆分，微服务也是一样。\n\n\n# 服务拆分原则\n\n这里我总结了微服务拆分时的几个原则：\n\n * 不同微服务，不要重复开发相同业务\n * 微服务数据独立，不要访问其它微服务的数据库\n * 微服务可以将自己的业务暴露为接口，供其它微服务调用\n\n\n\n\n# 服务拆分示例\n\n以课前资料中的微服务cloud-demo为例，其结构如下：\n\n\n\ncloud-demo：父工程，管理依赖\n\n * order-service：订单微服务，负责订单相关业务\n * user-service：用户微服务，负责用户相关业务\n\n要求：\n\n * 订单微服务和用户微服务都必须有各自的数据库，相互独立\n * 订单服务和用户服务都对外暴露Restful的接口\n * 订单服务如果需要查询用户信息，只能调用用户服务的Restful接口，不能查询用户数据库\n\n\n# 导入Sql语句\n\n首先，将课前资料提供的cloud-order.sql和cloud-user.sql导入到mysql中：\n\n\n\ncloud-user表中初始数据如下：\n\n\n\ncloud-order表中初始数据如下：\n\n\n\ncloud-order表中持有cloud-user表中的id字段。\n\n\n# 导入demo工程\n\n用IDEA导入课前资料提供的Demo：\n\n\n\n项目结构如下：\n\n\n\n导入后，会在IDEA右下角出现弹窗：\n\n\n\n点击弹窗，然后按下图选择：\n\n\n\n会出现这样的菜单：\n\n\n\n配置下项目使用的JDK：\n\n\n\n\n# 实现远程调用案例\n\n在order-service服务中，有一个根据id查询订单的接口：\n\n\n\n根据id查询订单，返回值是Order对象，如图：\n\n\n\n其中的user为null\n\n在user-service中有一个根据id查询用户的接口：\n\n\n\n查询的结果如图：\n\n\n\n\n# 案例需求：\n\n修改order-service中的根据id查询订单业务，要求在查询订单的同时，根据订单中包含的userId查询出用户信息，一起返回。\n\n\n\n因此，我们需要在order-service中 向user-service发起一个http的请求，调用http://localhost:8081/user/{userId}这个接口。\n\n大概的步骤是这样的：\n\n * 注册一个RestTemplate的实例到Spring容器\n * 修改order-service服务中的OrderService类中的queryOrderById方法，根据Order对象中的userId查询User\n * 将查询的User填充到Order对象，一起返回\n\n\n# 注册RestTemplate\n\n首先，我们在order-service服务中的OrderApplication启动类中，注册RestTemplate实例：\n\npackage cn.itcast.order;\n\nimport org.mybatis.spring.annotation.MapperScan;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.web.client.RestTemplate;\n\n@MapperScan("cn.itcast.order.mapper")\n@SpringBootApplication\npublic class OrderApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(OrderApplication.class, args);\n    }\n\n    @Bean\n    public RestTemplate restTemplate() {\n        return new RestTemplate();\n    }\n}\n\n\n\n# 实现远程调用\n\n修改order-service服务中的cn.itcast.order.service包下的OrderService类中的queryOrderById方法：\n\n\n\n\n# 提供者与消费者\n\n在服务调用关系中，会有两个不同的角色：\n\n服务提供者：一次业务中，被其它微服务调用的服务。（提供接口给其它微服务）\n\n服务消费者：一次业务中，调用其它微服务的服务。（调用其它微服务提供的接口）\n\n\n\n但是，服务提供者与服务消费者的角色并不是绝对的，而是相对于业务而言。\n\n如果服务A调用了服务B，而服务B又调用了服务C，服务B的角色是什么？\n\n * 对于A调用B的业务而言：A是服务消费者，B是服务提供者\n * 对于B调用C的业务而言：B是服务消费者，C是服务提供者\n\n因此，服务B既可以是服务提供者，也可以是服务消费者。',normalizedContent:'任何分布式架构都离不开服务的拆分，微服务也是一样。\n\n\n# 服务拆分原则\n\n这里我总结了微服务拆分时的几个原则：\n\n * 不同微服务，不要重复开发相同业务\n * 微服务数据独立，不要访问其它微服务的数据库\n * 微服务可以将自己的业务暴露为接口，供其它微服务调用\n\n\n\n\n# 服务拆分示例\n\n以课前资料中的微服务cloud-demo为例，其结构如下：\n\n\n\ncloud-demo：父工程，管理依赖\n\n * order-service：订单微服务，负责订单相关业务\n * user-service：用户微服务，负责用户相关业务\n\n要求：\n\n * 订单微服务和用户微服务都必须有各自的数据库，相互独立\n * 订单服务和用户服务都对外暴露restful的接口\n * 订单服务如果需要查询用户信息，只能调用用户服务的restful接口，不能查询用户数据库\n\n\n# 导入sql语句\n\n首先，将课前资料提供的cloud-order.sql和cloud-user.sql导入到mysql中：\n\n\n\ncloud-user表中初始数据如下：\n\n\n\ncloud-order表中初始数据如下：\n\n\n\ncloud-order表中持有cloud-user表中的id字段。\n\n\n# 导入demo工程\n\n用idea导入课前资料提供的demo：\n\n\n\n项目结构如下：\n\n\n\n导入后，会在idea右下角出现弹窗：\n\n\n\n点击弹窗，然后按下图选择：\n\n\n\n会出现这样的菜单：\n\n\n\n配置下项目使用的jdk：\n\n\n\n\n# 实现远程调用案例\n\n在order-service服务中，有一个根据id查询订单的接口：\n\n\n\n根据id查询订单，返回值是order对象，如图：\n\n\n\n其中的user为null\n\n在user-service中有一个根据id查询用户的接口：\n\n\n\n查询的结果如图：\n\n\n\n\n# 案例需求：\n\n修改order-service中的根据id查询订单业务，要求在查询订单的同时，根据订单中包含的userid查询出用户信息，一起返回。\n\n\n\n因此，我们需要在order-service中 向user-service发起一个http的请求，调用http://localhost:8081/user/{userid}这个接口。\n\n大概的步骤是这样的：\n\n * 注册一个resttemplate的实例到spring容器\n * 修改order-service服务中的orderservice类中的queryorderbyid方法，根据order对象中的userid查询user\n * 将查询的user填充到order对象，一起返回\n\n\n# 注册resttemplate\n\n首先，我们在order-service服务中的orderapplication启动类中，注册resttemplate实例：\n\npackage cn.itcast.order;\n\nimport org.mybatis.spring.annotation.mapperscan;\nimport org.springframework.boot.springapplication;\nimport org.springframework.boot.autoconfigure.springbootapplication;\nimport org.springframework.context.annotation.bean;\nimport org.springframework.web.client.resttemplate;\n\n@mapperscan("cn.itcast.order.mapper")\n@springbootapplication\npublic class orderapplication {\n\n    public static void main(string[] args) {\n        springapplication.run(orderapplication.class, args);\n    }\n\n    @bean\n    public resttemplate resttemplate() {\n        return new resttemplate();\n    }\n}\n\n\n\n# 实现远程调用\n\n修改order-service服务中的cn.itcast.order.service包下的orderservice类中的queryorderbyid方法：\n\n\n\n\n# 提供者与消费者\n\n在服务调用关系中，会有两个不同的角色：\n\n服务提供者：一次业务中，被其它微服务调用的服务。（提供接口给其它微服务）\n\n服务消费者：一次业务中，调用其它微服务的服务。（调用其它微服务提供的接口）\n\n\n\n但是，服务提供者与服务消费者的角色并不是绝对的，而是相对于业务而言。\n\n如果服务a调用了服务b，而服务b又调用了服务c，服务b的角色是什么？\n\n * 对于a调用b的业务而言：a是服务消费者，b是服务提供者\n * 对于b调用c的业务而言：b是服务消费者，c是服务提供者\n\n因此，服务b既可以是服务提供者，也可以是服务消费者。',charsets:{cjk:!0}},{title:"Ribbon负载均衡",frontmatter:{autoSort:97,title:"Ribbon负载均衡",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/5546ef/",categories:["后端","微服务","SpringCloud"],tags:["知识","微服务","SpringCloud"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/40.SpringCloud/30.Ribbon%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1.html",relativePath:"01.后端/60.微服务/40.SpringCloud/30.Ribbon负载均衡.md",key:"v-07eaf4f2",path:"/pages/5546ef/",headers:[{level:2,title:"负载均衡原理",slug:"负载均衡原理",normalizedTitle:"负载均衡原理",charIndex:50},{level:2,title:"源码跟踪",slug:"源码跟踪",normalizedTitle:"源码跟踪",charIndex:178},{level:3,title:"1）LoadBalancerIntercepor",slug:"_1-loadbalancerintercepor",normalizedTitle:"1）loadbalancerintercepor",charIndex:378},{level:3,title:"2）LoadBalancerClient",slug:"_2-loadbalancerclient",normalizedTitle:"2）loadbalancerclient",charIndex:675},{level:3,title:"3）负载均衡策略IRule",slug:"_3-负载均衡策略irule",normalizedTitle:"3）负载均衡策略irule",charIndex:932},{level:3,title:"4）总结",slug:"_4-总结",normalizedTitle:"4）总结",charIndex:1128},{level:2,title:"负载均衡策略",slug:"负载均衡策略",normalizedTitle:"负载均衡策略",charIndex:934},{level:3,title:"负载均衡策略",slug:"负载均衡策略-2",normalizedTitle:"负载均衡策略",charIndex:934},{level:3,title:"自定义负载均衡策略",slug:"自定义负载均衡策略",normalizedTitle:"自定义负载均衡策略",charIndex:2441},{level:2,title:"饥饿加载",slug:"饥饿加载",normalizedTitle:"饥饿加载",charIndex:2892},{level:2,title:"Ribbon负载均衡总结",slug:"ribbon负载均衡总结",normalizedTitle:"ribbon负载均衡总结",charIndex:3065},{level:3,title:"1. 负载均衡规则",slug:"_1-负载均衡规则",normalizedTitle:"1. 负载均衡规则",charIndex:3082},{level:3,title:"2. 负载均衡自定义方式",slug:"_2-负载均衡自定义方式",normalizedTitle:"2. 负载均衡自定义方式",charIndex:3167},{level:3,title:"3. 饥饿加载",slug:"_3-饥饿加载",normalizedTitle:"3. 饥饿加载",charIndex:3259}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"负载均衡原理 源码跟踪 1）LoadBalancerIntercepor 2）LoadBalancerClient 3）负载均衡策略IRule 4）总结 负载均衡策略 负载均衡策略 自定义负载均衡策略 饥饿加载 Ribbon负载均衡总结 1. 负载均衡规则 2. 负载均衡自定义方式 3. 饥饿加载",content:"上一节中，我们添加了@LoadBalanced注解，即可实现负载均衡功能，这是什么原理呢？\n\n\n# 负载均衡原理\n\nSpringCloud底层其实是利用了一个名为Ribbon的组件，来实现负载均衡功能的。\n\n\n\n那么我们发出的请求明明是http://userservice/user/1，怎么变成了http://localhost:8081的呢？\n\n\n# 源码跟踪\n\n为什么我们只输入了service名称就可以访问了呢？之前还要获取ip和端口。\n\n显然有人帮我们根据service名称，获取到了服务实例的ip和端口。它就是LoadBalancerInterceptor，这个类会在对RestTemplate的请求进行拦截，然后从Eureka根据服务id获取服务列表，随后利用负载均衡算法得到真实的服务地址信息，替换服务id。\n\n我们进行源码跟踪：\n\n\n# 1）LoadBalancerIntercepor\n\n\n\n可以看到这里的intercept方法，拦截了用户的HttpRequest请求，然后做了几件事：\n\n * request.getURI()：获取请求uri，本例中就是 http://user-service/user/8\n * originalUri.getHost()：获取uri路径的主机名，其实就是服务id，user-service\n * this.loadBalancer.execute()：处理服务id，和用户请求。\n\n这里的this.loadBalancer是LoadBalancerClient类型，我们继续跟入。\n\n\n# 2）LoadBalancerClient\n\n继续跟入execute方法：\n\n\n\n代码是这样的：\n\n * getLoadBalancer(serviceId)：根据服务id获取ILoadBalancer，而ILoadBalancer会拿着服务id去eureka中获取服务列表并保存起来。\n * getServer(loadBalancer)：利用内置的负载均衡算法，从服务列表中选择一个。本例中，可以看到获取了8082端口的服务\n\n放行后，再次访问并跟踪，发现获取的是8081：\n\n\n\n果然实现了负载均衡。\n\n\n# 3）负载均衡策略IRule\n\n在刚才的代码中，可以看到获取服务使通过一个getServer方法来做负载均衡:\n\n\n\n我们继续跟入：\n\n\n\n继续跟踪源码chooseServer方法，发现这么一段代码：\n\n\n\n我们看看这个rule是谁：\n\n\n\n这里的rule默认值是一个RoundRobinRule，看类的介绍：\n\n\n\n这不就是轮询的意思嘛。\n\n到这里，整个负载均衡的流程我们就清楚了。\n\n\n# 4）总结\n\nSpringCloudRibbon的底层采用了一个拦截器，拦截了RestTemplate发出的请求，对地址做了修改。用一幅图来总结一下：\n\n\n\n基本流程如下：\n\n * 拦截我们的RestTemplate请求http://userservice/user/1\n * RibbonLoadBalancerClient会从请求url中获取服务名称，也就是user-service\n * DynamicServerListLoadBalancer根据user-service到eureka拉取服务列表\n * eureka返回列表，localhost:8081、localhost:8082\n * IRule利用内置负载均衡规则，从列表中选择一个，例如localhost:8081\n * RibbonLoadBalancerClient修改请求地址，用localhost:8081替代userservice，得到http://localhost:8081/user/1，发起真实请求\n\n\n# 负载均衡策略\n\n\n# 负载均衡策略\n\n负载均衡的规则都定义在IRule接口中，而IRule有很多不同的实现类：\n\n\n\n不同规则的含义如下：\n\n内置负载均衡规则类                   规则描述\nRoundRobinRule              简单轮询服务列表来选择服务器。它是Ribbon默认的负载均衡规则。\nAvailabilityFilteringRule   对以下两种服务器进行忽略：\n                            （1）在默认情况下，这台服务器如果3次连接失败，这台服务器就会被设置为“短路”状态。短路状态将持续30秒，如果再次连接失败，短路的持续时间就会几何级地增加。\n                            （2）并发数过高的服务器。如果一个服务器的并发连接数过高，配置了AvailabilityFilteringRule规则的客户端也会将其忽略。并发连接数的上限，可以由客户端的<clientName>.<clientConfigNameSpace>.ActiveConnectionsLimit属性进行配置。\nWeightedResponseTimeRule    为每一个服务器赋予一个权重值。服务器响应时间越长，这个服务器的权重就越小。这个规则会随机选择服务器，这个权重值会影响服务器的选择。\nZoneAvoidanceRule （默认）      以区域可用的服务器为基础进行服务器的选择。使用Zone对服务器进行分类，这个Zone可以理解为一个机房、一个机架等。而后再对Zone内的多个服务做轮询。\nBestAvailableRule           忽略那些短路的服务器，并选择并发数较低的服务器。\nRandomRule                  随机选择一个可用的服务器。\nRetryRule                   重试机制的选择逻辑\n\n默认的实现就是ZoneAvoidanceRule，是一种轮询方案\n\n\n# 自定义负载均衡策略\n\n通过定义IRule实现可以修改负载均衡规则，有两种方式：\n\n 1. 代码方式：在order-service中的OrderApplication类中，定义一个新的IRule：\n    \n    这个是全局修改，不管访问哪个微服务，均采用这种负载均衡策略\n\n@Bean\npublic IRule randomRule(){\n    return new RandomRule();\n}\n\n\n 2. 配置文件方式：在order-service的application.yml文件中，添加新的配置也可以修改规则：\n    \n    给单独一个微服务修改规则\n\nuserservice: # 给某个微服务配置负载均衡规则，这里是userservice服务\n  ribbon:\n    NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule # 负载均衡规则 \n\n\n注意，一般用默认的负载均衡规则，不做修改。\n\n\n# 饥饿加载\n\nRibbon默认是采用懒加载，即第一次访问时才会去创建LoadBalanceClient，请求时间会很长。\n\n而饥饿加载则会在项目启动时创建，降低第一次访问的耗时，通过下面配置开启饥饿加载：\n\nribbon:\n  eager-load:\n    enabled: true\n    clients: userservice\n\n\n\n# Ribbon负载均衡总结\n\n\n# 1. 负载均衡规则\n\n * 规则接口是IRule\n * 默认实现是ZoneAvoidanceRule\n   * 首先根据zone选择服务列表\n   * 然后轮询\n\n\n# 2. 负载均衡自定义方式\n\n * 代码方式\n   * 全局配置，同时作用于所有的微服务项目\n * 配置方式-yml\n   * 需要单独对具体的微服务指定配置，无法做到全局配置\n\n\n# 3. 饥饿加载\n\n> 默认是懒加载，当访问到某个具体的微服务时，才会加载Ribbon。这样会导致第一次访问的时长过长。\n> \n> 饥饿加载则是，当项目启动时，就直接加载对指定微服务的Ribbon\n\n * 开启饥饿加载\n * 指定饥饿加载微服务名称",normalizedContent:"上一节中，我们添加了@loadbalanced注解，即可实现负载均衡功能，这是什么原理呢？\n\n\n# 负载均衡原理\n\nspringcloud底层其实是利用了一个名为ribbon的组件，来实现负载均衡功能的。\n\n\n\n那么我们发出的请求明明是http://userservice/user/1，怎么变成了http://localhost:8081的呢？\n\n\n# 源码跟踪\n\n为什么我们只输入了service名称就可以访问了呢？之前还要获取ip和端口。\n\n显然有人帮我们根据service名称，获取到了服务实例的ip和端口。它就是loadbalancerinterceptor，这个类会在对resttemplate的请求进行拦截，然后从eureka根据服务id获取服务列表，随后利用负载均衡算法得到真实的服务地址信息，替换服务id。\n\n我们进行源码跟踪：\n\n\n# 1）loadbalancerintercepor\n\n\n\n可以看到这里的intercept方法，拦截了用户的httprequest请求，然后做了几件事：\n\n * request.geturi()：获取请求uri，本例中就是 http://user-service/user/8\n * originaluri.gethost()：获取uri路径的主机名，其实就是服务id，user-service\n * this.loadbalancer.execute()：处理服务id，和用户请求。\n\n这里的this.loadbalancer是loadbalancerclient类型，我们继续跟入。\n\n\n# 2）loadbalancerclient\n\n继续跟入execute方法：\n\n\n\n代码是这样的：\n\n * getloadbalancer(serviceid)：根据服务id获取iloadbalancer，而iloadbalancer会拿着服务id去eureka中获取服务列表并保存起来。\n * getserver(loadbalancer)：利用内置的负载均衡算法，从服务列表中选择一个。本例中，可以看到获取了8082端口的服务\n\n放行后，再次访问并跟踪，发现获取的是8081：\n\n\n\n果然实现了负载均衡。\n\n\n# 3）负载均衡策略irule\n\n在刚才的代码中，可以看到获取服务使通过一个getserver方法来做负载均衡:\n\n\n\n我们继续跟入：\n\n\n\n继续跟踪源码chooseserver方法，发现这么一段代码：\n\n\n\n我们看看这个rule是谁：\n\n\n\n这里的rule默认值是一个roundrobinrule，看类的介绍：\n\n\n\n这不就是轮询的意思嘛。\n\n到这里，整个负载均衡的流程我们就清楚了。\n\n\n# 4）总结\n\nspringcloudribbon的底层采用了一个拦截器，拦截了resttemplate发出的请求，对地址做了修改。用一幅图来总结一下：\n\n\n\n基本流程如下：\n\n * 拦截我们的resttemplate请求http://userservice/user/1\n * ribbonloadbalancerclient会从请求url中获取服务名称，也就是user-service\n * dynamicserverlistloadbalancer根据user-service到eureka拉取服务列表\n * eureka返回列表，localhost:8081、localhost:8082\n * irule利用内置负载均衡规则，从列表中选择一个，例如localhost:8081\n * ribbonloadbalancerclient修改请求地址，用localhost:8081替代userservice，得到http://localhost:8081/user/1，发起真实请求\n\n\n# 负载均衡策略\n\n\n# 负载均衡策略\n\n负载均衡的规则都定义在irule接口中，而irule有很多不同的实现类：\n\n\n\n不同规则的含义如下：\n\n内置负载均衡规则类                   规则描述\nroundrobinrule              简单轮询服务列表来选择服务器。它是ribbon默认的负载均衡规则。\navailabilityfilteringrule   对以下两种服务器进行忽略：\n                            （1）在默认情况下，这台服务器如果3次连接失败，这台服务器就会被设置为“短路”状态。短路状态将持续30秒，如果再次连接失败，短路的持续时间就会几何级地增加。\n                            （2）并发数过高的服务器。如果一个服务器的并发连接数过高，配置了availabilityfilteringrule规则的客户端也会将其忽略。并发连接数的上限，可以由客户端的<clientname>.<clientconfignamespace>.activeconnectionslimit属性进行配置。\nweightedresponsetimerule    为每一个服务器赋予一个权重值。服务器响应时间越长，这个服务器的权重就越小。这个规则会随机选择服务器，这个权重值会影响服务器的选择。\nzoneavoidancerule （默认）      以区域可用的服务器为基础进行服务器的选择。使用zone对服务器进行分类，这个zone可以理解为一个机房、一个机架等。而后再对zone内的多个服务做轮询。\nbestavailablerule           忽略那些短路的服务器，并选择并发数较低的服务器。\nrandomrule                  随机选择一个可用的服务器。\nretryrule                   重试机制的选择逻辑\n\n默认的实现就是zoneavoidancerule，是一种轮询方案\n\n\n# 自定义负载均衡策略\n\n通过定义irule实现可以修改负载均衡规则，有两种方式：\n\n 1. 代码方式：在order-service中的orderapplication类中，定义一个新的irule：\n    \n    这个是全局修改，不管访问哪个微服务，均采用这种负载均衡策略\n\n@bean\npublic irule randomrule(){\n    return new randomrule();\n}\n\n\n 2. 配置文件方式：在order-service的application.yml文件中，添加新的配置也可以修改规则：\n    \n    给单独一个微服务修改规则\n\nuserservice: # 给某个微服务配置负载均衡规则，这里是userservice服务\n  ribbon:\n    nfloadbalancerruleclassname: com.netflix.loadbalancer.randomrule # 负载均衡规则 \n\n\n注意，一般用默认的负载均衡规则，不做修改。\n\n\n# 饥饿加载\n\nribbon默认是采用懒加载，即第一次访问时才会去创建loadbalanceclient，请求时间会很长。\n\n而饥饿加载则会在项目启动时创建，降低第一次访问的耗时，通过下面配置开启饥饿加载：\n\nribbon:\n  eager-load:\n    enabled: true\n    clients: userservice\n\n\n\n# ribbon负载均衡总结\n\n\n# 1. 负载均衡规则\n\n * 规则接口是irule\n * 默认实现是zoneavoidancerule\n   * 首先根据zone选择服务列表\n   * 然后轮询\n\n\n# 2. 负载均衡自定义方式\n\n * 代码方式\n   * 全局配置，同时作用于所有的微服务项目\n * 配置方式-yml\n   * 需要单独对具体的微服务指定配置，无法做到全局配置\n\n\n# 3. 饥饿加载\n\n> 默认是懒加载，当访问到某个具体的微服务时，才会加载ribbon。这样会导致第一次访问的时长过长。\n> \n> 饥饿加载则是，当项目启动时，就直接加载对指定微服务的ribbon\n\n * 开启饥饿加载\n * 指定饥饿加载微服务名称",charsets:{cjk:!0}},{title:"Eureka注册中心",frontmatter:{autoSort:98,title:"Eureka注册中心",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/938522/",categories:["后端","微服务","SpringCloud"],tags:["知识","微服务","SpringCloud"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/40.SpringCloud/20.Eureka%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83.html",relativePath:"01.后端/60.微服务/40.SpringCloud/20.Eureka注册中心.md",key:"v-fb5f7d52",path:"/pages/938522/",headers:[{level:2,title:"Eureka的结构和作用",slug:"eureka的结构和作用",normalizedTitle:"eureka的结构和作用",charIndex:206},{level:2,title:"搭建eureka-server",slug:"搭建eureka-server",normalizedTitle:"搭建eureka-server",charIndex:900},{level:3,title:"创建eureka-server服务",slug:"创建eureka-server服务",normalizedTitle:"创建eureka-server服务",charIndex:960},{level:3,title:"引入eureka依赖",slug:"引入eureka依赖",normalizedTitle:"引入eureka依赖",charIndex:1034},{level:3,title:"编写启动类",slug:"编写启动类",normalizedTitle:"编写启动类",charIndex:1234},{level:3,title:"编写配置文件",slug:"编写配置文件",normalizedTitle:"编写配置文件",charIndex:1731},{level:3,title:"启动服务",slug:"启动服务",normalizedTitle:"启动服务",charIndex:1927},{level:2,title:"服务注册",slug:"服务注册",normalizedTitle:"服务注册",charIndex:407},{level:3,title:"1）引入依赖",slug:"_1-引入依赖",normalizedTitle:"1）引入依赖",charIndex:2040},{level:3,title:"2）配置文件",slug:"_2-配置文件",normalizedTitle:"2）配置文件",charIndex:2245},{level:3,title:"3）启动多个user-service实例",slug:"_3-启动多个user-service实例",normalizedTitle:"3）启动多个user-service实例",charIndex:2440},{level:2,title:"服务发现",slug:"服务发现",normalizedTitle:"服务发现",charIndex:484},{level:3,title:"1）引入依赖",slug:"_1-引入依赖-2",normalizedTitle:"1）引入依赖",charIndex:2040},{level:3,title:"2）配置文件",slug:"_2-配置文件-2",normalizedTitle:"2）配置文件",charIndex:2245},{level:3,title:"3）服务拉取和负载均衡",slug:"_3-服务拉取和负载均衡",normalizedTitle:"3）服务拉取和负载均衡",charIndex:3353},{level:2,title:"Eureka注册中心 总结",slug:"eureka注册中心-总结",normalizedTitle:"eureka注册中心 总结",charIndex:3697},{level:3,title:"1.搭建EurekaServer",slug:"_1-搭建eurekaserver",normalizedTitle:"1.搭建eurekaserver",charIndex:3715},{level:3,title:"2.服务注册",slug:"_2-服务注册",normalizedTitle:"2.服务注册",charIndex:3825},{level:3,title:"3.服务发现",slug:"_3-服务发现",normalizedTitle:"3.服务发现",charIndex:3896}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"Eureka的结构和作用 搭建eureka-server 创建eureka-server服务 引入eureka依赖 编写启动类 编写配置文件 启动服务 服务注册 1）引入依赖 2）配置文件 3）启动多个user-service实例 服务发现 1）引入依赖 2）配置文件 3）服务拉取和负载均衡 Eureka注册中心 总结 1.搭建EurekaServer 2.服务注册 3.服务发现",content:"假如我们的服务提供者user-service部署了多个实例，如图：\n\n\n\n大家思考几个问题：\n\n * order-service在发起远程调用的时候，该如何得知user-service实例的ip地址和端口？\n * 有多个user-service实例地址，order-service调用时该如何选择？\n * order-service如何得知某个user-service实例是否依然健康，是不是已经宕机？\n\n\n# Eureka的结构和作用\n\n这些问题都需要利用SpringCloud中的注册中心来解决，其中最广为人知的注册中心就是Eureka，其结构如下：\n\n\n\n回答之前的各个问题。\n\n问题1：order-service如何得知user-service实例地址？\n\n获取地址信息的流程如下：\n\n * user-service服务实例启动后，将自己的信息注册到eureka-server（Eureka服务端）。这个叫服务注册\n * eureka-server保存服务名称到服务实例地址列表的映射关系\n * order-service根据服务名称，拉取实例地址列表。这个叫服务发现或服务拉取\n\n问题2：order-service如何从多个user-service实例中选择具体的实例？\n\n * order-service从实例列表中利用负载均衡算法选中一个实例地址\n * 向该实例地址发起远程调用\n\n问题3：order-service如何得知某个user-service实例是否依然健康，是不是已经宕机？\n\n * user-service会每隔一段时间（默认30秒）向eureka-server发起请求，报告自己状态，称为心跳\n * 当超过一定时间没有发送心跳时，eureka-server会认为微服务实例故障，将该实例从服务列表中剔除\n * order-service拉取服务时，就能将故障实例排除了\n\n> 注意：一个微服务，既可以是服务提供者，又可以是服务消费者，因此eureka将服务注册、服务发现等功能统一封装到了eureka-client端\n\n因此，接下来我们动手实践的步骤包括：\n\n\n\n\n# 搭建eureka-server\n\n首先大家注册中心服务端：eureka-server，这必须是一个独立的微服务\n\n\n# 创建eureka-server服务\n\n在cloud-demo父工程下，创建一个子模块：\n\n\n\n填写模块信息：\n\n\n\n然后填写服务信息：\n\n\n\n\n# 引入eureka依赖\n\n引入SpringCloud为eureka提供的starter依赖：\n\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-eureka-server</artifactId>\n</dependency>\n\n\n\n# 编写启动类\n\n给eureka-server服务编写一个启动类，一定要添加一个@EnableEurekaServer注解，开启eureka的注册中心功能：\n\npackage cn.itcast.eureka;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;\n\n@SpringBootApplication\n@EnableEurekaServer\npublic class EurekaApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(EurekaApplication.class, args);\n    }\n}\n\n\n\n# 编写配置文件\n\n编写一个application.yml文件，内容如下：\n\nserver:\n  port: 10086\nspring:\n  application:\n    name: eureka-server\neureka:\n  client:\n    service-url: \n      defaultZone: http://127.0.0.1:10086/eureka\n\n\n\n# 启动服务\n\n启动微服务，然后在浏览器访问：http://127.0.0.1:10086\n\n看到下面结果应该是成功了：\n\n\n\n\n# 服务注册\n\n下面，我们将user-service注册到eureka-server中去。\n\n\n# 1）引入依赖\n\n在user-service的pom文件中，引入下面的eureka-client依赖：\n\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>\n</dependency>\n\n\n\n# 2）配置文件\n\n在user-service中，修改application.yml文件，添加服务名称、eureka地址：\n\nspring:\n  application:\n    name: userservice\neureka:\n  client:\n    service-url:\n      defaultZone: http://127.0.0.1:10086/eureka\n\n\n\n# 3）启动多个user-service实例\n\n为了演示一个服务有多个实例的场景，我们添加一个SpringBoot的启动配置，再启动一个user-service。\n\n首先，复制原来的user-service启动配置：\n\n\n\n补充\n\n> 新版idea，下方这个页面不完全一样，注意要写在 VM options这个选项栏里\n> \n> 这里重新配置一个端口，会覆盖掉yml文件中的端口定义，避免端口冲突。\n\n然后，在弹出的窗口中，填写信息：\n\n\n\n现在，SpringBoot窗口会出现两个user-service启动配置：\n\n\n\n不过，第一个是8081端口，第二个是8082端口。\n\n启动两个user-service实例：\n\n\n\n查看eureka-server管理页面：\n\n\n\n\n# 服务发现\n\n下面，我们将order-service的逻辑修改：向eureka-server拉取user-service的信息，实现服务发现。\n\n\n# 1）引入依赖\n\n之前说过，服务发现、服务注册统一都封装在eureka-client依赖，因此这一步与服务注册时一致。\n\n在order-service的pom文件中，引入下面的eureka-client依赖：\n\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>\n</dependency>\n\n\n\n# 2）配置文件\n\n服务发现也需要知道eureka地址，因此第二步与服务注册一致，都是配置eureka信息：\n\n在order-service中，修改application.yml文件，添加服务名称、eureka地址：\n\nspring:\n  application:\n    name: orderservice\neureka:\n  client:\n    service-url:\n      defaultZone: http://127.0.0.1:10086/eureka\n\n\n\n# 3）服务拉取和负载均衡\n\n最后，我们要去eureka-server中拉取user-service服务的实例列表，并且实现负载均衡。\n\n不过这些动作不用我们去做，只需要添加一些注解即可。\n\n在order-service的OrderApplication中，给RestTemplate这个Bean添加一个@LoadBalanced注解：\n\n\n\n修改order-service服务中的cn.itcast.order.service包下的OrderService类中的queryOrderById方法。修改访问的url路径，用服务名代替ip、端口：\n\n\n\nspring会自动帮助我们从eureka-server端，根据userservice这个服务名称，获取实例列表，而后完成负载均衡。\n\n\n# Eureka注册中心 总结\n\n\n# 1.搭建EurekaServer\n\n * 引入eureka-server依赖\n * 添加@EnableEurekaServer注解开关\n * 在application.yml中配置服务名称，eureka服务地址\n\n\n# 2.服务注册\n\n * 引入eureka-client依赖\n * 在application.yml中配置服务名称，eureka服务地址\n\n\n# 3.服务发现\n\n * 引入eureka-client依赖\n * 在application.yml中配置服务名称，eureka服务地址\n * 给RestTemplate添加@LoadBalanced注解，实现负载均衡\n * 用服务者的服务名称（来代替网站的硬编码）实现远程调用",normalizedContent:"假如我们的服务提供者user-service部署了多个实例，如图：\n\n\n\n大家思考几个问题：\n\n * order-service在发起远程调用的时候，该如何得知user-service实例的ip地址和端口？\n * 有多个user-service实例地址，order-service调用时该如何选择？\n * order-service如何得知某个user-service实例是否依然健康，是不是已经宕机？\n\n\n# eureka的结构和作用\n\n这些问题都需要利用springcloud中的注册中心来解决，其中最广为人知的注册中心就是eureka，其结构如下：\n\n\n\n回答之前的各个问题。\n\n问题1：order-service如何得知user-service实例地址？\n\n获取地址信息的流程如下：\n\n * user-service服务实例启动后，将自己的信息注册到eureka-server（eureka服务端）。这个叫服务注册\n * eureka-server保存服务名称到服务实例地址列表的映射关系\n * order-service根据服务名称，拉取实例地址列表。这个叫服务发现或服务拉取\n\n问题2：order-service如何从多个user-service实例中选择具体的实例？\n\n * order-service从实例列表中利用负载均衡算法选中一个实例地址\n * 向该实例地址发起远程调用\n\n问题3：order-service如何得知某个user-service实例是否依然健康，是不是已经宕机？\n\n * user-service会每隔一段时间（默认30秒）向eureka-server发起请求，报告自己状态，称为心跳\n * 当超过一定时间没有发送心跳时，eureka-server会认为微服务实例故障，将该实例从服务列表中剔除\n * order-service拉取服务时，就能将故障实例排除了\n\n> 注意：一个微服务，既可以是服务提供者，又可以是服务消费者，因此eureka将服务注册、服务发现等功能统一封装到了eureka-client端\n\n因此，接下来我们动手实践的步骤包括：\n\n\n\n\n# 搭建eureka-server\n\n首先大家注册中心服务端：eureka-server，这必须是一个独立的微服务\n\n\n# 创建eureka-server服务\n\n在cloud-demo父工程下，创建一个子模块：\n\n\n\n填写模块信息：\n\n\n\n然后填写服务信息：\n\n\n\n\n# 引入eureka依赖\n\n引入springcloud为eureka提供的starter依赖：\n\n<dependency>\n    <groupid>org.springframework.cloud</groupid>\n    <artifactid>spring-cloud-starter-netflix-eureka-server</artifactid>\n</dependency>\n\n\n\n# 编写启动类\n\n给eureka-server服务编写一个启动类，一定要添加一个@enableeurekaserver注解，开启eureka的注册中心功能：\n\npackage cn.itcast.eureka;\n\nimport org.springframework.boot.springapplication;\nimport org.springframework.boot.autoconfigure.springbootapplication;\nimport org.springframework.cloud.netflix.eureka.server.enableeurekaserver;\n\n@springbootapplication\n@enableeurekaserver\npublic class eurekaapplication {\n    public static void main(string[] args) {\n        springapplication.run(eurekaapplication.class, args);\n    }\n}\n\n\n\n# 编写配置文件\n\n编写一个application.yml文件，内容如下：\n\nserver:\n  port: 10086\nspring:\n  application:\n    name: eureka-server\neureka:\n  client:\n    service-url: \n      defaultzone: http://127.0.0.1:10086/eureka\n\n\n\n# 启动服务\n\n启动微服务，然后在浏览器访问：http://127.0.0.1:10086\n\n看到下面结果应该是成功了：\n\n\n\n\n# 服务注册\n\n下面，我们将user-service注册到eureka-server中去。\n\n\n# 1）引入依赖\n\n在user-service的pom文件中，引入下面的eureka-client依赖：\n\n<dependency>\n    <groupid>org.springframework.cloud</groupid>\n    <artifactid>spring-cloud-starter-netflix-eureka-client</artifactid>\n</dependency>\n\n\n\n# 2）配置文件\n\n在user-service中，修改application.yml文件，添加服务名称、eureka地址：\n\nspring:\n  application:\n    name: userservice\neureka:\n  client:\n    service-url:\n      defaultzone: http://127.0.0.1:10086/eureka\n\n\n\n# 3）启动多个user-service实例\n\n为了演示一个服务有多个实例的场景，我们添加一个springboot的启动配置，再启动一个user-service。\n\n首先，复制原来的user-service启动配置：\n\n\n\n补充\n\n> 新版idea，下方这个页面不完全一样，注意要写在 vm options这个选项栏里\n> \n> 这里重新配置一个端口，会覆盖掉yml文件中的端口定义，避免端口冲突。\n\n然后，在弹出的窗口中，填写信息：\n\n\n\n现在，springboot窗口会出现两个user-service启动配置：\n\n\n\n不过，第一个是8081端口，第二个是8082端口。\n\n启动两个user-service实例：\n\n\n\n查看eureka-server管理页面：\n\n\n\n\n# 服务发现\n\n下面，我们将order-service的逻辑修改：向eureka-server拉取user-service的信息，实现服务发现。\n\n\n# 1）引入依赖\n\n之前说过，服务发现、服务注册统一都封装在eureka-client依赖，因此这一步与服务注册时一致。\n\n在order-service的pom文件中，引入下面的eureka-client依赖：\n\n<dependency>\n    <groupid>org.springframework.cloud</groupid>\n    <artifactid>spring-cloud-starter-netflix-eureka-client</artifactid>\n</dependency>\n\n\n\n# 2）配置文件\n\n服务发现也需要知道eureka地址，因此第二步与服务注册一致，都是配置eureka信息：\n\n在order-service中，修改application.yml文件，添加服务名称、eureka地址：\n\nspring:\n  application:\n    name: orderservice\neureka:\n  client:\n    service-url:\n      defaultzone: http://127.0.0.1:10086/eureka\n\n\n\n# 3）服务拉取和负载均衡\n\n最后，我们要去eureka-server中拉取user-service服务的实例列表，并且实现负载均衡。\n\n不过这些动作不用我们去做，只需要添加一些注解即可。\n\n在order-service的orderapplication中，给resttemplate这个bean添加一个@loadbalanced注解：\n\n\n\n修改order-service服务中的cn.itcast.order.service包下的orderservice类中的queryorderbyid方法。修改访问的url路径，用服务名代替ip、端口：\n\n\n\nspring会自动帮助我们从eureka-server端，根据userservice这个服务名称，获取实例列表，而后完成负载均衡。\n\n\n# eureka注册中心 总结\n\n\n# 1.搭建eurekaserver\n\n * 引入eureka-server依赖\n * 添加@enableeurekaserver注解开关\n * 在application.yml中配置服务名称，eureka服务地址\n\n\n# 2.服务注册\n\n * 引入eureka-client依赖\n * 在application.yml中配置服务名称，eureka服务地址\n\n\n# 3.服务发现\n\n * 引入eureka-client依赖\n * 在application.yml中配置服务名称，eureka服务地址\n * 给resttemplate添加@loadbalanced注解，实现负载均衡\n * 用服务者的服务名称（来代替网站的硬编码）实现远程调用",charsets:{cjk:!0}},{title:"Nacos配置管理",frontmatter:{autoSort:95,title:"Nacos配置管理",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/ffc82f/",categories:["后端","微服务","SpringCloud"],tags:["知识","微服务","SpringCloud"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/40.SpringCloud/50.Nacos%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86.html",relativePath:"01.后端/60.微服务/40.SpringCloud/50.Nacos配置管理.md",key:"v-ba455d3c",path:"/pages/ffc82f/",headers:[{level:2,title:"统一配置管理",slug:"统一配置管理",normalizedTitle:"统一配置管理",charIndex:33},{level:3,title:"在nacos中添加配置文件",slug:"在nacos中添加配置文件",normalizedTitle:"在nacos中添加配置文件",charIndex:173},{level:3,title:"从微服务拉取配置",slug:"从微服务拉取配置",normalizedTitle:"从微服务拉取配置",charIndex:292},{level:2,title:"配置热更新",slug:"配置热更新",normalizedTitle:"配置热更新",charIndex:2271},{level:3,title:"方式一",slug:"方式一",normalizedTitle:"方式一",charIndex:2350},{level:3,title:"方式二",slug:"方式二",normalizedTitle:"方式二",charIndex:2396},{level:2,title:"配置共享",slug:"配置共享",normalizedTitle:"配置共享",charIndex:3788},{level:3,title:"1）添加一个环境共享配置",slug:"_1-添加一个环境共享配置",normalizedTitle:"1）添加一个环境共享配置",charIndex:4034},{level:3,title:"2）在user-service中读取共享配置",slug:"_2-在user-service中读取共享配置",normalizedTitle:"2）在user-service中读取共享配置",charIndex:4087},{level:3,title:"3）运行两个UserApplication，使用不同的profile",slug:"_3-运行两个userapplication-使用不同的profile",normalizedTitle:"3）运行两个userapplication，使用不同的profile",charIndex:4210},{level:3,title:"4）配置共享的优先级",slug:"_4-配置共享的优先级",normalizedTitle:"4）配置共享的优先级",charIndex:4538}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"统一配置管理 在nacos中添加配置文件 从微服务拉取配置 配置热更新 方式一 方式二 配置共享 1）添加一个环境共享配置 2）在user-service中读取共享配置 3）运行两个UserApplication，使用不同的profile 4）配置共享的优先级",content:'Nacos除了可以做注册中心，同样可以做配置管理来使用。\n\n\n# 统一配置管理\n\n当微服务部署的实例越来越多，达到数十、数百时，逐个修改微服务配置就会让人抓狂，而且很容易出错。我们需要一种统一配置管理方案，可以集中管理所有实例的配置。\n\n\n\nNacos一方面可以将配置集中管理，另一方可以在配置变更时，及时通知微服务，实现配置的热更新。\n\n\n# 在nacos中添加配置文件\n\n如何在nacos中管理配置呢？\n\n\n\n然后在弹出的表单中，填写配置信息：\n\n\n\n注意：项目的核心配置，需要热更新的配置才有放到nacos管理的必要。基本不会变更的一些配置还是保存在微服务本地比较好。\n\n\n# 从微服务拉取配置\n\n微服务要拉取nacos中管理的配置，并且与本地的application.yml配置合并，才能完成项目启动。\n\n但如果尚未读取application.yml，又如何得知nacos地址呢？\n\n因此spring引入了一种新的配置文件：bootstrap.yaml文件，会在application.yml之前被读取，流程如下：\n\n\n\n1）引入nacos-config依赖\n\n首先，在user-service服务中，引入nacos-config的客户端依赖：\n\n\x3c!--nacos配置管理依赖--\x3e\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>\n</dependency>\n\n\n2）添加bootstrap.yaml\n\n然后，在user-service中添加一个bootstrap.yaml文件，内容如下：\n\nspring:\n  application:\n    name: userservice # 服务名称\n  profiles:\n    active: dev #开发环境，这里是dev \n  cloud:\n    nacos:\n      server-addr: localhost:8848 # Nacos地址\n      config:\n        file-extension: yaml # 文件后缀名\n\n\n这里会根据spring.cloud.nacos.server-addr获取nacos地址，再根据\n\n${spring.application.name}-${spring.profiles.active}.${spring.cloud.nacos.config.file-extension}作为文件id，来读取配置。\n\n本例中，就是去读取userservice-dev.yaml：\n\n这里需要添加配置命名空间的配置\n\ncloud:\n    nacos:\n      server-addr: localhost:8848 #nacos服务地址\n      config:\n        file-extension: yaml #后缀名\n        #这里需要额外配置上dev环境，默认是在public空间里\n        #这是配置管理的环境，跟服务管理的环境不一样\n        namespace: b1922d8f-22a7-4b31-b978-aa5e452ec77e #dev环境\n\n\n\n\n3）读取nacos配置\n\n在user-service中的UserController中添加业务逻辑，读取pattern.dateformat配置：\n\n\n\n完整代码：\n\npackage cn.itcast.user.web;\n\nimport cn.itcast.user.pojo.User;\nimport cn.itcast.user.service.UserService;\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.web.bind.annotation.*;\n\nimport java.time.LocalDateTime;\nimport java.time.format.DateTimeFormatter;\n\n@Slf4j\n@RestController\n@RequestMapping("/user")\npublic class UserController {\n\n    @Autowired\n    private UserService userService;\n\n    @Value("${pattern.dateformat}")\n    private String dateformat;\n    \n    @GetMapping("now")\n    public String now(){\n        return LocalDateTime.now().format(DateTimeFormatter.ofPattern(dateformat));\n    }\n    // ...略\n}\n\n\n在页面访问，可以看到效果：\n\n\n\n\n# 配置热更新\n\n我们最终的目的，是修改nacos中的配置后，微服务中无需重启即可让配置生效，也就是配置热更新。\n\n要实现配置热更新，可以使用两种方式：\n\n\n# 方式一\n\n在@Value注入的变量所在类上添加注解@RefreshScope：\n\n\n\n\n# 方式二\n\n使用@ConfigurationProperties注解代替@Value注解。\n\n在user-service服务中，添加一个类，读取patterrn.dateformat属性：\n\npackage cn.itcast.user.config;\n\nimport lombok.Data;\nimport org.springframework.boot.context.properties.ConfigurationProperties;\nimport org.springframework.stereotype.Component;\n\n@Component\n@Data\n@ConfigurationProperties(prefix = "pattern")\npublic class PatternProperties {\n    private String dateformat;\n}\n\n\n在UserController中使用这个类代替@Value：\n\n\n\n完整代码：\n\npackage cn.itcast.user.web;\n\nimport cn.itcast.user.config.PatternProperties;\nimport cn.itcast.user.pojo.User;\nimport cn.itcast.user.service.UserService;\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.PathVariable;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RestController;\n\nimport java.time.LocalDateTime;\nimport java.time.format.DateTimeFormatter;\n\n@Slf4j\n@RestController\n@RequestMapping("/user")\npublic class UserController {\n\n    @Autowired\n    private UserService userService;\n\n    @Autowired\n    private PatternProperties patternProperties;\n\n    @GetMapping("now")\n    public String now(){\n        return LocalDateTime.now().format(DateTimeFormatter.ofPattern(patternProperties.getDateformat()));\n    }\n\n    // 略\n}\n\n\n\n# 配置共享\n\n其实微服务启动时，会去nacos读取多个配置文件，例如：\n\n * [spring.application.name]-[spring.profiles.active].yaml，例如：userservice-dev.yaml\n\n * [spring.application.name].yaml，例如：userservice.yaml\n\n而[spring.application.name].yaml不包含环境，因此可以被多个环境共享。\n\n下面我们通过案例来测试配置共享\n\n\n# 1）添加一个环境共享配置\n\n我们在nacos中添加一个userservice.yaml文件：\n\n\n\n\n# 2）在user-service中读取共享配置\n\n在user-service服务中，修改PatternProperties类，读取新添加的属性：\n\n\n\n在user-service服务中，修改UserController，添加一个方法：\n\n\n\n\n# 3）运行两个UserApplication，使用不同的profile\n\n修改UserApplication2这个启动项，改变其profile值：\n\n\n\n\n\n这样，UserApplication(8081)使用的profile是dev，UserApplication2(8082)使用的profile是test。\n\n启动UserApplication和UserApplication2\n\n访问http://localhost:8081/user/prop，结果：\n\n\n\n访问http://localhost:8082/user/prop，结果：\n\n\n\n可以看出来，不管是dev，还是test环境，都读取到了envSharedValue这个属性的值。\n\n\n# 4）配置共享的优先级\n\n当nacos、服务本地同时出现相同属性时，优先级有高低之分：\n\n',normalizedContent:'nacos除了可以做注册中心，同样可以做配置管理来使用。\n\n\n# 统一配置管理\n\n当微服务部署的实例越来越多，达到数十、数百时，逐个修改微服务配置就会让人抓狂，而且很容易出错。我们需要一种统一配置管理方案，可以集中管理所有实例的配置。\n\n\n\nnacos一方面可以将配置集中管理，另一方可以在配置变更时，及时通知微服务，实现配置的热更新。\n\n\n# 在nacos中添加配置文件\n\n如何在nacos中管理配置呢？\n\n\n\n然后在弹出的表单中，填写配置信息：\n\n\n\n注意：项目的核心配置，需要热更新的配置才有放到nacos管理的必要。基本不会变更的一些配置还是保存在微服务本地比较好。\n\n\n# 从微服务拉取配置\n\n微服务要拉取nacos中管理的配置，并且与本地的application.yml配置合并，才能完成项目启动。\n\n但如果尚未读取application.yml，又如何得知nacos地址呢？\n\n因此spring引入了一种新的配置文件：bootstrap.yaml文件，会在application.yml之前被读取，流程如下：\n\n\n\n1）引入nacos-config依赖\n\n首先，在user-service服务中，引入nacos-config的客户端依赖：\n\n\x3c!--nacos配置管理依赖--\x3e\n<dependency>\n    <groupid>com.alibaba.cloud</groupid>\n    <artifactid>spring-cloud-starter-alibaba-nacos-config</artifactid>\n</dependency>\n\n\n2）添加bootstrap.yaml\n\n然后，在user-service中添加一个bootstrap.yaml文件，内容如下：\n\nspring:\n  application:\n    name: userservice # 服务名称\n  profiles:\n    active: dev #开发环境，这里是dev \n  cloud:\n    nacos:\n      server-addr: localhost:8848 # nacos地址\n      config:\n        file-extension: yaml # 文件后缀名\n\n\n这里会根据spring.cloud.nacos.server-addr获取nacos地址，再根据\n\n${spring.application.name}-${spring.profiles.active}.${spring.cloud.nacos.config.file-extension}作为文件id，来读取配置。\n\n本例中，就是去读取userservice-dev.yaml：\n\n这里需要添加配置命名空间的配置\n\ncloud:\n    nacos:\n      server-addr: localhost:8848 #nacos服务地址\n      config:\n        file-extension: yaml #后缀名\n        #这里需要额外配置上dev环境，默认是在public空间里\n        #这是配置管理的环境，跟服务管理的环境不一样\n        namespace: b1922d8f-22a7-4b31-b978-aa5e452ec77e #dev环境\n\n\n\n\n3）读取nacos配置\n\n在user-service中的usercontroller中添加业务逻辑，读取pattern.dateformat配置：\n\n\n\n完整代码：\n\npackage cn.itcast.user.web;\n\nimport cn.itcast.user.pojo.user;\nimport cn.itcast.user.service.userservice;\nimport lombok.extern.slf4j.slf4j;\nimport org.springframework.beans.factory.annotation.autowired;\nimport org.springframework.beans.factory.annotation.value;\nimport org.springframework.web.bind.annotation.*;\n\nimport java.time.localdatetime;\nimport java.time.format.datetimeformatter;\n\n@slf4j\n@restcontroller\n@requestmapping("/user")\npublic class usercontroller {\n\n    @autowired\n    private userservice userservice;\n\n    @value("${pattern.dateformat}")\n    private string dateformat;\n    \n    @getmapping("now")\n    public string now(){\n        return localdatetime.now().format(datetimeformatter.ofpattern(dateformat));\n    }\n    // ...略\n}\n\n\n在页面访问，可以看到效果：\n\n\n\n\n# 配置热更新\n\n我们最终的目的，是修改nacos中的配置后，微服务中无需重启即可让配置生效，也就是配置热更新。\n\n要实现配置热更新，可以使用两种方式：\n\n\n# 方式一\n\n在@value注入的变量所在类上添加注解@refreshscope：\n\n\n\n\n# 方式二\n\n使用@configurationproperties注解代替@value注解。\n\n在user-service服务中，添加一个类，读取patterrn.dateformat属性：\n\npackage cn.itcast.user.config;\n\nimport lombok.data;\nimport org.springframework.boot.context.properties.configurationproperties;\nimport org.springframework.stereotype.component;\n\n@component\n@data\n@configurationproperties(prefix = "pattern")\npublic class patternproperties {\n    private string dateformat;\n}\n\n\n在usercontroller中使用这个类代替@value：\n\n\n\n完整代码：\n\npackage cn.itcast.user.web;\n\nimport cn.itcast.user.config.patternproperties;\nimport cn.itcast.user.pojo.user;\nimport cn.itcast.user.service.userservice;\nimport lombok.extern.slf4j.slf4j;\nimport org.springframework.beans.factory.annotation.autowired;\nimport org.springframework.web.bind.annotation.getmapping;\nimport org.springframework.web.bind.annotation.pathvariable;\nimport org.springframework.web.bind.annotation.requestmapping;\nimport org.springframework.web.bind.annotation.restcontroller;\n\nimport java.time.localdatetime;\nimport java.time.format.datetimeformatter;\n\n@slf4j\n@restcontroller\n@requestmapping("/user")\npublic class usercontroller {\n\n    @autowired\n    private userservice userservice;\n\n    @autowired\n    private patternproperties patternproperties;\n\n    @getmapping("now")\n    public string now(){\n        return localdatetime.now().format(datetimeformatter.ofpattern(patternproperties.getdateformat()));\n    }\n\n    // 略\n}\n\n\n\n# 配置共享\n\n其实微服务启动时，会去nacos读取多个配置文件，例如：\n\n * [spring.application.name]-[spring.profiles.active].yaml，例如：userservice-dev.yaml\n\n * [spring.application.name].yaml，例如：userservice.yaml\n\n而[spring.application.name].yaml不包含环境，因此可以被多个环境共享。\n\n下面我们通过案例来测试配置共享\n\n\n# 1）添加一个环境共享配置\n\n我们在nacos中添加一个userservice.yaml文件：\n\n\n\n\n# 2）在user-service中读取共享配置\n\n在user-service服务中，修改patternproperties类，读取新添加的属性：\n\n\n\n在user-service服务中，修改usercontroller，添加一个方法：\n\n\n\n\n# 3）运行两个userapplication，使用不同的profile\n\n修改userapplication2这个启动项，改变其profile值：\n\n\n\n\n\n这样，userapplication(8081)使用的profile是dev，userapplication2(8082)使用的profile是test。\n\n启动userapplication和userapplication2\n\n访问http://localhost:8081/user/prop，结果：\n\n\n\n访问http://localhost:8082/user/prop，结果：\n\n\n\n可以看出来，不管是dev，还是test环境，都读取到了envsharedvalue这个属性的值。\n\n\n# 4）配置共享的优先级\n\n当nacos、服务本地同时出现相同属性时，优先级有高低之分：\n\n',charsets:{cjk:!0}},{title:"RabbitMQ进阶",frontmatter:{autoSort:97,title:"RabbitMQ进阶",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/bac98f/",categories:["后端","微服务","RabbitMQ"],tags:["知识","微服务","MQ"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/30.RabbitMQ/20.RabbitMQ%E8%BF%9B%E9%98%B6.html",relativePath:"01.后端/60.微服务/30.RabbitMQ/20.RabbitMQ进阶.md",key:"v-31baad0c",path:"/pages/bac98f/",headers:[{level:2,title:"消息可靠性",slug:"消息可靠性",normalizedTitle:"消息可靠性",charIndex:32},{level:3,title:"生产者消息确认",slug:"生产者消息确认",normalizedTitle:"生产者消息确认",charIndex:270},{level:3,title:"消息持久化",slug:"消息持久化",normalizedTitle:"消息持久化",charIndex:3353},{level:3,title:"消费者消息确认",slug:"消费者消息确认",normalizedTitle:"消费者消息确认",charIndex:4239},{level:3,title:"消费失败重试机制",slug:"消费失败重试机制",normalizedTitle:"消费失败重试机制",charIndex:5551},{level:3,title:"总结",slug:"总结-2",normalizedTitle:"总结",charIndex:3182},{level:2,title:"死信交换机",slug:"死信交换机",normalizedTitle:"死信交换机",charIndex:8842},{level:3,title:"初识死信交换机",slug:"初识死信交换机",normalizedTitle:"初识死信交换机",charIndex:8852},{level:3,title:"TTL",slug:"ttl",normalizedTitle:"ttl",charIndex:10546},{level:3,title:"延迟队列",slug:"延迟队列",normalizedTitle:"延迟队列",charIndex:12807},{level:2,title:"惰性队列",slug:"惰性队列",normalizedTitle:"惰性队列",charIndex:13678},{level:3,title:"消息堆积问题",slug:"消息堆积问题",normalizedTitle:"消息堆积问题",charIndex:13687},{level:3,title:"惰性队列",slug:"惰性队列-2",normalizedTitle:"惰性队列",charIndex:13678},{level:2,title:"MQ集群",slug:"mq集群",normalizedTitle:"mq集群",charIndex:14765},{level:3,title:"集群分类",slug:"集群分类",normalizedTitle:"集群分类",charIndex:14774},{level:3,title:"普通集群",slug:"普通集群",normalizedTitle:"普通集群",charIndex:14851},{level:3,title:"镜像集群",slug:"镜像集群",normalizedTitle:"镜像集群",charIndex:14896},{level:3,title:"仲裁队列",slug:"仲裁队列",normalizedTitle:"仲裁队列",charIndex:15009},{level:2,title:"总结",slug:"总结-7",normalizedTitle:"总结",charIndex:3182}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"消息可靠性 生产者消息确认 消息持久化 消费者消息确认 消费失败重试机制 总结 死信交换机 初识死信交换机 TTL 延迟队列 惰性队列 消息堆积问题 惰性队列 MQ集群 集群分类 普通集群 镜像集群 仲裁队列 总结",content:'消息队列在使用过程中，面临着很多实际问题需要思考：\n\n\n\n\n# 消息可靠性\n\n消息从发送，到消费者接收，会经理多个过程：\n\n\n\n其中的每一步都可能导致消息丢失，常见的丢失原因包括：\n\n * 发送时丢失：\n   * 生产者发送的消息未送达exchange\n   * 消息到达exchange后未到达queue\n * MQ宕机，queue将消息丢失\n * consumer接收到消息后未消费就宕机\n\n针对这些问题，RabbitMQ分别给出了解决方案：\n\n * 生产者确认机制\n * mq持久化\n * 消费者确认机制\n * 失败重试机制\n\n\n# 生产者消息确认\n\nRabbitMQ提供了publisher confirm机制来避免消息发送到MQ过程中丢失。这种机制必须给每个消息指定一个唯一ID。消息发送到MQ以后，会返回一个结果给发送者，表示消息是否处理成功。\n\n返回结果有两种方式：\n\n * publisher-confirm，发送者确认\n   * 消息成功投递到交换机，返回ack\n   * 消息未投递到交换机，返回nack\n * publisher-return，发送者回执\n   * 消息投递到交换机了，但是没有路由到队列。返回ACK，及路由失败原因。\n\n\n\n注意：\n\n\n\n# 修改配置\n\n首先，修改publisher服务中的application.yml文件，添加下面的内容：\n\nspring:\n  rabbitmq:\n    publisher-confirm-type: correlated\n    publisher-returns: true\n    template:\n      mandatory: true\n   \n\n\n说明：\n\n * publish-confirm-type：开启publisher-confirm，这里支持两种类型：\n   * simple：同步等待confirm结果，直到超时\n   * correlated：异步回调，定义ConfirmCallback，MQ返回结果时会回调这个ConfirmCallback\n * publish-returns：开启publish-return功能，同样是基于callback机制，不过是定义ReturnCallback\n * template.mandatory：定义消息路由失败时的策略。true，则调用ReturnCallback；false：则直接丢弃消息\n\n# 定义Return回调-- 投递到队列\n\n每个RabbitTemplate只能配置一个ReturnCallback，因此需要在项目加载时配置：\n\n修改publisher服务，添加一个：\n\npackage cn.itcast.mq.config;\n\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.amqp.rabbit.core.RabbitTemplate;\nimport org.springframework.beans.BeansException;\nimport org.springframework.context.ApplicationContext;\nimport org.springframework.context.ApplicationContextAware;\nimport org.springframework.context.annotation.Configuration;\n\n@Slf4j\n@Configuration\npublic class CommonConfig implements ApplicationContextAware {\n    @Override\n    public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {\n        // 获取RabbitTemplate\n        RabbitTemplate rabbitTemplate = applicationContext.getBean(RabbitTemplate.class);\n        // 设置ReturnCallback\n        rabbitTemplate.setReturnCallback((message, replyCode, replyText, exchange, routingKey) -> {\n            // 投递失败，记录日志\n            log.info("消息发送失败，应答码{}，原因{}，交换机{}，路由键{},消息{}",\n                     replyCode, replyText, exchange, routingKey, message.toString());\n            // 如果有业务需要，可以重发消息\n        });\n    }\n}\n\n\n# 定义ConfirmCallback -- 投递到交换机\n\nConfirmCallback可以在发送消息时指定，因为每个业务处理confirm成功或失败的逻辑不一定相同。\n\n在publisher服务的cn.itcast.mq.spring.SpringAmqpTest类中，定义一个单元测试方法：\n\npublic void testSendMessage2SimpleQueue() throws InterruptedException {\n    // 1.消息体\n    String message = "hello, spring amqp!";\n    // 2.全局唯一的消息ID，需要封装到CorrelationData中\n    CorrelationData correlationData = new CorrelationData(UUID.randomUUID().toString());\n    // 3.添加callback\n    correlationData.getFuture().addCallback(\n        result -> {\n            if(result.isAck()){\n                // 3.1.ack，消息成功\n                log.debug("消息发送成功, ID:{}", correlationData.getId());\n            }else{\n                // 3.2.nack，消息失败\n                log.error("消息发送失败, ID:{}, 原因{}",correlationData.getId(), result.getReason());\n            }\n        },\n        ex -> log.error("消息发送异常, ID:{}, 原因{}",correlationData.getId(),ex.getMessage())\n    );\n    // 4.发送消息\n    rabbitTemplate.convertAndSend("task.direct", "task", message, correlationData);\n\n    // 休眠一会儿，等待ack回执\n    Thread.sleep(2000);\n}\n\n\n# 总结\n\n消息确认的情况\n\n * publish-comfirm\n   * 消息成功发送到交换机，返回ack\n   * 消息发送失败，没有到达交换机，返回nack\n   * 消息发送过程出现异常，没有收到回执\n * publish-return\n   * 消息成功发送到交换机，但是没有发送到队列， 调用 ReturnCallback\n\n\n# 消息持久化\n\nSpringAMQP 声明的交换机,队列，消息都是持久化的\n\n生产者确认可以确保消息投递到RabbitMQ的队列中，但是消息发送到RabbitMQ以后，如果突然宕机，也可能导致消息丢失。\n\n要想确保消息在RabbitMQ中安全保存，必须开启消息持久化机制。\n\n * 交换机持久化\n * 队列持久化\n * 消息持久化\n\n# 交换机持久化\n\nRabbitMQ中交换机默认是非持久化的，mq重启后就丢失。\n\nSpringAMQP中可以通过代码指定交换机持久化：\n\n@Bean\npublic DirectExchange simpleExchange(){\n    // 三个参数：交换机名称、是否持久化、当没有queue与其绑定时是否自动删除\n    return new DirectExchange("simple.direct", true, false);\n}\n\n\n事实上，默认情况下，由SpringAMQP声明的交换机都是持久化的。\n\n可以在RabbitMQ控制台看到持久化的交换机都会带上D的标示：\n\n\n\n# 队列持久化\n\nRabbitMQ中队列默认是非持久化的，mq重启后就丢失。\n\nSpringAMQP中可以通过代码指定交换机持久化：\n\n@Bean\npublic Queue simpleQueue(){\n    // 使用QueueBuilder构建队列，durable就是持久化的\n    return QueueBuilder.durable("simple.queue").build();\n}\n\n\n事实上，默认情况下，由SpringAMQP声明的队列都是持久化的。\n\n可以在RabbitMQ控制台看到持久化的队列都会带上D的标示：\n\n\n\n# 消息持久化\n\n利用SpringAMQP发送消息时，可以设置消息的属性（MessageProperties），指定delivery-mode：\n\n * 1：非持久化\n * 2：持久化\n\n用java代码指定：\n\n\n\n默认情况下，SpringAMQP发出的任何消息都是持久化的，不用特意指定。\n\n\n# 消费者消息确认\n\nRabbitMQ是阅后即焚机制，RabbitMQ确认消息被消费者消费后会立刻删除。\n\n而RabbitMQ是通过消费者回执来确认消费者是否成功处理消息的：消费者获取消息后，应该向RabbitMQ发送ACK回执，表明自己已经处理消息。\n\n消息者消费消息异常情况：\n\n * 消费者没有接收到消息 nack\n * 消费者处理消息的时候出现了异常 nack\n * 消费者接收到消息，没有来的及处理，宕机了 没有回复\n\n设想这样的场景：\n\n * 1）RabbitMQ投递消息给消费者\n * 2）消费者获取消息后，返回ACK给RabbitMQ\n * 3）RabbitMQ删除消息\n * 4）消费者宕机，消息尚未处理\n\n这样，消息就丢失了。因此消费者返回ACK的时机非常重要。\n\n而SpringAMQP则允许配置三种确认模式：\n\n•manual：手动ack，需要在业务代码结束后，调用api发送ack。\n\n•auto：自动ack，由spring监测listener代码是否出现异常，没有异常则返回ack；抛出异常则返回nack\n\n•none：关闭ack，MQ假定消费者获取消息后会成功处理，因此消息投递后立即被删除\n\n由此可知：\n\n * none模式下，消息投递是不可靠的，可能丢失\n * auto模式类似事务机制，出现异常时返回nack，消息回滚到mq；没有异常，返回ack\n * manual：自己根据业务情况，判断什么时候该ack\n\n一般，我们都是使用默认的auto即可。\n\n# 演示none模式\n\n修改consumer服务的application.yml文件，添加下面内容：\n\nspring:\n  rabbitmq:\n    listener:\n      simple:\n        acknowledge-mode: none ## 关闭ack\n\n\n修改consumer服务的SpringRabbitListener类中的方法，模拟一个消息处理异常：\n\n@RabbitListener(queues = "simple.queue")\npublic void listenSimpleQueue(String msg) {\n    log.info("消费者接收到simple.queue的消息：【{}】", msg);\n    // 模拟异常\n    System.out.println(1 / 0);\n    log.debug("消息处理完成！");\n}\n\n\n测试可以发现，当消息处理抛异常时，消息依然被RabbitMQ删除了。\n\n# 演示auto模式\n\n再次把确认机制修改为auto:\n\nspring:\n  rabbitmq:\n    listener:\n      simple:\n        acknowledge-mode: auto ## 关闭ack\n\n\n在异常位置打断点，再次发送消息，程序卡在断点时，可以发现此时消息状态为unack（未确定状态）：\n\n\n\n抛出异常后，因为Spring会自动返回nack，所以消息恢复至Ready状态，并且没有被RabbitMQ删除：\n\n\n\n\n# 消费失败重试机制\n\n当消费者出现异常后，消息会不断requeue（重入队）到队列，再重新发送给消费者，然后再次异常，再次requeue，无限循环，导致mq的消息处理飙升，带来不必要的压力：\n\n\n\n怎么办呢？\n\n# 本地重试\n\n我们可以利用Spring的retry机制，在消费者出现异常时利用本地重试，而不是无限制的requeue到mq队列。\n\n修改consumer服务的application.yml文件，添加内容：\n\nspring:\n  rabbitmq:\n    listener:\n      simple:\n        retry:\n          enabled: true ## 开启消费者失败重试\n          initial-interval: 1000 ## 初识的失败等待时长为1秒\n          multiplier: 1 ## 失败的等待时长倍数，下次等待时长 = multiplier * last-interval\n          max-attempts: 3 ## 最大重试次数\n          stateless: true ## true无状态；false有状态。如果业务中包含事务，这里改为false\n\n\n重启consumer服务，重复之前的测试。可以发现：\n\n * 在重试3次后，SpringAMQP会抛出异常AmqpRejectAndDontRequeueException，说明本地重试触发了\n * 查看RabbitMQ控制台，发现消息被删除了，说明最后SpringAMQP返回的是ack，mq删除消息了\n\n结论：\n\n * 开启本地重试时，消息处理过程中抛出异常，不会requeue到队列，而是在消费者本地重试\n * 重试达到最大次数后，Spring会返回ack(reject)，消息会被丢弃\n\n# 失败策略\n\n在之前的测试中，达到最大重试次数后，消息会被丢弃，这是由Spring内部机制决定的。\n\n在开启重试模式后，重试次数耗尽，如果消息依然失败，则需要有MessageRecovery接口来处理，它包含三种不同的实现：\n\n * RejectAndDontRequeueRecoverer：重试耗尽后，直接reject，丢弃消息。默认就是这种方式\n\n * ImmediateRequeueMessageRecoverer：重试耗尽后，返回nack，消息重新入队\n\n * RepublishMessageRecoverer：重试耗尽后，将失败消息投递到指定的交换机——error.direct\n\n比较优雅的一种处理方案是RepublishMessageRecoverer，失败后将消息投递到一个指定的，专门存放异常消息的队列--error.direct，后续由人工集中处理。\n\n\n\n1）在consumer服务中定义处理失败消息的交换机和队列\n\n@Bean\npublic DirectExchange errorMessageExchange(){\n    return new DirectExchange("error.direct");\n}\n@Bean\npublic Queue errorQueue(){\n    return new Queue("error.queue", true);\n}\n@Bean\npublic Binding errorBinding(Queue errorQueue, DirectExchange errorMessageExchange){\n    return BindingBuilder.bind(errorQueue).to(errorMessageExchange).with("error");\n}\n\n\n2）定义一个RepublishMessageRecoverer，关联队列和交换机\n\n@Bean\npublic MessageRecoverer republishMessageRecoverer(RabbitTemplate rabbitTemplate){\n    return new RepublishMessageRecoverer(rabbitTemplate, "error.direct", "error");\n}\n\n\n完整代码：\n\npackage cn.itcast.mq.config;\n\nimport org.springframework.amqp.core.Binding;\nimport org.springframework.amqp.core.BindingBuilder;\nimport org.springframework.amqp.core.DirectExchange;\nimport org.springframework.amqp.core.Queue;\nimport org.springframework.amqp.rabbit.core.RabbitTemplate;\nimport org.springframework.amqp.rabbit.retry.MessageRecoverer;\nimport org.springframework.amqp.rabbit.retry.RepublishMessageRecoverer;\nimport org.springframework.context.annotation.Bean;\n\n@Configuration\npublic class ErrorMessageConfig {\n    @Bean\n    public DirectExchange errorMessageExchange(){\n        return new DirectExchange("error.direct");\n    }\n    @Bean\n    public Queue errorQueue(){\n        return new Queue("error.queue", true);\n    }\n    @Bean\n    public Binding errorBinding(Queue errorQueue, DirectExchange errorMessageExchange){\n        return BindingBuilder.bind(errorQueue).to(errorMessageExchange).with("error");\n    }\n    \n\t//TODO 重写 消息者消费消息失败 的 本地重试失败策略\n    // Ctrl + H 查看类结构层次\n    @Bean\n    public MessageRecoverer republishMessageRecoverer(RabbitTemplate rabbitTemplate){\n        return new RepublishMessageRecoverer(rabbitTemplate, "error.direct", "error");\n    }\n}\n\n\n\n# 总结\n\n如何确保RabbitMQ消息的可靠性？(3个方面)\n\n * 生产者方面\n   * 开启生产者确认机制，确保生产者的消息能到达队列\n * 持久化方面（mq）\n   * 开启持久化功能，确保消息未消费前在队列中不会丢失\n * 消费者方面\n   * 开启消费者确认机制为auto，由spring确认消息处理成功后完成ack\n   * 开启消费者失败重试机制，\n     * 默认是 多次失败后，直接将消息丢弃。RejectAndDontRequeueRecoverer\n     * 设置MessageRecoverer 策略为RepublishMessageRecoverer，多次重试失败后将消息投递到异常交换机，交由人工处理\n\n\n# 死信交换机\n\n\n# 初识死信交换机\n\n# 什么是死信交换机\n\n什么是死信？\n\n当一个队列中的消息满足下列情况之一时，可以成为死信（dead letter）：\n\n * 消费者使用basic.reject或 basic.nack声明消费失败，并且消息的requeue参数设置为false\n * 消息是一个过期消息，超时无人消费\n * 要投递的队列消息满了，无法投递\n\n如果这个包含死信的队列配置了dead-letter-exchange属性，指定了一个交换机，那么队列中的死信就会投递到这个交换机中，而这个交换机称为死信交换机（Dead Letter Exchange，检查DLX）。\n\n如图，一个消息被消费者拒绝了，变成了死信：\n\n\n\n因为simple.queue绑定了死信交换机 dl.direct，因此死信会投递给这个交换机：\n\n\n\n如果这个死信交换机也绑定了一个队列，则消息最终会进入这个存放死信的队列：\n\n\n\n另外，队列将死信投递给死信交换机时，必须知道两个信息：\n\n * 死信交换机名称 dead-letter-exchange\n * 死信交换机与死信队列绑定的RoutingKeydead-letter-routing-key\n\n这样才能确保投递的消息能到达死信交换机，并且正确的路由到死信队列。\n\n\n\n# 异常交换机 与 死信交换机对比\n\n>  1. 前者是由 消费者投递给异常交换机，后者是由队列投递给死信交换机\n\n\n\n# 利用死信交换机接收死信（拓展）\n\n在失败重试策略中，默认的RejectAndDontRequeueRecoverer会在本地重试次数耗尽后，发送reject给RabbitMQ，消息变成死信，被丢弃。\n\n我们可以给simple.queue添加一个死信交换机，给死信交换机绑定一个队列。这样消息变成死信后也不会丢弃，而是最终投递到死信交换机，路由到与死信交换机绑定的队列。\n\n\n\n我们在consumer服务中，定义一组死信交换机、死信队列：\n\n//MessageRecoverer的模式采用默认的  RejectAndDontRequeueRecoverer模式，本地重试失败后，丢弃消息，到死信交换机\n\n// 声明普通的 simple.queue队列，并且为其指定死信交换机：dl.direct\n@Bean\npublic Queue simpleQueue2(){\n    return QueueBuilder.durable("simple.queue") // 指定队列名称，并持久化\n        .deadLetterExchange("dl.direct") // 指定死信交换机\n        .deadLetterRoutingKey("dl") //指定死信交换机  RoutingKey\n        .build();\n}\n// 声明死信交换机 dl.direct\n@Bean\npublic DirectExchange dlExchange(){\n    return new DirectExchange("dl.direct", true, false);\n}\n// 声明存储死信的队列 dl.queue\n@Bean\npublic Queue dlQueue(){\n    return new Queue("dl.queue", true);\n}\n// 将死信队列 与 死信交换机绑定\n@Bean\npublic Binding dlBinding(){\n    return BindingBuilder.bind(dlQueue()).to(dlExchange()).with("dl");\n}\n\n\n# 总结\n\n什么样的消息会成为死信？\n\n * 消息被消费者reject或者返回nack\n * 消息超时未消费\n * 队列满了\n\n死信交换机的使用场景是什么？\n\n * 如果队列绑定了死信交换机，死信会投递到死信交换机；\n * 可以利用死信交换机收集所有消费者处理失败的消息（死信），交由人工处理，进一步提高消息队列的可靠性。\n\n\n# TTL\n\n一个队列中的消息如果超时未消费，则会变为死信，超时分为两种情况：\n\n * 消息所在的队列设置了超时时间\n * 消息本身设置了超时时间\n * 当两者都存在时间时，取较短的一个\n\n\n\n# 接收超时死信的死信交换机\n\n原理\n\n> TTL的交换机不设置消费者监听，让消息在队列中超时，自动投递到死信交换机中,让消费者监听死信交换机，这样消费者收到生产者的消息就实现了延时。\n\n在consumer服务的SpringRabbitListener中，定义一个新的消费者，并且声明 死信交换机、死信队列：\n\n@RabbitListener(bindings = @QueueBinding(\n    value = @Queue(name = "dl.ttl.queue", durable = "true"),\n    exchange = @Exchange(name = "dl.ttl.direct"),\n    key = "ttl"\n))\npublic void listenDlQueue(String msg){\n    log.info("接收到 dl.ttl.queue的延迟消息：{}", msg);\n}\n\n\n# 声明一个队列，并且指定TTL\n\n要给队列设置超时时间，需要在声明队列时配置x-message-ttl属性：\n\n@Bean\npublic Queue ttlQueue(){\n    return QueueBuilder.durable("ttl.queue") // 指定队列名称，并持久化\n        .ttl(10000) // 设置队列的超时时间，10秒\n        .deadLetterExchange("dl.ttl.direct") // 指定死信交换机\n        .deadLetterRoutingKey("dl") //指定死信交换机  RoutingKey\n        .build();\n}\n\n\n注意，这个队列设定了死信交换机为dl.ttl.direct\n\n声明交换机，将ttl与交换机绑定：\n\n@Bean\npublic DirectExchange ttlExchange(){\n    return new DirectExchange("ttl.direct");\n}\n@Bean\npublic Binding ttlBinding(){\n    return BindingBuilder.bind(ttlQueue()).to(ttlExchange()).with("ttl");\n}\n\n\n发送消息，但是不要指定TTL：\n\n@Test\npublic void testTTLQueue() {\n    // 创建消息\n    String message = "hello, ttl queue";\n    // 消息ID，需要封装到CorrelationData中\n    CorrelationData correlationData = new CorrelationData(UUID.randomUUID().toString());\n    // 发送消息\n    rabbitTemplate.convertAndSend("ttl.direct", "ttl", message, correlationData);\n    // 记录日志\n    log.debug("发送消息成功");\n}\n\n\n发送消息的日志：\n\n\n\n查看下接收消息的日志：\n\n\n\n因为队列的TTL值是10000ms，也就是10秒。可以看到消息发送与接收之间的时差刚好是10秒。\n\n# 发送消息时，设定TTL\n\n在发送消息时，也可以指定TTL：\n\n@Test\npublic void testTTLMsg() {\n    // 创建消息\n    Message message = MessageBuilder\n        .withBody("hello, ttl message".getBytes(StandardCharsets.UTF_8))\n        .setExpiration("5000")\n        .build();\n    // 消息ID，需要封装到CorrelationData中\n    CorrelationData correlationData = new CorrelationData(UUID.randomUUID().toString());\n    // 发送消息\n    rabbitTemplate.convertAndSend("ttl.direct", "ttl", message, correlationData);\n    log.debug("发送消息成功");\n}\n\n\n查看发送消息日志：\n\n\n\n接收消息日志：\n\n\n\n这次，发送与接收的延迟只有5秒。说明当队列、消息都设置了TTL时，任意一个到期就会成为死信。\n\n# 总结\n\n消息超时的两种方式是？\n\n * 给队列设置ttl属性，进入队列后超过ttl时间的消息变为死信\n * 给消息设置ttl属性，队列接收到消息超过ttl时间后变为死信\n\n如何实现发送一个消息20秒后消费者才收到消息？\n\n * 给消息的目标队列指定死信交换机\n * 将消费者监听的队列绑定到死信交换机\n * 发送消息时给消息设置超时时间为20秒\n\n\n# 延迟队列\n\n利用TTL结合死信交换机，我们实现了消息发出后，消费者延迟收到消息的效果。这种消息模式就称为延迟队列（Delay Queue）模式。\n\n延迟队列的使用场景包括：\n\n * 延迟发送短信\n * 用户下单，如果用户在15 分钟内未支付，则自动取消\n * 预约工作会议，20分钟后自动通知所有参会人员\n\n因为延迟队列的需求非常多，所以RabbitMQ的官方也推出了一个插件，原生支持延迟队列效果。\n\n这个插件就是DelayExchange插件。参考RabbitMQ的插件列表页面：https://www.rabbitmq.com/community-plugins.html\n\n\n\n使用方式可以参考官网地址：https://blog.rabbitmq.com/posts/2015/04/scheduling-messages-with-rabbitmq\n\n# DelayExchange原理\n\nDelayExchange需要将一个交换机声明为delayed类型。当我们发送消息到delayExchange时，流程如下：\n\n * 接收消息\n * 判断消息是否具备x-delay属性\n * 如果有x-delay属性，说明是延迟消息，持久化到硬盘，读取x-delay值，作为延迟时间\n * 返回routing not found结果给消息发送者\n * x-delay时间到期后，重新投递消息到指定队列\n\n# 使用DelayExchange\n\n插件的使用也非常简单：声明一个交换机，交换机的类型可以是任意类型，只需要设定delayed属性为true即可，然后声明队列与其绑定即可。\n\n# 1）声明DelayExchange交换机\n\n基于注解方式（推荐）：\n\n\n\n也可以基于@Bean的方式：\n\n\n\n# 2）发送消息\n\n发送消息时，一定要携带x-delay属性，指定延迟的时间：\n\n\n\n# 总结\n\n延迟队列插件的使用步骤包括哪些？\n\n•声明一个交换机，添加delayed属性为true\n\n•发送消息时，添加x-delay头，值为超时时间\n\n\n# 惰性队列\n\n\n# 消息堆积问题\n\n当生产者发送消息的速度超过了消费者处理消息的速度，就会导致队列中的消息堆积，直到队列存储消息达到上限。之后发送的消息就会成为死信，可能会被丢弃，这就是消息堆积问题。\n\n\n\n解决消息堆积有三种思路：\n\n * 增加更多消费者，提高消费速度。也就是我们之前说的work queue模式\n * 在消费者内开启线程池加快消息处理速度\n * 扩大队列容积，提高堆积上限\n\n要提升队列容积，把消息保存在内存中显然是不行的。\n\n\n# 惰性队列\n\n从RabbitMQ的3.6.0版本开始，就增加了Lazy Queues的概念，也就是惰性队列。惰性队列的特征如下：\n\n * 接收到消息后直接存入磁盘而非内存\n * 消费者要消费消息时才会从磁盘中读取并加载到内存\n * 支持数百万条的消息存储\n\n# 基于命令行设置lazy-queue\n\n而要设置一个队列为惰性队列，只需要在声明队列时，指定x-queue-mode属性为lazy即可。可以通过命令行将一个运行中的队列修改为惰性队列：\n\nrabbitmqctl set_policy Lazy "^lazy-queue$" \'{"queue-mode":"lazy"}\' --apply-to queues  \n\n\n命令解读：\n\n * rabbitmqctl ：RabbitMQ的命令行工具\n * set_policy ：添加一个策略\n * Lazy ：策略名称，可以自定义\n * "^lazy-queue$" ：用正则表达式匹配队列的名字\n * \'{"queue-mode":"lazy"}\' ：设置队列模式为lazy模式\n * --apply-to queues：策略的作用对象，是所有的队列\n\n# 基于@Bean声明lazy-queue\n\n\n\n# 基于@RabbitListener声明LazyQueue\n\n\n\n# 总结\n\n消息堆积问题的解决方案？\n\n * 队列上绑定多个消费者，提高消费速度\n * 给消费者开启线程池，提高消费速度\n * 使用惰性队列，可以再mq中保存更多消息\n\n惰性队列的优点有哪些？\n\n * 基于磁盘存储，消息上限高\n * 没有间歇性的page-out，性能比较稳定\n * Lazy 队列\n   * 消息直接存到磁盘，当消费者要消费时，才将消息读到磁盘\n\n\n\n * 普通队列\n   \n   * 先往内存中存，内存快满了后，将内存中的数据存到磁盘中，内存在重新接收数据\n   \n   \n\n惰性队列的缺点有哪些？\n\n * 基于磁盘存储，消息时效性会降低\n * 性能受限于磁盘的IO\n\n\n# MQ集群\n\n\n# 集群分类\n\nRabbitMQ的是基于Erlang语言编写，而Erlang又是一个面向并发的语言，天然支持集群模式。RabbitMQ的集群有两种模式：\n\n•普通集群：是一种分布式集群，将队列分散到集群的各个节点，从而提高整个集群的并发能力。\n\n•镜像集群：是一种主从集群，普通集群的基础上，添加了主从备份功能，提高集群的数据可用性。\n\n镜像集群虽然支持主从，但主从同步并不是强一致的，某些情况下可能有数据丢失的风险。因此在RabbitMQ的3.8版本以后，推出了新的功能：仲裁队列来代替镜像集群，底层采用Raft协议确保主从的数据一致性。\n\n\n# 普通集群\n\n# 集群结构和特征\n\n普通集群，或者叫标准集群（classic cluster），具备下列特征：\n\n * 会在集群的各个节点间共享部分数据，包括：交换机、队列元信息。不包含队列中的消息。\n   \n   * 元信息就是，能看到队列叫啥，在哪等信息\n\n * 当访问集群某节点时，如果队列不在该节点，会从数据所在节点传递到当前节点并返回\n\n * 队列所在节点宕机，队列中的消息就会丢失\n\n结构如图：\n\n\n\n\n# 镜像集群\n\n# 集群结构和特征\n\n镜像集群：本质是主从模式，具备下面的特征：\n\n类似于 es\n\n * 交换机、队列、队列中的消息会在各个mq的镜像节点之间同步备份。\n * 创建队列的节点被称为该队列的主节点，备份到的其它节点叫做该队列的镜像节点。\n * 一个队列的主节点可能是另一个队列的镜像节点\n * 所有操作都是主节点完成，然后同步给镜像节点\n * 主宕机后，镜像节点会替代成新的主\n\n结构如图：\n\n\n\n\n# 仲裁队列\n\n# 集群特征\n\n仲裁队列：仲裁队列是3.8版本以后才有的新功能，用来替代镜像队列，具备下列特征：\n\n * 与镜像队列一样，都是主从模式，支持主从数据同步\n * 使用非常简单，没有复杂的配置\n * 主从同步基于Raft协议，强一致\n\n# Java代码创建仲裁队列\n\n这里除了创建队列时不一样，其他写和监听都一样的\n\n@Bean\npublic Queue quorumQueue() {\n    return QueueBuilder\n        .durable("quorum.queue") // 持久化\n        .quorum() // 仲裁队列\n        .build();\n}\n\n\n# SpringAMQP连接MQ集群\n\n注意，这里用address来代替host、port方式\n\nspring:\n  rabbitmq:\n    addresses: 192.168.159.100:8071, 192.168.159.100:8072, 192.168.159.100:8073\n    username: itcast\n    password: 123321\n    virtual-host: /\n\n\n\n# 总结\n\n * 四个问题\n   * 消息可靠性\n     * 生产者消息确认\n       * ReturnCallback\n       * ConfirmCallback\n     * 消息持久化存储 - durable\n       * 默认队列，交换机，消息都是持久化的\n     * 消费者消息确认\n       * 本地重试\n       * 失败策略\n         * 异常交换机，异常队列\n   * 延时消息\n     * 死信交换机\n       * 死信处理\n       * 与异常交换机的区别\n     * 延迟队列\n       * 利用死信交换机完成TTL\n     * 延迟交换机\n       * 安装插件，让交换机临时存储数据，达到延时转发的目的\n   * 消息堆积\n     * 解决方法\n     * 惰性队列\n       * 特性\n       * 与普通队列区别\n   * 高可用\n     * 普通集群\n     * 镜像集群\n     * 仲裁队列',normalizedContent:'消息队列在使用过程中，面临着很多实际问题需要思考：\n\n\n\n\n# 消息可靠性\n\n消息从发送，到消费者接收，会经理多个过程：\n\n\n\n其中的每一步都可能导致消息丢失，常见的丢失原因包括：\n\n * 发送时丢失：\n   * 生产者发送的消息未送达exchange\n   * 消息到达exchange后未到达queue\n * mq宕机，queue将消息丢失\n * consumer接收到消息后未消费就宕机\n\n针对这些问题，rabbitmq分别给出了解决方案：\n\n * 生产者确认机制\n * mq持久化\n * 消费者确认机制\n * 失败重试机制\n\n\n# 生产者消息确认\n\nrabbitmq提供了publisher confirm机制来避免消息发送到mq过程中丢失。这种机制必须给每个消息指定一个唯一id。消息发送到mq以后，会返回一个结果给发送者，表示消息是否处理成功。\n\n返回结果有两种方式：\n\n * publisher-confirm，发送者确认\n   * 消息成功投递到交换机，返回ack\n   * 消息未投递到交换机，返回nack\n * publisher-return，发送者回执\n   * 消息投递到交换机了，但是没有路由到队列。返回ack，及路由失败原因。\n\n\n\n注意：\n\n\n\n# 修改配置\n\n首先，修改publisher服务中的application.yml文件，添加下面的内容：\n\nspring:\n  rabbitmq:\n    publisher-confirm-type: correlated\n    publisher-returns: true\n    template:\n      mandatory: true\n   \n\n\n说明：\n\n * publish-confirm-type：开启publisher-confirm，这里支持两种类型：\n   * simple：同步等待confirm结果，直到超时\n   * correlated：异步回调，定义confirmcallback，mq返回结果时会回调这个confirmcallback\n * publish-returns：开启publish-return功能，同样是基于callback机制，不过是定义returncallback\n * template.mandatory：定义消息路由失败时的策略。true，则调用returncallback；false：则直接丢弃消息\n\n# 定义return回调-- 投递到队列\n\n每个rabbittemplate只能配置一个returncallback，因此需要在项目加载时配置：\n\n修改publisher服务，添加一个：\n\npackage cn.itcast.mq.config;\n\nimport lombok.extern.slf4j.slf4j;\nimport org.springframework.amqp.rabbit.core.rabbittemplate;\nimport org.springframework.beans.beansexception;\nimport org.springframework.context.applicationcontext;\nimport org.springframework.context.applicationcontextaware;\nimport org.springframework.context.annotation.configuration;\n\n@slf4j\n@configuration\npublic class commonconfig implements applicationcontextaware {\n    @override\n    public void setapplicationcontext(applicationcontext applicationcontext) throws beansexception {\n        // 获取rabbittemplate\n        rabbittemplate rabbittemplate = applicationcontext.getbean(rabbittemplate.class);\n        // 设置returncallback\n        rabbittemplate.setreturncallback((message, replycode, replytext, exchange, routingkey) -> {\n            // 投递失败，记录日志\n            log.info("消息发送失败，应答码{}，原因{}，交换机{}，路由键{},消息{}",\n                     replycode, replytext, exchange, routingkey, message.tostring());\n            // 如果有业务需要，可以重发消息\n        });\n    }\n}\n\n\n# 定义confirmcallback -- 投递到交换机\n\nconfirmcallback可以在发送消息时指定，因为每个业务处理confirm成功或失败的逻辑不一定相同。\n\n在publisher服务的cn.itcast.mq.spring.springamqptest类中，定义一个单元测试方法：\n\npublic void testsendmessage2simplequeue() throws interruptedexception {\n    // 1.消息体\n    string message = "hello, spring amqp!";\n    // 2.全局唯一的消息id，需要封装到correlationdata中\n    correlationdata correlationdata = new correlationdata(uuid.randomuuid().tostring());\n    // 3.添加callback\n    correlationdata.getfuture().addcallback(\n        result -> {\n            if(result.isack()){\n                // 3.1.ack，消息成功\n                log.debug("消息发送成功, id:{}", correlationdata.getid());\n            }else{\n                // 3.2.nack，消息失败\n                log.error("消息发送失败, id:{}, 原因{}",correlationdata.getid(), result.getreason());\n            }\n        },\n        ex -> log.error("消息发送异常, id:{}, 原因{}",correlationdata.getid(),ex.getmessage())\n    );\n    // 4.发送消息\n    rabbittemplate.convertandsend("task.direct", "task", message, correlationdata);\n\n    // 休眠一会儿，等待ack回执\n    thread.sleep(2000);\n}\n\n\n# 总结\n\n消息确认的情况\n\n * publish-comfirm\n   * 消息成功发送到交换机，返回ack\n   * 消息发送失败，没有到达交换机，返回nack\n   * 消息发送过程出现异常，没有收到回执\n * publish-return\n   * 消息成功发送到交换机，但是没有发送到队列， 调用 returncallback\n\n\n# 消息持久化\n\nspringamqp 声明的交换机,队列，消息都是持久化的\n\n生产者确认可以确保消息投递到rabbitmq的队列中，但是消息发送到rabbitmq以后，如果突然宕机，也可能导致消息丢失。\n\n要想确保消息在rabbitmq中安全保存，必须开启消息持久化机制。\n\n * 交换机持久化\n * 队列持久化\n * 消息持久化\n\n# 交换机持久化\n\nrabbitmq中交换机默认是非持久化的，mq重启后就丢失。\n\nspringamqp中可以通过代码指定交换机持久化：\n\n@bean\npublic directexchange simpleexchange(){\n    // 三个参数：交换机名称、是否持久化、当没有queue与其绑定时是否自动删除\n    return new directexchange("simple.direct", true, false);\n}\n\n\n事实上，默认情况下，由springamqp声明的交换机都是持久化的。\n\n可以在rabbitmq控制台看到持久化的交换机都会带上d的标示：\n\n\n\n# 队列持久化\n\nrabbitmq中队列默认是非持久化的，mq重启后就丢失。\n\nspringamqp中可以通过代码指定交换机持久化：\n\n@bean\npublic queue simplequeue(){\n    // 使用queuebuilder构建队列，durable就是持久化的\n    return queuebuilder.durable("simple.queue").build();\n}\n\n\n事实上，默认情况下，由springamqp声明的队列都是持久化的。\n\n可以在rabbitmq控制台看到持久化的队列都会带上d的标示：\n\n\n\n# 消息持久化\n\n利用springamqp发送消息时，可以设置消息的属性（messageproperties），指定delivery-mode：\n\n * 1：非持久化\n * 2：持久化\n\n用java代码指定：\n\n\n\n默认情况下，springamqp发出的任何消息都是持久化的，不用特意指定。\n\n\n# 消费者消息确认\n\nrabbitmq是阅后即焚机制，rabbitmq确认消息被消费者消费后会立刻删除。\n\n而rabbitmq是通过消费者回执来确认消费者是否成功处理消息的：消费者获取消息后，应该向rabbitmq发送ack回执，表明自己已经处理消息。\n\n消息者消费消息异常情况：\n\n * 消费者没有接收到消息 nack\n * 消费者处理消息的时候出现了异常 nack\n * 消费者接收到消息，没有来的及处理，宕机了 没有回复\n\n设想这样的场景：\n\n * 1）rabbitmq投递消息给消费者\n * 2）消费者获取消息后，返回ack给rabbitmq\n * 3）rabbitmq删除消息\n * 4）消费者宕机，消息尚未处理\n\n这样，消息就丢失了。因此消费者返回ack的时机非常重要。\n\n而springamqp则允许配置三种确认模式：\n\n•manual：手动ack，需要在业务代码结束后，调用api发送ack。\n\n•auto：自动ack，由spring监测listener代码是否出现异常，没有异常则返回ack；抛出异常则返回nack\n\n•none：关闭ack，mq假定消费者获取消息后会成功处理，因此消息投递后立即被删除\n\n由此可知：\n\n * none模式下，消息投递是不可靠的，可能丢失\n * auto模式类似事务机制，出现异常时返回nack，消息回滚到mq；没有异常，返回ack\n * manual：自己根据业务情况，判断什么时候该ack\n\n一般，我们都是使用默认的auto即可。\n\n# 演示none模式\n\n修改consumer服务的application.yml文件，添加下面内容：\n\nspring:\n  rabbitmq:\n    listener:\n      simple:\n        acknowledge-mode: none ## 关闭ack\n\n\n修改consumer服务的springrabbitlistener类中的方法，模拟一个消息处理异常：\n\n@rabbitlistener(queues = "simple.queue")\npublic void listensimplequeue(string msg) {\n    log.info("消费者接收到simple.queue的消息：【{}】", msg);\n    // 模拟异常\n    system.out.println(1 / 0);\n    log.debug("消息处理完成！");\n}\n\n\n测试可以发现，当消息处理抛异常时，消息依然被rabbitmq删除了。\n\n# 演示auto模式\n\n再次把确认机制修改为auto:\n\nspring:\n  rabbitmq:\n    listener:\n      simple:\n        acknowledge-mode: auto ## 关闭ack\n\n\n在异常位置打断点，再次发送消息，程序卡在断点时，可以发现此时消息状态为unack（未确定状态）：\n\n\n\n抛出异常后，因为spring会自动返回nack，所以消息恢复至ready状态，并且没有被rabbitmq删除：\n\n\n\n\n# 消费失败重试机制\n\n当消费者出现异常后，消息会不断requeue（重入队）到队列，再重新发送给消费者，然后再次异常，再次requeue，无限循环，导致mq的消息处理飙升，带来不必要的压力：\n\n\n\n怎么办呢？\n\n# 本地重试\n\n我们可以利用spring的retry机制，在消费者出现异常时利用本地重试，而不是无限制的requeue到mq队列。\n\n修改consumer服务的application.yml文件，添加内容：\n\nspring:\n  rabbitmq:\n    listener:\n      simple:\n        retry:\n          enabled: true ## 开启消费者失败重试\n          initial-interval: 1000 ## 初识的失败等待时长为1秒\n          multiplier: 1 ## 失败的等待时长倍数，下次等待时长 = multiplier * last-interval\n          max-attempts: 3 ## 最大重试次数\n          stateless: true ## true无状态；false有状态。如果业务中包含事务，这里改为false\n\n\n重启consumer服务，重复之前的测试。可以发现：\n\n * 在重试3次后，springamqp会抛出异常amqprejectanddontrequeueexception，说明本地重试触发了\n * 查看rabbitmq控制台，发现消息被删除了，说明最后springamqp返回的是ack，mq删除消息了\n\n结论：\n\n * 开启本地重试时，消息处理过程中抛出异常，不会requeue到队列，而是在消费者本地重试\n * 重试达到最大次数后，spring会返回ack(reject)，消息会被丢弃\n\n# 失败策略\n\n在之前的测试中，达到最大重试次数后，消息会被丢弃，这是由spring内部机制决定的。\n\n在开启重试模式后，重试次数耗尽，如果消息依然失败，则需要有messagerecovery接口来处理，它包含三种不同的实现：\n\n * rejectanddontrequeuerecoverer：重试耗尽后，直接reject，丢弃消息。默认就是这种方式\n\n * immediaterequeuemessagerecoverer：重试耗尽后，返回nack，消息重新入队\n\n * republishmessagerecoverer：重试耗尽后，将失败消息投递到指定的交换机——error.direct\n\n比较优雅的一种处理方案是republishmessagerecoverer，失败后将消息投递到一个指定的，专门存放异常消息的队列--error.direct，后续由人工集中处理。\n\n\n\n1）在consumer服务中定义处理失败消息的交换机和队列\n\n@bean\npublic directexchange errormessageexchange(){\n    return new directexchange("error.direct");\n}\n@bean\npublic queue errorqueue(){\n    return new queue("error.queue", true);\n}\n@bean\npublic binding errorbinding(queue errorqueue, directexchange errormessageexchange){\n    return bindingbuilder.bind(errorqueue).to(errormessageexchange).with("error");\n}\n\n\n2）定义一个republishmessagerecoverer，关联队列和交换机\n\n@bean\npublic messagerecoverer republishmessagerecoverer(rabbittemplate rabbittemplate){\n    return new republishmessagerecoverer(rabbittemplate, "error.direct", "error");\n}\n\n\n完整代码：\n\npackage cn.itcast.mq.config;\n\nimport org.springframework.amqp.core.binding;\nimport org.springframework.amqp.core.bindingbuilder;\nimport org.springframework.amqp.core.directexchange;\nimport org.springframework.amqp.core.queue;\nimport org.springframework.amqp.rabbit.core.rabbittemplate;\nimport org.springframework.amqp.rabbit.retry.messagerecoverer;\nimport org.springframework.amqp.rabbit.retry.republishmessagerecoverer;\nimport org.springframework.context.annotation.bean;\n\n@configuration\npublic class errormessageconfig {\n    @bean\n    public directexchange errormessageexchange(){\n        return new directexchange("error.direct");\n    }\n    @bean\n    public queue errorqueue(){\n        return new queue("error.queue", true);\n    }\n    @bean\n    public binding errorbinding(queue errorqueue, directexchange errormessageexchange){\n        return bindingbuilder.bind(errorqueue).to(errormessageexchange).with("error");\n    }\n    \n\t//todo 重写 消息者消费消息失败 的 本地重试失败策略\n    // ctrl + h 查看类结构层次\n    @bean\n    public messagerecoverer republishmessagerecoverer(rabbittemplate rabbittemplate){\n        return new republishmessagerecoverer(rabbittemplate, "error.direct", "error");\n    }\n}\n\n\n\n# 总结\n\n如何确保rabbitmq消息的可靠性？(3个方面)\n\n * 生产者方面\n   * 开启生产者确认机制，确保生产者的消息能到达队列\n * 持久化方面（mq）\n   * 开启持久化功能，确保消息未消费前在队列中不会丢失\n * 消费者方面\n   * 开启消费者确认机制为auto，由spring确认消息处理成功后完成ack\n   * 开启消费者失败重试机制，\n     * 默认是 多次失败后，直接将消息丢弃。rejectanddontrequeuerecoverer\n     * 设置messagerecoverer 策略为republishmessagerecoverer，多次重试失败后将消息投递到异常交换机，交由人工处理\n\n\n# 死信交换机\n\n\n# 初识死信交换机\n\n# 什么是死信交换机\n\n什么是死信？\n\n当一个队列中的消息满足下列情况之一时，可以成为死信（dead letter）：\n\n * 消费者使用basic.reject或 basic.nack声明消费失败，并且消息的requeue参数设置为false\n * 消息是一个过期消息，超时无人消费\n * 要投递的队列消息满了，无法投递\n\n如果这个包含死信的队列配置了dead-letter-exchange属性，指定了一个交换机，那么队列中的死信就会投递到这个交换机中，而这个交换机称为死信交换机（dead letter exchange，检查dlx）。\n\n如图，一个消息被消费者拒绝了，变成了死信：\n\n\n\n因为simple.queue绑定了死信交换机 dl.direct，因此死信会投递给这个交换机：\n\n\n\n如果这个死信交换机也绑定了一个队列，则消息最终会进入这个存放死信的队列：\n\n\n\n另外，队列将死信投递给死信交换机时，必须知道两个信息：\n\n * 死信交换机名称 dead-letter-exchange\n * 死信交换机与死信队列绑定的routingkeydead-letter-routing-key\n\n这样才能确保投递的消息能到达死信交换机，并且正确的路由到死信队列。\n\n\n\n# 异常交换机 与 死信交换机对比\n\n>  1. 前者是由 消费者投递给异常交换机，后者是由队列投递给死信交换机\n\n\n\n# 利用死信交换机接收死信（拓展）\n\n在失败重试策略中，默认的rejectanddontrequeuerecoverer会在本地重试次数耗尽后，发送reject给rabbitmq，消息变成死信，被丢弃。\n\n我们可以给simple.queue添加一个死信交换机，给死信交换机绑定一个队列。这样消息变成死信后也不会丢弃，而是最终投递到死信交换机，路由到与死信交换机绑定的队列。\n\n\n\n我们在consumer服务中，定义一组死信交换机、死信队列：\n\n//messagerecoverer的模式采用默认的  rejectanddontrequeuerecoverer模式，本地重试失败后，丢弃消息，到死信交换机\n\n// 声明普通的 simple.queue队列，并且为其指定死信交换机：dl.direct\n@bean\npublic queue simplequeue2(){\n    return queuebuilder.durable("simple.queue") // 指定队列名称，并持久化\n        .deadletterexchange("dl.direct") // 指定死信交换机\n        .deadletterroutingkey("dl") //指定死信交换机  routingkey\n        .build();\n}\n// 声明死信交换机 dl.direct\n@bean\npublic directexchange dlexchange(){\n    return new directexchange("dl.direct", true, false);\n}\n// 声明存储死信的队列 dl.queue\n@bean\npublic queue dlqueue(){\n    return new queue("dl.queue", true);\n}\n// 将死信队列 与 死信交换机绑定\n@bean\npublic binding dlbinding(){\n    return bindingbuilder.bind(dlqueue()).to(dlexchange()).with("dl");\n}\n\n\n# 总结\n\n什么样的消息会成为死信？\n\n * 消息被消费者reject或者返回nack\n * 消息超时未消费\n * 队列满了\n\n死信交换机的使用场景是什么？\n\n * 如果队列绑定了死信交换机，死信会投递到死信交换机；\n * 可以利用死信交换机收集所有消费者处理失败的消息（死信），交由人工处理，进一步提高消息队列的可靠性。\n\n\n# ttl\n\n一个队列中的消息如果超时未消费，则会变为死信，超时分为两种情况：\n\n * 消息所在的队列设置了超时时间\n * 消息本身设置了超时时间\n * 当两者都存在时间时，取较短的一个\n\n\n\n# 接收超时死信的死信交换机\n\n原理\n\n> ttl的交换机不设置消费者监听，让消息在队列中超时，自动投递到死信交换机中,让消费者监听死信交换机，这样消费者收到生产者的消息就实现了延时。\n\n在consumer服务的springrabbitlistener中，定义一个新的消费者，并且声明 死信交换机、死信队列：\n\n@rabbitlistener(bindings = @queuebinding(\n    value = @queue(name = "dl.ttl.queue", durable = "true"),\n    exchange = @exchange(name = "dl.ttl.direct"),\n    key = "ttl"\n))\npublic void listendlqueue(string msg){\n    log.info("接收到 dl.ttl.queue的延迟消息：{}", msg);\n}\n\n\n# 声明一个队列，并且指定ttl\n\n要给队列设置超时时间，需要在声明队列时配置x-message-ttl属性：\n\n@bean\npublic queue ttlqueue(){\n    return queuebuilder.durable("ttl.queue") // 指定队列名称，并持久化\n        .ttl(10000) // 设置队列的超时时间，10秒\n        .deadletterexchange("dl.ttl.direct") // 指定死信交换机\n        .deadletterroutingkey("dl") //指定死信交换机  routingkey\n        .build();\n}\n\n\n注意，这个队列设定了死信交换机为dl.ttl.direct\n\n声明交换机，将ttl与交换机绑定：\n\n@bean\npublic directexchange ttlexchange(){\n    return new directexchange("ttl.direct");\n}\n@bean\npublic binding ttlbinding(){\n    return bindingbuilder.bind(ttlqueue()).to(ttlexchange()).with("ttl");\n}\n\n\n发送消息，但是不要指定ttl：\n\n@test\npublic void testttlqueue() {\n    // 创建消息\n    string message = "hello, ttl queue";\n    // 消息id，需要封装到correlationdata中\n    correlationdata correlationdata = new correlationdata(uuid.randomuuid().tostring());\n    // 发送消息\n    rabbittemplate.convertandsend("ttl.direct", "ttl", message, correlationdata);\n    // 记录日志\n    log.debug("发送消息成功");\n}\n\n\n发送消息的日志：\n\n\n\n查看下接收消息的日志：\n\n\n\n因为队列的ttl值是10000ms，也就是10秒。可以看到消息发送与接收之间的时差刚好是10秒。\n\n# 发送消息时，设定ttl\n\n在发送消息时，也可以指定ttl：\n\n@test\npublic void testttlmsg() {\n    // 创建消息\n    message message = messagebuilder\n        .withbody("hello, ttl message".getbytes(standardcharsets.utf_8))\n        .setexpiration("5000")\n        .build();\n    // 消息id，需要封装到correlationdata中\n    correlationdata correlationdata = new correlationdata(uuid.randomuuid().tostring());\n    // 发送消息\n    rabbittemplate.convertandsend("ttl.direct", "ttl", message, correlationdata);\n    log.debug("发送消息成功");\n}\n\n\n查看发送消息日志：\n\n\n\n接收消息日志：\n\n\n\n这次，发送与接收的延迟只有5秒。说明当队列、消息都设置了ttl时，任意一个到期就会成为死信。\n\n# 总结\n\n消息超时的两种方式是？\n\n * 给队列设置ttl属性，进入队列后超过ttl时间的消息变为死信\n * 给消息设置ttl属性，队列接收到消息超过ttl时间后变为死信\n\n如何实现发送一个消息20秒后消费者才收到消息？\n\n * 给消息的目标队列指定死信交换机\n * 将消费者监听的队列绑定到死信交换机\n * 发送消息时给消息设置超时时间为20秒\n\n\n# 延迟队列\n\n利用ttl结合死信交换机，我们实现了消息发出后，消费者延迟收到消息的效果。这种消息模式就称为延迟队列（delay queue）模式。\n\n延迟队列的使用场景包括：\n\n * 延迟发送短信\n * 用户下单，如果用户在15 分钟内未支付，则自动取消\n * 预约工作会议，20分钟后自动通知所有参会人员\n\n因为延迟队列的需求非常多，所以rabbitmq的官方也推出了一个插件，原生支持延迟队列效果。\n\n这个插件就是delayexchange插件。参考rabbitmq的插件列表页面：https://www.rabbitmq.com/community-plugins.html\n\n\n\n使用方式可以参考官网地址：https://blog.rabbitmq.com/posts/2015/04/scheduling-messages-with-rabbitmq\n\n# delayexchange原理\n\ndelayexchange需要将一个交换机声明为delayed类型。当我们发送消息到delayexchange时，流程如下：\n\n * 接收消息\n * 判断消息是否具备x-delay属性\n * 如果有x-delay属性，说明是延迟消息，持久化到硬盘，读取x-delay值，作为延迟时间\n * 返回routing not found结果给消息发送者\n * x-delay时间到期后，重新投递消息到指定队列\n\n# 使用delayexchange\n\n插件的使用也非常简单：声明一个交换机，交换机的类型可以是任意类型，只需要设定delayed属性为true即可，然后声明队列与其绑定即可。\n\n# 1）声明delayexchange交换机\n\n基于注解方式（推荐）：\n\n\n\n也可以基于@bean的方式：\n\n\n\n# 2）发送消息\n\n发送消息时，一定要携带x-delay属性，指定延迟的时间：\n\n\n\n# 总结\n\n延迟队列插件的使用步骤包括哪些？\n\n•声明一个交换机，添加delayed属性为true\n\n•发送消息时，添加x-delay头，值为超时时间\n\n\n# 惰性队列\n\n\n# 消息堆积问题\n\n当生产者发送消息的速度超过了消费者处理消息的速度，就会导致队列中的消息堆积，直到队列存储消息达到上限。之后发送的消息就会成为死信，可能会被丢弃，这就是消息堆积问题。\n\n\n\n解决消息堆积有三种思路：\n\n * 增加更多消费者，提高消费速度。也就是我们之前说的work queue模式\n * 在消费者内开启线程池加快消息处理速度\n * 扩大队列容积，提高堆积上限\n\n要提升队列容积，把消息保存在内存中显然是不行的。\n\n\n# 惰性队列\n\n从rabbitmq的3.6.0版本开始，就增加了lazy queues的概念，也就是惰性队列。惰性队列的特征如下：\n\n * 接收到消息后直接存入磁盘而非内存\n * 消费者要消费消息时才会从磁盘中读取并加载到内存\n * 支持数百万条的消息存储\n\n# 基于命令行设置lazy-queue\n\n而要设置一个队列为惰性队列，只需要在声明队列时，指定x-queue-mode属性为lazy即可。可以通过命令行将一个运行中的队列修改为惰性队列：\n\nrabbitmqctl set_policy lazy "^lazy-queue$" \'{"queue-mode":"lazy"}\' --apply-to queues  \n\n\n命令解读：\n\n * rabbitmqctl ：rabbitmq的命令行工具\n * set_policy ：添加一个策略\n * lazy ：策略名称，可以自定义\n * "^lazy-queue$" ：用正则表达式匹配队列的名字\n * \'{"queue-mode":"lazy"}\' ：设置队列模式为lazy模式\n * --apply-to queues：策略的作用对象，是所有的队列\n\n# 基于@bean声明lazy-queue\n\n\n\n# 基于@rabbitlistener声明lazyqueue\n\n\n\n# 总结\n\n消息堆积问题的解决方案？\n\n * 队列上绑定多个消费者，提高消费速度\n * 给消费者开启线程池，提高消费速度\n * 使用惰性队列，可以再mq中保存更多消息\n\n惰性队列的优点有哪些？\n\n * 基于磁盘存储，消息上限高\n * 没有间歇性的page-out，性能比较稳定\n * lazy 队列\n   * 消息直接存到磁盘，当消费者要消费时，才将消息读到磁盘\n\n\n\n * 普通队列\n   \n   * 先往内存中存，内存快满了后，将内存中的数据存到磁盘中，内存在重新接收数据\n   \n   \n\n惰性队列的缺点有哪些？\n\n * 基于磁盘存储，消息时效性会降低\n * 性能受限于磁盘的io\n\n\n# mq集群\n\n\n# 集群分类\n\nrabbitmq的是基于erlang语言编写，而erlang又是一个面向并发的语言，天然支持集群模式。rabbitmq的集群有两种模式：\n\n•普通集群：是一种分布式集群，将队列分散到集群的各个节点，从而提高整个集群的并发能力。\n\n•镜像集群：是一种主从集群，普通集群的基础上，添加了主从备份功能，提高集群的数据可用性。\n\n镜像集群虽然支持主从，但主从同步并不是强一致的，某些情况下可能有数据丢失的风险。因此在rabbitmq的3.8版本以后，推出了新的功能：仲裁队列来代替镜像集群，底层采用raft协议确保主从的数据一致性。\n\n\n# 普通集群\n\n# 集群结构和特征\n\n普通集群，或者叫标准集群（classic cluster），具备下列特征：\n\n * 会在集群的各个节点间共享部分数据，包括：交换机、队列元信息。不包含队列中的消息。\n   \n   * 元信息就是，能看到队列叫啥，在哪等信息\n\n * 当访问集群某节点时，如果队列不在该节点，会从数据所在节点传递到当前节点并返回\n\n * 队列所在节点宕机，队列中的消息就会丢失\n\n结构如图：\n\n\n\n\n# 镜像集群\n\n# 集群结构和特征\n\n镜像集群：本质是主从模式，具备下面的特征：\n\n类似于 es\n\n * 交换机、队列、队列中的消息会在各个mq的镜像节点之间同步备份。\n * 创建队列的节点被称为该队列的主节点，备份到的其它节点叫做该队列的镜像节点。\n * 一个队列的主节点可能是另一个队列的镜像节点\n * 所有操作都是主节点完成，然后同步给镜像节点\n * 主宕机后，镜像节点会替代成新的主\n\n结构如图：\n\n\n\n\n# 仲裁队列\n\n# 集群特征\n\n仲裁队列：仲裁队列是3.8版本以后才有的新功能，用来替代镜像队列，具备下列特征：\n\n * 与镜像队列一样，都是主从模式，支持主从数据同步\n * 使用非常简单，没有复杂的配置\n * 主从同步基于raft协议，强一致\n\n# java代码创建仲裁队列\n\n这里除了创建队列时不一样，其他写和监听都一样的\n\n@bean\npublic queue quorumqueue() {\n    return queuebuilder\n        .durable("quorum.queue") // 持久化\n        .quorum() // 仲裁队列\n        .build();\n}\n\n\n# springamqp连接mq集群\n\n注意，这里用address来代替host、port方式\n\nspring:\n  rabbitmq:\n    addresses: 192.168.159.100:8071, 192.168.159.100:8072, 192.168.159.100:8073\n    username: itcast\n    password: 123321\n    virtual-host: /\n\n\n\n# 总结\n\n * 四个问题\n   * 消息可靠性\n     * 生产者消息确认\n       * returncallback\n       * confirmcallback\n     * 消息持久化存储 - durable\n       * 默认队列，交换机，消息都是持久化的\n     * 消费者消息确认\n       * 本地重试\n       * 失败策略\n         * 异常交换机，异常队列\n   * 延时消息\n     * 死信交换机\n       * 死信处理\n       * 与异常交换机的区别\n     * 延迟队列\n       * 利用死信交换机完成ttl\n     * 延迟交换机\n       * 安装插件，让交换机临时存储数据，达到延时转发的目的\n   * 消息堆积\n     * 解决方法\n     * 惰性队列\n       * 特性\n       * 与普通队列区别\n   * 高可用\n     * 普通集群\n     * 镜像集群\n     * 仲裁队列',charsets:{cjk:!0}},{title:"Nacos注册中心",frontmatter:{autoSort:96,title:"Nacos注册中心",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/9b099d/",categories:["后端","微服务","SpringCloud"],tags:["知识","微服务","SpringCloud"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/40.SpringCloud/40.Nacos%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83.html",relativePath:"01.后端/60.微服务/40.SpringCloud/40.Nacos注册中心.md",key:"v-4000ac32",path:"/pages/9b099d/",headers:[{level:2,title:"服务注册到nacos",slug:"服务注册到nacos",normalizedTitle:"服务注册到nacos",charIndex:66},{level:3,title:"1）引入依赖",slug:"_1-引入依赖",normalizedTitle:"1）引入依赖",charIndex:222},{level:3,title:"2）配置nacos地址",slug:"_2-配置nacos地址",normalizedTitle:"2）配置nacos地址",charIndex:748},{level:3,title:"3）重启",slug:"_3-重启",normalizedTitle:"3）重启",charIndex:907},{level:2,title:"服务分级存储模型",slug:"服务分级存储模型",normalizedTitle:"服务分级存储模型",charIndex:949},{level:3,title:"给user-service配置集群",slug:"给user-service配置集群",normalizedTitle:"给user-service配置集群",charIndex:1346},{level:3,title:"同集群优先的负载均衡",slug:"同集群优先的负载均衡",normalizedTitle:"同集群优先的负载均衡",charIndex:1714},{level:2,title:"权重配置",slug:"权重配置",normalizedTitle:"权重配置",charIndex:2246},{level:2,title:"环境隔离",slug:"环境隔离",normalizedTitle:"环境隔离",charIndex:2603},{level:3,title:"创建namespace",slug:"创建namespace",normalizedTitle:"创建namespace",charIndex:2742},{level:3,title:"给微服务配置namespace",slug:"给微服务配置namespace",normalizedTitle:"给微服务配置namespace",charIndex:2880},{level:2,title:"Nacos与Eureka的区别",slug:"nacos与eureka的区别",normalizedTitle:"nacos与eureka的区别",charIndex:3241}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"服务注册到nacos 1）引入依赖 2）配置nacos地址 3）重启 服务分级存储模型 给user-service配置集群 同集群优先的负载均衡 权重配置 环境隔离 创建namespace 给微服务配置namespace Nacos与Eureka的区别",content:"Nacos是阿里巴巴的产品，现在是SpringCloud中的一个组件。相比Eureka功能更加丰富，在国内受欢迎程度较高。\n\n\n# 服务注册到nacos\n\nNacos是SpringCloudAlibaba的组件，而SpringCloudAlibaba也遵循SpringCloud中定义的服务注册、服务发现规范。因此使用Nacos和使用Eureka对于微服务来说，并没有太大区别。\n\n主要差异在于：\n\n * 依赖不同\n * 服务地址不同\n\n\n# 1）引入依赖\n\n在cloud-demo父工程的pom文件中的<dependencyManagement>中引入SpringCloudAlibaba的依赖：\n\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-alibaba-dependencies</artifactId>\n    <version>2.2.6.RELEASE</version>\n    <type>pom</type>\n    <scope>import</scope>\n</dependency>\n\n\n然后在user-service和order-service中的pom文件中引入nacos-discovery依赖：\n\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n\n\n> 注意：不要忘了注释掉eureka的依赖。\n\n\n# 2）配置nacos地址\n\n在user-service和order-service的application.yml中添加nacos地址：\n\nspring:\n  cloud:\n    nacos:\n      server-addr: localhost:8848\n\n\n> 注意：不要忘了注释掉eureka的地址\n\n\n# 3）重启\n\n重启微服务后，登录nacos管理页面，可以看到微服务信息：\n\n\n\n\n# 服务分级存储模型\n\n一个服务可以有多个实例，例如我们的user-service，可以有:\n\n * 127.0.0.1:8081\n * 127.0.0.1:8082\n * 127.0.0.1:8083\n\n假如这些实例分布于全国各地的不同机房，例如：\n\n * 127.0.0.1:8081，在上海机房\n * 127.0.0.1:8082，在上海机房\n * 127.0.0.1:8083，在杭州机房\n\nNacos就将同一机房内的实例 划分为一个集群。\n\n也就是说，user-service是服务，一个服务可以包含多个集群，如杭州、上海，每个集群下可以有多个实例，形成分级模型，如图：\n\n\n\n微服务互相访问时，应该尽可能访问同集群实例，因为本地访问速度更快。当本集群内不可用时，才访问其它集群。例如：\n\n\n\n杭州机房内的order-service应该优先访问同机房的user-service。\n\n\n# 给user-service配置集群\n\n修改user-service的application.yml文件，添加集群配置：\n\nspring:\n  cloud:\n    nacos:\n      server-addr: localhost:8848\n      discovery:\n        cluster-name: HZ # 集群名称\n\n\n重启两个user-service实例后，我们可以在nacos控制台看到下面结果：\n\n\n\n我们再次复制一个user-service启动配置，添加属性：\n\n-Dserver.port=8083 -Dspring.cloud.nacos.discovery.cluster-name=SH\n\n\n配置如图所示：\n\n\n\n启动UserApplication3后再次查看nacos控制台：\n\n\n\n\n# 同集群优先的负载均衡\n\n默认的ZoneAvoidanceRule并不能实现根据同集群优先来实现负载均衡。\n\n因此Nacos中提供了一个NacosRule的实现，可以优先从同集群中挑选实例。\n\n1）给order-service配置集群信息\n\n修改order-service的application.yml文件，添加集群配置：\n\nspring:\n  cloud:\n    nacos:\n      server-addr: localhost:8848\n      discovery:\n        cluster-name: HZ # 集群名称\n\n\n2）修改负载均衡规则\n\n修改order-service的application.yml文件，修改负载均衡规则：\n\n访问规则\n\n>  1. 优先访问本地集群内部，然后对于内部的多个实例，采用随机访问的方式\n>  2. 当本地集群全部挂掉的时候，也会访问外部集群，但是会给出warning\n\nuserservice:\n  ribbon:\n    NFLoadBalancerRuleClassName: com.alibaba.cloud.nacos.ribbon.NacosRule # 负载均衡规则 \n\n\n\n# 权重配置\n\n0到1之间\n\n实际部署中会出现这样的场景：\n\n服务器设备性能有差异，部分实例所在机器性能较好，另一些较差，我们希望性能好的机器承担更多的用户请求。\n\n但默认情况下NacosRule是同集群内随机挑选，不会考虑机器的性能问题。\n\n因此，Nacos提供了权重配置来控制访问频率，权重越大则访问频率越高。\n\n在nacos控制台，找到user-service的实例列表，点击编辑，即可修改权重：\n\n\n\n在弹出的编辑窗口，修改权重：\n\n\n\n权重作用\n\n> 可以实现丝滑的系统升级。\n> \n> 比如，让系统宕机，更新，可以将权重设置为0；\n> \n> 然后系统重新启动后，可以设置一个较小的权值，测试下服务器的性能；\n> \n> 没问题的话，在提高权值。\n\n> 注意：如果权重修改为0，则该实例永远不会被访问\n\n\n# 环境隔离\n\nNacos提供了namespace来实现环境隔离功能。\n\n * nacos中可以有多个namespace\n * namespace下可以有group、service等\n * 不同namespace之间相互隔离，例如不同namespace的服务互相不可见\n\n\n\n\n# 创建namespace\n\n默认情况下，所有service、data、group都在同一个namespace，名为public：\n\n\n\n我们可以点击页面新增按钮，添加一个namespace：\n\n\n\n然后，填写表单：\n\n\n\n就能在页面看到一个新的namespace：\n\n\n\n\n# 给微服务配置namespace\n\n给微服务配置namespace只能通过修改配置来实现。\n\n例如，修改order-service的application.yml文件：\n\nspring:\n  cloud:\n    nacos:\n      server-addr: localhost:8848\n      discovery:\n        cluster-name: HZ\n        namespace: 492a7d5d-237b-46a1-a99a-fa8e98e4b0f9 # 命名空间，填ID\n\n\n重启order-service后，访问控制台，可以看到下面的结果：\n\n\n\n\n\n此时访问order-service，因为namespace不同，会导致找不到userservice，控制台会报错：\n\n\n\n\n# Nacos与Eureka的区别\n\nNacos的服务实例分为两种l类型：\n\n * 临时实例：如果实例宕机超过一定时间，会从服务列表剔除，默认的类型。\n\n * 非临时实例：如果实例宕机，不会从服务列表剔除，也可以叫永久实例。\n\n配置一个服务实例为永久实例：\n\nspring:\n  cloud:\n    nacos:\n      discovery:\n        ephemeral: false # 设置为非临时实例\n\n\nNacos和Eureka整体结构类似，服务注册、服务拉取、心跳等待，但是也存在一些差异：\n\n\n\n * Nacos与eureka的共同点\n   * 都支持服务注册和服务拉取\n   * 都支持服务提供者心跳方式做健康检测\n * Nacos与Eureka的区别\n   * Nacos支持服务端主动检测提供者状态：临时实例采用心跳模式，非临时实例采用主动检测模式\n     * 主动检测对服务器压力大，推荐使用费临时实例\n   * 临时实例心跳不正常会被剔除，非临时实例则不会被剔除\n   * Nacos支持服务列表变更的消息推送模式，服务列表更新更及时\n   * Nacos集群默认采用AP方式，当集群中存在非临时实例时，采用CP模式；Eureka采用AP方式",normalizedContent:"nacos是阿里巴巴的产品，现在是springcloud中的一个组件。相比eureka功能更加丰富，在国内受欢迎程度较高。\n\n\n# 服务注册到nacos\n\nnacos是springcloudalibaba的组件，而springcloudalibaba也遵循springcloud中定义的服务注册、服务发现规范。因此使用nacos和使用eureka对于微服务来说，并没有太大区别。\n\n主要差异在于：\n\n * 依赖不同\n * 服务地址不同\n\n\n# 1）引入依赖\n\n在cloud-demo父工程的pom文件中的<dependencymanagement>中引入springcloudalibaba的依赖：\n\n<dependency>\n    <groupid>com.alibaba.cloud</groupid>\n    <artifactid>spring-cloud-alibaba-dependencies</artifactid>\n    <version>2.2.6.release</version>\n    <type>pom</type>\n    <scope>import</scope>\n</dependency>\n\n\n然后在user-service和order-service中的pom文件中引入nacos-discovery依赖：\n\n<dependency>\n    <groupid>com.alibaba.cloud</groupid>\n    <artifactid>spring-cloud-starter-alibaba-nacos-discovery</artifactid>\n</dependency>\n\n\n> 注意：不要忘了注释掉eureka的依赖。\n\n\n# 2）配置nacos地址\n\n在user-service和order-service的application.yml中添加nacos地址：\n\nspring:\n  cloud:\n    nacos:\n      server-addr: localhost:8848\n\n\n> 注意：不要忘了注释掉eureka的地址\n\n\n# 3）重启\n\n重启微服务后，登录nacos管理页面，可以看到微服务信息：\n\n\n\n\n# 服务分级存储模型\n\n一个服务可以有多个实例，例如我们的user-service，可以有:\n\n * 127.0.0.1:8081\n * 127.0.0.1:8082\n * 127.0.0.1:8083\n\n假如这些实例分布于全国各地的不同机房，例如：\n\n * 127.0.0.1:8081，在上海机房\n * 127.0.0.1:8082，在上海机房\n * 127.0.0.1:8083，在杭州机房\n\nnacos就将同一机房内的实例 划分为一个集群。\n\n也就是说，user-service是服务，一个服务可以包含多个集群，如杭州、上海，每个集群下可以有多个实例，形成分级模型，如图：\n\n\n\n微服务互相访问时，应该尽可能访问同集群实例，因为本地访问速度更快。当本集群内不可用时，才访问其它集群。例如：\n\n\n\n杭州机房内的order-service应该优先访问同机房的user-service。\n\n\n# 给user-service配置集群\n\n修改user-service的application.yml文件，添加集群配置：\n\nspring:\n  cloud:\n    nacos:\n      server-addr: localhost:8848\n      discovery:\n        cluster-name: hz # 集群名称\n\n\n重启两个user-service实例后，我们可以在nacos控制台看到下面结果：\n\n\n\n我们再次复制一个user-service启动配置，添加属性：\n\n-dserver.port=8083 -dspring.cloud.nacos.discovery.cluster-name=sh\n\n\n配置如图所示：\n\n\n\n启动userapplication3后再次查看nacos控制台：\n\n\n\n\n# 同集群优先的负载均衡\n\n默认的zoneavoidancerule并不能实现根据同集群优先来实现负载均衡。\n\n因此nacos中提供了一个nacosrule的实现，可以优先从同集群中挑选实例。\n\n1）给order-service配置集群信息\n\n修改order-service的application.yml文件，添加集群配置：\n\nspring:\n  cloud:\n    nacos:\n      server-addr: localhost:8848\n      discovery:\n        cluster-name: hz # 集群名称\n\n\n2）修改负载均衡规则\n\n修改order-service的application.yml文件，修改负载均衡规则：\n\n访问规则\n\n>  1. 优先访问本地集群内部，然后对于内部的多个实例，采用随机访问的方式\n>  2. 当本地集群全部挂掉的时候，也会访问外部集群，但是会给出warning\n\nuserservice:\n  ribbon:\n    nfloadbalancerruleclassname: com.alibaba.cloud.nacos.ribbon.nacosrule # 负载均衡规则 \n\n\n\n# 权重配置\n\n0到1之间\n\n实际部署中会出现这样的场景：\n\n服务器设备性能有差异，部分实例所在机器性能较好，另一些较差，我们希望性能好的机器承担更多的用户请求。\n\n但默认情况下nacosrule是同集群内随机挑选，不会考虑机器的性能问题。\n\n因此，nacos提供了权重配置来控制访问频率，权重越大则访问频率越高。\n\n在nacos控制台，找到user-service的实例列表，点击编辑，即可修改权重：\n\n\n\n在弹出的编辑窗口，修改权重：\n\n\n\n权重作用\n\n> 可以实现丝滑的系统升级。\n> \n> 比如，让系统宕机，更新，可以将权重设置为0；\n> \n> 然后系统重新启动后，可以设置一个较小的权值，测试下服务器的性能；\n> \n> 没问题的话，在提高权值。\n\n> 注意：如果权重修改为0，则该实例永远不会被访问\n\n\n# 环境隔离\n\nnacos提供了namespace来实现环境隔离功能。\n\n * nacos中可以有多个namespace\n * namespace下可以有group、service等\n * 不同namespace之间相互隔离，例如不同namespace的服务互相不可见\n\n\n\n\n# 创建namespace\n\n默认情况下，所有service、data、group都在同一个namespace，名为public：\n\n\n\n我们可以点击页面新增按钮，添加一个namespace：\n\n\n\n然后，填写表单：\n\n\n\n就能在页面看到一个新的namespace：\n\n\n\n\n# 给微服务配置namespace\n\n给微服务配置namespace只能通过修改配置来实现。\n\n例如，修改order-service的application.yml文件：\n\nspring:\n  cloud:\n    nacos:\n      server-addr: localhost:8848\n      discovery:\n        cluster-name: hz\n        namespace: 492a7d5d-237b-46a1-a99a-fa8e98e4b0f9 # 命名空间，填id\n\n\n重启order-service后，访问控制台，可以看到下面的结果：\n\n\n\n\n\n此时访问order-service，因为namespace不同，会导致找不到userservice，控制台会报错：\n\n\n\n\n# nacos与eureka的区别\n\nnacos的服务实例分为两种l类型：\n\n * 临时实例：如果实例宕机超过一定时间，会从服务列表剔除，默认的类型。\n\n * 非临时实例：如果实例宕机，不会从服务列表剔除，也可以叫永久实例。\n\n配置一个服务实例为永久实例：\n\nspring:\n  cloud:\n    nacos:\n      discovery:\n        ephemeral: false # 设置为非临时实例\n\n\nnacos和eureka整体结构类似，服务注册、服务拉取、心跳等待，但是也存在一些差异：\n\n\n\n * nacos与eureka的共同点\n   * 都支持服务注册和服务拉取\n   * 都支持服务提供者心跳方式做健康检测\n * nacos与eureka的区别\n   * nacos支持服务端主动检测提供者状态：临时实例采用心跳模式，非临时实例采用主动检测模式\n     * 主动检测对服务器压力大，推荐使用费临时实例\n   * 临时实例心跳不正常会被剔除，非临时实例则不会被剔除\n   * nacos支持服务列表变更的消息推送模式，服务列表更新更及时\n   * nacos集群默认采用ap方式，当集群中存在非临时实例时，采用cp模式；eureka采用ap方式",charsets:{cjk:!0}},{title:"Feign远程调用",frontmatter:{autoSort:94,title:"Feign远程调用",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/314ef0/",categories:["后端","微服务","SpringCloud"],tags:["知识","微服务","SpringCloud"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/40.SpringCloud/60.Feign%E8%BF%9C%E7%A8%8B%E8%B0%83%E7%94%A8.html",relativePath:"01.后端/60.微服务/40.SpringCloud/60.Feign远程调用.md",key:"v-06d0c40e",path:"/pages/314ef0/",headers:[{level:2,title:"Feign替代RestTemplate",slug:"feign替代resttemplate",normalizedTitle:"feign替代resttemplate",charIndex:178},{level:3,title:"1）引入依赖",slug:"_1-引入依赖",normalizedTitle:"1）引入依赖",charIndex:217},{level:3,title:"2）添加注解",slug:"_2-添加注解",normalizedTitle:"2）添加注解",charIndex:404},{level:3,title:"3）编写Feign的客户端",slug:"_3-编写feign的客户端",normalizedTitle:"3）编写feign的客户端",charIndex:452},{level:3,title:"4）测试",slug:"_4-测试",normalizedTitle:"4）测试",charIndex:1052},{level:3,title:"5）总结",slug:"_5-总结",normalizedTitle:"5）总结",charIndex:1152},{level:2,title:"自定义配置",slug:"自定义配置",normalizedTitle:"自定义配置",charIndex:1265},{level:3,title:"配置文件方式",slug:"配置文件方式",normalizedTitle:"配置文件方式",charIndex:1712},{level:3,title:"Java代码方式",slug:"java代码方式",normalizedTitle:"java代码方式",charIndex:2136},{level:2,title:"Feign使用优化",slug:"feign使用优化",normalizedTitle:"feign使用优化",charIndex:2599},{level:2,title:"最佳实践",slug:"最佳实践",normalizedTitle:"最佳实践",charIndex:3521},{level:3,title:"继承方式",slug:"继承方式",normalizedTitle:"继承方式",charIndex:3660},{level:3,title:"抽取方式",slug:"抽取方式",normalizedTitle:"抽取方式",charIndex:3853},{level:3,title:"实现基于抽取的最佳实践",slug:"实现基于抽取的最佳实践",normalizedTitle:"实现基于抽取的最佳实践",charIndex:3999}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"Feign替代RestTemplate 1）引入依赖 2）添加注解 3）编写Feign的客户端 4）测试 5）总结 自定义配置 配置文件方式 Java代码方式 Feign使用优化 最佳实践 继承方式 抽取方式 实现基于抽取的最佳实践",content:'先来看我们以前利用RestTemplate发起远程调用的代码：\n\n\n\n存在下面的问题：\n\n•代码可读性差，编程体验不统一\n\n•参数复杂URL难以维护\n\nFeign是一个声明式的http客户端，官方地址：https://github.com/OpenFeign/feign\n\n其作用就是帮助我们优雅的实现http请求的发送，解决上面提到的问题。\n\n\n\n\n# Feign替代RestTemplate\n\nFegin的使用步骤如下：\n\n\n# 1）引入依赖\n\n我们在order-service服务的pom文件中引入feign的依赖：\n\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-openfeign</artifactId>\n</dependency>\n\n\n\n# 2）添加注解\n\n在order-service的启动类添加注解开启Feign的功能：\n\n\n\n\n# 3）编写Feign的客户端\n\n在order-service中新建一个接口，内容如下：\n\npackage cn.itcast.order.client;\n\nimport cn.itcast.order.pojo.User;\nimport org.springframework.cloud.openfeign.FeignClient;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.PathVariable;\n\n@FeignClient("userservice")\npublic interface UserClient {\n    @GetMapping("/user/{id}")\n    User findById(@PathVariable("id") Long id);\n}\n\n\n这个客户端主要是基于SpringMVC的注解来声明远程调用的信息，比如：\n\n * 服务名称：userservice\n * 请求方式：GET\n * 请求路径：/user/{id}\n * 请求参数：Long id\n * 返回值类型：User\n\n这样，Feign就可以帮助我们发送http请求，无需自己使用RestTemplate来发送了。\n\n\n# 4）测试\n\n修改order-service中的OrderService类中的queryOrderById方法，使用Feign客户端代替RestTemplate：\n\n\n\n是不是看起来优雅多了。\n\n\n# 5）总结\n\n使用Feign的步骤：\n\n① 引入依赖\n\n② 添加@EnableFeignClients注解\n\n③ 编写FeignClient接口\n\n④ 使用FeignClient中定义的方法代替RestTemplate\n\n\n# 自定义配置\n\nFeign可以支持很多的自定义配置，如下表所示：\n\n类型                    作用         说明\nfeign.Logger.Level    修改日志级别     包含四种不同的级别：NONE、BASIC、HEADERS、FULL\nfeign.codec.Decoder   响应结果的解析器   http远程调用的结果做解析，例如解析json字符串为java对象\nfeign.codec.Encoder   请求参数编码     将请求参数编码，便于通过http请求发送\nfeign. Contract       支持的注解格式    默认是SpringMVC的注解\nfeign. Retryer        失败重试机制     请求失败的重试机制，默认是没有，不过会使用Ribbon的重试\n\n一般情况下，默认值就能满足我们使用，如果要自定义时，只需要创建自定义的@Bean覆盖默认Bean即可。\n\n下面以日志为例来演示如何自定义配置。\n\n\n# 配置文件方式\n\n基于配置文件修改feign的日志级别可以针对单个服务：\n\nfeign:  \n  client:\n    config: \n      userservice: # 针对某个微服务的配置\n        loggerLevel: FULL #  日志级别 \n\n\n也可以针对所有服务：\n\nfeign:  \n  client:\n    config: \n      default: # 这里用default就是全局配置，如果是写服务名称，则是针对某个微服务的配置\n        loggerLevel: FULL #  日志级别 \n\n\n而日志的级别分为四种：\n\n * NONE：不记录任何日志信息，这是默认值。\n * BASIC：仅记录请求的方法，URL以及响应状态码和执行时间\n * HEADERS：在BASIC的基础上，额外记录了请求和响应的头信息\n * FULL：记录所有请求和响应的明细，包括头信息、请求体、元数据。\n\n\n# Java代码方式\n\n也可以基于Java代码来修改日志级别，先声明一个类，然后声明一个Logger.Level的对象：\n\npublic class DefaultFeignConfiguration  {\n    @Bean\n    public Logger.Level feignLogLevel(){\n        return Logger.Level.BASIC; // 日志级别为BASIC\n    }\n}\n\n\n如果要全局生效，将其放到启动类的@EnableFeignClients这个注解中：\n\n@EnableFeignClients(defaultConfiguration = DefaultFeignConfiguration .class) \n\n\n如果是局部生效，则把它放到对应的@FeignClient这个注解中：\n\n@FeignClient(value = "userservice", configuration = DefaultFeignConfiguration .class) \n\n\n\n# Feign使用优化\n\nFeign底层发起http请求，依赖于其它的框架。其底层客户端实现包括：\n\n•URLConnection：默认实现，不支持连接池\n\n•Apache HttpClient ：支持连接池\n\n•OKHttp：支持连接池\n\n因此提高Feign的性能主要手段就是使用连接池代替默认的URLConnection。\n\n这里我们用Apache的HttpClient来演示。\n\n1）引入依赖\n\n在order-service的pom文件中引入Apache的HttpClient依赖：\n\n\x3c!--httpClient的依赖 --\x3e\n<dependency>\n    <groupId>io.github.openfeign</groupId>\n    <artifactId>feign-httpclient</artifactId>\n</dependency>\n\n\n2）配置连接池\n\n在order-service的application.yml中添加配置：\n\nfeign:\n  client:\n    config:\n      default: # default全局的配置\n        loggerLevel: BASIC # 日志级别，BASIC就是基本的请求和响应信息\n  httpclient:\n    enabled: true # 开启feign对HttpClient的支持\n    max-connections: 200 # 最大的连接数\n    max-connections-per-route: 50 # 每个路径的最大连接数\n\n\n接下来，在FeignClientFactoryBean中的loadBalance方法中打断点：\n\n\n\nDebug方式启动order-service服务，可以看到这里的client，底层就是Apache HttpClient：\n\n\n\n总结，Feign的优化：\n\n1.日志级别尽量用basic\n\n2.使用HttpClient或OKHttp代替URLConnection\n\n① 引入feign-httpClient依赖\n\n② 配置文件开启httpClient功能，设置连接池参数\n\n\n# 最佳实践\n\n所谓最近实践，就是使用过程中总结的经验，最好的一种使用方式。\n\n自习观察可以发现，Feign的客户端与服务提供者的controller代码非常相似：\n\nfeign客户端：\n\n\n\nUserController：\n\n\n\n有没有一种办法简化这种重复的代码编写呢？\n\n\n# 继承方式\n\n一样的代码可以通过继承来共享：\n\n1）定义一个API接口，利用定义方法，并基于SpringMVC注解做声明。\n\n2）Feign客户端和Controller都集成改接口\n\n\n\n优点：\n\n * 简单\n * 实现了代码共享\n\n缺点：\n\n * 服务提供方、服务消费方紧耦合\n\n * 参数列表中的注解映射并不会继承，因此Controller中必须再次声明方法、参数列表、注解\n\n\n# 抽取方式\n\n将Feign的Client抽取为独立模块，并且把接口有关的POJO、默认的Feign配置都放到这个模块中，提供给所有消费者使用。\n\n例如，将UserClient、User、Feign的默认配置都抽取到一个feign-api包中，所有微服务引用该依赖包，即可直接使用。\n\n\n\n\n# 实现基于抽取的最佳实践\n\n# 1）抽取\n\n首先创建一个module，命名为feign-api：\n\n\n\n项目结构：\n\n\n\n在feign-api中然后引入feign的starter依赖\n\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-openfeign</artifactId>\n</dependency>\n\n\n然后，order-service中编写的UserClient、User、DefaultFeignConfiguration都复制到feign-api项目中\n\n\n\n# 2）在order-service中使用feign-api\n\n首先，删除order-service中的UserClient、User、DefaultFeignConfiguration等类或接口。\n\n在order-service的pom文件中中引入feign-api的依赖：\n\n<dependency>\n    <groupId>cn.itcast.demo</groupId>\n    <artifactId>feign-api</artifactId>\n    <version>1.0</version>\n</dependency>\n\n\n修改order-service中的所有与上述三个组件有关的导包部分，改成导入feign-api中的包\n\n# 3）重启测试\n\n重启后，发现服务报错了：\n\n\n\n这是因为UserClient现在在cn.itcast.feign.clients包下，\n\n而order-service的@EnableFeignClients注解是在cn.itcast.order包下，不在同一个包，无法扫描到UserClient。\n\n# 4）解决扫描包问题\n\n方式一：\n\n指定Feign应该扫描的包：\n\n@EnableFeignClients(basePackages = "cn.itcast.feign.clients")\n\n\n方式二：\n\n指定需要加载的Client接口：\n\n@EnableFeignClients(clients = {UserClient.class})\n',normalizedContent:'先来看我们以前利用resttemplate发起远程调用的代码：\n\n\n\n存在下面的问题：\n\n•代码可读性差，编程体验不统一\n\n•参数复杂url难以维护\n\nfeign是一个声明式的http客户端，官方地址：https://github.com/openfeign/feign\n\n其作用就是帮助我们优雅的实现http请求的发送，解决上面提到的问题。\n\n\n\n\n# feign替代resttemplate\n\nfegin的使用步骤如下：\n\n\n# 1）引入依赖\n\n我们在order-service服务的pom文件中引入feign的依赖：\n\n<dependency>\n    <groupid>org.springframework.cloud</groupid>\n    <artifactid>spring-cloud-starter-openfeign</artifactid>\n</dependency>\n\n\n\n# 2）添加注解\n\n在order-service的启动类添加注解开启feign的功能：\n\n\n\n\n# 3）编写feign的客户端\n\n在order-service中新建一个接口，内容如下：\n\npackage cn.itcast.order.client;\n\nimport cn.itcast.order.pojo.user;\nimport org.springframework.cloud.openfeign.feignclient;\nimport org.springframework.web.bind.annotation.getmapping;\nimport org.springframework.web.bind.annotation.pathvariable;\n\n@feignclient("userservice")\npublic interface userclient {\n    @getmapping("/user/{id}")\n    user findbyid(@pathvariable("id") long id);\n}\n\n\n这个客户端主要是基于springmvc的注解来声明远程调用的信息，比如：\n\n * 服务名称：userservice\n * 请求方式：get\n * 请求路径：/user/{id}\n * 请求参数：long id\n * 返回值类型：user\n\n这样，feign就可以帮助我们发送http请求，无需自己使用resttemplate来发送了。\n\n\n# 4）测试\n\n修改order-service中的orderservice类中的queryorderbyid方法，使用feign客户端代替resttemplate：\n\n\n\n是不是看起来优雅多了。\n\n\n# 5）总结\n\n使用feign的步骤：\n\n① 引入依赖\n\n② 添加@enablefeignclients注解\n\n③ 编写feignclient接口\n\n④ 使用feignclient中定义的方法代替resttemplate\n\n\n# 自定义配置\n\nfeign可以支持很多的自定义配置，如下表所示：\n\n类型                    作用         说明\nfeign.logger.level    修改日志级别     包含四种不同的级别：none、basic、headers、full\nfeign.codec.decoder   响应结果的解析器   http远程调用的结果做解析，例如解析json字符串为java对象\nfeign.codec.encoder   请求参数编码     将请求参数编码，便于通过http请求发送\nfeign. contract       支持的注解格式    默认是springmvc的注解\nfeign. retryer        失败重试机制     请求失败的重试机制，默认是没有，不过会使用ribbon的重试\n\n一般情况下，默认值就能满足我们使用，如果要自定义时，只需要创建自定义的@bean覆盖默认bean即可。\n\n下面以日志为例来演示如何自定义配置。\n\n\n# 配置文件方式\n\n基于配置文件修改feign的日志级别可以针对单个服务：\n\nfeign:  \n  client:\n    config: \n      userservice: # 针对某个微服务的配置\n        loggerlevel: full #  日志级别 \n\n\n也可以针对所有服务：\n\nfeign:  \n  client:\n    config: \n      default: # 这里用default就是全局配置，如果是写服务名称，则是针对某个微服务的配置\n        loggerlevel: full #  日志级别 \n\n\n而日志的级别分为四种：\n\n * none：不记录任何日志信息，这是默认值。\n * basic：仅记录请求的方法，url以及响应状态码和执行时间\n * headers：在basic的基础上，额外记录了请求和响应的头信息\n * full：记录所有请求和响应的明细，包括头信息、请求体、元数据。\n\n\n# java代码方式\n\n也可以基于java代码来修改日志级别，先声明一个类，然后声明一个logger.level的对象：\n\npublic class defaultfeignconfiguration  {\n    @bean\n    public logger.level feignloglevel(){\n        return logger.level.basic; // 日志级别为basic\n    }\n}\n\n\n如果要全局生效，将其放到启动类的@enablefeignclients这个注解中：\n\n@enablefeignclients(defaultconfiguration = defaultfeignconfiguration .class) \n\n\n如果是局部生效，则把它放到对应的@feignclient这个注解中：\n\n@feignclient(value = "userservice", configuration = defaultfeignconfiguration .class) \n\n\n\n# feign使用优化\n\nfeign底层发起http请求，依赖于其它的框架。其底层客户端实现包括：\n\n•urlconnection：默认实现，不支持连接池\n\n•apache httpclient ：支持连接池\n\n•okhttp：支持连接池\n\n因此提高feign的性能主要手段就是使用连接池代替默认的urlconnection。\n\n这里我们用apache的httpclient来演示。\n\n1）引入依赖\n\n在order-service的pom文件中引入apache的httpclient依赖：\n\n\x3c!--httpclient的依赖 --\x3e\n<dependency>\n    <groupid>io.github.openfeign</groupid>\n    <artifactid>feign-httpclient</artifactid>\n</dependency>\n\n\n2）配置连接池\n\n在order-service的application.yml中添加配置：\n\nfeign:\n  client:\n    config:\n      default: # default全局的配置\n        loggerlevel: basic # 日志级别，basic就是基本的请求和响应信息\n  httpclient:\n    enabled: true # 开启feign对httpclient的支持\n    max-connections: 200 # 最大的连接数\n    max-connections-per-route: 50 # 每个路径的最大连接数\n\n\n接下来，在feignclientfactorybean中的loadbalance方法中打断点：\n\n\n\ndebug方式启动order-service服务，可以看到这里的client，底层就是apache httpclient：\n\n\n\n总结，feign的优化：\n\n1.日志级别尽量用basic\n\n2.使用httpclient或okhttp代替urlconnection\n\n① 引入feign-httpclient依赖\n\n② 配置文件开启httpclient功能，设置连接池参数\n\n\n# 最佳实践\n\n所谓最近实践，就是使用过程中总结的经验，最好的一种使用方式。\n\n自习观察可以发现，feign的客户端与服务提供者的controller代码非常相似：\n\nfeign客户端：\n\n\n\nusercontroller：\n\n\n\n有没有一种办法简化这种重复的代码编写呢？\n\n\n# 继承方式\n\n一样的代码可以通过继承来共享：\n\n1）定义一个api接口，利用定义方法，并基于springmvc注解做声明。\n\n2）feign客户端和controller都集成改接口\n\n\n\n优点：\n\n * 简单\n * 实现了代码共享\n\n缺点：\n\n * 服务提供方、服务消费方紧耦合\n\n * 参数列表中的注解映射并不会继承，因此controller中必须再次声明方法、参数列表、注解\n\n\n# 抽取方式\n\n将feign的client抽取为独立模块，并且把接口有关的pojo、默认的feign配置都放到这个模块中，提供给所有消费者使用。\n\n例如，将userclient、user、feign的默认配置都抽取到一个feign-api包中，所有微服务引用该依赖包，即可直接使用。\n\n\n\n\n# 实现基于抽取的最佳实践\n\n# 1）抽取\n\n首先创建一个module，命名为feign-api：\n\n\n\n项目结构：\n\n\n\n在feign-api中然后引入feign的starter依赖\n\n<dependency>\n    <groupid>org.springframework.cloud</groupid>\n    <artifactid>spring-cloud-starter-openfeign</artifactid>\n</dependency>\n\n\n然后，order-service中编写的userclient、user、defaultfeignconfiguration都复制到feign-api项目中\n\n\n\n# 2）在order-service中使用feign-api\n\n首先，删除order-service中的userclient、user、defaultfeignconfiguration等类或接口。\n\n在order-service的pom文件中中引入feign-api的依赖：\n\n<dependency>\n    <groupid>cn.itcast.demo</groupid>\n    <artifactid>feign-api</artifactid>\n    <version>1.0</version>\n</dependency>\n\n\n修改order-service中的所有与上述三个组件有关的导包部分，改成导入feign-api中的包\n\n# 3）重启测试\n\n重启后，发现服务报错了：\n\n\n\n这是因为userclient现在在cn.itcast.feign.clients包下，\n\n而order-service的@enablefeignclients注解是在cn.itcast.order包下，不在同一个包，无法扫描到userclient。\n\n# 4）解决扫描包问题\n\n方式一：\n\n指定feign应该扫描的包：\n\n@enablefeignclients(basepackages = "cn.itcast.feign.clients")\n\n\n方式二：\n\n指定需要加载的client接口：\n\n@enablefeignclients(clients = {userclient.class})\n',charsets:{cjk:!0}},{title:"多级缓存",frontmatter:{autoSort:100,title:"多级缓存",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/267113/",categories:["后端","微服务","进阶","多级缓存"],tags:["知识","微服务","多级缓存"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/50.%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/05.%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98.html",relativePath:"01.后端/60.微服务/50.多级缓存/05.多级缓存.md",key:"v-4ae8fe0c",path:"/pages/267113/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"传统的缓存策略一般是请求到达Tomcat后，先查询Redis，如果未命中则查询数据库，如图：\n\n\n\n存在下面的问题：\n\n•请求要经过Tomcat处理，Tomcat的性能成为整个系统的瓶颈\n\n•Redis缓存失效时，会对数据库产生冲击\n\n多级缓存就是充分利用请求处理的每个环节，分别添加缓存，减轻Tomcat压力，提升服务性能：\n\n * 浏览器访问静态资源时，优先读取浏览器本地缓存\n * 访问非静态资源（ajax查询数据）时，访问服务端\n * 请求到达Nginx后，优先读取Nginx本地缓存\n * 如果Nginx本地缓存未命中，则去直接查询Redis（不经过Tomcat）\n * 如果Redis查询未命中，则查询Tomcat\n * 请求进入Tomcat后，优先查询JVM进程缓存\n * 如果JVM进程缓存未命中，则查询数据库\n\n\n\n在多级缓存架构中，Nginx内部需要编写本地缓存查询、Redis查询、Tomcat查询的业务逻辑，因此这样的nginx服务不再是一个反向代理服务器，而是一个编写业务的Web服务器了。\n\n因此这样的业务Nginx服务也需要搭建集群来提高并发，再有专门的nginx服务来做反向代理，如图：\n\n\n\n另外，我们的Tomcat服务将来也会部署为集群模式：\n\n\n\n可见，多级缓存的关键有两个：\n\n * 一个是在nginx中编写业务，实现nginx本地缓存、Redis、Tomcat的查询\n\n * 另一个就是在Tomcat中实现JVM进程缓存\n\n其中Nginx编程则会用到OpenResty框架结合Lua这样的语言。",normalizedContent:"传统的缓存策略一般是请求到达tomcat后，先查询redis，如果未命中则查询数据库，如图：\n\n\n\n存在下面的问题：\n\n•请求要经过tomcat处理，tomcat的性能成为整个系统的瓶颈\n\n•redis缓存失效时，会对数据库产生冲击\n\n多级缓存就是充分利用请求处理的每个环节，分别添加缓存，减轻tomcat压力，提升服务性能：\n\n * 浏览器访问静态资源时，优先读取浏览器本地缓存\n * 访问非静态资源（ajax查询数据）时，访问服务端\n * 请求到达nginx后，优先读取nginx本地缓存\n * 如果nginx本地缓存未命中，则去直接查询redis（不经过tomcat）\n * 如果redis查询未命中，则查询tomcat\n * 请求进入tomcat后，优先查询jvm进程缓存\n * 如果jvm进程缓存未命中，则查询数据库\n\n\n\n在多级缓存架构中，nginx内部需要编写本地缓存查询、redis查询、tomcat查询的业务逻辑，因此这样的nginx服务不再是一个反向代理服务器，而是一个编写业务的web服务器了。\n\n因此这样的业务nginx服务也需要搭建集群来提高并发，再有专门的nginx服务来做反向代理，如图：\n\n\n\n另外，我们的tomcat服务将来也会部署为集群模式：\n\n\n\n可见，多级缓存的关键有两个：\n\n * 一个是在nginx中编写业务，实现nginx本地缓存、redis、tomcat的查询\n\n * 另一个就是在tomcat中实现jvm进程缓存\n\n其中nginx编程则会用到openresty框架结合lua这样的语言。",charsets:{cjk:!0}},{title:"Lua语法入门",frontmatter:{autoSort:98,title:"Lua语法入门",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/1725f1/",categories:["后端","微服务","进阶","多级缓存"],tags:["知识","微服务","多级缓存"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/50.%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/15.Lua%E8%AF%AD%E6%B3%95%E5%85%A5%E9%97%A8.html",relativePath:"01.后端/60.微服务/50.多级缓存/15.Lua语法入门.md",key:"v-3a15b156",path:"/pages/1725f1/",headers:[{level:2,title:"初识Lua",slug:"初识lua",normalizedTitle:"初识lua",charIndex:40},{level:2,title:"HelloWorld",slug:"helloworld",normalizedTitle:"helloworld",charIndex:213},{level:2,title:"变量和循环",slug:"变量和循环",normalizedTitle:"变量和循环",charIndex:348},{level:3,title:"Lua的数据类型",slug:"lua的数据类型",normalizedTitle:"lua的数据类型",charIndex:391},{level:3,title:"声明变量",slug:"声明变量",normalizedTitle:"声明变量",charIndex:458},{level:3,title:"循环",slug:"循环",normalizedTitle:"循环",charIndex:351},{level:2,title:"条件控制、函数",slug:"条件控制、函数",normalizedTitle:"条件控制、函数",charIndex:1373},{level:3,title:"函数",slug:"函数",normalizedTitle:"函数",charIndex:436},{level:3,title:"条件控制",slug:"条件控制",normalizedTitle:"条件控制",charIndex:1373},{level:3,title:"案例",slug:"案例",normalizedTitle:"案例",charIndex:1780}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"初识Lua HelloWorld 变量和循环 Lua的数据类型 声明变量 循环 条件控制、函数 函数 条件控制 案例",content:"Nginx编程需要用到Lua语言，因此我们必须先入门Lua的基本语法。\n\n\n# 初识Lua\n\nLua 是一种轻量小巧的脚本语言，用标准C语言编写并以源代码形式开放， 其设计目的是为了嵌入应用程序中，从而为应用程序提供灵活的扩展和定制功能。官网：https://www.lua.org/\n\n\n\nLua经常嵌入到C语言开发的程序中，例如游戏开发、游戏插件等。\n\nNginx本身也是C语言开发，因此也允许基于Lua做拓展。\n\n\n# HelloWorld\n\nCentOS7默认已经安装了Lua语言环境，所以可以直接运行Lua代码。\n\n1）在Linux虚拟机的任意目录下，新建一个hello.lua文件\n\n\n\n2）添加下面的内容\n\nprint(\"Hello World!\")  \n\n\n3）运行\n\n\n\n\n# 变量和循环\n\n学习任何语言必然离不开变量，而变量的声明必须先知道数据的类型。\n\n\n# Lua的数据类型\n\nLua中支持的常见数据类型包括：\n\n\n\n另外，Lua提供了type()函数来判断一个变量的数据类型：\n\n\n\n\n# 声明变量\n\nLua声明变量的时候无需指定数据类型，而是用local来声明变量为局部变量：\n\n-- 声明字符串，可以用单引号或双引号，\nlocal str = 'hello'\n-- 字符串拼接可以使用 ..\nlocal str2 = 'hello' .. 'world'\n-- 声明数字\nlocal num = 21\n-- 声明布尔类型\nlocal flag = true\n\n\n**命令行格式下。**局部变量在一行才能读到，过了一行就读不到了。\n\n\n\nLua中的table类型既可以作为数组，又可以作为Java中的map来使用。数组就是特殊的table，key是数组角标而已：\n\n-- 声明数组 ，key为角标的 table\nlocal arr = {'java', 'python', 'lua'}\n-- 就相当于 {1='java', 2='python', 3='lua'}\n\n-- 声明table，类似java的map\nlocal map =  {name='Jack', age=21}\n\n\nLua中的数组角标是从1开始，访问的时候与Java中类似：\n\n-- 访问数组，lua数组的角标从1开始\nprint(arr[1])\n\n\nLua中的table可以用key来访问：\n\n-- 访问table\nprint(map['name'])\nprint(map.name)\n\n\n\n# 循环\n\n对于table，我们可以利用for循环来遍历。不过数组和普通table遍历略有差异。\n\n遍历数组：\n\n-- 声明数组 key为索引的 table\nlocal arr = {'java', 'python', 'lua'}\n-- 遍历数组\nfor index,value in ipairs(arr) do\n    print(index, value) \nend\n\n\n遍历普通table\n\n-- 声明map，也就是table\nlocal map = {name='Jack', age=21}\n-- 遍历table\nfor key,value in pairs(map) do\n   print(key, value) \nend\n\n\n\n# 条件控制、函数\n\nLua中的条件控制和函数声明与Java类似。\n\n\n# 函数\n\n定义函数的语法：\n\nfunction 函数名( argument1, argument2..., argumentn)\n    -- 函数体\n    return 返回值\nend\n\n\n例如，定义一个函数，用来打印数组：\n\nfunction printArr(arr)\n    for index, value in ipairs(arr) do\n        print(value)\n    end\nend\n\n\n\n# 条件控制\n\n类似Java的条件控制，例如if、else语法：\n\nif(布尔表达式)\nthen\n   --[ 布尔表达式为 true 时执行该语句块 --]\nelse\n   --[ 布尔表达式为 false 时执行该语句块 --]\nend\n\n\n\n与java不同，布尔表达式中的逻辑运算是基于英文单词：\n\n\n\n\n# 案例\n\n需求：自定义一个函数，可以打印table，当参数为nil时，打印错误信息\n\nfunction printarr(arr)\n   -- 如果参数为nil 打印为空 \n   if (not arr) then\n     print('数组为空！')\n   else -- 否则，循环打印数组\n   \t\tfor index, value in  ipairs(arr) do\n         \tprint(index, value)\n   \t\tend\n  end\nend\n-- 测试样例 \nprintarr(arr)\narr = {'java','python'}\nprintarr(arr)\n",normalizedContent:"nginx编程需要用到lua语言，因此我们必须先入门lua的基本语法。\n\n\n# 初识lua\n\nlua 是一种轻量小巧的脚本语言，用标准c语言编写并以源代码形式开放， 其设计目的是为了嵌入应用程序中，从而为应用程序提供灵活的扩展和定制功能。官网：https://www.lua.org/\n\n\n\nlua经常嵌入到c语言开发的程序中，例如游戏开发、游戏插件等。\n\nnginx本身也是c语言开发，因此也允许基于lua做拓展。\n\n\n# helloworld\n\ncentos7默认已经安装了lua语言环境，所以可以直接运行lua代码。\n\n1）在linux虚拟机的任意目录下，新建一个hello.lua文件\n\n\n\n2）添加下面的内容\n\nprint(\"hello world!\")  \n\n\n3）运行\n\n\n\n\n# 变量和循环\n\n学习任何语言必然离不开变量，而变量的声明必须先知道数据的类型。\n\n\n# lua的数据类型\n\nlua中支持的常见数据类型包括：\n\n\n\n另外，lua提供了type()函数来判断一个变量的数据类型：\n\n\n\n\n# 声明变量\n\nlua声明变量的时候无需指定数据类型，而是用local来声明变量为局部变量：\n\n-- 声明字符串，可以用单引号或双引号，\nlocal str = 'hello'\n-- 字符串拼接可以使用 ..\nlocal str2 = 'hello' .. 'world'\n-- 声明数字\nlocal num = 21\n-- 声明布尔类型\nlocal flag = true\n\n\n**命令行格式下。**局部变量在一行才能读到，过了一行就读不到了。\n\n\n\nlua中的table类型既可以作为数组，又可以作为java中的map来使用。数组就是特殊的table，key是数组角标而已：\n\n-- 声明数组 ，key为角标的 table\nlocal arr = {'java', 'python', 'lua'}\n-- 就相当于 {1='java', 2='python', 3='lua'}\n\n-- 声明table，类似java的map\nlocal map =  {name='jack', age=21}\n\n\nlua中的数组角标是从1开始，访问的时候与java中类似：\n\n-- 访问数组，lua数组的角标从1开始\nprint(arr[1])\n\n\nlua中的table可以用key来访问：\n\n-- 访问table\nprint(map['name'])\nprint(map.name)\n\n\n\n# 循环\n\n对于table，我们可以利用for循环来遍历。不过数组和普通table遍历略有差异。\n\n遍历数组：\n\n-- 声明数组 key为索引的 table\nlocal arr = {'java', 'python', 'lua'}\n-- 遍历数组\nfor index,value in ipairs(arr) do\n    print(index, value) \nend\n\n\n遍历普通table\n\n-- 声明map，也就是table\nlocal map = {name='jack', age=21}\n-- 遍历table\nfor key,value in pairs(map) do\n   print(key, value) \nend\n\n\n\n# 条件控制、函数\n\nlua中的条件控制和函数声明与java类似。\n\n\n# 函数\n\n定义函数的语法：\n\nfunction 函数名( argument1, argument2..., argumentn)\n    -- 函数体\n    return 返回值\nend\n\n\n例如，定义一个函数，用来打印数组：\n\nfunction printarr(arr)\n    for index, value in ipairs(arr) do\n        print(value)\n    end\nend\n\n\n\n# 条件控制\n\n类似java的条件控制，例如if、else语法：\n\nif(布尔表达式)\nthen\n   --[ 布尔表达式为 true 时执行该语句块 --]\nelse\n   --[ 布尔表达式为 false 时执行该语句块 --]\nend\n\n\n\n与java不同，布尔表达式中的逻辑运算是基于英文单词：\n\n\n\n\n# 案例\n\n需求：自定义一个函数，可以打印table，当参数为nil时，打印错误信息\n\nfunction printarr(arr)\n   -- 如果参数为nil 打印为空 \n   if (not arr) then\n     print('数组为空！')\n   else -- 否则，循环打印数组\n   \t\tfor index, value in  ipairs(arr) do\n         \tprint(index, value)\n   \t\tend\n  end\nend\n-- 测试样例 \nprintarr(arr)\narr = {'java','python'}\nprintarr(arr)\n",charsets:{cjk:!0}},{title:"JVM进程缓存",frontmatter:{autoSort:99,title:"JVM进程缓存",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/dc166e/",categories:["后端","微服务","进阶","多级缓存"],tags:["知识","微服务","多级缓存"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/50.%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/10.JVM%E8%BF%9B%E7%A8%8B%E7%BC%93%E5%AD%98.html",relativePath:"01.后端/60.微服务/50.多级缓存/10.JVM进程缓存.md",key:"v-2fa7be82",path:"/pages/dc166e/",headers:[{level:2,title:"初识Caffeine",slug:"初识caffeine",normalizedTitle:"初识caffeine",charIndex:32},{level:2,title:"实现JVM进程缓存",slug:"实现jvm进程缓存",normalizedTitle:"实现jvm进程缓存",charIndex:345},{level:3,title:"需求",slug:"需求",normalizedTitle:"需求",charIndex:1645},{level:3,title:"实现",slug:"实现",normalizedTitle:"实现",charIndex:345}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"初识Caffeine 实现JVM进程缓存 需求 实现",content:'为了演示多级缓存的案例，我们先准备一个商品查询的业务。\n\n\n# 初识Caffeine\n\n缓存在日常开发中启动至关重要的作用，由于是存储在内存中，数据的读取速度是非常快的，能大量减少对数据库的访问，减少数据库的压力。我们把缓存分为两类：\n\n * 分布式缓存，例如Redis：\n   * 优点：存储容量更大、可靠性更好、可以在集群间共享\n   * 缺点：访问缓存有网络开销\n   * 场景：缓存数据量较大、可靠性要求较高、需要在集群间共享\n * 进程本地缓存，例如HashMap、GuavaCache：\n   * 优点：读取本地内存，没有网络开销，速度更快\n   * 缺点：存储容量有限、可靠性较低、无法共享\n   * 场景：性能要求较高，缓存数据量较小\n\n我们今天会利用Caffeine框架来实现JVM进程缓存。\n\nCaffeine是一个基于Java8开发的，提供了近乎最佳命中率的高性能的本地缓存库。目前Spring内部的缓存使用的就是Caffeine。GitHub地址：https://github.com/ben-manes/caffeine\n\nCaffeine的性能非常好，下图是官方给出的性能对比：\n\n\n\n可以看到Caffeine的性能遥遥领先！\n\n缓存使用的基本API：\n\n@Test\nvoid testBasicOps() {\n    // 构建cache对象\n    Cache<String, String> cache = Caffeine.newBuilder().build();\n\n    // 存数据\n    cache.put("gf", "迪丽热巴");\n\n    // 取数据\n    String gf = cache.getIfPresent("gf");\n    System.out.println("gf = " + gf);\n\n    // 取数据，包含两个参数：\n    // 参数一：缓存的key\n    // 参数二：Lambda表达式，表达式参数就是缓存的key，方法体是查询数据库的逻辑\n    // 优先根据key查询JVM缓存，如果未命中，则执行参数二的Lambda表达式\n    String defaultGF = cache.get("defaultGF", key -> {\n        // 根据key去数据库查询数据\n        return "柳岩";\n    });\n    System.out.println("defaultGF = " + defaultGF);\n}\n\n\nCaffeine既然是缓存的一种，肯定需要有缓存的清除策略，不然的话内存总会有耗尽的时候。\n\nCaffeine提供了三种缓存驱逐策略：\n\n * 基于容量：设置缓存的数量上限\n   \n   // 创建缓存对象\n   Cache<String, String> cache = Caffeine.newBuilder()\n       .maximumSize(1) // 设置缓存大小上限为 1\n       .build();\n   \n\n * 基于时间：设置缓存的有效时间\n   \n   // 创建缓存对象\n   Cache<String, String> cache = Caffeine.newBuilder()\n       // 设置缓存有效期为 10 秒，从最后一次写入开始计时 \n       .expireAfterWrite(Duration.ofSeconds(10)) \n       .build();\n   \n   \n\n * 基于引用：设置缓存为软引用或弱引用，利用GC来回收缓存数据。性能较差，不建议使用。\n\n> 注意：在默认情况下，当一个缓存元素过期的时候，Caffeine不会自动立即将其清理和驱逐。而是在一次读或写操作后，或者在空闲时间完成对失效数据的驱逐。\n\n\n# 实现JVM进程缓存\n\n\n# 需求\n\n利用Caffeine实现下列需求：\n\n * 给根据id查询商品的业务添加缓存，缓存未命中时查询数据库\n * 给根据id查询商品库存的业务添加缓存，缓存未命中时查询数据库\n * 缓存初始大小为100\n * 缓存上限为10000\n\n\n# 实现\n\n首先，我们需要定义两个Caffeine的缓存对象，分别保存商品、库存的缓存数据。\n\n在item-service的com.heima.item.config包下定义CaffeineConfig类：\n\npackage com.heima.item.config;\n\nimport com.github.benmanes.caffeine.cache.Cache;\nimport com.github.benmanes.caffeine.cache.Caffeine;\nimport com.heima.item.pojo.Item;\nimport com.heima.item.pojo.ItemStock;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\n@Configuration\npublic class CaffeineConfig {\n\n    @Bean\n    public Cache<Long, Item> itemCache(){\n        return Caffeine.newBuilder()\n                .initialCapacity(100)\n                .maximumSize(10_000)\n                .build();\n    }\n\n    @Bean\n    public Cache<Long, ItemStock> stockCache(){\n        return Caffeine.newBuilder()\n                .initialCapacity(100)\n                .maximumSize(10_000)\n                .build();\n    }\n}\n\n\n然后，修改item-service中的com.heima.item.web包下的ItemController类，添加缓存逻辑：\n\n@RestController\n@RequestMapping("item")\npublic class ItemController {\n\n    @Autowired\n    private IItemService itemService;\n    @Autowired\n    private IItemStockService stockService;\n\n    @Autowired\n    private Cache<Long, Item> itemCache;\n    @Autowired\n    private Cache<Long, ItemStock> stockCache;\n    \n    // ...其它略\n    \n    @GetMapping("/{id}")\n    public Item findById(@PathVariable("id") Long id) {\n        return itemCache.get(id, key -> itemService.query()\n                .ne("status", 3).eq("id", key)\n                .one()\n        );\n    }\n\n    @GetMapping("/stock/{id}")\n    public ItemStock findStockById(@PathVariable("id") Long id) {\n        return stockCache.get(id, key -> stockService.getById(key));\n    }\n}\n\n\n',normalizedContent:'为了演示多级缓存的案例，我们先准备一个商品查询的业务。\n\n\n# 初识caffeine\n\n缓存在日常开发中启动至关重要的作用，由于是存储在内存中，数据的读取速度是非常快的，能大量减少对数据库的访问，减少数据库的压力。我们把缓存分为两类：\n\n * 分布式缓存，例如redis：\n   * 优点：存储容量更大、可靠性更好、可以在集群间共享\n   * 缺点：访问缓存有网络开销\n   * 场景：缓存数据量较大、可靠性要求较高、需要在集群间共享\n * 进程本地缓存，例如hashmap、guavacache：\n   * 优点：读取本地内存，没有网络开销，速度更快\n   * 缺点：存储容量有限、可靠性较低、无法共享\n   * 场景：性能要求较高，缓存数据量较小\n\n我们今天会利用caffeine框架来实现jvm进程缓存。\n\ncaffeine是一个基于java8开发的，提供了近乎最佳命中率的高性能的本地缓存库。目前spring内部的缓存使用的就是caffeine。github地址：https://github.com/ben-manes/caffeine\n\ncaffeine的性能非常好，下图是官方给出的性能对比：\n\n\n\n可以看到caffeine的性能遥遥领先！\n\n缓存使用的基本api：\n\n@test\nvoid testbasicops() {\n    // 构建cache对象\n    cache<string, string> cache = caffeine.newbuilder().build();\n\n    // 存数据\n    cache.put("gf", "迪丽热巴");\n\n    // 取数据\n    string gf = cache.getifpresent("gf");\n    system.out.println("gf = " + gf);\n\n    // 取数据，包含两个参数：\n    // 参数一：缓存的key\n    // 参数二：lambda表达式，表达式参数就是缓存的key，方法体是查询数据库的逻辑\n    // 优先根据key查询jvm缓存，如果未命中，则执行参数二的lambda表达式\n    string defaultgf = cache.get("defaultgf", key -> {\n        // 根据key去数据库查询数据\n        return "柳岩";\n    });\n    system.out.println("defaultgf = " + defaultgf);\n}\n\n\ncaffeine既然是缓存的一种，肯定需要有缓存的清除策略，不然的话内存总会有耗尽的时候。\n\ncaffeine提供了三种缓存驱逐策略：\n\n * 基于容量：设置缓存的数量上限\n   \n   // 创建缓存对象\n   cache<string, string> cache = caffeine.newbuilder()\n       .maximumsize(1) // 设置缓存大小上限为 1\n       .build();\n   \n\n * 基于时间：设置缓存的有效时间\n   \n   // 创建缓存对象\n   cache<string, string> cache = caffeine.newbuilder()\n       // 设置缓存有效期为 10 秒，从最后一次写入开始计时 \n       .expireafterwrite(duration.ofseconds(10)) \n       .build();\n   \n   \n\n * 基于引用：设置缓存为软引用或弱引用，利用gc来回收缓存数据。性能较差，不建议使用。\n\n> 注意：在默认情况下，当一个缓存元素过期的时候，caffeine不会自动立即将其清理和驱逐。而是在一次读或写操作后，或者在空闲时间完成对失效数据的驱逐。\n\n\n# 实现jvm进程缓存\n\n\n# 需求\n\n利用caffeine实现下列需求：\n\n * 给根据id查询商品的业务添加缓存，缓存未命中时查询数据库\n * 给根据id查询商品库存的业务添加缓存，缓存未命中时查询数据库\n * 缓存初始大小为100\n * 缓存上限为10000\n\n\n# 实现\n\n首先，我们需要定义两个caffeine的缓存对象，分别保存商品、库存的缓存数据。\n\n在item-service的com.heima.item.config包下定义caffeineconfig类：\n\npackage com.heima.item.config;\n\nimport com.github.benmanes.caffeine.cache.cache;\nimport com.github.benmanes.caffeine.cache.caffeine;\nimport com.heima.item.pojo.item;\nimport com.heima.item.pojo.itemstock;\nimport org.springframework.context.annotation.bean;\nimport org.springframework.context.annotation.configuration;\n\n@configuration\npublic class caffeineconfig {\n\n    @bean\n    public cache<long, item> itemcache(){\n        return caffeine.newbuilder()\n                .initialcapacity(100)\n                .maximumsize(10_000)\n                .build();\n    }\n\n    @bean\n    public cache<long, itemstock> stockcache(){\n        return caffeine.newbuilder()\n                .initialcapacity(100)\n                .maximumsize(10_000)\n                .build();\n    }\n}\n\n\n然后，修改item-service中的com.heima.item.web包下的itemcontroller类，添加缓存逻辑：\n\n@restcontroller\n@requestmapping("item")\npublic class itemcontroller {\n\n    @autowired\n    private iitemservice itemservice;\n    @autowired\n    private iitemstockservice stockservice;\n\n    @autowired\n    private cache<long, item> itemcache;\n    @autowired\n    private cache<long, itemstock> stockcache;\n    \n    // ...其它略\n    \n    @getmapping("/{id}")\n    public item findbyid(@pathvariable("id") long id) {\n        return itemcache.get(id, key -> itemservice.query()\n                .ne("status", 3).eq("id", key)\n                .one()\n        );\n    }\n\n    @getmapping("/stock/{id}")\n    public itemstock findstockbyid(@pathvariable("id") long id) {\n        return stockcache.get(id, key -> stockservice.getbyid(key));\n    }\n}\n\n\n',charsets:{cjk:!0}},{title:"Gateway服务网关",frontmatter:{autoSort:93,title:"Gateway服务网关",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/89fc28/",categories:["后端","微服务","SpringCloud"],tags:["知识","微服务","SpringCloud"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/40.SpringCloud/70.Gateway%E6%9C%8D%E5%8A%A1%E7%BD%91%E5%85%B3.html",relativePath:"01.后端/60.微服务/40.SpringCloud/70.Gateway服务网关.md",key:"v-3ba4cd3c",path:"/pages/89fc28/",headers:[{level:2,title:"为什么需要网关",slug:"为什么需要网关",normalizedTitle:"为什么需要网关",charIndex:152},{level:2,title:"gateway快速入门",slug:"gateway快速入门",normalizedTitle:"gateway快速入门",charIndex:565},{level:3,title:"1）创建gateway服务，引入依赖",slug:"_1-创建gateway服务-引入依赖",normalizedTitle:"1）创建gateway服务，引入依赖",charIndex:684},{level:3,title:"2）编写启动类",slug:"_2-编写启动类",normalizedTitle:"2）编写启动类",charIndex:1030},{level:3,title:"3）编写基础配置和路由规则",slug:"_3-编写基础配置和路由规则",normalizedTitle:"3）编写基础配置和路由规则",charIndex:1355},{level:3,title:"4）重启测试",slug:"_4-重启测试",normalizedTitle:"4）重启测试",charIndex:1955},{level:3,title:"5）网关路由的流程图",slug:"_5-网关路由的流程图",normalizedTitle:"5）网关路由的流程图",charIndex:2062},{level:2,title:"断言工厂",slug:"断言工厂",normalizedTitle:"断言工厂",charIndex:2343},{level:2,title:"过滤器工厂",slug:"过滤器工厂",normalizedTitle:"过滤器工厂",charIndex:3444},{level:3,title:"3.4.1.路由过滤器的种类",slug:"_3-4-1-路由过滤器的种类",normalizedTitle:"3.4.1.路由过滤器的种类",charIndex:3507},{level:3,title:"请求头过滤器",slug:"请求头过滤器",normalizedTitle:"请求头过滤器",charIndex:3758},{level:3,title:"默认过滤器",slug:"默认过滤器",normalizedTitle:"默认过滤器",charIndex:4200},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:2088},{level:2,title:"全局过滤器",slug:"全局过滤器",normalizedTitle:"全局过滤器",charIndex:4599},{level:3,title:"全局过滤器作用",slug:"全局过滤器作用",normalizedTitle:"全局过滤器作用",charIndex:4672},{level:3,title:"自定义全局过滤器",slug:"自定义全局过滤器",normalizedTitle:"自定义全局过滤器",charIndex:5209},{level:3,title:"过滤器执行顺序",slug:"过滤器执行顺序",normalizedTitle:"过滤器执行顺序",charIndex:6439},{level:2,title:"跨域问题",slug:"跨域问题",normalizedTitle:"跨域问题",charIndex:7207},{level:3,title:"什么是跨域问题",slug:"什么是跨域问题",normalizedTitle:"什么是跨域问题",charIndex:7216},{level:3,title:"模拟跨域问题",slug:"模拟跨域问题",normalizedTitle:"模拟跨域问题",charIndex:7497},{level:3,title:"解决跨域问题",slug:"解决跨域问题",normalizedTitle:"解决跨域问题",charIndex:7628}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"为什么需要网关 gateway快速入门 1）创建gateway服务，引入依赖 2）编写启动类 3）编写基础配置和路由规则 4）重启测试 5）网关路由的流程图 断言工厂 过滤器工厂 3.4.1.路由过滤器的种类 请求头过滤器 默认过滤器 总结 全局过滤器 全局过滤器作用 自定义全局过滤器 过滤器执行顺序 跨域问题 什么是跨域问题 模拟跨域问题 解决跨域问题",content:'Spring Cloud Gateway 是 Spring Cloud 的一个全新项目，该项目是基于 Spring 5.0，Spring Boot 2.0 和 Project Reactor 等响应式编程和事件流技术开发的网关，它旨在为微服务架构提供一种简单有效的统一的 API 路由管理方式。\n\n\n# 为什么需要网关\n\nGateway网关是我们服务的守门神，所有微服务的统一入口。\n\n网关的核心功能特性：\n\n * 请求路由、负载均衡\n * 权限控制、身份认证\n * 请求限流\n\n架构图：\n\n\n\n权限控制：网关作为微服务入口，需要校验用户是是否有请求资格，如果没有则进行拦截。\n\n路由和负载均衡：一切请求都必须先经过gateway，但网关不处理业务，而是根据某种规则，把请求转发到某个微服务，这个过程叫做路由。当然路由的目标服务有多个时，还需要做负载均衡。\n\n限流：当请求流量过高时，在网关中按照下流的微服务能够接受的速度来放行请求，避免服务压力过大。\n\n在SpringCloud中网关的实现包括两种：\n\n * gateway\n * zuul\n\nZuul是基于Servlet的实现，属于阻塞式编程。而SpringCloudGateway则是基于Spring5中提供的WebFlux，属于响应式编程的实现，具备更好的性能。\n\n\n# gateway快速入门\n\n下面，我们就演示下网关的基本路由功能。基本步骤如下：\n\n 1. 创建SpringBoot工程gateway，引入网关依赖\n 2. 编写启动类\n 3. 编写基础配置和路由规则\n 4. 启动网关服务进行测试\n\n\n# 1）创建gateway服务，引入依赖\n\n创建服务：\n\n\n\n引入依赖：\n\n\x3c!--网关--\x3e\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-gateway</artifactId>\n</dependency>\n\x3c!--nacos服务发现依赖--\x3e\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n\n\n\n# 2）编写启动类\n\npackage cn.itcast.gateway;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n\n@SpringBootApplication\npublic class GatewayApplication {\n\n\tpublic static void main(String[] args) {\n\t\tSpringApplication.run(GatewayApplication.class, args);\n\t}\n}\n\n\n\n# 3）编写基础配置和路由规则\n\n创建application.yml文件，内容如下：\n\nserver:\n  port: 10010 # 网关端口\nspring:\n  application:\n    name: gateway # 服务名称\n  cloud:\n    nacos:\n      server-addr: localhost:8848 # nacos地址\n    gateway:\n      routes: # 网关路由配置\n        - id: user-service # 路由id，自定义，只要唯一即可\n          # uri: http://127.0.0.1:8081 # 路由的目标地址 http就是固定地址\n          uri: lb://userservice # 路由的目标地址 lb就是负载均衡，后面跟服务名称\n          predicates: # 路由断言，也就是判断请求是否符合路由规则的条件\n            - Path=/user/** # 这个是按照路径匹配，只要以/user/开头就符合要求\n\n\n我们将符合Path 规则的一切请求，都代理到 uri参数指定的地址。\n\n本例中，我们将 /user/**开头的请求，代理到lb://userservice，lb是负载均衡，根据服务名拉取服务列表，实现负载均衡。\n\n\n# 4）重启测试\n\n重启网关，访问http://localhost:10010/user/1时，符合/user/**规则，请求转发到uri：http://userservice/user/1，得到了结果：\n\n\n\n\n# 5）网关路由的流程图\n\n整个访问的流程如下：\n\n\n\n总结：\n\n网关搭建步骤：\n\n 1. 创建项目，引入nacos服务发现和gateway依赖\n\n 2. 配置application.yml，包括服务基本信息、nacos地址、路由\n\n路由配置包括：\n\n 1. 路由id：路由的唯一标示\n\n 2. 路由目标（uri）：路由的目标地址，http代表固定地址，lb代表根据服务名负载均衡\n\n 3. 路由断言（predicates）：判断路由的规则，\n\n 4. 路由过滤器（filters）：对请求或响应做处理\n\n接下来，就重点来学习路由断言和路由过滤器的详细知识\n\n\n# 断言工厂\n\n断言工厂官方文档\n\n我们在配置文件中写的断言规则只是字符串，这些字符串会被Predicate Factory读取并处理，转变为路由判断的条件\n\n例如Path=/user/**是按照路径匹配，这个规则是由\n\norg.springframework.cloud.gateway.handler.predicate.PathRoutePredicateFactory类来\n\n处理的，像这样的断言工厂在SpringCloudGateway还有十几个:\n\n名称           说明                  示例\nAfter        是某个时间点后的请求          - After=2037-01-20T17:42:47.789-07:00[America/Denver]\nBefore       是某个时间点之前的请求         - Before=2031-04-13T15:14:47.433+08:00[Asia/Shanghai]\nBetween      是某两个时间点之前的请求        - Between=2037-01-20T17:42:47.789-07:00[America/Denver],\n                                 2037-01-21T17:42:47.789-07:00[America/Denver]\nCookie       请求必须包含某些cookie      - Cookie=chocolate, ch.p\nHeader       请求必须包含某些header      - Header=X-Request-Id, \\d+\nHost         请求必须是访问某个host（域名）   - Host=.somehost.org,.anotherhost.org\nMethod       请求方式必须是指定方式         - Method=GET,POST\nPath         请求路径必须符合指定规则        - Path=/red/{segment},/blue/**\nQuery        请求参数必须包含指定参数        - Query=name, Jack或者- Query=name\nRemoteAddr   请求者的ip必须是指定范围       - RemoteAddr=192.168.1.1/24\nWeight       权重处理                \n\n我们只需要掌握Path这种路由工程就可以了。\n\n\n# 过滤器工厂\n\nGatewayFilter是网关中提供的一种过滤器，可以对进入网关的请求和微服务返回的响应做处理：\n\n\n\n\n# 3.4.1.路由过滤器的种类\n\nSpring提供了31种不同的路由过滤器工厂。例如：\n\n名称                     说明\nAddRequestHeader       给当前请求添加一个请求头\nRemoveRequestHeader    移除请求中的一个请求头\nAddResponseHeader      给响应结果中添加一个响应头\nRemoveResponseHeader   从响应结果中移除有一个响应头\nRequestRateLimiter     限制请求的流量\n\n\n# 请求头过滤器\n\n下面我们以AddRequestHeader 为例来讲解。\n\n> 需求：给所有进入userservice的请求添加一个请求头：Truth=itcast is freaking awesome!\n\n只需要修改gateway服务的application.yml文件，添加路由过滤即可：\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: user-service \n        uri: lb://userservice \n        predicates: \n        - Path=/user/** \n        filters: # 过滤器\n        - AddRequestHeader=Truth, Itcast is freaking awesome! # 添加请求头\n\n\n当前过滤器写在userservice路由下，因此仅仅对访问userservice的请求有效。\n\n\n# 默认过滤器\n\n如果要对所有的路由都生效，则可以将过滤器工厂写到default下。格式如下：\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: user-service \n        uri: lb://userservice \n        predicates: \n        - Path=/user/**\n      default-filters: # 默认过滤项\n      - AddRequestHeader=Truth, Itcast is freaking awesome! \n\n\n\n# 总结\n\n过滤器的作用是什么？\n\n① 对路由的请求或响应做加工处理，比如添加请求头\n\n② 配置在路由下的过滤器只对当前路由的请求生效\n\ndefaultFilters的作用是什么？\n\n① 对所有路由都生效的过滤器\n\n\n# 全局过滤器\n\n上一节学习的过滤器，网关提供了31种，但每一种过滤器的作用都是固定的。如果我们希望拦截请求，做自己的业务逻辑则没办法实现。\n\n\n# 全局过滤器作用\n\n全局过滤器的作用也是处理一切进入网关的请求和微服务响应，与GatewayFilter的作用一样。区别在于GatewayFilter通过配置定义，处理逻辑是固定的；而GlobalFilter的逻辑需要自己写代码实现。\n\n定义方式是实现GlobalFilter接口。\n\npublic interface GlobalFilter {\n    /**\n     *  处理当前请求，有必要的话通过{@link GatewayFilterChain}将请求交给下一个过滤器处理\n     *\n     * @param exchange 请求上下文，里面可以获取Request、Response等信息\n     * @param chain 用来把请求委托给下一个过滤器 \n     * @return {@code Mono<Void>} 返回标示当前过滤器业务结束\n     */\n    Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain);\n}\n\n\n在filter中编写自定义逻辑，可以实现下列功能：\n\n * 登录状态判断\n * 权限校验\n * 请求限流等\n\n\n# 自定义全局过滤器\n\n需求：定义全局过滤器，拦截请求，判断请求的参数是否满足下面条件：\n\n * 参数中是否有authorization，\n\n * authorization参数值是否为admin\n\n如果同时满足则放行，否则拦截\n\n实现：\n\n在gateway中定义一个过滤器：\n\npackage cn.itcast.gateway.filters;\n\nimport org.springframework.cloud.gateway.filter.GatewayFilterChain;\nimport org.springframework.cloud.gateway.filter.GlobalFilter;\nimport org.springframework.core.annotation.Order;\nimport org.springframework.http.HttpStatus;\nimport org.springframework.stereotype.Component;\nimport org.springframework.web.server.ServerWebExchange;\nimport reactor.core.publisher.Mono;\n\n@Order(-1)\n@Component\npublic class AuthorizeFilter implements GlobalFilter {\n    @Override\n    public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {\n        // 1.获取请求参数\n        MultiValueMap<String, String> params = exchange.getRequest().getQueryParams();\n        // 2.获取authorization参数\n        String auth = params.getFirst("authorization");\n        // 3.校验\n        if ("admin".equals(auth)) {\n            // 放行\n            return chain.filter(exchange);\n        }\n        // 4.拦截\n        // 4.1.禁止访问，设置状态码\n        exchange.getResponse().setStatusCode(HttpStatus.FORBIDDEN);\n        // 4.2.结束处理\n        return exchange.getResponse().setComplete();\n    }\n}\n\n\n\n# 过滤器执行顺序\n\n请求进入网关会碰到三类过滤器：当前路由的过滤器、DefaultFilter、GlobalFilter\n\n请求路由后，会将当前路由过滤器和DefaultFilter、GlobalFilter，合并到一个过滤器链（集合）中，排序后依次执行每个过滤器：\n\n> 路由过滤器和DefaultFilter过滤器是一类过滤器，只不过是作用范围不一样，都是GatewayFilter\n> \n> 而GlobalFilter，在网关内部，会被一个过滤器适配器（GatewayFilterAdapter）转换成GatewayFilter\n\n\n\n排序的规则是什么呢？\n\n * 每一个过滤器都必须指定一个int类型的order值，order值越小，优先级越高，执行顺序越靠前。\n * GlobalFilter通过实现Ordered接口，或者添加@Order注解来指定order值，由我们自己指定\n * 路由过滤器和defaultFilter的order由Spring指定，默认是按照声明顺序从1递增。\n * 当过滤器的order值一样时，会按照 defaultFilter > 路由过滤器 > GlobalFilter的顺序执行。\n\n详细内容，可以查看源码：\n\norg.springframework.cloud.gateway.route.RouteDefinitionRouteLocator#getFilters()方法是先加载defaultFilters，然后再加载某个route的filters，然后合并。\n\norg.springframework.cloud.gateway.handler.FilteringWebHandler#handle()方法会加载全局过滤器，与前面的过滤器合并后根据order排序，组织过滤器链\n\n\n# 跨域问题\n\n\n# 什么是跨域问题\n\n跨域：域名不一致就是跨域，主要包括：\n\n * 域名不同： www.taobao.com 和 www.taobao.org 和 www.jd.com 和 miaosha.jd.com\n\n * 域名相同，端口不同：localhost:8080和localhost8081\n\n跨域问题：浏览器禁止请求的发起者与服务端发生跨域ajax请求，请求被浏览器拦截的问题\n\n解决方案：CORS，这个以前应该学习过，这里不再赘述了。不知道的小伙伴可以查看https://www.ruanyifeng.com/blog/2016/04/cors.html\n\n\n# 模拟跨域问题\n\n找到课前资料的页面文件：\n\n\n\n放入tomcat或者nginx这样的web服务器中，启动并访问。\n\n可以在浏览器控制台看到下面的错误：\n\n\n\n从localhost:8090访问localhost:10010，端口不同，显然是跨域的请求。\n\n\n# 解决跨域问题\n\n在gateway服务的application.yml文件中，添加下面的配置：\n\nspring:\n  cloud:\n    gateway:\n      # 。。。\n      globalcors: # 全局的跨域处理\n        add-to-simple-url-handler-mapping: true # 解决options请求被拦截问题\n        corsConfigurations:\n          \'[/**]\':\n            allowedOrigins: # 允许哪些网站的跨域请求 \n              - "http://localhost:8090"\n            allowedMethods: # 允许的跨域ajax的请求方式\n              - "GET"\n              - "POST"\n              - "DELETE"\n              - "PUT"\n              - "OPTIONS"\n            allowedHeaders: "*" # 允许在请求中携带的头信息\n            allowCredentials: true # 是否允许携带cookie\n            maxAge: 360000 # 这次跨域检测的有效期\n',normalizedContent:'spring cloud gateway 是 spring cloud 的一个全新项目，该项目是基于 spring 5.0，spring boot 2.0 和 project reactor 等响应式编程和事件流技术开发的网关，它旨在为微服务架构提供一种简单有效的统一的 api 路由管理方式。\n\n\n# 为什么需要网关\n\ngateway网关是我们服务的守门神，所有微服务的统一入口。\n\n网关的核心功能特性：\n\n * 请求路由、负载均衡\n * 权限控制、身份认证\n * 请求限流\n\n架构图：\n\n\n\n权限控制：网关作为微服务入口，需要校验用户是是否有请求资格，如果没有则进行拦截。\n\n路由和负载均衡：一切请求都必须先经过gateway，但网关不处理业务，而是根据某种规则，把请求转发到某个微服务，这个过程叫做路由。当然路由的目标服务有多个时，还需要做负载均衡。\n\n限流：当请求流量过高时，在网关中按照下流的微服务能够接受的速度来放行请求，避免服务压力过大。\n\n在springcloud中网关的实现包括两种：\n\n * gateway\n * zuul\n\nzuul是基于servlet的实现，属于阻塞式编程。而springcloudgateway则是基于spring5中提供的webflux，属于响应式编程的实现，具备更好的性能。\n\n\n# gateway快速入门\n\n下面，我们就演示下网关的基本路由功能。基本步骤如下：\n\n 1. 创建springboot工程gateway，引入网关依赖\n 2. 编写启动类\n 3. 编写基础配置和路由规则\n 4. 启动网关服务进行测试\n\n\n# 1）创建gateway服务，引入依赖\n\n创建服务：\n\n\n\n引入依赖：\n\n\x3c!--网关--\x3e\n<dependency>\n    <groupid>org.springframework.cloud</groupid>\n    <artifactid>spring-cloud-starter-gateway</artifactid>\n</dependency>\n\x3c!--nacos服务发现依赖--\x3e\n<dependency>\n    <groupid>com.alibaba.cloud</groupid>\n    <artifactid>spring-cloud-starter-alibaba-nacos-discovery</artifactid>\n</dependency>\n\n\n\n# 2）编写启动类\n\npackage cn.itcast.gateway;\n\nimport org.springframework.boot.springapplication;\nimport org.springframework.boot.autoconfigure.springbootapplication;\n\n@springbootapplication\npublic class gatewayapplication {\n\n\tpublic static void main(string[] args) {\n\t\tspringapplication.run(gatewayapplication.class, args);\n\t}\n}\n\n\n\n# 3）编写基础配置和路由规则\n\n创建application.yml文件，内容如下：\n\nserver:\n  port: 10010 # 网关端口\nspring:\n  application:\n    name: gateway # 服务名称\n  cloud:\n    nacos:\n      server-addr: localhost:8848 # nacos地址\n    gateway:\n      routes: # 网关路由配置\n        - id: user-service # 路由id，自定义，只要唯一即可\n          # uri: http://127.0.0.1:8081 # 路由的目标地址 http就是固定地址\n          uri: lb://userservice # 路由的目标地址 lb就是负载均衡，后面跟服务名称\n          predicates: # 路由断言，也就是判断请求是否符合路由规则的条件\n            - path=/user/** # 这个是按照路径匹配，只要以/user/开头就符合要求\n\n\n我们将符合path 规则的一切请求，都代理到 uri参数指定的地址。\n\n本例中，我们将 /user/**开头的请求，代理到lb://userservice，lb是负载均衡，根据服务名拉取服务列表，实现负载均衡。\n\n\n# 4）重启测试\n\n重启网关，访问http://localhost:10010/user/1时，符合/user/**规则，请求转发到uri：http://userservice/user/1，得到了结果：\n\n\n\n\n# 5）网关路由的流程图\n\n整个访问的流程如下：\n\n\n\n总结：\n\n网关搭建步骤：\n\n 1. 创建项目，引入nacos服务发现和gateway依赖\n\n 2. 配置application.yml，包括服务基本信息、nacos地址、路由\n\n路由配置包括：\n\n 1. 路由id：路由的唯一标示\n\n 2. 路由目标（uri）：路由的目标地址，http代表固定地址，lb代表根据服务名负载均衡\n\n 3. 路由断言（predicates）：判断路由的规则，\n\n 4. 路由过滤器（filters）：对请求或响应做处理\n\n接下来，就重点来学习路由断言和路由过滤器的详细知识\n\n\n# 断言工厂\n\n断言工厂官方文档\n\n我们在配置文件中写的断言规则只是字符串，这些字符串会被predicate factory读取并处理，转变为路由判断的条件\n\n例如path=/user/**是按照路径匹配，这个规则是由\n\norg.springframework.cloud.gateway.handler.predicate.pathroutepredicatefactory类来\n\n处理的，像这样的断言工厂在springcloudgateway还有十几个:\n\n名称           说明                  示例\nafter        是某个时间点后的请求          - after=2037-01-20t17:42:47.789-07:00[america/denver]\nbefore       是某个时间点之前的请求         - before=2031-04-13t15:14:47.433+08:00[asia/shanghai]\nbetween      是某两个时间点之前的请求        - between=2037-01-20t17:42:47.789-07:00[america/denver],\n                                 2037-01-21t17:42:47.789-07:00[america/denver]\ncookie       请求必须包含某些cookie      - cookie=chocolate, ch.p\nheader       请求必须包含某些header      - header=x-request-id, \\d+\nhost         请求必须是访问某个host（域名）   - host=.somehost.org,.anotherhost.org\nmethod       请求方式必须是指定方式         - method=get,post\npath         请求路径必须符合指定规则        - path=/red/{segment},/blue/**\nquery        请求参数必须包含指定参数        - query=name, jack或者- query=name\nremoteaddr   请求者的ip必须是指定范围       - remoteaddr=192.168.1.1/24\nweight       权重处理                \n\n我们只需要掌握path这种路由工程就可以了。\n\n\n# 过滤器工厂\n\ngatewayfilter是网关中提供的一种过滤器，可以对进入网关的请求和微服务返回的响应做处理：\n\n\n\n\n# 3.4.1.路由过滤器的种类\n\nspring提供了31种不同的路由过滤器工厂。例如：\n\n名称                     说明\naddrequestheader       给当前请求添加一个请求头\nremoverequestheader    移除请求中的一个请求头\naddresponseheader      给响应结果中添加一个响应头\nremoveresponseheader   从响应结果中移除有一个响应头\nrequestratelimiter     限制请求的流量\n\n\n# 请求头过滤器\n\n下面我们以addrequestheader 为例来讲解。\n\n> 需求：给所有进入userservice的请求添加一个请求头：truth=itcast is freaking awesome!\n\n只需要修改gateway服务的application.yml文件，添加路由过滤即可：\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: user-service \n        uri: lb://userservice \n        predicates: \n        - path=/user/** \n        filters: # 过滤器\n        - addrequestheader=truth, itcast is freaking awesome! # 添加请求头\n\n\n当前过滤器写在userservice路由下，因此仅仅对访问userservice的请求有效。\n\n\n# 默认过滤器\n\n如果要对所有的路由都生效，则可以将过滤器工厂写到default下。格式如下：\n\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: user-service \n        uri: lb://userservice \n        predicates: \n        - path=/user/**\n      default-filters: # 默认过滤项\n      - addrequestheader=truth, itcast is freaking awesome! \n\n\n\n# 总结\n\n过滤器的作用是什么？\n\n① 对路由的请求或响应做加工处理，比如添加请求头\n\n② 配置在路由下的过滤器只对当前路由的请求生效\n\ndefaultfilters的作用是什么？\n\n① 对所有路由都生效的过滤器\n\n\n# 全局过滤器\n\n上一节学习的过滤器，网关提供了31种，但每一种过滤器的作用都是固定的。如果我们希望拦截请求，做自己的业务逻辑则没办法实现。\n\n\n# 全局过滤器作用\n\n全局过滤器的作用也是处理一切进入网关的请求和微服务响应，与gatewayfilter的作用一样。区别在于gatewayfilter通过配置定义，处理逻辑是固定的；而globalfilter的逻辑需要自己写代码实现。\n\n定义方式是实现globalfilter接口。\n\npublic interface globalfilter {\n    /**\n     *  处理当前请求，有必要的话通过{@link gatewayfilterchain}将请求交给下一个过滤器处理\n     *\n     * @param exchange 请求上下文，里面可以获取request、response等信息\n     * @param chain 用来把请求委托给下一个过滤器 \n     * @return {@code mono<void>} 返回标示当前过滤器业务结束\n     */\n    mono<void> filter(serverwebexchange exchange, gatewayfilterchain chain);\n}\n\n\n在filter中编写自定义逻辑，可以实现下列功能：\n\n * 登录状态判断\n * 权限校验\n * 请求限流等\n\n\n# 自定义全局过滤器\n\n需求：定义全局过滤器，拦截请求，判断请求的参数是否满足下面条件：\n\n * 参数中是否有authorization，\n\n * authorization参数值是否为admin\n\n如果同时满足则放行，否则拦截\n\n实现：\n\n在gateway中定义一个过滤器：\n\npackage cn.itcast.gateway.filters;\n\nimport org.springframework.cloud.gateway.filter.gatewayfilterchain;\nimport org.springframework.cloud.gateway.filter.globalfilter;\nimport org.springframework.core.annotation.order;\nimport org.springframework.http.httpstatus;\nimport org.springframework.stereotype.component;\nimport org.springframework.web.server.serverwebexchange;\nimport reactor.core.publisher.mono;\n\n@order(-1)\n@component\npublic class authorizefilter implements globalfilter {\n    @override\n    public mono<void> filter(serverwebexchange exchange, gatewayfilterchain chain) {\n        // 1.获取请求参数\n        multivaluemap<string, string> params = exchange.getrequest().getqueryparams();\n        // 2.获取authorization参数\n        string auth = params.getfirst("authorization");\n        // 3.校验\n        if ("admin".equals(auth)) {\n            // 放行\n            return chain.filter(exchange);\n        }\n        // 4.拦截\n        // 4.1.禁止访问，设置状态码\n        exchange.getresponse().setstatuscode(httpstatus.forbidden);\n        // 4.2.结束处理\n        return exchange.getresponse().setcomplete();\n    }\n}\n\n\n\n# 过滤器执行顺序\n\n请求进入网关会碰到三类过滤器：当前路由的过滤器、defaultfilter、globalfilter\n\n请求路由后，会将当前路由过滤器和defaultfilter、globalfilter，合并到一个过滤器链（集合）中，排序后依次执行每个过滤器：\n\n> 路由过滤器和defaultfilter过滤器是一类过滤器，只不过是作用范围不一样，都是gatewayfilter\n> \n> 而globalfilter，在网关内部，会被一个过滤器适配器（gatewayfilteradapter）转换成gatewayfilter\n\n\n\n排序的规则是什么呢？\n\n * 每一个过滤器都必须指定一个int类型的order值，order值越小，优先级越高，执行顺序越靠前。\n * globalfilter通过实现ordered接口，或者添加@order注解来指定order值，由我们自己指定\n * 路由过滤器和defaultfilter的order由spring指定，默认是按照声明顺序从1递增。\n * 当过滤器的order值一样时，会按照 defaultfilter > 路由过滤器 > globalfilter的顺序执行。\n\n详细内容，可以查看源码：\n\norg.springframework.cloud.gateway.route.routedefinitionroutelocator#getfilters()方法是先加载defaultfilters，然后再加载某个route的filters，然后合并。\n\norg.springframework.cloud.gateway.handler.filteringwebhandler#handle()方法会加载全局过滤器，与前面的过滤器合并后根据order排序，组织过滤器链\n\n\n# 跨域问题\n\n\n# 什么是跨域问题\n\n跨域：域名不一致就是跨域，主要包括：\n\n * 域名不同： www.taobao.com 和 www.taobao.org 和 www.jd.com 和 miaosha.jd.com\n\n * 域名相同，端口不同：localhost:8080和localhost8081\n\n跨域问题：浏览器禁止请求的发起者与服务端发生跨域ajax请求，请求被浏览器拦截的问题\n\n解决方案：cors，这个以前应该学习过，这里不再赘述了。不知道的小伙伴可以查看https://www.ruanyifeng.com/blog/2016/04/cors.html\n\n\n# 模拟跨域问题\n\n找到课前资料的页面文件：\n\n\n\n放入tomcat或者nginx这样的web服务器中，启动并访问。\n\n可以在浏览器控制台看到下面的错误：\n\n\n\n从localhost:8090访问localhost:10010，端口不同，显然是跨域的请求。\n\n\n# 解决跨域问题\n\n在gateway服务的application.yml文件中，添加下面的配置：\n\nspring:\n  cloud:\n    gateway:\n      # 。。。\n      globalcors: # 全局的跨域处理\n        add-to-simple-url-handler-mapping: true # 解决options请求被拦截问题\n        corsconfigurations:\n          \'[/**]\':\n            allowedorigins: # 允许哪些网站的跨域请求 \n              - "http://localhost:8090"\n            allowedmethods: # 允许的跨域ajax的请求方式\n              - "get"\n              - "post"\n              - "delete"\n              - "put"\n              - "options"\n            allowedheaders: "*" # 允许在请求中携带的头信息\n            allowcredentials: true # 是否允许携带cookie\n            maxage: 360000 # 这次跨域检测的有效期\n',charsets:{cjk:!0}},{title:"分布式事务理论基础",frontmatter:{autoSort:99,title:"分布式事务理论基础",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/8ffa15/",categories:["后端","微服务","进阶","分布式事务"],tags:["知识","微服务","事务"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/60.%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/10.%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80.html",relativePath:"01.后端/60.微服务/60.分布式事务/10.理论基础.md",key:"v-af4ce9ea",path:"/pages/8ffa15/",headers:[{level:2,title:"CAP定理",slug:"cap定理",normalizedTitle:"cap定理",charIndex:36},{level:3,title:"一致性",slug:"一致性",normalizedTitle:"一致性",charIndex:106},{level:3,title:"可用性",slug:"可用性",normalizedTitle:"可用性",charIndex:129},{level:3,title:"分区容错",slug:"分区容错",normalizedTitle:"分区容错",charIndex:160},{level:3,title:"矛盾",slug:"矛盾",normalizedTitle:"矛盾",charIndex:622},{level:2,title:"BASE理论",slug:"base理论",normalizedTitle:"base理论",charIndex:977},{level:2,title:"解决分布式事务的思路",slug:"解决分布式事务的思路",normalizedTitle:"解决分布式事务的思路",charIndex:1188}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"CAP定理 一致性 可用性 分区容错 矛盾 BASE理论 解决分布式事务的思路",content:"解决分布式事务问题，需要一些分布式系统的基础知识作为理论指导。\n\n\n# CAP定理\n\n1998年，加州大学的计算机科学家 Eric Brewer 提出，分布式系统有三个指标。\n\n>  * Consistency（一致性）\n>  * Availability（可用性）\n>  * Partition tolerance （分区容错性）\n\n\n\n它们的第一个字母分别是 C、A、P。\n\nEric Brewer 说，这三个指标不可能同时做到。这个结论就叫做 CAP 定理。\n\n\n# 一致性\n\nConsistency（一致性）：用户访问分布式系统中的任意节点，得到的数据必须一致。\n\n比如现在包含两个节点，其中的初始数据是一致的：\n\n\n\n当我们修改其中一个节点的数据时，两者的数据产生了差异：\n\n\n\n要想保住一致性，就必须实现node01 到 node02的数据 同步：\n\n\n\n\n# 可用性\n\nAvailability （可用性）：用户访问集群中的任意健康节点，必须能得到响应，而不是超时或拒绝。\n\n如图，有三个节点的集群，访问任何一个都可以及时得到响应：\n\n\n\n当有部分节点因为网络故障或其它原因无法访问时，代表节点不可用：\n\n\n\n\n# 分区容错\n\nPartition（分区）：因为网络故障或其它原因导致分布式系统中的部分节点与其它节点失去连接，形成独立分区。\n\n\n\nTolerance（容错）：在集群出现分区时，整个系统也要持续对外提供服务\n\n\n# 矛盾\n\n在分布式系统中，系统间的网络不能100%保证健康，一定会有故障的时候，而服务有必须对外保证服务。因此Partition Tolerance不可避免。\n\n当节点接收到新的数据变更时，就会出现问题了：\n\n\n\n如果此时要保证一致性-C，就必须等待网络恢复，完成数据同步后，整个集群才对外提供服务，服务处于阻塞状态，不可用。\n\n如果此时要保证可用性-A，就不能等待网络恢复，那node01、node02与node03之间就会出现数据不一致。\n\n也就是说，在P一定会出现的情况下，A和C之间只能实现一个。\n\n思考\n\nelasticsearch集群是CP还是AP？——CP\n\n * ES集群出现问题时，会剔除故障节点，数据分片会重新分配到其他节点，保证数据一致性\n * 因此是低可用性，高一致性。——CP\n\n\n# BASE理论\n\nBASE理论是对CAP的一种解决思路，包含三个思想：\n\n * Basically Available （基本可用）：分布式系统在出现故障时，允许损失部分可用性，即保证核心可用。\n * Soft State（软状态）：在一定时间内，允许出现中间状态，比如临时的不一致状态。\n * Eventually Consistent（最终一致性）：虽然无法保证强一致性，但是在软状态结束后，最终达到数据一致。\n\n\n# 解决分布式事务的思路\n\n分布式事务最大的问题是各个子事务的一致性问题，因此可以借鉴CAP定理和BASE理论，有两种解决思路：\n\n * 最终一致思想\n   \n   * AP模式：各子事务分别执行和提交，允许出现结果不一致，然后采用弥补措施恢复数据即可，实现最终一致。\n\n * 强一致思想\n   \n   * CP模式：各个子事务执行后互相等待，同时提交，同时回滚，达成强一致。但事务等待过程中，处于弱可用状态。\n\n但不管是哪一种模式，都需要在子系统事务之间互相通讯，协调事务状态，也就是需要一个事务协调者(TC)：\n\n\n\n这里的子系统事务，称为分支事务；\n\n有关联的各个分支事务在一起称为全局事务。",normalizedContent:"解决分布式事务问题，需要一些分布式系统的基础知识作为理论指导。\n\n\n# cap定理\n\n1998年，加州大学的计算机科学家 eric brewer 提出，分布式系统有三个指标。\n\n>  * consistency（一致性）\n>  * availability（可用性）\n>  * partition tolerance （分区容错性）\n\n\n\n它们的第一个字母分别是 c、a、p。\n\neric brewer 说，这三个指标不可能同时做到。这个结论就叫做 cap 定理。\n\n\n# 一致性\n\nconsistency（一致性）：用户访问分布式系统中的任意节点，得到的数据必须一致。\n\n比如现在包含两个节点，其中的初始数据是一致的：\n\n\n\n当我们修改其中一个节点的数据时，两者的数据产生了差异：\n\n\n\n要想保住一致性，就必须实现node01 到 node02的数据 同步：\n\n\n\n\n# 可用性\n\navailability （可用性）：用户访问集群中的任意健康节点，必须能得到响应，而不是超时或拒绝。\n\n如图，有三个节点的集群，访问任何一个都可以及时得到响应：\n\n\n\n当有部分节点因为网络故障或其它原因无法访问时，代表节点不可用：\n\n\n\n\n# 分区容错\n\npartition（分区）：因为网络故障或其它原因导致分布式系统中的部分节点与其它节点失去连接，形成独立分区。\n\n\n\ntolerance（容错）：在集群出现分区时，整个系统也要持续对外提供服务\n\n\n# 矛盾\n\n在分布式系统中，系统间的网络不能100%保证健康，一定会有故障的时候，而服务有必须对外保证服务。因此partition tolerance不可避免。\n\n当节点接收到新的数据变更时，就会出现问题了：\n\n\n\n如果此时要保证一致性-c，就必须等待网络恢复，完成数据同步后，整个集群才对外提供服务，服务处于阻塞状态，不可用。\n\n如果此时要保证可用性-a，就不能等待网络恢复，那node01、node02与node03之间就会出现数据不一致。\n\n也就是说，在p一定会出现的情况下，a和c之间只能实现一个。\n\n思考\n\nelasticsearch集群是cp还是ap？——cp\n\n * es集群出现问题时，会剔除故障节点，数据分片会重新分配到其他节点，保证数据一致性\n * 因此是低可用性，高一致性。——cp\n\n\n# base理论\n\nbase理论是对cap的一种解决思路，包含三个思想：\n\n * basically available （基本可用）：分布式系统在出现故障时，允许损失部分可用性，即保证核心可用。\n * soft state（软状态）：在一定时间内，允许出现中间状态，比如临时的不一致状态。\n * eventually consistent（最终一致性）：虽然无法保证强一致性，但是在软状态结束后，最终达到数据一致。\n\n\n# 解决分布式事务的思路\n\n分布式事务最大的问题是各个子事务的一致性问题，因此可以借鉴cap定理和base理论，有两种解决思路：\n\n * 最终一致思想\n   \n   * ap模式：各子事务分别执行和提交，允许出现结果不一致，然后采用弥补措施恢复数据即可，实现最终一致。\n\n * 强一致思想\n   \n   * cp模式：各个子事务执行后互相等待，同时提交，同时回滚，达成强一致。但事务等待过程中，处于弱可用状态。\n\n但不管是哪一种模式，都需要在子系统事务之间互相通讯，协调事务状态，也就是需要一个事务协调者(tc)：\n\n\n\n这里的子系统事务，称为分支事务；\n\n有关联的各个分支事务在一起称为全局事务。",charsets:{cjk:!0}},{title:"多级缓存总结",frontmatter:{autoSort:95,title:"多级缓存总结",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/52d2bc/",categories:["后端","微服务","进阶","多级缓存"],tags:["知识","微服务","多级缓存"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/50.%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/100.%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98%E6%80%BB%E7%BB%93.html",relativePath:"01.后端/60.微服务/50.多级缓存/100.多级缓存总结.md",key:"v-7c74bfe6",path:"/pages/52d2bc/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:" * 第一级nginx 负责 静态资源存储和反向代理\n   \n   * 将请求路由到openResty的nginx集群中\n\n * 第二级nginx集群 实现第一级缓存，即nginx本地缓存\n   \n    # 共享字典，也就是本地缓存，名称叫做：item_cache，大小150M\n    lua_shared_dict item_cache 150m; \n   \n   \n   * 当请求路由到此时，它优先查询本地缓存，本地缓存没有在查询redis缓存，redis缓存没有在去访问tomcat端口获得信息\n     * 访问的代码逻辑，用lua语言编写，在/usr/local/openresty/nginx/lua/item.lua中\n     * 编写了一些 封装好的函数，在 /usr/local/openresty/lualib/common.lua中\n   * 在访问tomcat集群的时候，因为tomcat内部做了进程缓存，但是进程之间缓存不共享，所以在路由请求时不采用默认的轮询测策略，而采用根据访问路径 hash 的方法，基于ID负载均衡。\n     * 如果访问同一个路径，则必然得到同一个hash值，则必然访问到同一个tomcat进程。\n     * 这样就可以极大的利用tomcat的进程缓存。\n\n * 第二级缓存即为redis缓存\n   \n   * 为了避免项目刚启动时，因为reids缓存中无数据导致对mysql冲击较大，所以要进行redis缓存预热，将热点信息提前放到reids中，减少数据库的压力\n   * 实现 接口 InitializingBean 及其内部定义的方法afterPropertiesSet()\n   * 在设置键值对的时候， 键要加前缀，归位同一类，避免键冲突\n     * item:id:10001\n     * item:stock:id:10001\n\n * 第三级缓存即为tomcat进程缓存\n   \n   * 借助 Caffeine框架实现进程缓存\n   * 各个进程间 缓存不共享\n\n * 缓存同步\n   \n   * nginx本地缓存 的缓存同步策略为\n     * 设置有效期：给缓存设置有效期，到期后自动删除。再次查询时更新\n   * redis缓存和tomcat进程缓存 的缓存同步策略为\n     * **异步通知：**修改数据库时发送事件通知，相关服务监听到通知后修改缓存数据\n       * 使用canal监听数据库变化，当数据发生变化时，自动修改缓存\n         * 使用 canal做监听器，可以实现代码0侵入，只需要实现接口EntryHandler<Item>\n           * 实现接口的三个方法\n           * 在实体类字段加上一些注解，来更好的与数据库建立联系\n         * canal监听的原理与mysql的主从同步一致\n           * 它伪装成一个slave，通过监听数据库master的binary log，来实现实时的数据监听\n       * 当然也可以使用 mq",normalizedContent:" * 第一级nginx 负责 静态资源存储和反向代理\n   \n   * 将请求路由到openresty的nginx集群中\n\n * 第二级nginx集群 实现第一级缓存，即nginx本地缓存\n   \n    # 共享字典，也就是本地缓存，名称叫做：item_cache，大小150m\n    lua_shared_dict item_cache 150m; \n   \n   \n   * 当请求路由到此时，它优先查询本地缓存，本地缓存没有在查询redis缓存，redis缓存没有在去访问tomcat端口获得信息\n     * 访问的代码逻辑，用lua语言编写，在/usr/local/openresty/nginx/lua/item.lua中\n     * 编写了一些 封装好的函数，在 /usr/local/openresty/lualib/common.lua中\n   * 在访问tomcat集群的时候，因为tomcat内部做了进程缓存，但是进程之间缓存不共享，所以在路由请求时不采用默认的轮询测策略，而采用根据访问路径 hash 的方法，基于id负载均衡。\n     * 如果访问同一个路径，则必然得到同一个hash值，则必然访问到同一个tomcat进程。\n     * 这样就可以极大的利用tomcat的进程缓存。\n\n * 第二级缓存即为redis缓存\n   \n   * 为了避免项目刚启动时，因为reids缓存中无数据导致对mysql冲击较大，所以要进行redis缓存预热，将热点信息提前放到reids中，减少数据库的压力\n   * 实现 接口 initializingbean 及其内部定义的方法afterpropertiesset()\n   * 在设置键值对的时候， 键要加前缀，归位同一类，避免键冲突\n     * item:id:10001\n     * item:stock:id:10001\n\n * 第三级缓存即为tomcat进程缓存\n   \n   * 借助 caffeine框架实现进程缓存\n   * 各个进程间 缓存不共享\n\n * 缓存同步\n   \n   * nginx本地缓存 的缓存同步策略为\n     * 设置有效期：给缓存设置有效期，到期后自动删除。再次查询时更新\n   * redis缓存和tomcat进程缓存 的缓存同步策略为\n     * **异步通知：**修改数据库时发送事件通知，相关服务监听到通知后修改缓存数据\n       * 使用canal监听数据库变化，当数据发生变化时，自动修改缓存\n         * 使用 canal做监听器，可以实现代码0侵入，只需要实现接口entryhandler<item>\n           * 实现接口的三个方法\n           * 在实体类字段加上一些注解，来更好的与数据库建立联系\n         * canal监听的原理与mysql的主从同步一致\n           * 它伪装成一个slave，通过监听数据库master的binary log，来实现实时的数据监听\n       * 当然也可以使用 mq",charsets:{cjk:!0}},{title:"分布式事务问题",frontmatter:{autoSort:100,title:"分布式事务问题",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/d48085/",categories:["后端","微服务","进阶","分布式事务"],tags:["知识","微服务","事务"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/60.%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/05.%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E9%97%AE%E9%A2%98.html",relativePath:"01.后端/60.微服务/60.分布式事务/05.分布式事务问题.md",key:"v-b54bf03c",path:"/pages/d48085/",headers:[{level:2,title:"本地事务",slug:"本地事务",normalizedTitle:"本地事务",charIndex:2},{level:2,title:"分布式事务",slug:"分布式事务",normalizedTitle:"分布式事务",charIndex:51},{level:2,title:"演示分布式事务问题",slug:"演示分布式事务问题",normalizedTitle:"演示分布式事务问题",charIndex:431}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"本地事务 分布式事务 演示分布式事务问题",content:"# 本地事务\n\n本地事务，也就是传统的单机事务。在传统数据库事务中，必须要满足四个原则：\n\n\n\n\n# 分布式事务\n\n分布式事务，就是指不是在单个服务或单个数据库架构下，产生的事务，例如：\n\n * 跨数据源的分布式事务\n * 跨服务的分布式事务\n * 综合情况\n\n在数据库水平拆分、服务垂直拆分之后，一个业务操作通常要跨多个数据库、服务才能完成。例如电商行业中比较常见的下单付款案例，包括下面几个行为：\n\n * 创建新订单\n * 扣减商品库存\n * 从用户账户余额扣除金额\n\n完成上面的操作需要访问三个不同的微服务和三个不同的数据库。\n\n\n\n订单的创建、库存的扣减、账户扣款在每一个服务和数据库内是一个本地事务，可以保证ACID原则。\n\n但是当我们把三件事情看做一个\"业务\"，要满足保证“业务”的原子性，要么所有操作全部成功，要么全部失败，不允许出现部分成功部分失败的现象，这就是分布式系统下的事务了。\n\n此时ACID难以满足，这是分布式事务要解决的问题\n\n\n# 演示分布式事务问题\n\n我们通过一个案例来演示分布式事务的问题：\n\n1）创建数据库，名为seata_demo，然后导入课前资料提供的SQL文件：\n\n\n\n2）导入课前资料提供的微服务：\n\n\n\n微服务结构如下：\n\n\n\n其中：\n\nseata-demo：父工程，负责管理项目依赖\n\n * account-service：账户服务，负责管理用户的资金账户。提供扣减余额的接口\n * storage-service：库存服务，负责管理商品库存。提供扣减库存的接口\n * order-service：订单服务，负责管理订单。创建订单时，需要调用account-service和storage-service\n\n3）启动nacos、所有微服务\n\n4）测试下单功能，发出Post请求：\n\n请求如下：\n\ncurl --location --request POST 'http://localhost:8082/order?userId=user202103032042012&commodityCode=100202003032041&count=20&money=200'\n\n\n如图：\n\n\n\n测试发现，当库存不足时，如果余额已经扣减，并不会回滚，出现了分布式事务问题。",normalizedContent:"# 本地事务\n\n本地事务，也就是传统的单机事务。在传统数据库事务中，必须要满足四个原则：\n\n\n\n\n# 分布式事务\n\n分布式事务，就是指不是在单个服务或单个数据库架构下，产生的事务，例如：\n\n * 跨数据源的分布式事务\n * 跨服务的分布式事务\n * 综合情况\n\n在数据库水平拆分、服务垂直拆分之后，一个业务操作通常要跨多个数据库、服务才能完成。例如电商行业中比较常见的下单付款案例，包括下面几个行为：\n\n * 创建新订单\n * 扣减商品库存\n * 从用户账户余额扣除金额\n\n完成上面的操作需要访问三个不同的微服务和三个不同的数据库。\n\n\n\n订单的创建、库存的扣减、账户扣款在每一个服务和数据库内是一个本地事务，可以保证acid原则。\n\n但是当我们把三件事情看做一个\"业务\"，要满足保证“业务”的原子性，要么所有操作全部成功，要么全部失败，不允许出现部分成功部分失败的现象，这就是分布式系统下的事务了。\n\n此时acid难以满足，这是分布式事务要解决的问题\n\n\n# 演示分布式事务问题\n\n我们通过一个案例来演示分布式事务的问题：\n\n1）创建数据库，名为seata_demo，然后导入课前资料提供的sql文件：\n\n\n\n2）导入课前资料提供的微服务：\n\n\n\n微服务结构如下：\n\n\n\n其中：\n\nseata-demo：父工程，负责管理项目依赖\n\n * account-service：账户服务，负责管理用户的资金账户。提供扣减余额的接口\n * storage-service：库存服务，负责管理商品库存。提供扣减库存的接口\n * order-service：订单服务，负责管理订单。创建订单时，需要调用account-service和storage-service\n\n3）启动nacos、所有微服务\n\n4）测试下单功能，发出post请求：\n\n请求如下：\n\ncurl --location --request post 'http://localhost:8082/order?userid=user202103032042012&commoditycode=100202003032041&count=20&money=200'\n\n\n如图：\n\n\n\n测试发现，当库存不足时，如果余额已经扣减，并不会回滚，出现了分布式事务问题。",charsets:{cjk:!0}},{title:"缓存同步",frontmatter:{autoSort:96,title:"缓存同步",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/92ac01/",categories:["后端","微服务","进阶","多级缓存"],tags:["知识","微服务","多级缓存"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/50.%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/25.%E7%BC%93%E5%AD%98%E5%90%8C%E6%AD%A5.html",relativePath:"01.后端/60.微服务/50.多级缓存/25.缓存同步.md",key:"v-3f4db490",path:"/pages/92ac01/",headers:[{level:2,title:"数据同步策略",slug:"数据同步策略",normalizedTitle:"数据同步策略",charIndex:96},{level:2,title:"认识Canal",slug:"认识canal",normalizedTitle:"认识canal",charIndex:648},{level:2,title:"监听Canal",slug:"监听canal",normalizedTitle:"监听canal",charIndex:1106},{level:3,title:"引入依赖：",slug:"引入依赖",normalizedTitle:"引入依赖：",charIndex:1356},{level:3,title:"编写配置：",slug:"编写配置",normalizedTitle:"编写配置：",charIndex:1523},{level:3,title:"修改Item实体类",slug:"修改item实体类",normalizedTitle:"修改item实体类",charIndex:1638},{level:3,title:"编写监听器",slug:"编写监听器",normalizedTitle:"编写监听器",charIndex:2819}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"数据同步策略 认识Canal 监听Canal 引入依赖： 编写配置： 修改Item实体类 编写监听器",content:'大多数情况下，浏览器查询到的都是缓存数据，如果缓存数据与数据库数据存在较大差异，可能会产生比较严重的后果。\n\n所以我们必须保证数据库数据、缓存数据的一致性，这就是缓存与数据库的同步。\n\n\n# 数据同步策略\n\n缓存数据同步的常见方式有三种：\n\n设置有效期：给缓存设置有效期，到期后自动删除。再次查询时更新\n\n * 优势：简单、方便\n * 缺点：时效性差，缓存过期之前可能不一致\n * 场景：更新频率较低，时效性要求低的业务\n\n同步双写：在修改数据库的同时，直接修改缓存\n\n * 优势：时效性强，缓存与数据库强一致\n * 缺点：有代码侵入，耦合度高；\n * 场景：对一致性、时效性要求较高的缓存数据\n\n**异步通知：**修改数据库时发送事件通知，相关服务监听到通知后修改缓存数据\n\n * 优势：低耦合，可以同时通知多个缓存服务\n * 缺点：时效性一般，可能存在中间不一致状态\n * 场景：时效性要求一般，有多个服务需要同步\n\n而异步实现又可以基于MQ或者Canal来实现：\n\n1）基于MQ的异步通知：\n\n\n\n解读：\n\n * 商品服务完成对数据的修改后，只需要发送一条消息到MQ中。\n * 缓存服务监听MQ消息，然后完成对缓存的更新\n\n依然有少量的代码侵入。\n\n2）基于Canal的通知\n\n\n\n解读：\n\n * 商品服务完成商品修改后，业务直接结束，没有任何代码侵入\n * Canal监听MySQL变化，当发现变化后，立即通知缓存服务\n * 缓存服务接收到canal通知，更新缓存\n\n代码零侵入\n\n\n# 认识Canal\n\nCanal [kə\'næl]，译意为水道/管道/沟渠，canal是阿里巴巴旗下的一款开源项目，基于Java开发。基于数据库增量日志解析，提供增量数据订阅&消费。GitHub的地址：https://github.com/alibaba/canal\n\nCanal是基于mysql的主从同步来实现的，MySQL主从同步的原理如下：\n\n\n\n * 1）MySQL master 将数据变更写入二进制日志( binary log），其中记录的数据叫做binary log events\n * 2）MySQL slave 将 master 的 binary log events拷贝到它的中继日志(relay log)\n * 3）MySQL slave 重放 relay log 中事件，将数据变更反映它自己的数据\n\n而Canal就是把自己伪装成MySQL的一个slave节点，从而监听master的binary log变化。再把得到的变化信息通知给Canal的客户端，进而完成对其它数据库的同步。\n\n\n\n\n# 监听Canal\n\nCanal提供了各种语言的客户端，当Canal监听到binlog变化时，会通知Canal的客户端。\n\n\n\n我们可以利用Canal提供的Java客户端，监听Canal通知消息。当收到变化的消息时，完成对缓存的更新。\n\n不过这里我们会使用GitHub上的第三方开源的canal-starter客户端。地址：https://github.com/NormanGyllenhaal/canal-client\n\n与SpringBoot完美整合，自动装配，比官方客户端要简单好用很多。\n\n\n# 引入依赖：\n\n<dependency>\n    <groupId>top.javatool</groupId>\n    <artifactId>canal-spring-boot-starter</artifactId>\n    <version>1.2.1-RELEASE</version>\n</dependency>\n\n\n\n# 编写配置：\n\ncanal:\n  destination: diane # canal的集群名字，要与安装canal时设置的名称一致\n  server: 192.168.159.100:11111 # canal服务地址\n\n\n\n# 修改Item实体类\n\n通过@Id、@Column、等注解完成Item与数据库表字段的映射：\n\npackage com.heima.item.pojo;\n\nimport com.baomidou.mybatisplus.annotation.IdType;\nimport com.baomidou.mybatisplus.annotation.TableField;\nimport com.baomidou.mybatisplus.annotation.TableId;\nimport com.baomidou.mybatisplus.annotation.TableName;\nimport lombok.Data;\nimport org.springframework.data.annotation.Id;\nimport org.springframework.data.annotation.Transient;\n\nimport javax.persistence.Column;\nimport java.util.Date;\n\n@Data\n@TableName("tb_item")\npublic class Item {\n    @TableId(type = IdType.AUTO)\n    @Id //表示主键\n    private Long id;//商品id\n    @Column(name = "name") //非主键部分，当数据库名称与字段名称不一致时打标注\n    private String name;//商品名称\n    private String title;//商品标题\n    private Long price;//价格（分）\n    private String image;//商品图片\n    private String category;//分类名称\n    private String brand;//品牌名称\n    private String spec;//规格\n    private Integer status;//商品状态 1-正常，2-下架\n    private Date createTime;//创建时间\n    private Date updateTime;//更新时间\n    @TableField(exist = false)//mybatis注解  表示该属性 数据库的表中没有\n    @Transient//canal 标记不属于数据库表中的字段\n    private Integer stock;\n    @TableField(exist = false)\n    @Transient\n    private Integer sold;\n}\n\n\n\n# 编写监听器\n\n通过实现EntryHandler<T>接口编写监听器，监听Canal消息。注意两点：\n\n * 实现类通过@CanalTable("tb_item")指定监听的表信息\n * EntryHandler的泛型是与表对应的实体类\n\npackage com.heima.item.canal;\n\nimport com.github.benmanes.caffeine.cache.Cache;\nimport com.heima.item.config.RedisHandler;\nimport com.heima.item.pojo.Item;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.stereotype.Component;\nimport top.javatool.canal.client.annotation.CanalTable;\nimport top.javatool.canal.client.handler.EntryHandler;\n\n@CanalTable("tb_item")\n@Component\npublic class ItemHandler implements EntryHandler<Item> {\n\n    @Autowired\n    private RedisHandler redisHandler;\n    @Autowired\n    private Cache<Long, Item> itemCache;\n\n    @Override\n    public void insert(Item item) {\n        // 写数据到JVM进程缓存\n        itemCache.put(item.getId(), item);\n        // 写数据到redis\n        redisHandler.saveItem(item);\n    }\n\n    @Override\n    public void update(Item before, Item after) {\n        // 写数据到JVM进程缓存\n        itemCache.put(after.getId(), after);\n        // 写数据到redis\n        redisHandler.saveItem(after);\n    }\n\n    @Override\n    public void delete(Item item) {\n        // 删除数据到JVM进程缓存\n        itemCache.invalidate(item.getId());\n        // 删除数据到redis\n        redisHandler.deleteItemById(item.getId());\n    }\n}\n\n\n在这里对Redis的操作都封装到了RedisHandler这个对象中，是我们之前做缓存预热时编写的一个类，内容如下：\n\npackage com.heima.item.config;\n\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.heima.item.pojo.Item;\nimport com.heima.item.pojo.ItemStock;\nimport com.heima.item.service.IItemService;\nimport com.heima.item.service.IItemStockService;\nimport org.springframework.beans.factory.InitializingBean;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.data.redis.core.StringRedisTemplate;\nimport org.springframework.stereotype.Component;\n\nimport java.util.List;\n\n@Component\npublic class RedisHandler implements InitializingBean {\n\n    @Autowired\n    private StringRedisTemplate redisTemplate;\n\n    @Autowired\n    private IItemService itemService;\n    @Autowired\n    private IItemStockService stockService;\n\n    private static final ObjectMapper MAPPER = new ObjectMapper();\n\n    @Override\n    public void afterPropertiesSet() throws Exception {\n        // 初始化缓存\n        // 1.查询商品信息\n        List<Item> itemList = itemService.list();\n        // 2.放入缓存\n        for (Item item : itemList) {\n            // 2.1.item序列化为JSON\n            String json = MAPPER.writeValueAsString(item);\n            // 2.2.存入redis\n            redisTemplate.opsForValue().set("item:id:" + item.getId(), json);\n        }\n\n        // 3.查询商品库存信息\n        List<ItemStock> stockList = stockService.list();\n        // 4.放入缓存\n        for (ItemStock stock : stockList) {\n            // 2.1.item序列化为JSON\n            String json = MAPPER.writeValueAsString(stock);\n            // 2.2.存入redis\n            redisTemplate.opsForValue().set("item:stock:id:" + stock.getId(), json);\n        }\n    }\n\n    public void saveItem(Item item) {\n        try {\n            String json = MAPPER.writeValueAsString(item);\n            redisTemplate.opsForValue().set("item:id:" + item.getId(), json);\n        } catch (JsonProcessingException e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n    public void deleteItemById(Long id) {\n        redisTemplate.delete("item:id:" + id);\n    }\n}\n',normalizedContent:'大多数情况下，浏览器查询到的都是缓存数据，如果缓存数据与数据库数据存在较大差异，可能会产生比较严重的后果。\n\n所以我们必须保证数据库数据、缓存数据的一致性，这就是缓存与数据库的同步。\n\n\n# 数据同步策略\n\n缓存数据同步的常见方式有三种：\n\n设置有效期：给缓存设置有效期，到期后自动删除。再次查询时更新\n\n * 优势：简单、方便\n * 缺点：时效性差，缓存过期之前可能不一致\n * 场景：更新频率较低，时效性要求低的业务\n\n同步双写：在修改数据库的同时，直接修改缓存\n\n * 优势：时效性强，缓存与数据库强一致\n * 缺点：有代码侵入，耦合度高；\n * 场景：对一致性、时效性要求较高的缓存数据\n\n**异步通知：**修改数据库时发送事件通知，相关服务监听到通知后修改缓存数据\n\n * 优势：低耦合，可以同时通知多个缓存服务\n * 缺点：时效性一般，可能存在中间不一致状态\n * 场景：时效性要求一般，有多个服务需要同步\n\n而异步实现又可以基于mq或者canal来实现：\n\n1）基于mq的异步通知：\n\n\n\n解读：\n\n * 商品服务完成对数据的修改后，只需要发送一条消息到mq中。\n * 缓存服务监听mq消息，然后完成对缓存的更新\n\n依然有少量的代码侵入。\n\n2）基于canal的通知\n\n\n\n解读：\n\n * 商品服务完成商品修改后，业务直接结束，没有任何代码侵入\n * canal监听mysql变化，当发现变化后，立即通知缓存服务\n * 缓存服务接收到canal通知，更新缓存\n\n代码零侵入\n\n\n# 认识canal\n\ncanal [kə\'næl]，译意为水道/管道/沟渠，canal是阿里巴巴旗下的一款开源项目，基于java开发。基于数据库增量日志解析，提供增量数据订阅&消费。github的地址：https://github.com/alibaba/canal\n\ncanal是基于mysql的主从同步来实现的，mysql主从同步的原理如下：\n\n\n\n * 1）mysql master 将数据变更写入二进制日志( binary log），其中记录的数据叫做binary log events\n * 2）mysql slave 将 master 的 binary log events拷贝到它的中继日志(relay log)\n * 3）mysql slave 重放 relay log 中事件，将数据变更反映它自己的数据\n\n而canal就是把自己伪装成mysql的一个slave节点，从而监听master的binary log变化。再把得到的变化信息通知给canal的客户端，进而完成对其它数据库的同步。\n\n\n\n\n# 监听canal\n\ncanal提供了各种语言的客户端，当canal监听到binlog变化时，会通知canal的客户端。\n\n\n\n我们可以利用canal提供的java客户端，监听canal通知消息。当收到变化的消息时，完成对缓存的更新。\n\n不过这里我们会使用github上的第三方开源的canal-starter客户端。地址：https://github.com/normangyllenhaal/canal-client\n\n与springboot完美整合，自动装配，比官方客户端要简单好用很多。\n\n\n# 引入依赖：\n\n<dependency>\n    <groupid>top.javatool</groupid>\n    <artifactid>canal-spring-boot-starter</artifactid>\n    <version>1.2.1-release</version>\n</dependency>\n\n\n\n# 编写配置：\n\ncanal:\n  destination: diane # canal的集群名字，要与安装canal时设置的名称一致\n  server: 192.168.159.100:11111 # canal服务地址\n\n\n\n# 修改item实体类\n\n通过@id、@column、等注解完成item与数据库表字段的映射：\n\npackage com.heima.item.pojo;\n\nimport com.baomidou.mybatisplus.annotation.idtype;\nimport com.baomidou.mybatisplus.annotation.tablefield;\nimport com.baomidou.mybatisplus.annotation.tableid;\nimport com.baomidou.mybatisplus.annotation.tablename;\nimport lombok.data;\nimport org.springframework.data.annotation.id;\nimport org.springframework.data.annotation.transient;\n\nimport javax.persistence.column;\nimport java.util.date;\n\n@data\n@tablename("tb_item")\npublic class item {\n    @tableid(type = idtype.auto)\n    @id //表示主键\n    private long id;//商品id\n    @column(name = "name") //非主键部分，当数据库名称与字段名称不一致时打标注\n    private string name;//商品名称\n    private string title;//商品标题\n    private long price;//价格（分）\n    private string image;//商品图片\n    private string category;//分类名称\n    private string brand;//品牌名称\n    private string spec;//规格\n    private integer status;//商品状态 1-正常，2-下架\n    private date createtime;//创建时间\n    private date updatetime;//更新时间\n    @tablefield(exist = false)//mybatis注解  表示该属性 数据库的表中没有\n    @transient//canal 标记不属于数据库表中的字段\n    private integer stock;\n    @tablefield(exist = false)\n    @transient\n    private integer sold;\n}\n\n\n\n# 编写监听器\n\n通过实现entryhandler<t>接口编写监听器，监听canal消息。注意两点：\n\n * 实现类通过@canaltable("tb_item")指定监听的表信息\n * entryhandler的泛型是与表对应的实体类\n\npackage com.heima.item.canal;\n\nimport com.github.benmanes.caffeine.cache.cache;\nimport com.heima.item.config.redishandler;\nimport com.heima.item.pojo.item;\nimport org.springframework.beans.factory.annotation.autowired;\nimport org.springframework.stereotype.component;\nimport top.javatool.canal.client.annotation.canaltable;\nimport top.javatool.canal.client.handler.entryhandler;\n\n@canaltable("tb_item")\n@component\npublic class itemhandler implements entryhandler<item> {\n\n    @autowired\n    private redishandler redishandler;\n    @autowired\n    private cache<long, item> itemcache;\n\n    @override\n    public void insert(item item) {\n        // 写数据到jvm进程缓存\n        itemcache.put(item.getid(), item);\n        // 写数据到redis\n        redishandler.saveitem(item);\n    }\n\n    @override\n    public void update(item before, item after) {\n        // 写数据到jvm进程缓存\n        itemcache.put(after.getid(), after);\n        // 写数据到redis\n        redishandler.saveitem(after);\n    }\n\n    @override\n    public void delete(item item) {\n        // 删除数据到jvm进程缓存\n        itemcache.invalidate(item.getid());\n        // 删除数据到redis\n        redishandler.deleteitembyid(item.getid());\n    }\n}\n\n\n在这里对redis的操作都封装到了redishandler这个对象中，是我们之前做缓存预热时编写的一个类，内容如下：\n\npackage com.heima.item.config;\n\nimport com.fasterxml.jackson.core.jsonprocessingexception;\nimport com.fasterxml.jackson.databind.objectmapper;\nimport com.heima.item.pojo.item;\nimport com.heima.item.pojo.itemstock;\nimport com.heima.item.service.iitemservice;\nimport com.heima.item.service.iitemstockservice;\nimport org.springframework.beans.factory.initializingbean;\nimport org.springframework.beans.factory.annotation.autowired;\nimport org.springframework.data.redis.core.stringredistemplate;\nimport org.springframework.stereotype.component;\n\nimport java.util.list;\n\n@component\npublic class redishandler implements initializingbean {\n\n    @autowired\n    private stringredistemplate redistemplate;\n\n    @autowired\n    private iitemservice itemservice;\n    @autowired\n    private iitemstockservice stockservice;\n\n    private static final objectmapper mapper = new objectmapper();\n\n    @override\n    public void afterpropertiesset() throws exception {\n        // 初始化缓存\n        // 1.查询商品信息\n        list<item> itemlist = itemservice.list();\n        // 2.放入缓存\n        for (item item : itemlist) {\n            // 2.1.item序列化为json\n            string json = mapper.writevalueasstring(item);\n            // 2.2.存入redis\n            redistemplate.opsforvalue().set("item:id:" + item.getid(), json);\n        }\n\n        // 3.查询商品库存信息\n        list<itemstock> stocklist = stockservice.list();\n        // 4.放入缓存\n        for (itemstock stock : stocklist) {\n            // 2.1.item序列化为json\n            string json = mapper.writevalueasstring(stock);\n            // 2.2.存入redis\n            redistemplate.opsforvalue().set("item:stock:id:" + stock.getid(), json);\n        }\n    }\n\n    public void saveitem(item item) {\n        try {\n            string json = mapper.writevalueasstring(item);\n            redistemplate.opsforvalue().set("item:id:" + item.getid(), json);\n        } catch (jsonprocessingexception e) {\n            throw new runtimeexception(e);\n        }\n    }\n\n    public void deleteitembyid(long id) {\n        redistemplate.delete("item:id:" + id);\n    }\n}\n',charsets:{cjk:!0}},{title:"实现多级缓存",frontmatter:{autoSort:97,title:"实现多级缓存",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/387b25/",categories:["后端","微服务","进阶","多级缓存"],tags:["知识","微服务","多级缓存"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/50.%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/20.%E5%AE%9E%E7%8E%B0%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98.html",relativePath:"01.后端/60.微服务/50.多级缓存/20.实现多级缓存.md",key:"v-4fafeb1f",path:"/pages/387b25/",headers:[{level:2,title:"安装OpenResty",slug:"安装openresty",normalizedTitle:"安装openresty",charIndex:45},{level:2,title:"OpenResty快速入门",slug:"openresty快速入门",normalizedTitle:"openresty快速入门",charIndex:259},{level:3,title:"反向代理流程",slug:"反向代理流程",normalizedTitle:"反向代理流程",charIndex:386},{level:3,title:"OpenResty监听请求",slug:"openresty监听请求",normalizedTitle:"openresty监听请求",charIndex:600},{level:3,title:"编写item.lua",slug:"编写item-lua",normalizedTitle:"编写item.lua",charIndex:1293},{level:2,title:"请求参数处理",slug:"请求参数处理",normalizedTitle:"请求参数处理",charIndex:2039},{level:3,title:"获取参数的API",slug:"获取参数的api",normalizedTitle:"获取参数的api",charIndex:2140},{level:3,title:"获取参数并返回",slug:"获取参数并返回",normalizedTitle:"获取参数并返回",charIndex:2217},{level:2,title:"查询Tomcat",slug:"查询tomcat",normalizedTitle:"查询tomcat",charIndex:3126},{level:3,title:"发送http请求的API",slug:"发送http请求的api",normalizedTitle:"发送http请求的api",charIndex:3285},{level:3,title:"封装http工具",slug:"封装http工具",normalizedTitle:"封装http工具",charIndex:3746},{level:3,title:"CJSON工具类",slug:"cjson工具类",normalizedTitle:"cjson工具类",charIndex:5269},{level:3,title:"实现Tomcat查询",slug:"实现tomcat查询",normalizedTitle:"实现tomcat查询",charIndex:5649},{level:3,title:"基于ID负载均衡",slug:"基于id负载均衡",normalizedTitle:"基于id负载均衡",charIndex:6195},{level:2,title:"Redis缓存预热",slug:"redis缓存预热",normalizedTitle:"redis缓存预热",charIndex:7352},{level:2,title:"查询Redis缓存",slug:"查询redis缓存",normalizedTitle:"查询redis缓存",charIndex:9716},{level:3,title:"封装Redis工具",slug:"封装redis工具",normalizedTitle:"封装redis工具",charIndex:9849},{level:3,title:"实现Redis查询",slug:"实现redis查询",normalizedTitle:"实现redis查询",charIndex:12609},{level:2,title:"Nginx本地缓存",slug:"nginx本地缓存",normalizedTitle:"nginx本地缓存",charIndex:14241},{level:3,title:"本地缓存API",slug:"本地缓存api",normalizedTitle:"本地缓存api",charIndex:14294},{level:3,title:"实现本地缓存查询",slug:"实现本地缓存查询",normalizedTitle:"实现本地缓存查询",charIndex:14676}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"安装OpenResty OpenResty快速入门 反向代理流程 OpenResty监听请求 编写item.lua 请求参数处理 获取参数的API 获取参数并返回 查询Tomcat 发送http请求的API 封装http工具 CJSON工具类 实现Tomcat查询 基于ID负载均衡 Redis缓存预热 查询Redis缓存 封装Redis工具 实现Redis查询 Nginx本地缓存 本地缓存API 实现本地缓存查询",content:'多级缓存的实现离不开Nginx编程，而Nginx编程又离不开OpenResty。\n\n\n# 安装OpenResty\n\nOpenResty® 是一个基于 Nginx的高性能 Web 平台，用于方便地搭建能够处理超高并发、扩展性极高的动态 Web 应用、Web 服务和动态网关。具备下列特点：\n\n * 具备Nginx的完整功能\n * 基于Lua语言进行扩展，集成了大量精良的 Lua 库、第三方模块\n * 允许使用Lua自定义业务逻辑、自定义库\n\n官方网站： https://openresty.org/cn/\n\n\n\n\n# OpenResty快速入门\n\n我们希望达到的多级缓存架构如图：\n\n\n\n其中：\n\n * windows上的nginx用来做反向代理服务，将前端的查询商品的ajax请求代理到OpenResty集群\n\n * OpenResty集群用来编写多级缓存业务\n\n\n# 反向代理流程\n\n现在，商品详情页使用的是假的商品数据。不过在浏览器中，可以看到页面有发起ajax请求查询真实商品数据。\n\n这个请求如下：\n\n\n\n请求地址是localhost，端口是80，就被windows上安装的Nginx服务给接收到了。然后代理给了OpenResty集群：\n\n\n\n我们需要在OpenResty中编写业务，查询商品数据并返回到浏览器。\n\n但是这次，我们先在OpenResty接收请求，返回假的商品数据。\n\n\n# OpenResty监听请求\n\nOpenResty的很多功能都依赖于其目录下的Lua库，需要在nginx.conf中指定依赖库的目录，并导入依赖：\n\n1）添加对OpenResty的Lua模块的加载\n\n修改/usr/local/openresty/nginx/conf/nginx.conf文件，在其中的http下面，添加下面代码：\n\n#lua 模块\nlua_package_path "/usr/local/openresty/lualib/?.lua;;";\n#c模块     \nlua_package_cpath "/usr/local/openresty/lualib/?.so;;";  \n\n\n2）监听/api/item路径\n\n修改/usr/local/openresty/nginx/conf/nginx.conf文件，在nginx.conf的server下面，添加对/api/item这个路径的监听：\n\nlocation  /api/item {\n    # 默认的响应类型\n    default_type application/json;\n    # 响应结果由lua/item.lua文件来决定\n    content_by_lua_file lua/item.lua;\n}\n\n\n这个监听，就类似于SpringMVC中的@GetMapping("/api/item")做路径映射。\n\n而content_by_lua_file lua/item.lua则相当于调用item.lua这个文件，执行其中的业务，把结果返回给用户。相当于java中调用service。\n\n\n# 编写item.lua\n\n1）在/usr/loca/openresty/nginx目录创建文件夹：lua\n\n\n\n2）在/usr/loca/openresty/nginx/lua文件夹下，新建文件：item.lua\n\n\n\n3）编写item.lua，返回假数据\n\nitem.lua中，利用ngx.say()函数返回数据到Response中\n\nngx.say(\'{"id":10001,"name":"SALSA AIR","title":"RIMOWA 21寸托运箱拉杆箱 SALSA AIR系列果绿色 820.70.36.4","price":17900,"image":"https://m.360buyimg.com/mobilecms/s720x720_jfs/t6934/364/1195375010/84676/e9f2c55f/597ece38N0ddcbc77.jpg!q70.jpg.webp","category":"拉杆箱","brand":"RIMOWA","spec":"","status":1,"createTime":"2019-04-30T16:00:00.000+00:00","updateTime":"2019-04-30T16:00:00.000+00:00","stock":2999,"sold":31290}\')\n\n\n4）重新加载配置\n\nnginx -s reload\n\n\n刷新商品页面：http://localhost/item.html?id=1001，即可看到效果：\n\n\n\n注意\n\n这里要重新启动一下 windows上负责反向代理的nginx\n\n * 关闭nginx\n   * taskkill /f /t /im nginx.exe\n\n\n# 请求参数处理\n\n上一节中，我们在OpenResty接收前端请求，但是返回的是假数据。\n\n要返回真实数据，必须根据前端传递来的商品id，查询商品信息才可以。\n\n那么如何获取前端传递的商品参数呢？\n\n\n# 获取参数的API\n\nOpenResty中提供了一些API用来获取不同类型的前端请求参数：\n\n * 路径占位符\n   * ~表示使用正则表达式\n\n\n\n\n# 获取参数并返回\n\n在前端发起的ajax请求如图：\n\n\n\n可以看到商品id是以路径占位符方式传递的，因此可以利用正则表达式匹配的方式来获取ID\n\n1）获取商品id\n\n修改/usr/loca/openresty/nginx/nginx.conf文件中监听/api/item的代码，利用正则表达式获取ID：\n\nlocation ~ /api/item/(\\d+) {\n    # 默认的响应类型\n    default_type application/json;\n    # 响应结果由lua/item.lua文件来决定\n    content_by_lua_file lua/item.lua;\n}\n\n\n2）拼接ID并返回\n\n修改/usr/loca/openresty/nginx/lua/item.lua文件，获取id并拼接到结果中返回：\n\n-- 获取商品id\nlocal id = ngx.var[1]\n-- 拼接并返回\nngx.say(\'{"id":\' .. id .. \',"name":"SALSA AIR","title":"RIMOWA 21寸托运箱拉杆箱 SALSA AIR系列果绿色 820.70.36.4","price":17900,"image":"https://m.360buyimg.com/mobilecms/s720x720_jfs/t6934/364/1195375010/84676/e9f2c55f/597ece38N0ddcbc77.jpg!q70.jpg.webp","category":"拉杆箱","brand":"RIMOWA","spec":"","status":1,"createTime":"2019-04-30T16:00:00.000+00:00","updateTime":"2019-04-30T16:00:00.000+00:00","stock":2999,"sold":31290}\')\n\n\n3）重新加载并测试\n\n运行命令以重新加载OpenResty配置：\n\nnginx -s reload\n\n\n刷新页面可以看到结果中已经带上了ID：\n\n\n\n\n# 查询Tomcat\n\n拿到商品ID后，本应去缓存中查询商品信息，不过目前我们还未建立nginx、redis缓存。因此，这里我们先根据商品id去tomcat查询商品信息。我们实现如图部分：\n\n\n\n需要注意的是，我们的OpenResty是在虚拟机，Tomcat是在Windows电脑上。两者IP一定不要搞错了。\n\n\n\n\n# 发送http请求的API\n\nnginx提供了内部API用以发送http请求：\n\nlocal resp = ngx.location.capture("/path",{\n    method = ngx.HTTP_GET,   -- 请求方式\n    args = {a=1,b=2},  -- get方式传参数\n})\n\n\n返回的响应内容包括：\n\n * resp.status：响应状态码\n * resp.header：响应头，是一个table\n * resp.body：响应体，就是响应数据\n\n注意：这里的path是路径，并不包含IP和端口。这个请求会被nginx内部的server监听并处理。\n\n但是我们希望这个请求发送到Tomcat服务器，所以还需要编写一个server来对这个路径做反向代理：\n\n location /path {\n     # 这里是windows电脑的ip和Java服务端口\n     proxy_pass http://192.168.159.1:8081;\n }\n\n\n原理如图：\n\n\n\n\n# 封装http工具\n\n下面，我们封装一个发送Http请求的工具，基于ngx.location.capture来实现查询tomcat。\n\n1）添加反向代理，到windows的Java服务\n\n因为item-service中的接口都是/item开头，所以我们监听/item路径，代理到windows上的tomcat服务。\n\n修改 /usr/local/openresty/nginx/conf/nginx.conf文件，添加一个location：\n\nlocation /item {\n    proxy_pass http://192.168.159.1:8081;\n}\n\n\n以后，只要我们调用ngx.location.capture("/item")，就一定能发送请求到windows的tomcat服务。\n\n2）封装工具类\n\n之前我们说过，OpenResty启动时会加载以下两个目录中的工具文件：\n\n\n\n所以，自定义的http工具也需要放到这个目录下。\n\n在/usr/local/openresty/lualib目录下，新建一个common.lua文件：\n\nvi /usr/local/openresty/lualib/common.lua\n\n\n内容如下:\n\n-- 封装函数，发送http请求，并解析响应\nlocal function read_http(path, params)\n    local resp = ngx.location.capture(path,{\n        method = ngx.HTTP_GET,\n        args = params,\n    })\n    if not resp then\n        -- 记录错误信息，返回404\n        ngx.log(ngx.ERR, "http请求查询失败, path: ", path , ", args: ", args)\n        ngx.exit(404)\n    end\n    return resp.body\nend\n-- 将方法导出\nlocal _M = {  \n    read_http = read_http\n}  \nreturn _M\n\n\n这个工具将read_http函数封装到_M这个table类型的变量中，并且返回，这类似于导出。\n\n使用的时候，可以利用require(\'common\')来导入该函数库，这里的common是函数库的文件名。\n\n3）实现商品查询\n\n最后，我们修改/usr/local/openresty/lua/item.lua文件，利用刚刚封装的函数库实现对tomcat的查询：\n\n-- 引入自定义common工具模块，返回值是common中返回的 _M\nlocal common = require("common")\n-- 从 common中获取read_http这个函数\nlocal read_http = common.read_http\n-- 获取路径参数\nlocal id = ngx.var[1]\n-- 根据id查询商品\nlocal itemJSON = read_http("/item/".. id, nil)\n-- 根据id查询商品库存\nlocal itemStockJSON = read_http("/item/stock/".. id, nil)\n\n\n这里查询到的结果是json字符串，并且包含商品、库存两个json字符串，页面最终需要的是把两个json拼接为一个json：\n\n\n\n这就需要我们先把JSON变为lua的table，完成数据整合后，再转为JSON。\n\n\n# CJSON工具类\n\nOpenResty提供了一个cjson的模块用来处理JSON的序列化和反序列化。\n\n官方地址： https://github.com/openresty/lua-cjson/\n\n1）引入cjson模块：\n\nlocal cjson = require "cjson"\n\n\n2）序列化：\n\nlocal obj = {\n    name = \'jack\',\n    age = 21\n}\n-- 把 table 序列化为 json\nlocal json = cjson.encode(obj)\n\n\n3）反序列化：\n\nlocal json = \'{"name": "jack", "age": 21}\'\n-- 反序列化 json为 table\nlocal obj = cjson.decode(json);\nprint(obj.name)\n\n\n\n# 实现Tomcat查询\n\n下面，我们修改之前的item.lua中的业务，添加json处理功能：\n\n-- 导入common函数库\nlocal common = require(\'common\')\nlocal read_http = common.read_http\n-- 导入cjson库\nlocal cjson = require(\'cjson\')\n\n-- 获取路径参数\nlocal id = ngx.var[1]\n-- 根据id查询商品\nlocal itemJSON = read_http("/item/".. id, nil)\n-- 根据id查询商品库存\nlocal itemStockJSON = read_http("/item/stock/".. id, nil)\n\n-- JSON转化为lua的table\nlocal item = cjson.decode(itemJSON)\nlocal stock = cjson.decode(stockJSON)\n\n-- 组合数据\nitem.stock = stock.stock\nitem.sold = stock.sold\n\n-- 把item序列化为json 返回结果\nngx.say(cjson.encode(item))\n\n\n\n# 基于ID负载均衡\n\n刚才的代码中，我们的tomcat是单机部署。而实际开发中，tomcat一定是集群模式：\n\n\n\n因此，OpenResty需要对tomcat集群做负载均衡。\n\n而默认的负载均衡规则是轮询模式，当我们查询/item/10001时：\n\n * 第一次会访问8081端口的tomcat服务，在该服务内部就形成了JVM进程缓存\n * 第二次会访问8082端口的tomcat服务，该服务内部没有JVM缓存（因为JVM缓存无法共享），会查询数据库\n * ...\n\n你看，因为轮询的原因，第一次查询8081形成的JVM缓存并未生效，直到下一次再次访问到8081时才可以生效，缓存命中率太低了。\n\n怎么办？\n\n如果能让同一个商品，每次查询时都访问同一个tomcat服务，那么JVM缓存就一定能生效了。\n\n也就是说，我们需要根据商品id做负载均衡，而不是轮询。\n\n# 1）原理\n\nnginx提供了基于请求路径做负载均衡的算法：\n\nnginx根据请求路径做hash运算，把得到的数值对tomcat服务的数量取余，余数是几，就访问第几个服务，实现负载均衡。\n\n例如：\n\n * 我们的请求路径是 /item/10001\n * tomcat总数为2台（8081、8082）\n * 对请求路径/item/1001做hash运算求余的结果为1\n * 则访问第一个tomcat服务，也就是8081\n\n只要id不变，每次hash运算结果也不会变，那就可以保证同一个商品，一直访问同一个tomcat服务，确保JVM缓存生效。\n\n# 2）实现\n\n修改/usr/local/openresty/nginx/conf/nginx.conf文件，实现基于ID做负载均衡。\n\n首先，定义tomcat集群，并设置基于路径做负载均衡：\n\nupstream tomcat-cluster {\n    #这里使用基于路径做负载均衡算法\n    #原理是 使用路径做哈希运算 只要路径一样 永远访问的是同一台服务器\n    hash $request_uri;\n    server 192.168.159.1:8081;\n    server 192.168.159.1:8082;\n}\n\n\n然后，修改对tomcat服务的反向代理，目标指向tomcat集群：\n\nlocation /item {\n    proxy_pass http://tomcat-cluster;\n}\n\n\n重新加载OpenResty\n\nnginx -s reload\n\n\n# 3）测试\n\n启动两台tomcat服务：\n\n-Dserver.port=8082\n\n\n\n同时启动：\n\n\n\n清空日志后，再次访问页面，可以看到不同id的商品，访问到了不同的tomcat服务：\n\n\n\n\n\n\n# Redis缓存预热\n\nRedis缓存会面临冷启动问题：\n\n冷启动：服务刚刚启动时，Redis中并没有缓存，如果所有商品数据都在第一次查询时添加缓存，可能会给数据库带来较大压力。\n\n缓存预热：在实际开发中，我们可以利用大数据统计用户访问的热点数据，在项目启动时将这些热点数据提前查询并保存到Redis中。\n\n我们数据量较少，并且没有数据统计相关功能，目前可以在启动时将所有数据都放入缓存中。\n\n1）利用Docker安装Redis\n\n#做数据持久化\ndocker run --name redis -p 6379:6379 -d redis redis-server --appendonly yes\n\n\n2）在item-service服务中引入Redis依赖\n\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-data-redis</artifactId>\n</dependency>\n\n\n3）配置Redis地址\n\nspring:\n  redis:\n    host: 192.168.159.100\n\n\n4）编写初始化类\n\n缓存预热需要在项目启动时完成，并且必须是拿到RedisTemplate之后。\n\n这里我们利用InitializingBean接口来实现，因为InitializingBean可以在对象被Spring创建并且成员变量全部注入后执行。\n\npackage com.heima.item.config;\n\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.heima.item.pojo.Item;\nimport com.heima.item.pojo.ItemStock;\nimport com.heima.item.service.IItemService;\nimport com.heima.item.service.IItemStockService;\nimport org.springframework.beans.factory.InitializingBean;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.data.redis.core.StringRedisTemplate;\nimport org.springframework.stereotype.Component;\n\nimport java.util.List;\n\n@Component\npublic class RedisHandler implements InitializingBean {\n\n    @Autowired\n    private StringRedisTemplate redisTemplate;\n\n    @Autowired\n    private IItemService itemService;\n    @Autowired\n    private IItemStockService stockService;\n\n    private static final ObjectMapper MAPPER = new ObjectMapper();\n\n    @Override\n    public void afterPropertiesSet() throws Exception {\n        // 初始化缓存\n        // 1.查询商品信息\n        List<Item> itemList = itemService.list();\n        // 2.放入缓存\n        for (Item item : itemList) {\n            // 2.1.item序列化为JSON\n            String json = MAPPER.writeValueAsString(item);\n            // 2.2.存入redis\n            //TODO 在设置键值的时候， 键要加前缀，归位同一类，避免键冲突\n            redisTemplate.opsForValue().set("item:id:" + item.getId(), json);\n        }\n\n        // 3.查询商品库存信息\n        List<ItemStock> stockList = stockService.list();\n        // 4.放入缓存\n        for (ItemStock stock : stockList) {\n            // 2.1.item序列化为JSON\n            String json = MAPPER.writeValueAsString(stock);\n            // 2.2.存入redis\n            redisTemplate.opsForValue().set("item:stock:id:" + stock.getId(), json);\n        }\n    }\n}\n\n\n\n# 查询Redis缓存\n\n现在，Redis缓存已经准备就绪，我们可以再OpenResty中实现查询Redis的逻辑了。如下图红框所示：\n\n\n\n当请求进入OpenResty之后：\n\n * 优先查询Redis缓存\n * 如果Redis缓存未命中，再查询Tomcat\n\n\n# 封装Redis工具\n\nOpenResty提供了操作Redis的模块，我们只要引入该模块就能直接使用。但是为了方便，我们将Redis操作封装到之前的common.lua工具库中。\n\n修改/usr/local/openresty/lualib/common.lua文件：\n\n1）引入Redis模块，并初始化Redis对象\n\n-- 导入redis\nlocal redis = require(\'resty.redis\')\n-- 初始化redis\nlocal red = redis:new()\n-- 设置redis超时时间\n-- 建立链接的超时时间，发送请求的超时时间，响应结果的超时时间  均为1s\nred:set_timeouts(1000, 1000, 1000)\n\n\n2）封装函数，用来释放Redis连接，其实是放入连接池\n\n-- 关闭redis连接的工具方法，其实是放入连接池\nlocal function close_redis(red)\n    local pool_max_idle_time = 10000 -- 连接的空闲时间，单位是毫秒\n    local pool_size = 100 --连接池大小\n    local ok, err = red:set_keepalive(pool_max_idle_time, pool_size)\n    if not ok then\n        ngx.log(ngx.ERR, "放入redis连接池失败: ", err)\n    end\nend\n\n\n3）封装函数，根据key查询Redis数据\n\n-- 查询redis的方法 ip和port是redis地址，key是查询的key\nlocal function read_redis(ip, port, key)\n    -- 获取一个连接\n    local ok, err = red:connect(ip, port)\n    if not ok then\n        ngx.log(ngx.ERR, "连接redis失败 : ", err)\n        return nil\n    end\n    -- 查询redis\n    local resp, err = red:get(key)\n    -- 查询失败处理\n    if not resp then\n        ngx.log(ngx.ERR, "查询Redis失败: ", err, ", key = " , key)\n    end\n    --得到的数据为空处理\n    if resp ** ngx.null then\n        resp = nil\n        ngx.log(ngx.ERR, "查询Redis数据为空, key = ", key)\n    end\n    close_redis(red)\n    return resp\nend\n\n\n4）导出\n\n-- 将方法导出\nlocal _M = {  \n    read_http = read_http,\n    read_redis = read_redis\n}  \nreturn _M\n\n\n完整的common.lua：\n\n-- 导入redis\nlocal redis = require(\'resty.redis\')\n-- 初始化redis\nlocal red = redis:new()\nred:set_timeouts(1000, 1000, 1000)\n\n-- 关闭redis连接的工具方法，其实是放入连接池\nlocal function close_redis(red)\n    local pool_max_idle_time = 10000 -- 连接的空闲时间，单位是毫秒\n    local pool_size = 100 --连接池大小\n    local ok, err = red:set_keepalive(pool_max_idle_time, pool_size)\n    if not ok then\n        ngx.log(ngx.ERR, "放入redis连接池失败: ", err)\n    end\nend\n\n-- 查询redis的方法 ip和port是redis地址，key是查询的key\nlocal function read_redis(ip, port, key)\n    -- 获取一个连接\n    local ok, err = red:connect(ip, port)\n    if not ok then\n        ngx.log(ngx.ERR, "连接redis失败 : ", err)\n        return nil\n    end\n    -- 查询redis\n    local resp, err = red:get(key)\n    -- 查询失败处理\n    if not resp then\n        ngx.log(ngx.ERR, "查询Redis失败: ", err, ", key = " , key)\n    end\n    --得到的数据为空处理\n    if resp ** ngx.null then\n        resp = nil\n        ngx.log(ngx.ERR, "查询Redis数据为空, key = ", key)\n    end\n    close_redis(red)\n    return resp\nend\n\n-- 封装函数，发送http请求，并解析响应\nlocal function read_http(path, params)\n    local resp = ngx.location.capture(path,{\n        method = ngx.HTTP_GET,\n        args = params,\n    })\n    if not resp then\n        -- 记录错误信息，返回404\n        ngx.log(ngx.ERR, "http查询失败, path: ", path , ", args: ", args)\n        ngx.exit(404)\n    end\n    return resp.body\nend\n-- 将方法导出\nlocal _M = {  \n    read_http = read_http,\n    read_redis = read_redis\n}  \nreturn _M\n\n\n\n# 实现Redis查询\n\n接下来，我们就可以去修改item.lua文件，实现对Redis的查询了。\n\n查询逻辑是：\n\n * 根据id查询Redis\n * 如果查询失败则继续查询Tomcat\n * 将查询结果返回\n\n1）修改/usr/local/openresty/lua/item.lua文件，添加一个查询函数：\n\n-- 导入common函数库\nlocal common = require(\'common\')\nlocal read_http = common.read_http\nlocal read_redis = common.read_redis\n-- 封装查询函数\nfunction read_data(key, path, params)\n    -- 查询本地缓存\n    local val = read_redis("192.168.159.100", 6379, key)\n    -- 判断查询结果\n    if not val then\n        ngx.log(ngx.ERR, "redis查询失败，尝试查询http， key: ", key)\n        -- redis查询失败，去查询http\n        val = read_http(path, params)\n    end\n    -- 返回数据\n    return val\nend\n\n\n2）而后修改商品查询、库存查询的业务：\n\n\n\n3）完整的item.lua代码：\n\n-- 导入common函数库\nlocal common = require(\'common\')\nlocal read_http = common.read_http\nlocal read_redis = common.read_redis\n-- 导入cjson库\nlocal cjson = require(\'cjson\')\n\n-- 封装查询函数\nfunction read_data(key, path, params)\n    -- 查询本地缓存\n    local val = read_redis("192.168.159.100", 6379, key)\n    -- 判断查询结果\n    if not val then\n        ngx.log(ngx.ERR, "redis查询失败，尝试查询http， key: ", key)\n        -- redis查询失败，去查询http\n        val = read_http(path, params)\n    end\n    -- 返回数据\n    return val\nend\n\n-- 获取路径参数\nlocal id = ngx.var[1]\n\n-- 查询信息，先从reids缓存中查，如果未命中再去访问tomcat端口\n-- 查询商品信息\nlocal itemJSON = read_data("item:id:" .. id,  "/item/" .. id, nil)\n-- 查询库存信息\nlocal stockJSON = read_data("item:stock:id:" .. id, "/item/stock/" .. id, nil)\n\n-- JSON转化为lua的table\nlocal item = cjson.decode(itemJSON)\nlocal stock = cjson.decode(stockJSON)\n-- 组合数据\nitem.stock = stock.stock\nitem.sold = stock.sold\n\n-- 把item序列化为json 返回结果\nngx.say(cjson.encode(item))\n\n\n * 这样即使将 tomcat 服务关闭， 也能从 redis中查到数据（redis 预热以后）\n * 记得看日志，日志很重要！！！\n\n\n# Nginx本地缓存\n\n现在，整个多级缓存中只差最后一环，也就是nginx的本地缓存了。如图：\n\n\n\n\n# 本地缓存API\n\nOpenResty为Nginx提供了shard dict的功能，可以在nginx的多个worker之间共享数据，实现缓存功能。\n\n1）开启共享字典，在nginx.conf的http下添加配置：\n\n # 共享字典，也就是本地缓存，名称叫做：item_cache，大小150M\n lua_shared_dict item_cache 150m; \n\n\n2）操作共享字典：\n\n-- 获取本地缓存对象\nlocal item_cache = ngx.shared.item_cache\n-- 存储, 指定key、value、过期时间，单位s，默认为0代表永不过期\n-- 1000s后自动删除\nitem_cache:set(\'key\', \'value\', 1000)\n-- 读取\nlocal val = item_cache:get(\'key\')\n\n\n\n# 实现本地缓存查询\n\n1）修改/usr/local/openresty/lua/item.lua文件，修改read_data查询函数，添加本地缓存逻辑：\n\n-- 导入共享词典，本地缓存\nlocal item_cache = ngx.shared.item_cache\n\n-- 封装查询函数\nfunction read_data(key, expire, path, params)\n    -- 查询本地缓存\n    local val = item_cache:get(key)\n    if not val then\n        ngx.log(ngx.ERR, "本地缓存查询失败，尝试查询Redis， key: ", key)\n        -- 查询redis\n        val = read_redis("127.0.0.1", 6379, key)\n        -- 判断查询结果\n        if not val then\n            ngx.log(ngx.ERR, "redis查询失败，尝试查询http， key: ", key)\n            -- redis查询失败，去查询http\n            val = read_http(path, params)\n        end\n    end\n    -- 查询成功，把数据写入本地缓存\n    item_cache:set(key, val, expire)\n    -- 返回数据\n    return val\nend\n\n\n2）修改item.lua中查询商品和库存的业务，实现最新的read_data函数：\n\n\n\n其实就是多了缓存时间参数，过期后nginx缓存会自动删除，下次访问即可更新缓存。\n\n这里给商品基本信息设置超时时间为30分钟，库存为1分钟。\n\n因为库存更新频率较高，如果缓存时间过长，可能与数据库差异较大。\n\n3）完整的item.lua文件：\n\n-- 导入common函数库\nlocal common = require(\'common\')\nlocal read_http = common.read_http\nlocal read_redis = common.read_redis\n-- 导入cjson库\nlocal cjson = require(\'cjson\')\n-- 导入共享词典，本地缓存\nlocal item_cache = ngx.shared.item_cache\n\n-- 封装查询函数\n-- TODO 查询逻辑，想查询nginx本地缓存信息，再查询redis缓存信息，再访问tomcat端口\nfunction read_data(key, expire, path, params)\n    -- 查询本地缓存\n    local val = item_cache:get(key)\n    if not val then\n        ngx.log(ngx.ERR, "本地缓存查询失败，尝试查询Redis， key: ", key)\n        -- 查询redis\n        val = read_redis("127.0.0.1", 6379, key)\n        -- 判断查询结果\n        if not val then\n            ngx.log(ngx.ERR, "redis查询失败，尝试查询http， key: ", key)\n            -- redis查询失败，去查询http\n            val = read_http(path, params)\n        end\n    end\n    -- 查询成功，把数据写入本地缓存\n    item_cache:set(key, val, expire)\n    -- 返回数据\n    return val\nend\n\n-- 获取路径参数\nlocal id = ngx.var[1]\n\n-- 查询商品信息\nlocal itemJSON = read_data("item:id:" .. id, 1800,  "/item/" .. id, nil)\n-- 查询库存信息\nlocal stockJSON = read_data("item:stock:id:" .. id, 60, "/item/stock/" .. id, nil)\n\n-- JSON转化为lua的table\nlocal item = cjson.decode(itemJSON)\nlocal stock = cjson.decode(stockJSON)\n-- 组合数据\nitem.stock = stock.stock\nitem.sold = stock.sold\n\n-- 把item序列化为json 返回结果\nngx.say(cjson.encode(item))\n',normalizedContent:'多级缓存的实现离不开nginx编程，而nginx编程又离不开openresty。\n\n\n# 安装openresty\n\nopenresty® 是一个基于 nginx的高性能 web 平台，用于方便地搭建能够处理超高并发、扩展性极高的动态 web 应用、web 服务和动态网关。具备下列特点：\n\n * 具备nginx的完整功能\n * 基于lua语言进行扩展，集成了大量精良的 lua 库、第三方模块\n * 允许使用lua自定义业务逻辑、自定义库\n\n官方网站： https://openresty.org/cn/\n\n\n\n\n# openresty快速入门\n\n我们希望达到的多级缓存架构如图：\n\n\n\n其中：\n\n * windows上的nginx用来做反向代理服务，将前端的查询商品的ajax请求代理到openresty集群\n\n * openresty集群用来编写多级缓存业务\n\n\n# 反向代理流程\n\n现在，商品详情页使用的是假的商品数据。不过在浏览器中，可以看到页面有发起ajax请求查询真实商品数据。\n\n这个请求如下：\n\n\n\n请求地址是localhost，端口是80，就被windows上安装的nginx服务给接收到了。然后代理给了openresty集群：\n\n\n\n我们需要在openresty中编写业务，查询商品数据并返回到浏览器。\n\n但是这次，我们先在openresty接收请求，返回假的商品数据。\n\n\n# openresty监听请求\n\nopenresty的很多功能都依赖于其目录下的lua库，需要在nginx.conf中指定依赖库的目录，并导入依赖：\n\n1）添加对openresty的lua模块的加载\n\n修改/usr/local/openresty/nginx/conf/nginx.conf文件，在其中的http下面，添加下面代码：\n\n#lua 模块\nlua_package_path "/usr/local/openresty/lualib/?.lua;;";\n#c模块     \nlua_package_cpath "/usr/local/openresty/lualib/?.so;;";  \n\n\n2）监听/api/item路径\n\n修改/usr/local/openresty/nginx/conf/nginx.conf文件，在nginx.conf的server下面，添加对/api/item这个路径的监听：\n\nlocation  /api/item {\n    # 默认的响应类型\n    default_type application/json;\n    # 响应结果由lua/item.lua文件来决定\n    content_by_lua_file lua/item.lua;\n}\n\n\n这个监听，就类似于springmvc中的@getmapping("/api/item")做路径映射。\n\n而content_by_lua_file lua/item.lua则相当于调用item.lua这个文件，执行其中的业务，把结果返回给用户。相当于java中调用service。\n\n\n# 编写item.lua\n\n1）在/usr/loca/openresty/nginx目录创建文件夹：lua\n\n\n\n2）在/usr/loca/openresty/nginx/lua文件夹下，新建文件：item.lua\n\n\n\n3）编写item.lua，返回假数据\n\nitem.lua中，利用ngx.say()函数返回数据到response中\n\nngx.say(\'{"id":10001,"name":"salsa air","title":"rimowa 21寸托运箱拉杆箱 salsa air系列果绿色 820.70.36.4","price":17900,"image":"https://m.360buyimg.com/mobilecms/s720x720_jfs/t6934/364/1195375010/84676/e9f2c55f/597ece38n0ddcbc77.jpg!q70.jpg.webp","category":"拉杆箱","brand":"rimowa","spec":"","status":1,"createtime":"2019-04-30t16:00:00.000+00:00","updatetime":"2019-04-30t16:00:00.000+00:00","stock":2999,"sold":31290}\')\n\n\n4）重新加载配置\n\nnginx -s reload\n\n\n刷新商品页面：http://localhost/item.html?id=1001，即可看到效果：\n\n\n\n注意\n\n这里要重新启动一下 windows上负责反向代理的nginx\n\n * 关闭nginx\n   * taskkill /f /t /im nginx.exe\n\n\n# 请求参数处理\n\n上一节中，我们在openresty接收前端请求，但是返回的是假数据。\n\n要返回真实数据，必须根据前端传递来的商品id，查询商品信息才可以。\n\n那么如何获取前端传递的商品参数呢？\n\n\n# 获取参数的api\n\nopenresty中提供了一些api用来获取不同类型的前端请求参数：\n\n * 路径占位符\n   * ~表示使用正则表达式\n\n\n\n\n# 获取参数并返回\n\n在前端发起的ajax请求如图：\n\n\n\n可以看到商品id是以路径占位符方式传递的，因此可以利用正则表达式匹配的方式来获取id\n\n1）获取商品id\n\n修改/usr/loca/openresty/nginx/nginx.conf文件中监听/api/item的代码，利用正则表达式获取id：\n\nlocation ~ /api/item/(\\d+) {\n    # 默认的响应类型\n    default_type application/json;\n    # 响应结果由lua/item.lua文件来决定\n    content_by_lua_file lua/item.lua;\n}\n\n\n2）拼接id并返回\n\n修改/usr/loca/openresty/nginx/lua/item.lua文件，获取id并拼接到结果中返回：\n\n-- 获取商品id\nlocal id = ngx.var[1]\n-- 拼接并返回\nngx.say(\'{"id":\' .. id .. \',"name":"salsa air","title":"rimowa 21寸托运箱拉杆箱 salsa air系列果绿色 820.70.36.4","price":17900,"image":"https://m.360buyimg.com/mobilecms/s720x720_jfs/t6934/364/1195375010/84676/e9f2c55f/597ece38n0ddcbc77.jpg!q70.jpg.webp","category":"拉杆箱","brand":"rimowa","spec":"","status":1,"createtime":"2019-04-30t16:00:00.000+00:00","updatetime":"2019-04-30t16:00:00.000+00:00","stock":2999,"sold":31290}\')\n\n\n3）重新加载并测试\n\n运行命令以重新加载openresty配置：\n\nnginx -s reload\n\n\n刷新页面可以看到结果中已经带上了id：\n\n\n\n\n# 查询tomcat\n\n拿到商品id后，本应去缓存中查询商品信息，不过目前我们还未建立nginx、redis缓存。因此，这里我们先根据商品id去tomcat查询商品信息。我们实现如图部分：\n\n\n\n需要注意的是，我们的openresty是在虚拟机，tomcat是在windows电脑上。两者ip一定不要搞错了。\n\n\n\n\n# 发送http请求的api\n\nnginx提供了内部api用以发送http请求：\n\nlocal resp = ngx.location.capture("/path",{\n    method = ngx.http_get,   -- 请求方式\n    args = {a=1,b=2},  -- get方式传参数\n})\n\n\n返回的响应内容包括：\n\n * resp.status：响应状态码\n * resp.header：响应头，是一个table\n * resp.body：响应体，就是响应数据\n\n注意：这里的path是路径，并不包含ip和端口。这个请求会被nginx内部的server监听并处理。\n\n但是我们希望这个请求发送到tomcat服务器，所以还需要编写一个server来对这个路径做反向代理：\n\n location /path {\n     # 这里是windows电脑的ip和java服务端口\n     proxy_pass http://192.168.159.1:8081;\n }\n\n\n原理如图：\n\n\n\n\n# 封装http工具\n\n下面，我们封装一个发送http请求的工具，基于ngx.location.capture来实现查询tomcat。\n\n1）添加反向代理，到windows的java服务\n\n因为item-service中的接口都是/item开头，所以我们监听/item路径，代理到windows上的tomcat服务。\n\n修改 /usr/local/openresty/nginx/conf/nginx.conf文件，添加一个location：\n\nlocation /item {\n    proxy_pass http://192.168.159.1:8081;\n}\n\n\n以后，只要我们调用ngx.location.capture("/item")，就一定能发送请求到windows的tomcat服务。\n\n2）封装工具类\n\n之前我们说过，openresty启动时会加载以下两个目录中的工具文件：\n\n\n\n所以，自定义的http工具也需要放到这个目录下。\n\n在/usr/local/openresty/lualib目录下，新建一个common.lua文件：\n\nvi /usr/local/openresty/lualib/common.lua\n\n\n内容如下:\n\n-- 封装函数，发送http请求，并解析响应\nlocal function read_http(path, params)\n    local resp = ngx.location.capture(path,{\n        method = ngx.http_get,\n        args = params,\n    })\n    if not resp then\n        -- 记录错误信息，返回404\n        ngx.log(ngx.err, "http请求查询失败, path: ", path , ", args: ", args)\n        ngx.exit(404)\n    end\n    return resp.body\nend\n-- 将方法导出\nlocal _m = {  \n    read_http = read_http\n}  \nreturn _m\n\n\n这个工具将read_http函数封装到_m这个table类型的变量中，并且返回，这类似于导出。\n\n使用的时候，可以利用require(\'common\')来导入该函数库，这里的common是函数库的文件名。\n\n3）实现商品查询\n\n最后，我们修改/usr/local/openresty/lua/item.lua文件，利用刚刚封装的函数库实现对tomcat的查询：\n\n-- 引入自定义common工具模块，返回值是common中返回的 _m\nlocal common = require("common")\n-- 从 common中获取read_http这个函数\nlocal read_http = common.read_http\n-- 获取路径参数\nlocal id = ngx.var[1]\n-- 根据id查询商品\nlocal itemjson = read_http("/item/".. id, nil)\n-- 根据id查询商品库存\nlocal itemstockjson = read_http("/item/stock/".. id, nil)\n\n\n这里查询到的结果是json字符串，并且包含商品、库存两个json字符串，页面最终需要的是把两个json拼接为一个json：\n\n\n\n这就需要我们先把json变为lua的table，完成数据整合后，再转为json。\n\n\n# cjson工具类\n\nopenresty提供了一个cjson的模块用来处理json的序列化和反序列化。\n\n官方地址： https://github.com/openresty/lua-cjson/\n\n1）引入cjson模块：\n\nlocal cjson = require "cjson"\n\n\n2）序列化：\n\nlocal obj = {\n    name = \'jack\',\n    age = 21\n}\n-- 把 table 序列化为 json\nlocal json = cjson.encode(obj)\n\n\n3）反序列化：\n\nlocal json = \'{"name": "jack", "age": 21}\'\n-- 反序列化 json为 table\nlocal obj = cjson.decode(json);\nprint(obj.name)\n\n\n\n# 实现tomcat查询\n\n下面，我们修改之前的item.lua中的业务，添加json处理功能：\n\n-- 导入common函数库\nlocal common = require(\'common\')\nlocal read_http = common.read_http\n-- 导入cjson库\nlocal cjson = require(\'cjson\')\n\n-- 获取路径参数\nlocal id = ngx.var[1]\n-- 根据id查询商品\nlocal itemjson = read_http("/item/".. id, nil)\n-- 根据id查询商品库存\nlocal itemstockjson = read_http("/item/stock/".. id, nil)\n\n-- json转化为lua的table\nlocal item = cjson.decode(itemjson)\nlocal stock = cjson.decode(stockjson)\n\n-- 组合数据\nitem.stock = stock.stock\nitem.sold = stock.sold\n\n-- 把item序列化为json 返回结果\nngx.say(cjson.encode(item))\n\n\n\n# 基于id负载均衡\n\n刚才的代码中，我们的tomcat是单机部署。而实际开发中，tomcat一定是集群模式：\n\n\n\n因此，openresty需要对tomcat集群做负载均衡。\n\n而默认的负载均衡规则是轮询模式，当我们查询/item/10001时：\n\n * 第一次会访问8081端口的tomcat服务，在该服务内部就形成了jvm进程缓存\n * 第二次会访问8082端口的tomcat服务，该服务内部没有jvm缓存（因为jvm缓存无法共享），会查询数据库\n * ...\n\n你看，因为轮询的原因，第一次查询8081形成的jvm缓存并未生效，直到下一次再次访问到8081时才可以生效，缓存命中率太低了。\n\n怎么办？\n\n如果能让同一个商品，每次查询时都访问同一个tomcat服务，那么jvm缓存就一定能生效了。\n\n也就是说，我们需要根据商品id做负载均衡，而不是轮询。\n\n# 1）原理\n\nnginx提供了基于请求路径做负载均衡的算法：\n\nnginx根据请求路径做hash运算，把得到的数值对tomcat服务的数量取余，余数是几，就访问第几个服务，实现负载均衡。\n\n例如：\n\n * 我们的请求路径是 /item/10001\n * tomcat总数为2台（8081、8082）\n * 对请求路径/item/1001做hash运算求余的结果为1\n * 则访问第一个tomcat服务，也就是8081\n\n只要id不变，每次hash运算结果也不会变，那就可以保证同一个商品，一直访问同一个tomcat服务，确保jvm缓存生效。\n\n# 2）实现\n\n修改/usr/local/openresty/nginx/conf/nginx.conf文件，实现基于id做负载均衡。\n\n首先，定义tomcat集群，并设置基于路径做负载均衡：\n\nupstream tomcat-cluster {\n    #这里使用基于路径做负载均衡算法\n    #原理是 使用路径做哈希运算 只要路径一样 永远访问的是同一台服务器\n    hash $request_uri;\n    server 192.168.159.1:8081;\n    server 192.168.159.1:8082;\n}\n\n\n然后，修改对tomcat服务的反向代理，目标指向tomcat集群：\n\nlocation /item {\n    proxy_pass http://tomcat-cluster;\n}\n\n\n重新加载openresty\n\nnginx -s reload\n\n\n# 3）测试\n\n启动两台tomcat服务：\n\n-dserver.port=8082\n\n\n\n同时启动：\n\n\n\n清空日志后，再次访问页面，可以看到不同id的商品，访问到了不同的tomcat服务：\n\n\n\n\n\n\n# redis缓存预热\n\nredis缓存会面临冷启动问题：\n\n冷启动：服务刚刚启动时，redis中并没有缓存，如果所有商品数据都在第一次查询时添加缓存，可能会给数据库带来较大压力。\n\n缓存预热：在实际开发中，我们可以利用大数据统计用户访问的热点数据，在项目启动时将这些热点数据提前查询并保存到redis中。\n\n我们数据量较少，并且没有数据统计相关功能，目前可以在启动时将所有数据都放入缓存中。\n\n1）利用docker安装redis\n\n#做数据持久化\ndocker run --name redis -p 6379:6379 -d redis redis-server --appendonly yes\n\n\n2）在item-service服务中引入redis依赖\n\n<dependency>\n    <groupid>org.springframework.boot</groupid>\n    <artifactid>spring-boot-starter-data-redis</artifactid>\n</dependency>\n\n\n3）配置redis地址\n\nspring:\n  redis:\n    host: 192.168.159.100\n\n\n4）编写初始化类\n\n缓存预热需要在项目启动时完成，并且必须是拿到redistemplate之后。\n\n这里我们利用initializingbean接口来实现，因为initializingbean可以在对象被spring创建并且成员变量全部注入后执行。\n\npackage com.heima.item.config;\n\nimport com.fasterxml.jackson.core.jsonprocessingexception;\nimport com.fasterxml.jackson.databind.objectmapper;\nimport com.heima.item.pojo.item;\nimport com.heima.item.pojo.itemstock;\nimport com.heima.item.service.iitemservice;\nimport com.heima.item.service.iitemstockservice;\nimport org.springframework.beans.factory.initializingbean;\nimport org.springframework.beans.factory.annotation.autowired;\nimport org.springframework.data.redis.core.stringredistemplate;\nimport org.springframework.stereotype.component;\n\nimport java.util.list;\n\n@component\npublic class redishandler implements initializingbean {\n\n    @autowired\n    private stringredistemplate redistemplate;\n\n    @autowired\n    private iitemservice itemservice;\n    @autowired\n    private iitemstockservice stockservice;\n\n    private static final objectmapper mapper = new objectmapper();\n\n    @override\n    public void afterpropertiesset() throws exception {\n        // 初始化缓存\n        // 1.查询商品信息\n        list<item> itemlist = itemservice.list();\n        // 2.放入缓存\n        for (item item : itemlist) {\n            // 2.1.item序列化为json\n            string json = mapper.writevalueasstring(item);\n            // 2.2.存入redis\n            //todo 在设置键值的时候， 键要加前缀，归位同一类，避免键冲突\n            redistemplate.opsforvalue().set("item:id:" + item.getid(), json);\n        }\n\n        // 3.查询商品库存信息\n        list<itemstock> stocklist = stockservice.list();\n        // 4.放入缓存\n        for (itemstock stock : stocklist) {\n            // 2.1.item序列化为json\n            string json = mapper.writevalueasstring(stock);\n            // 2.2.存入redis\n            redistemplate.opsforvalue().set("item:stock:id:" + stock.getid(), json);\n        }\n    }\n}\n\n\n\n# 查询redis缓存\n\n现在，redis缓存已经准备就绪，我们可以再openresty中实现查询redis的逻辑了。如下图红框所示：\n\n\n\n当请求进入openresty之后：\n\n * 优先查询redis缓存\n * 如果redis缓存未命中，再查询tomcat\n\n\n# 封装redis工具\n\nopenresty提供了操作redis的模块，我们只要引入该模块就能直接使用。但是为了方便，我们将redis操作封装到之前的common.lua工具库中。\n\n修改/usr/local/openresty/lualib/common.lua文件：\n\n1）引入redis模块，并初始化redis对象\n\n-- 导入redis\nlocal redis = require(\'resty.redis\')\n-- 初始化redis\nlocal red = redis:new()\n-- 设置redis超时时间\n-- 建立链接的超时时间，发送请求的超时时间，响应结果的超时时间  均为1s\nred:set_timeouts(1000, 1000, 1000)\n\n\n2）封装函数，用来释放redis连接，其实是放入连接池\n\n-- 关闭redis连接的工具方法，其实是放入连接池\nlocal function close_redis(red)\n    local pool_max_idle_time = 10000 -- 连接的空闲时间，单位是毫秒\n    local pool_size = 100 --连接池大小\n    local ok, err = red:set_keepalive(pool_max_idle_time, pool_size)\n    if not ok then\n        ngx.log(ngx.err, "放入redis连接池失败: ", err)\n    end\nend\n\n\n3）封装函数，根据key查询redis数据\n\n-- 查询redis的方法 ip和port是redis地址，key是查询的key\nlocal function read_redis(ip, port, key)\n    -- 获取一个连接\n    local ok, err = red:connect(ip, port)\n    if not ok then\n        ngx.log(ngx.err, "连接redis失败 : ", err)\n        return nil\n    end\n    -- 查询redis\n    local resp, err = red:get(key)\n    -- 查询失败处理\n    if not resp then\n        ngx.log(ngx.err, "查询redis失败: ", err, ", key = " , key)\n    end\n    --得到的数据为空处理\n    if resp ** ngx.null then\n        resp = nil\n        ngx.log(ngx.err, "查询redis数据为空, key = ", key)\n    end\n    close_redis(red)\n    return resp\nend\n\n\n4）导出\n\n-- 将方法导出\nlocal _m = {  \n    read_http = read_http,\n    read_redis = read_redis\n}  \nreturn _m\n\n\n完整的common.lua：\n\n-- 导入redis\nlocal redis = require(\'resty.redis\')\n-- 初始化redis\nlocal red = redis:new()\nred:set_timeouts(1000, 1000, 1000)\n\n-- 关闭redis连接的工具方法，其实是放入连接池\nlocal function close_redis(red)\n    local pool_max_idle_time = 10000 -- 连接的空闲时间，单位是毫秒\n    local pool_size = 100 --连接池大小\n    local ok, err = red:set_keepalive(pool_max_idle_time, pool_size)\n    if not ok then\n        ngx.log(ngx.err, "放入redis连接池失败: ", err)\n    end\nend\n\n-- 查询redis的方法 ip和port是redis地址，key是查询的key\nlocal function read_redis(ip, port, key)\n    -- 获取一个连接\n    local ok, err = red:connect(ip, port)\n    if not ok then\n        ngx.log(ngx.err, "连接redis失败 : ", err)\n        return nil\n    end\n    -- 查询redis\n    local resp, err = red:get(key)\n    -- 查询失败处理\n    if not resp then\n        ngx.log(ngx.err, "查询redis失败: ", err, ", key = " , key)\n    end\n    --得到的数据为空处理\n    if resp ** ngx.null then\n        resp = nil\n        ngx.log(ngx.err, "查询redis数据为空, key = ", key)\n    end\n    close_redis(red)\n    return resp\nend\n\n-- 封装函数，发送http请求，并解析响应\nlocal function read_http(path, params)\n    local resp = ngx.location.capture(path,{\n        method = ngx.http_get,\n        args = params,\n    })\n    if not resp then\n        -- 记录错误信息，返回404\n        ngx.log(ngx.err, "http查询失败, path: ", path , ", args: ", args)\n        ngx.exit(404)\n    end\n    return resp.body\nend\n-- 将方法导出\nlocal _m = {  \n    read_http = read_http,\n    read_redis = read_redis\n}  \nreturn _m\n\n\n\n# 实现redis查询\n\n接下来，我们就可以去修改item.lua文件，实现对redis的查询了。\n\n查询逻辑是：\n\n * 根据id查询redis\n * 如果查询失败则继续查询tomcat\n * 将查询结果返回\n\n1）修改/usr/local/openresty/lua/item.lua文件，添加一个查询函数：\n\n-- 导入common函数库\nlocal common = require(\'common\')\nlocal read_http = common.read_http\nlocal read_redis = common.read_redis\n-- 封装查询函数\nfunction read_data(key, path, params)\n    -- 查询本地缓存\n    local val = read_redis("192.168.159.100", 6379, key)\n    -- 判断查询结果\n    if not val then\n        ngx.log(ngx.err, "redis查询失败，尝试查询http， key: ", key)\n        -- redis查询失败，去查询http\n        val = read_http(path, params)\n    end\n    -- 返回数据\n    return val\nend\n\n\n2）而后修改商品查询、库存查询的业务：\n\n\n\n3）完整的item.lua代码：\n\n-- 导入common函数库\nlocal common = require(\'common\')\nlocal read_http = common.read_http\nlocal read_redis = common.read_redis\n-- 导入cjson库\nlocal cjson = require(\'cjson\')\n\n-- 封装查询函数\nfunction read_data(key, path, params)\n    -- 查询本地缓存\n    local val = read_redis("192.168.159.100", 6379, key)\n    -- 判断查询结果\n    if not val then\n        ngx.log(ngx.err, "redis查询失败，尝试查询http， key: ", key)\n        -- redis查询失败，去查询http\n        val = read_http(path, params)\n    end\n    -- 返回数据\n    return val\nend\n\n-- 获取路径参数\nlocal id = ngx.var[1]\n\n-- 查询信息，先从reids缓存中查，如果未命中再去访问tomcat端口\n-- 查询商品信息\nlocal itemjson = read_data("item:id:" .. id,  "/item/" .. id, nil)\n-- 查询库存信息\nlocal stockjson = read_data("item:stock:id:" .. id, "/item/stock/" .. id, nil)\n\n-- json转化为lua的table\nlocal item = cjson.decode(itemjson)\nlocal stock = cjson.decode(stockjson)\n-- 组合数据\nitem.stock = stock.stock\nitem.sold = stock.sold\n\n-- 把item序列化为json 返回结果\nngx.say(cjson.encode(item))\n\n\n * 这样即使将 tomcat 服务关闭， 也能从 redis中查到数据（redis 预热以后）\n * 记得看日志，日志很重要！！！\n\n\n# nginx本地缓存\n\n现在，整个多级缓存中只差最后一环，也就是nginx的本地缓存了。如图：\n\n\n\n\n# 本地缓存api\n\nopenresty为nginx提供了shard dict的功能，可以在nginx的多个worker之间共享数据，实现缓存功能。\n\n1）开启共享字典，在nginx.conf的http下添加配置：\n\n # 共享字典，也就是本地缓存，名称叫做：item_cache，大小150m\n lua_shared_dict item_cache 150m; \n\n\n2）操作共享字典：\n\n-- 获取本地缓存对象\nlocal item_cache = ngx.shared.item_cache\n-- 存储, 指定key、value、过期时间，单位s，默认为0代表永不过期\n-- 1000s后自动删除\nitem_cache:set(\'key\', \'value\', 1000)\n-- 读取\nlocal val = item_cache:get(\'key\')\n\n\n\n# 实现本地缓存查询\n\n1）修改/usr/local/openresty/lua/item.lua文件，修改read_data查询函数，添加本地缓存逻辑：\n\n-- 导入共享词典，本地缓存\nlocal item_cache = ngx.shared.item_cache\n\n-- 封装查询函数\nfunction read_data(key, expire, path, params)\n    -- 查询本地缓存\n    local val = item_cache:get(key)\n    if not val then\n        ngx.log(ngx.err, "本地缓存查询失败，尝试查询redis， key: ", key)\n        -- 查询redis\n        val = read_redis("127.0.0.1", 6379, key)\n        -- 判断查询结果\n        if not val then\n            ngx.log(ngx.err, "redis查询失败，尝试查询http， key: ", key)\n            -- redis查询失败，去查询http\n            val = read_http(path, params)\n        end\n    end\n    -- 查询成功，把数据写入本地缓存\n    item_cache:set(key, val, expire)\n    -- 返回数据\n    return val\nend\n\n\n2）修改item.lua中查询商品和库存的业务，实现最新的read_data函数：\n\n\n\n其实就是多了缓存时间参数，过期后nginx缓存会自动删除，下次访问即可更新缓存。\n\n这里给商品基本信息设置超时时间为30分钟，库存为1分钟。\n\n因为库存更新频率较高，如果缓存时间过长，可能与数据库差异较大。\n\n3）完整的item.lua文件：\n\n-- 导入common函数库\nlocal common = require(\'common\')\nlocal read_http = common.read_http\nlocal read_redis = common.read_redis\n-- 导入cjson库\nlocal cjson = require(\'cjson\')\n-- 导入共享词典，本地缓存\nlocal item_cache = ngx.shared.item_cache\n\n-- 封装查询函数\n-- todo 查询逻辑，想查询nginx本地缓存信息，再查询redis缓存信息，再访问tomcat端口\nfunction read_data(key, expire, path, params)\n    -- 查询本地缓存\n    local val = item_cache:get(key)\n    if not val then\n        ngx.log(ngx.err, "本地缓存查询失败，尝试查询redis， key: ", key)\n        -- 查询redis\n        val = read_redis("127.0.0.1", 6379, key)\n        -- 判断查询结果\n        if not val then\n            ngx.log(ngx.err, "redis查询失败，尝试查询http， key: ", key)\n            -- redis查询失败，去查询http\n            val = read_http(path, params)\n        end\n    end\n    -- 查询成功，把数据写入本地缓存\n    item_cache:set(key, val, expire)\n    -- 返回数据\n    return val\nend\n\n-- 获取路径参数\nlocal id = ngx.var[1]\n\n-- 查询商品信息\nlocal itemjson = read_data("item:id:" .. id, 1800,  "/item/" .. id, nil)\n-- 查询库存信息\nlocal stockjson = read_data("item:stock:id:" .. id, 60, "/item/stock/" .. id, nil)\n\n-- json转化为lua的table\nlocal item = cjson.decode(itemjson)\nlocal stock = cjson.decode(stockjson)\n-- 组合数据\nitem.stock = stock.stock\nitem.sold = stock.sold\n\n-- 把item序列化为json 返回结果\nngx.say(cjson.encode(item))\n',charsets:{cjk:!0}},{title:"Seata基础",frontmatter:{autoSort:98,title:"Seata基础",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/f26c95/",categories:["后端","微服务","进阶","分布式事务"],tags:["知识","微服务","事务"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/60.%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/20.Seata%E5%9F%BA%E7%A1%80.html",relativePath:"01.后端/60.微服务/60.分布式事务/20.Seata基础.md",key:"v-716c9de8",path:"/pages/f26c95/",headers:[{level:2,title:"Seata的架构",slug:"seata的架构",normalizedTitle:"seata的架构",charIndex:137},{level:2,title:"微服务集成Seata",slug:"微服务集成seata",normalizedTitle:"微服务集成seata",charIndex:602},{level:3,title:"引入依赖",slug:"引入依赖",normalizedTitle:"引入依赖",charIndex:641},{level:3,title:"配置TC地址",slug:"配置tc地址",normalizedTitle:"配置tc地址",charIndex:1222},{level:3,title:"其它服务",slug:"其它服务",normalizedTitle:"其它服务",charIndex:2e3}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"Seata的架构 微服务集成Seata 引入依赖 配置TC地址 其它服务",content:'Seata是 2019 年 1 月份蚂蚁金服和阿里巴巴共同开源的分布式事务解决方案。致力于提供高性能和简单易用的分布式事务服务，为用户打造一站式的分布式解决方案。\n\n官网地址：http://seata.io/，其中的文档、播客中提供了大量的使用说明、源码分析。\n\n\n\n\n# Seata的架构\n\nSeata事务管理中有三个重要的角色：\n\n * TC (Transaction Coordinator) - **事务协调者：**维护全局和分支事务的状态，协调全局事务提交或回滚。\n\n * TM (Transaction Manager) - **事务管理器：**定义全局事务的范围、开始全局事务、提交或回滚全局事务。\n\n * RM (Resource Manager) - **资源管理器：**管理分支事务处理的资源，与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚。\n\n整体的架构如图：\n\n\n\nSeata基于上述架构提供了四种不同的分布式事务解决方案：\n\n * **XA模式：**强一致性分阶段事务模式，牺牲了一定的可用性，无业务侵入\n * TCC模式：最终一致的分阶段事务模式，有业务侵入\n * AT模式：**最终一致的分阶段事务模式，无业务侵入，也是Seata的默认模式**\n * SAGA模式：长事务模式，有业务侵入\n\n无论哪种方案，都离不开TC，也就是事务的协调者。\n\n\n# 微服务集成Seata\n\n我们以order-service为例来演示。\n\n\n# 引入依赖\n\n首先，在order-service中引入依赖：\n\n\x3c!--seata--\x3e\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-seata</artifactId>\n    <exclusions>\n        \x3c!--版本较低，1.3.0，因此排除--\x3e \n        <exclusion>\n            <artifactId>seata-spring-boot-starter</artifactId>\n            <groupId>io.seata</groupId>\n        </exclusion>\n    </exclusions>\n</dependency>\n<dependency>\n    <groupId>io.seata</groupId>\n    <artifactId>seata-spring-boot-starter</artifactId>\n    \x3c!--seata starter 采用1.4.2版本--\x3e\n    <version>${seata.version}</version>\n</dependency>\n\n\n\n# 配置TC地址\n\n在order-service中的application.yml中，配置TC服务信息，通过注册中心nacos，结合服务名称获取TC地址：\n\nseata:\n  registry: # TC服务注册中心的配置，微服务根据这些信息去注册中心获取tc服务地址\n    type: nacos # 注册中心类型 nacos\n    nacos:\n      server-addr: 127.0.0.1:8848 # nacos地址\n      namespace: "" # namespace，默认为空\n      group: DEFAULT_GROUP # 分组，默认是DEFAULT_GROUP\n      application: seata-tc-server # seata服务名称\n      username: nacos\n      password: nacos\n  tx-service-group: seata-demo # 事务组名称\n  service:\n    vgroup-mapping: # 事务组与cluster的映射关系\n      seata-demo: SH\n\n\n微服务如何根据这些配置寻找TC的地址呢？\n\n我们知道注册到Nacos中的微服务，确定一个具体实例需要四个信息：\n\n * namespace：命名空间\n * group：分组\n * application：服务名\n * cluster：集群名\n\n以上四个信息，在刚才的yaml文件中都能找到：\n\n\n\nnamespace为空，就是默认的public\n\n结合起来，TC服务的信息就是：public@DEFAULT_GROUP@seata-tc-server@SH，这样就能确定TC服务集群了。然后就可以去Nacos拉取对应的实例信息了。\n\n\n# 其它服务\n\n其它两个微服务也都参考order-service的步骤来做，完全一样。',normalizedContent:'seata是 2019 年 1 月份蚂蚁金服和阿里巴巴共同开源的分布式事务解决方案。致力于提供高性能和简单易用的分布式事务服务，为用户打造一站式的分布式解决方案。\n\n官网地址：http://seata.io/，其中的文档、播客中提供了大量的使用说明、源码分析。\n\n\n\n\n# seata的架构\n\nseata事务管理中有三个重要的角色：\n\n * tc (transaction coordinator) - **事务协调者：**维护全局和分支事务的状态，协调全局事务提交或回滚。\n\n * tm (transaction manager) - **事务管理器：**定义全局事务的范围、开始全局事务、提交或回滚全局事务。\n\n * rm (resource manager) - **资源管理器：**管理分支事务处理的资源，与tc交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚。\n\n整体的架构如图：\n\n\n\nseata基于上述架构提供了四种不同的分布式事务解决方案：\n\n * **xa模式：**强一致性分阶段事务模式，牺牲了一定的可用性，无业务侵入\n * tcc模式：最终一致的分阶段事务模式，有业务侵入\n * at模式：**最终一致的分阶段事务模式，无业务侵入，也是seata的默认模式**\n * saga模式：长事务模式，有业务侵入\n\n无论哪种方案，都离不开tc，也就是事务的协调者。\n\n\n# 微服务集成seata\n\n我们以order-service为例来演示。\n\n\n# 引入依赖\n\n首先，在order-service中引入依赖：\n\n\x3c!--seata--\x3e\n<dependency>\n    <groupid>com.alibaba.cloud</groupid>\n    <artifactid>spring-cloud-starter-alibaba-seata</artifactid>\n    <exclusions>\n        \x3c!--版本较低，1.3.0，因此排除--\x3e \n        <exclusion>\n            <artifactid>seata-spring-boot-starter</artifactid>\n            <groupid>io.seata</groupid>\n        </exclusion>\n    </exclusions>\n</dependency>\n<dependency>\n    <groupid>io.seata</groupid>\n    <artifactid>seata-spring-boot-starter</artifactid>\n    \x3c!--seata starter 采用1.4.2版本--\x3e\n    <version>${seata.version}</version>\n</dependency>\n\n\n\n# 配置tc地址\n\n在order-service中的application.yml中，配置tc服务信息，通过注册中心nacos，结合服务名称获取tc地址：\n\nseata:\n  registry: # tc服务注册中心的配置，微服务根据这些信息去注册中心获取tc服务地址\n    type: nacos # 注册中心类型 nacos\n    nacos:\n      server-addr: 127.0.0.1:8848 # nacos地址\n      namespace: "" # namespace，默认为空\n      group: default_group # 分组，默认是default_group\n      application: seata-tc-server # seata服务名称\n      username: nacos\n      password: nacos\n  tx-service-group: seata-demo # 事务组名称\n  service:\n    vgroup-mapping: # 事务组与cluster的映射关系\n      seata-demo: sh\n\n\n微服务如何根据这些配置寻找tc的地址呢？\n\n我们知道注册到nacos中的微服务，确定一个具体实例需要四个信息：\n\n * namespace：命名空间\n * group：分组\n * application：服务名\n * cluster：集群名\n\n以上四个信息，在刚才的yaml文件中都能找到：\n\n\n\nnamespace为空，就是默认的public\n\n结合起来，tc服务的信息就是：public@default_group@seata-tc-server@sh，这样就能确定tc服务集群了。然后就可以去nacos拉取对应的实例信息了。\n\n\n# 其它服务\n\n其它两个微服务也都参考order-service的步骤来做，完全一样。',charsets:{cjk:!0}},{title:"Seata事务模式",frontmatter:{autoSort:97,title:"Seata事务模式",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/2bcba9/",categories:["后端","微服务","进阶","分布式事务"],tags:["知识","微服务","事务"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/60.%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/25.Seata%E4%BA%8B%E5%8A%A1%E6%A8%A1%E5%BC%8F.html",relativePath:"01.后端/60.微服务/60.分布式事务/25.Seata事务模式.md",key:"v-18ec41e8",path:"/pages/2bcba9/",headers:[{level:2,title:"XA模式",slug:"xa模式",normalizedTitle:"xa模式",charIndex:32},{level:3,title:"两阶段提交",slug:"两阶段提交",normalizedTitle:"两阶段提交",charIndex:164},{level:3,title:"Seata的XA模型",slug:"seata的xa模型",normalizedTitle:"seata的xa模型",charIndex:403},{level:3,title:"优缺点",slug:"优缺点",normalizedTitle:"优缺点",charIndex:629},{level:3,title:"实现XA模式",slug:"实现xa模式",normalizedTitle:"实现xa模式",charIndex:777},{level:2,title:"AT模式",slug:"at模式",normalizedTitle:"at模式",charIndex:1040},{level:3,title:"Seata的AT模型",slug:"seata的at模型",normalizedTitle:"seata的at模型",charIndex:1106},{level:3,title:"流程梳理",slug:"流程梳理",normalizedTitle:"流程梳理",charIndex:1279},{level:3,title:"AT与XA的区别",slug:"at与xa的区别",normalizedTitle:"at与xa的区别",charIndex:1762},{level:3,title:"脏写问题",slug:"脏写问题",normalizedTitle:"脏写问题",charIndex:1895},{level:3,title:"优缺点",slug:"优缺点-2",normalizedTitle:"优缺点",charIndex:629},{level:3,title:"实现AT模式",slug:"实现at模式",normalizedTitle:"实现at模式",charIndex:2304},{level:2,title:"TCC模式",slug:"tcc模式",normalizedTitle:"tcc模式",charIndex:2610},{level:3,title:"流程分析",slug:"流程分析",normalizedTitle:"流程分析",charIndex:2775},{level:3,title:"Seata的TCC模型",slug:"seata的tcc模型",normalizedTitle:"seata的tcc模型",charIndex:3141},{level:3,title:"优缺点",slug:"优缺点-3",normalizedTitle:"优缺点",charIndex:629},{level:3,title:"事务悬挂和空回滚",slug:"事务悬挂和空回滚",normalizedTitle:"事务悬挂和空回滚",charIndex:3536},{level:3,title:"实现TCC模式",slug:"实现tcc模式",normalizedTitle:"实现tcc模式",charIndex:3836},{level:2,title:"SAGA模式",slug:"saga模式",normalizedTitle:"saga模式",charIndex:7514},{level:3,title:"原理",slug:"原理",normalizedTitle:"原理",charIndex:196},{level:3,title:"优缺点",slug:"优缺点-4",normalizedTitle:"优缺点",charIndex:629},{level:2,title:"四种模式对比",slug:"四种模式对比",normalizedTitle:"四种模式对比",charIndex:8052}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"XA模式 两阶段提交 Seata的XA模型 优缺点 实现XA模式 AT模式 Seata的AT模型 流程梳理 AT与XA的区别 脏写问题 优缺点 实现AT模式 TCC模式 流程分析 Seata的TCC模型 优缺点 事务悬挂和空回滚 实现TCC模式 SAGA模式 原理 优缺点 四种模式对比",content:'下面我们就一起学习下Seata中的四种不同的事务模式。\n\n\n# XA模式\n\nXA 规范 是 X/Open 组织定义的分布式事务处理（DTP，Distributed Transaction Processing）标准，XA 规范 描述了全局的TM与局部的RM之间的接口，几乎所有主流的数据库都对 XA 规范 提供了支持。\n\n\n# 两阶段提交\n\nXA是规范，目前主流数据库都实现了这种规范，实现的原理都是基于两阶段提交。\n\n正常情况：\n\n\n\n异常情况：\n\n\n\n一阶段：\n\n * 事务协调者通知每个事物参与者执行本地事务\n * 本地事务执行完成后报告事务执行状态给事务协调者，此时事务不提交，继续持有数据库锁\n\n二阶段：\n\n * 事务协调者基于一阶段的报告来判断下一步操作\n   * 如果一阶段都成功，则通知所有事务参与者，提交事务\n   * 如果一阶段任意一个参与者失败，则通知所有事务参与者回滚事务\n\n\n# Seata的XA模型\n\nSeata对原始的XA模式做了简单的封装和改造，以适应自己的事务模型，基本架构如图：\n\n\n\nRM一阶段的工作：\n\n① 注册分支事务到TC\n\n② 执行分支业务sql但不提交\n\n③ 报告执行状态到TC\n\nTC二阶段的工作：\n\n * TC检测各分支事务执行状态\n   \n   a.如果都成功，通知所有RM提交事务\n   \n   b.如果有失败，通知所有RM回滚事务\n\nRM二阶段的工作：\n\n * 接收TC指令，提交或回滚事务\n\n\n# 优缺点\n\nXA模式的优点是什么？\n\n * 事务的强一致性，满足ACID原则。\n * 常用数据库都支持，实现简单，并且没有代码侵入\n\nXA模式的缺点是什么？\n\n * 因为一阶段需要锁定数据库资源，等待二阶段结束才释放，性能较差\n * 依赖关系型数据库实现事务，如果数据库不支持，则不会成功\n\n\n# 实现XA模式\n\nSeata的starter已经完成了XA模式的自动装配，实现非常简单，步骤如下：\n\n1）修改application.yml文件（每个参与事务的微服务），开启XA模式：\n\nseata:\n  data-source-proxy-mode: XA\n\n\n2）给发起全局事务的入口方法添加@GlobalTransactional注解:\n\n本例中是OrderServiceImpl中的create方法.\n\n\n\n3）重启服务并测试\n\n重启order-service，再次测试，发现无论怎样，三个微服务都能成功回滚。\n\n\n# AT模式\n\nSeata默认是AT模式\n\nAT模式同样是分阶段提交的事务模型，不过缺弥补了XA模型中资源锁定周期过长的缺陷。\n\n\n# Seata的AT模型\n\n基本流程图：\n\n\n\n阶段一RM的工作：\n\n * 注册分支事务\n * 记录undo-log（数据快照）\n * 执行业务sql并提交\n * 报告事务状态\n\n阶段二提交时RM的工作：\n\n * 删除undo-log即可\n\n阶段二回滚时RM的工作：\n\n * 根据undo-log恢复数据到更新前\n * 删除undo-log\n\n\n# 流程梳理\n\n我们用一个真实的业务来梳理下AT模式的原理。\n\n比如，现在又一个数据库表，记录用户余额：\n\nID   MONEY\n1    100\n\n其中一个分支业务要执行的SQL为：\n\nupdate tb_account set money = money - 10 where id = 1\n\n\nAT模式下，当前分支事务执行流程如下：\n\n一阶段：\n\n1）TM发起并注册全局事务到TC\n\n2）TM调用分支事务\n\n3）分支事务准备执行业务SQL\n\n4）RM拦截业务SQL，根据where条件查询原始数据，形成快照。\n\n{\n    "id": 1, "money": 100\n}\n\n\n5）RM执行业务SQL，提交本地事务，释放数据库锁。此时 money = 90\n\n6）RM报告本地事务状态给TC\n\n二阶段：\n\n1）TM通知TC事务结束\n\n2）TC检查分支事务状态\n\na）如果都成功，则立即删除快照\n\nb）如果有分支事务失败，需要回滚。读取快照数据（{"id": 1, "money": 100}），将快照恢复到数据库。此时数据库再次恢复为100\n\n流程图：\n\n\n\n\n# AT与XA的区别\n\n简述AT模式与XA模式最大的区别是什么？\n\n * XA模式一阶段不提交事务，锁定资源；AT模式一阶段直接提交，不锁定资源。\n * XA模式依赖数据库机制实现回滚；AT模式利用数据快照实现数据回滚。\n * XA模式强一致；AT模式最终一致\n\n\n# 脏写问题\n\n在多线程并发访问AT模式的分布式事务时，有可能出现脏写问题，如图：\n\n\n\n解决思路就是引入了全局锁的概念。在释放DB锁之前，先拿到全局锁。避免同一时刻有另外一个事务来操作当前数据。\n\n\n\n全局锁与DB锁\n\n * DB锁不释放，其他事务无法对数据库进行任何操作。——XA\n   * DB锁是数据库的\n * 全局锁只锁定该事务正在修改的某个表的某行数据的某列，其他事务可以对数据库中的其他数据进行操作。——AT\n   * 全局锁是 Seata的\n\n极端情况处理\n\n * 非Seata管理全局事务介入同一事物。\n\n\n\n\n# 优缺点\n\nAT模式的优点：\n\n * 一阶段完成直接提交事务，释放数据库资源，性能比较好\n * 利用全局锁实现读写隔离\n * 没有代码侵入，框架自动完成回滚和提交\n\nAT模式的缺点：\n\n * 两阶段之间属于软状态，属于最终一致\n * 框架的快照功能会影响性能，但比XA模式要好很多\n\n\n# 实现AT模式\n\nAT模式中的快照生成、回滚等动作都是由框架自动完成，没有任何代码侵入，因此实现非常简单。\n\n只不过，AT模式需要一个表来记录全局锁、另一张表来记录数据快照undo_log。\n\n1）导入数据库表，记录全局锁\n\n导入课前资料提供的Sql文件：seata-at.sql，其中lock_table导入到TC服务关联的数据库，undo_log表导入到微服务关联的数据库：\n\n\n\n2）修改application.yml文件，将事务模式修改为AT模式即可：\n\nseata:\n  data-source-proxy-mode: AT # 默认就是AT\n\n\n3）重启服务并测试\n\n两个表中的数据用了就删了\n\n\n# TCC模式\n\nTCC模式与AT模式非常相似，每阶段都是独立事务，不同的是TCC通过人工编码来实现数据恢复。需要实现三个方法：\n\n * Try：资源的检测和预留；\n\n * Confirm：完成资源操作业务；要求 Try 成功 Confirm 一定要能成功。\n\n * Cancel：预留资源释放，可以理解为try的反向操作。\n\n\n# 流程分析\n\n举例，一个扣减用户余额的业务。假设账户A原来余额是100，需要余额扣减30元。\n\n * 阶段一（ Try ）：检查余额是否充足，如果充足则冻结金额增加30元，可用余额扣除30\n\n初识余额：\n\n\n\n余额充足，可以冻结：\n\n\n\n此时，总金额 = 冻结金额 + 可用金额，数量依然是100不变。事务直接提交无需等待其它事务。\n\n * 阶段二（Confirm)：假如要提交（Confirm），则冻结金额扣减30\n\n确认可以提交，不过之前可用金额已经扣减过了，这里只要清除冻结金额就好了：\n\n\n\n此时，总金额 = 冻结金额 + 可用金额 = 0 + 70 = 70元\n\n * 阶段二(Canncel)：如果要回滚（Cancel），则冻结金额扣减30，可用余额增加30\n\n需要回滚，那么就要释放冻结金额，恢复可用金额：\n\n\n\n\n# Seata的TCC模型\n\nSeata中的TCC模型依然延续之前的事务架构，如图：\n\n\n\n\n# 优缺点\n\nTCC模式的每个阶段是做什么的？\n\n * Try：资源检查和预留\n * Confirm：业务执行和提交\n * Cancel：预留资源的释放\n\nTCC的优点是什么？\n\n * 一阶段完成直接提交事务，释放数据库资源，性能好\n * 相比AT模型，无需生成快照，无需使用全局锁，性能最强\n * 不依赖数据库事务，而是依赖补偿操作，可以用于非事务型数据库\n\nTCC的缺点是什么？\n\n * 有代码侵入，需要人为编写try、Confirm和Cancel接口，太麻烦\n * 软状态，事务是最终一致\n * 需要考虑Confirm和Cancel的失败情况，做好幂等处理\n   * 幂等 就是 执行几次结果都是一样的。\n   * 因为任务执行失败后，seata会重新尝试执行，有可能造成多次提交事务\n\n\n# 事务悬挂和空回滚\n\n# 1）空回滚\n\n当某分支事务的try阶段阻塞时，可能导致全局事务超时而触发二阶段的cancel操作。在未执行try操作时先执行了cancel操作，这时cancel不能做回滚，就是空回滚。\n\n如图：\n\n\n\n执行cancel操作时，应当判断try是否已经执行，如果尚未执行，则应该空回滚。\n\n# 2）业务悬挂\n\n对于已经空回滚的业务，之前被阻塞的try操作恢复，继续执行try，就永远不可能confirm或cancel ，事务一直处于中间状态，这就是业务悬挂。\n\n执行try操作时，应当判断cancel是否已经执行过了，如果已经执行，应当阻止空回滚后的try操作，避免悬挂\n\n\n# 实现TCC模式\n\n解决空回滚和业务悬挂问题，必须要记录当前事务状态，是在try、还是cancel？\n\n# 1）思路分析\n\n这里我们定义一张表：\n\nCREATE TABLE `account_freeze_tbl` (\n  `xid` varchar(128) NOT NULL,\n  `user_id` varchar(255) DEFAULT NULL COMMENT \'用户id\',\n  `freeze_money` int(11) unsigned DEFAULT \'0\' COMMENT \'冻结金额\',\n  `state` int(1) DEFAULT NULL COMMENT \'事务状态，0:try，1:confirm，2:cancel\',\n  PRIMARY KEY (`xid`) USING BTREE\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=COMPACT;\n\n\n\n其中：\n\n * xid：是全局事务id\n * freeze_money：用来记录用户冻结金额\n * state：用来记录事务状态\n\n那此时，我们的业务开怎么做呢？\n\n * Try业务：\n   * 记录冻结金额和事务状态到account_freeze表\n   * 扣减account表可用金额\n * Confirm业务\n   * 根据xid删除account_freeze表的冻结记录\n * Cancel业务\n   * 修改account_freeze表，冻结金额为0，state为2\n   * 修改account表，恢复可用金额\n * 如何判断是否空回滚？\n   * cancel业务中，根据xid查询account_freeze，如果为null则说明try还没做，需要空回滚\n * 如何避免业务悬挂？\n   * try业务中，根据xid查询account_freeze ，如果已经存在则证明Cancel已经执行，拒绝执行try业务\n\n接下来，我们改造account-service，利用TCC实现余额扣减功能。\n\n# 2）声明TCC接口\n\nTCC的Try、Confirm、Cancel方法都需要在接口中基于注解来声明，\n\n我们在account-service项目中的cn.itcast.account.service包中新建一个接口，声明TCC三个接口：\n\npackage cn.itcast.account.service;\n\nimport io.seata.rm.tcc.api.BusinessActionContext;\nimport io.seata.rm.tcc.api.BusinessActionContextParameter;\nimport io.seata.rm.tcc.api.LocalTCC;\nimport io.seata.rm.tcc.api.TwoPhaseBusinessAction;\n\n@LocalTCC\npublic interface AccountTCCService {\n\n    @TwoPhaseBusinessAction(name = "deduct", commitMethod = "confirm", rollbackMethod = "cancel")\n    void deduct(@BusinessActionContextParameter(paramName = "userId") String userId,\n                @BusinessActionContextParameter(paramName = "money")int money);\n\n    boolean confirm(BusinessActionContext ctx);\n\n    boolean cancel(BusinessActionContext ctx);\n}\n\n\n# 3）编写实现类\n\n在account-service服务中的cn.itcast.account.service.impl包下新建一个类，实现TCC业务：\n\npackage cn.itcast.account.service.impl;\n\nimport cn.itcast.account.entity.AccountFreeze;\nimport cn.itcast.account.mapper.AccountFreezeMapper;\nimport cn.itcast.account.mapper.AccountMapper;\nimport cn.itcast.account.service.AccountTCCService;\nimport io.seata.core.context.RootContext;\nimport io.seata.rm.tcc.api.BusinessActionContext;\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.stereotype.Service;\nimport org.springframework.transaction.annotation.Transactional;\n\n@Service\n@Slf4j\npublic class AccountTCCServiceImpl implements AccountTCCService {\n\n    @Autowired\n    private AccountMapper accountMapper;\n    @Autowired\n    private AccountFreezeMapper freezeMapper;\n\n    @Override\n    @Transactional\n    public void deduct(String userId, int money) {\n        // 0.获取事务id\n        String xid = RootContext.getXID();\n        // 1.扣减可用余额\n        accountMapper.deduct(userId, money);\n        // 2.记录冻结金额，事务状态\n        AccountFreeze freeze = new AccountFreeze();\n        freeze.setUserId(userId);\n        freeze.setFreezeMoney(money);\n        freeze.setState(AccountFreeze.State.TRY);\n        freeze.setXid(xid);\n        freezeMapper.insert(freeze);\n    }\n\n    @Override\n    public boolean confirm(BusinessActionContext ctx) {\n        // 1.获取事务id\n        String xid = ctx.getXid();\n        // 2.根据id删除冻结记录\n        int count = freezeMapper.deleteById(xid);\n        return count ** 1;\n    }\n\n    @Override\n    public boolean cancel(BusinessActionContext ctx) {\n        // 0.查询冻结记录\n        String xid = ctx.getXid();\n        AccountFreeze freeze = freezeMapper.selectById(xid);\n\n        // 1.恢复可用余额\n        accountMapper.refund(freeze.getUserId(), freeze.getFreezeMoney());\n        // 2.将冻结金额清零，状态改为CANCEL\n        freeze.setFreezeMoney(0);\n        freeze.setState(AccountFreeze.State.CANCEL);\n        int count = freezeMapper.updateById(freeze);\n        return count ** 1;\n    }\n}\n\n\n\n# SAGA模式\n\nSaga 模式是 Seata 即将开源的长事务解决方案，将由蚂蚁金服主要贡献。\n\n其理论基础是Hector & Kenneth 在1987年发表的论文Sagas。\n\nSeata官网对于Saga的指南：https://seata.io/zh-cn/docs/user/saga.html\n\n\n# 原理\n\n在 Saga 模式下，分布式事务内有多个参与者，每一个参与者都是一个冲正补偿服务，需要用户根据业务场景实现其正向操作和逆向回滚操作。\n\n分布式事务执行过程中，依次执行各参与者的正向操作，如果所有正向操作均执行成功，那么分布式事务提交。如果任何一个正向操作执行失败，那么分布式事务会去退回去执行前面各参与者的逆向回滚操作，回滚已提交的参与者，使分布式事务回到初始状态。\n\n\n\nSaga也分为两个阶段：\n\n * 一阶段：直接提交本地事务\n * 二阶段：成功则什么都不做；失败则通过编写补偿业务来回滚\n\n\n# 优缺点\n\n优点：\n\n * 事务参与者可以基于事件驱动实现异步调用，吞吐高\n * 一阶段直接提交事务，无锁，性能好\n * 不用编写TCC中的三个阶段，实现简单\n\n缺点：\n\n * 软状态持续时间不确定，时效性差\n * 没有锁，没有事务隔离，会有脏写\n\n\n# 四种模式对比\n\nAT最多\n\n追求极致性能，或者设计到非关系型数据库时 使用 TCC\n\n我们从以下几个方面来对比四种实现：\n\n * 一致性：能否保证事务的一致性？强一致还是最终一致？\n * 隔离性：事务之间的隔离性如何？\n * 代码侵入：是否需要对业务代码改造？\n * 性能：有无性能损耗？\n * 场景：常见的业务场景\n\n如图：\n\n',normalizedContent:'下面我们就一起学习下seata中的四种不同的事务模式。\n\n\n# xa模式\n\nxa 规范 是 x/open 组织定义的分布式事务处理（dtp，distributed transaction processing）标准，xa 规范 描述了全局的tm与局部的rm之间的接口，几乎所有主流的数据库都对 xa 规范 提供了支持。\n\n\n# 两阶段提交\n\nxa是规范，目前主流数据库都实现了这种规范，实现的原理都是基于两阶段提交。\n\n正常情况：\n\n\n\n异常情况：\n\n\n\n一阶段：\n\n * 事务协调者通知每个事物参与者执行本地事务\n * 本地事务执行完成后报告事务执行状态给事务协调者，此时事务不提交，继续持有数据库锁\n\n二阶段：\n\n * 事务协调者基于一阶段的报告来判断下一步操作\n   * 如果一阶段都成功，则通知所有事务参与者，提交事务\n   * 如果一阶段任意一个参与者失败，则通知所有事务参与者回滚事务\n\n\n# seata的xa模型\n\nseata对原始的xa模式做了简单的封装和改造，以适应自己的事务模型，基本架构如图：\n\n\n\nrm一阶段的工作：\n\n① 注册分支事务到tc\n\n② 执行分支业务sql但不提交\n\n③ 报告执行状态到tc\n\ntc二阶段的工作：\n\n * tc检测各分支事务执行状态\n   \n   a.如果都成功，通知所有rm提交事务\n   \n   b.如果有失败，通知所有rm回滚事务\n\nrm二阶段的工作：\n\n * 接收tc指令，提交或回滚事务\n\n\n# 优缺点\n\nxa模式的优点是什么？\n\n * 事务的强一致性，满足acid原则。\n * 常用数据库都支持，实现简单，并且没有代码侵入\n\nxa模式的缺点是什么？\n\n * 因为一阶段需要锁定数据库资源，等待二阶段结束才释放，性能较差\n * 依赖关系型数据库实现事务，如果数据库不支持，则不会成功\n\n\n# 实现xa模式\n\nseata的starter已经完成了xa模式的自动装配，实现非常简单，步骤如下：\n\n1）修改application.yml文件（每个参与事务的微服务），开启xa模式：\n\nseata:\n  data-source-proxy-mode: xa\n\n\n2）给发起全局事务的入口方法添加@globaltransactional注解:\n\n本例中是orderserviceimpl中的create方法.\n\n\n\n3）重启服务并测试\n\n重启order-service，再次测试，发现无论怎样，三个微服务都能成功回滚。\n\n\n# at模式\n\nseata默认是at模式\n\nat模式同样是分阶段提交的事务模型，不过缺弥补了xa模型中资源锁定周期过长的缺陷。\n\n\n# seata的at模型\n\n基本流程图：\n\n\n\n阶段一rm的工作：\n\n * 注册分支事务\n * 记录undo-log（数据快照）\n * 执行业务sql并提交\n * 报告事务状态\n\n阶段二提交时rm的工作：\n\n * 删除undo-log即可\n\n阶段二回滚时rm的工作：\n\n * 根据undo-log恢复数据到更新前\n * 删除undo-log\n\n\n# 流程梳理\n\n我们用一个真实的业务来梳理下at模式的原理。\n\n比如，现在又一个数据库表，记录用户余额：\n\nid   money\n1    100\n\n其中一个分支业务要执行的sql为：\n\nupdate tb_account set money = money - 10 where id = 1\n\n\nat模式下，当前分支事务执行流程如下：\n\n一阶段：\n\n1）tm发起并注册全局事务到tc\n\n2）tm调用分支事务\n\n3）分支事务准备执行业务sql\n\n4）rm拦截业务sql，根据where条件查询原始数据，形成快照。\n\n{\n    "id": 1, "money": 100\n}\n\n\n5）rm执行业务sql，提交本地事务，释放数据库锁。此时 money = 90\n\n6）rm报告本地事务状态给tc\n\n二阶段：\n\n1）tm通知tc事务结束\n\n2）tc检查分支事务状态\n\na）如果都成功，则立即删除快照\n\nb）如果有分支事务失败，需要回滚。读取快照数据（{"id": 1, "money": 100}），将快照恢复到数据库。此时数据库再次恢复为100\n\n流程图：\n\n\n\n\n# at与xa的区别\n\n简述at模式与xa模式最大的区别是什么？\n\n * xa模式一阶段不提交事务，锁定资源；at模式一阶段直接提交，不锁定资源。\n * xa模式依赖数据库机制实现回滚；at模式利用数据快照实现数据回滚。\n * xa模式强一致；at模式最终一致\n\n\n# 脏写问题\n\n在多线程并发访问at模式的分布式事务时，有可能出现脏写问题，如图：\n\n\n\n解决思路就是引入了全局锁的概念。在释放db锁之前，先拿到全局锁。避免同一时刻有另外一个事务来操作当前数据。\n\n\n\n全局锁与db锁\n\n * db锁不释放，其他事务无法对数据库进行任何操作。——xa\n   * db锁是数据库的\n * 全局锁只锁定该事务正在修改的某个表的某行数据的某列，其他事务可以对数据库中的其他数据进行操作。——at\n   * 全局锁是 seata的\n\n极端情况处理\n\n * 非seata管理全局事务介入同一事物。\n\n\n\n\n# 优缺点\n\nat模式的优点：\n\n * 一阶段完成直接提交事务，释放数据库资源，性能比较好\n * 利用全局锁实现读写隔离\n * 没有代码侵入，框架自动完成回滚和提交\n\nat模式的缺点：\n\n * 两阶段之间属于软状态，属于最终一致\n * 框架的快照功能会影响性能，但比xa模式要好很多\n\n\n# 实现at模式\n\nat模式中的快照生成、回滚等动作都是由框架自动完成，没有任何代码侵入，因此实现非常简单。\n\n只不过，at模式需要一个表来记录全局锁、另一张表来记录数据快照undo_log。\n\n1）导入数据库表，记录全局锁\n\n导入课前资料提供的sql文件：seata-at.sql，其中lock_table导入到tc服务关联的数据库，undo_log表导入到微服务关联的数据库：\n\n\n\n2）修改application.yml文件，将事务模式修改为at模式即可：\n\nseata:\n  data-source-proxy-mode: at # 默认就是at\n\n\n3）重启服务并测试\n\n两个表中的数据用了就删了\n\n\n# tcc模式\n\ntcc模式与at模式非常相似，每阶段都是独立事务，不同的是tcc通过人工编码来实现数据恢复。需要实现三个方法：\n\n * try：资源的检测和预留；\n\n * confirm：完成资源操作业务；要求 try 成功 confirm 一定要能成功。\n\n * cancel：预留资源释放，可以理解为try的反向操作。\n\n\n# 流程分析\n\n举例，一个扣减用户余额的业务。假设账户a原来余额是100，需要余额扣减30元。\n\n * 阶段一（ try ）：检查余额是否充足，如果充足则冻结金额增加30元，可用余额扣除30\n\n初识余额：\n\n\n\n余额充足，可以冻结：\n\n\n\n此时，总金额 = 冻结金额 + 可用金额，数量依然是100不变。事务直接提交无需等待其它事务。\n\n * 阶段二（confirm)：假如要提交（confirm），则冻结金额扣减30\n\n确认可以提交，不过之前可用金额已经扣减过了，这里只要清除冻结金额就好了：\n\n\n\n此时，总金额 = 冻结金额 + 可用金额 = 0 + 70 = 70元\n\n * 阶段二(canncel)：如果要回滚（cancel），则冻结金额扣减30，可用余额增加30\n\n需要回滚，那么就要释放冻结金额，恢复可用金额：\n\n\n\n\n# seata的tcc模型\n\nseata中的tcc模型依然延续之前的事务架构，如图：\n\n\n\n\n# 优缺点\n\ntcc模式的每个阶段是做什么的？\n\n * try：资源检查和预留\n * confirm：业务执行和提交\n * cancel：预留资源的释放\n\ntcc的优点是什么？\n\n * 一阶段完成直接提交事务，释放数据库资源，性能好\n * 相比at模型，无需生成快照，无需使用全局锁，性能最强\n * 不依赖数据库事务，而是依赖补偿操作，可以用于非事务型数据库\n\ntcc的缺点是什么？\n\n * 有代码侵入，需要人为编写try、confirm和cancel接口，太麻烦\n * 软状态，事务是最终一致\n * 需要考虑confirm和cancel的失败情况，做好幂等处理\n   * 幂等 就是 执行几次结果都是一样的。\n   * 因为任务执行失败后，seata会重新尝试执行，有可能造成多次提交事务\n\n\n# 事务悬挂和空回滚\n\n# 1）空回滚\n\n当某分支事务的try阶段阻塞时，可能导致全局事务超时而触发二阶段的cancel操作。在未执行try操作时先执行了cancel操作，这时cancel不能做回滚，就是空回滚。\n\n如图：\n\n\n\n执行cancel操作时，应当判断try是否已经执行，如果尚未执行，则应该空回滚。\n\n# 2）业务悬挂\n\n对于已经空回滚的业务，之前被阻塞的try操作恢复，继续执行try，就永远不可能confirm或cancel ，事务一直处于中间状态，这就是业务悬挂。\n\n执行try操作时，应当判断cancel是否已经执行过了，如果已经执行，应当阻止空回滚后的try操作，避免悬挂\n\n\n# 实现tcc模式\n\n解决空回滚和业务悬挂问题，必须要记录当前事务状态，是在try、还是cancel？\n\n# 1）思路分析\n\n这里我们定义一张表：\n\ncreate table `account_freeze_tbl` (\n  `xid` varchar(128) not null,\n  `user_id` varchar(255) default null comment \'用户id\',\n  `freeze_money` int(11) unsigned default \'0\' comment \'冻结金额\',\n  `state` int(1) default null comment \'事务状态，0:try，1:confirm，2:cancel\',\n  primary key (`xid`) using btree\n) engine=innodb default charset=utf8 row_format=compact;\n\n\n\n其中：\n\n * xid：是全局事务id\n * freeze_money：用来记录用户冻结金额\n * state：用来记录事务状态\n\n那此时，我们的业务开怎么做呢？\n\n * try业务：\n   * 记录冻结金额和事务状态到account_freeze表\n   * 扣减account表可用金额\n * confirm业务\n   * 根据xid删除account_freeze表的冻结记录\n * cancel业务\n   * 修改account_freeze表，冻结金额为0，state为2\n   * 修改account表，恢复可用金额\n * 如何判断是否空回滚？\n   * cancel业务中，根据xid查询account_freeze，如果为null则说明try还没做，需要空回滚\n * 如何避免业务悬挂？\n   * try业务中，根据xid查询account_freeze ，如果已经存在则证明cancel已经执行，拒绝执行try业务\n\n接下来，我们改造account-service，利用tcc实现余额扣减功能。\n\n# 2）声明tcc接口\n\ntcc的try、confirm、cancel方法都需要在接口中基于注解来声明，\n\n我们在account-service项目中的cn.itcast.account.service包中新建一个接口，声明tcc三个接口：\n\npackage cn.itcast.account.service;\n\nimport io.seata.rm.tcc.api.businessactioncontext;\nimport io.seata.rm.tcc.api.businessactioncontextparameter;\nimport io.seata.rm.tcc.api.localtcc;\nimport io.seata.rm.tcc.api.twophasebusinessaction;\n\n@localtcc\npublic interface accounttccservice {\n\n    @twophasebusinessaction(name = "deduct", commitmethod = "confirm", rollbackmethod = "cancel")\n    void deduct(@businessactioncontextparameter(paramname = "userid") string userid,\n                @businessactioncontextparameter(paramname = "money")int money);\n\n    boolean confirm(businessactioncontext ctx);\n\n    boolean cancel(businessactioncontext ctx);\n}\n\n\n# 3）编写实现类\n\n在account-service服务中的cn.itcast.account.service.impl包下新建一个类，实现tcc业务：\n\npackage cn.itcast.account.service.impl;\n\nimport cn.itcast.account.entity.accountfreeze;\nimport cn.itcast.account.mapper.accountfreezemapper;\nimport cn.itcast.account.mapper.accountmapper;\nimport cn.itcast.account.service.accounttccservice;\nimport io.seata.core.context.rootcontext;\nimport io.seata.rm.tcc.api.businessactioncontext;\nimport lombok.extern.slf4j.slf4j;\nimport org.springframework.beans.factory.annotation.autowired;\nimport org.springframework.stereotype.service;\nimport org.springframework.transaction.annotation.transactional;\n\n@service\n@slf4j\npublic class accounttccserviceimpl implements accounttccservice {\n\n    @autowired\n    private accountmapper accountmapper;\n    @autowired\n    private accountfreezemapper freezemapper;\n\n    @override\n    @transactional\n    public void deduct(string userid, int money) {\n        // 0.获取事务id\n        string xid = rootcontext.getxid();\n        // 1.扣减可用余额\n        accountmapper.deduct(userid, money);\n        // 2.记录冻结金额，事务状态\n        accountfreeze freeze = new accountfreeze();\n        freeze.setuserid(userid);\n        freeze.setfreezemoney(money);\n        freeze.setstate(accountfreeze.state.try);\n        freeze.setxid(xid);\n        freezemapper.insert(freeze);\n    }\n\n    @override\n    public boolean confirm(businessactioncontext ctx) {\n        // 1.获取事务id\n        string xid = ctx.getxid();\n        // 2.根据id删除冻结记录\n        int count = freezemapper.deletebyid(xid);\n        return count ** 1;\n    }\n\n    @override\n    public boolean cancel(businessactioncontext ctx) {\n        // 0.查询冻结记录\n        string xid = ctx.getxid();\n        accountfreeze freeze = freezemapper.selectbyid(xid);\n\n        // 1.恢复可用余额\n        accountmapper.refund(freeze.getuserid(), freeze.getfreezemoney());\n        // 2.将冻结金额清零，状态改为cancel\n        freeze.setfreezemoney(0);\n        freeze.setstate(accountfreeze.state.cancel);\n        int count = freezemapper.updatebyid(freeze);\n        return count ** 1;\n    }\n}\n\n\n\n# saga模式\n\nsaga 模式是 seata 即将开源的长事务解决方案，将由蚂蚁金服主要贡献。\n\n其理论基础是hector & kenneth 在1987年发表的论文sagas。\n\nseata官网对于saga的指南：https://seata.io/zh-cn/docs/user/saga.html\n\n\n# 原理\n\n在 saga 模式下，分布式事务内有多个参与者，每一个参与者都是一个冲正补偿服务，需要用户根据业务场景实现其正向操作和逆向回滚操作。\n\n分布式事务执行过程中，依次执行各参与者的正向操作，如果所有正向操作均执行成功，那么分布式事务提交。如果任何一个正向操作执行失败，那么分布式事务会去退回去执行前面各参与者的逆向回滚操作，回滚已提交的参与者，使分布式事务回到初始状态。\n\n\n\nsaga也分为两个阶段：\n\n * 一阶段：直接提交本地事务\n * 二阶段：成功则什么都不做；失败则通过编写补偿业务来回滚\n\n\n# 优缺点\n\n优点：\n\n * 事务参与者可以基于事件驱动实现异步调用，吞吐高\n * 一阶段直接提交事务，无锁，性能好\n * 不用编写tcc中的三个阶段，实现简单\n\n缺点：\n\n * 软状态持续时间不确定，时效性差\n * 没有锁，没有事务隔离，会有脏写\n\n\n# 四种模式对比\n\nat最多\n\n追求极致性能，或者设计到非关系型数据库时 使用 tcc\n\n我们从以下几个方面来对比四种实现：\n\n * 一致性：能否保证事务的一致性？强一致还是最终一致？\n * 隔离性：事务之间的隔离性如何？\n * 代码侵入：是否需要对业务代码改造？\n * 性能：有无性能损耗？\n * 场景：常见的业务场景\n\n如图：\n\n',charsets:{cjk:!0}},{title:"流量控制",frontmatter:{autoSort:99,title:"流量控制",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/0f114c/",categories:["后端","微服务","进阶","微服务保护"],tags:["知识","微服务","微服务保护"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/70.%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/20.%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6.html",relativePath:"01.后端/60.微服务/70.微服务保护/20.流量控制.md",key:"v-02b6dc42",path:"/pages/0f114c/",headers:[{level:2,title:"簇点链路",slug:"簇点链路",normalizedTitle:"簇点链路",charIndex:61},{level:2,title:"快速入门",slug:"快速入门",normalizedTitle:"快速入门",charIndex:444},{level:3,title:"示例",slug:"示例",normalizedTitle:"示例",charIndex:453},{level:3,title:"练习：",slug:"练习",normalizedTitle:"练习：",charIndex:581},{level:2,title:"流控模式",slug:"流控模式",normalizedTitle:"流控模式",charIndex:877},{level:3,title:"关联模式",slug:"关联模式",normalizedTitle:"关联模式",charIndex:1046},{level:3,title:"链路模式",slug:"链路模式",normalizedTitle:"链路模式",charIndex:1896},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:1859},{level:2,title:"流控效果",slug:"流控效果",normalizedTitle:"流控效果",charIndex:3797},{level:3,title:"warm up",slug:"warm-up",normalizedTitle:"warm up",charIndex:3915},{level:3,title:"排队等待",slug:"排队等待",normalizedTitle:"排队等待",charIndex:3981},{level:3,title:"总结",slug:"总结-2",normalizedTitle:"总结",charIndex:1859},{level:2,title:"热点参数限流",slug:"热点参数限流",normalizedTitle:"热点参数限流",charIndex:412},{level:3,title:"全局参数限流",slug:"全局参数限流",normalizedTitle:"全局参数限流",charIndex:5526},{level:3,title:"热点参数限流",slug:"热点参数限流-2",normalizedTitle:"热点参数限流",charIndex:412},{level:3,title:"案例",slug:"案例",normalizedTitle:"案例",charIndex:2046}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"簇点链路 快速入门 示例 练习： 流控模式 关联模式 链路模式 总结 流控效果 warm up 排队等待 总结 热点参数限流 全局参数限流 热点参数限流 案例",content:'雪崩问题虽然有四种方案，但是限流是避免服务因突发的流量而发生故障，是对微服务雪崩问题的预防。我们先学习这种模式。\n\n\n# 簇点链路\n\n当请求进入微服务时，首先会访问DispatcherServlet，然后进入Controller、Service、Mapper，这样的一个调用链就叫做簇点链路。簇点链路中被监控的每一个接口就是一个资源。\n\n默认情况下sentinel会监控SpringMVC的每一个端点（Endpoint，也就是controller中的方法），因此SpringMVC的每一个端点（Endpoint）就是调用链路中的一个资源。\n\n例如，我们刚才访问的order-service中的OrderController中的端点：/order/{orderId}\n\n\n\n流控、熔断等都是针对簇点链路中的资源来设置的，因此我们可以点击对应资源后面的按钮来设置规则：\n\n * 流控：流量控制\n * 降级：降级熔断\n * 热点：热点参数限流，是限流的一种\n * 授权：请求的权限控制\n\n\n# 快速入门\n\n\n# 示例\n\n点击资源/order/{orderId}后面的流控按钮，就可以弹出表单。\n\n\n\n表单中可以填写限流规则，如下：\n\n\n\n其含义是限制 /order/{orderId}这个资源的单机QPS为1，即每秒只允许1次请求，超出的请求会被拦截并报错。\n\n\n# 练习：\n\n需求：给 /order/{orderId}这个资源设置流控规则，QPS不能超过 5，然后测试。\n\n1）首先在sentinel控制台添加限流规则\n\n\n\n2）利用jmeter测试\n\n如果没有用过jmeter，可以参考课前资料提供的文档《Jmeter快速入门.md》\n\n课前资料提供了编写好的Jmeter测试样例：\n\n\n\n打开jmeter，导入课前资料提供的测试样例：\n\n\n\n选择：\n\n\n\n20个用户，2秒内运行完，QPS是10，超过了5.\n\n选中流控入门，QPS<5右键运行：\n\n\n\n> 注意，不要点击菜单中的执行按钮来运行。\n\n结果：\n\n\n\n可以看到，成功的请求每次只有5个\n\n\n# 流控模式\n\n在添加限流规则时，点击高级选项，可以选择三种流控模式：\n\n * 直接：统计当前资源的请求，触发阈值时对当前资源直接限流，也是默认的模式\n * 关联：统计与当前资源相关的另一个资源，触发阈值时，对当前资源限流\n * 链路：统计从指定链路访问到本资源的请求，触发阈值时，对指定链路限流\n\n\n\n快速入门测试的就是直接模式。\n\n\n# 关联模式\n\n关联模式：统计与当前资源相关的另一个资源，触发阈值时，对当前资源限流\n\n配置规则：\n\n\n\n语法说明：当/write资源访问量触发阈值时，就会对/read资源限流，避免影响/write资源。\n\n使用场景：比如用户支付时需要修改订单状态，同时用户要查询订单。查询和修改操作会争抢数据库锁，产生竞争。业务需求是优先支付和更新订单的业务，因此当修改订单业务触发阈值时，需要对查询订单业务限流。\n\n需求说明：\n\n * 在OrderController新建两个端点：/order/query和/order/update，无需实现业务\n\n * 配置流控规则，当/order/ update资源被访问的QPS超过5时，对/order/query请求限流\n\n1）定义/order/query端点，模拟订单查询\n\n@GetMapping("/query")\npublic String queryOrder() {\n    return "查询订单成功";\n}\n\n\n2）定义/order/update端点，模拟订单更新\n\n@GetMapping("/update")\npublic String updateOrder() {\n    return "更新订单成功";\n}\n\n\n重启服务，查看sentinel控制台的簇点链路：\n\n\n\n3）配置流控规则\n\n对哪个端点限流，就点击哪个端点后面的按钮。我们是对订单查询/order/query限流，因此点击它后面的按钮：\n\n\n\n在表单中填写流控规则：\n\n\n\n4）在Jmeter测试\n\n选择《流控模式-关联》：\n\n\n\n可以看到1000个用户，100秒，因此QPS为10，超过了我们设定的阈值：5\n\n查看http请求：\n\n\n\n请求的目标是/order/update，这样这个断点就会触发阈值。\n\n但限流的目标是/order/query，我们在浏览器访问，可以发现：\n\n\n\n确实被限流了。\n\n5）总结\n\n当优先级高的服务触发阈值时，对优先级低的服务进行限制\n\n\n\n\n# 链路模式\n\n链路模式：只针对从指定链路访问到本资源的请求做统计，判断是否超过阈值。\n\n配置示例：\n\n例如有两条请求链路：\n\n * /test1 --\x3e /common\n\n * /test2 --\x3e /common\n\n如果只希望统计从/test2进入到/common的请求，则可以这样配置：\n\n\n\n实战案例\n\n需求：有查询订单和创建订单业务，两者都需要查询商品。针对从查询订单进入到查询商品的请求统计，并设置限流。\n\n步骤：\n\n 1. 在OrderService中添加一个queryGoods方法，不用实现业务\n\n 2. 在OrderController中，改造/order/query端点，调用OrderService中的queryGoods方法\n\n 3. 在OrderController中添加一个/order/save的端点，调用OrderService的queryGoods方法\n\n 4. 给queryGoods设置限流规则，从/order/query进入queryGoods的方法限制QPS必须小于2\n\n实现：\n\n# 1）添加查询商品方法\n\n在order-service服务中，给OrderService类添加一个queryGoods方法：\n\npublic void queryGoods(){\n    System.err.println("查询商品");\n}\n\n\n# 2）查询订单时，查询商品\n\n在order-service的OrderController中，修改/order/query端点的业务逻辑：\n\n@GetMapping("/query")\npublic String queryOrder() {\n    // 查询商品\n    orderService.queryGoods();\n    // 查询订单\n    System.out.println("查询订单");\n    return "查询订单成功";\n}\n\n\n# 3）新增订单，查询商品\n\n在order-service的OrderController中，修改/order/save端点，模拟新增订单：\n\n@GetMapping("/save")\npublic String saveOrder() {\n    // 查询商品\n    orderService.queryGoods();\n    // 查询订单\n    System.err.println("新增订单");\n    return "新增订单成功";\n}\n\n\n# 4）给查询商品添加资源标记\n\n默认情况下，OrderService中的方法是不被Sentinel监控的，需要我们自己通过注解来标记要监控的方法。\n\n给OrderService的queryGoods方法添加@SentinelResource注解：\n\n@SentinelResource("goods")\npublic void queryGoods(){\n    System.err.println("查询商品");\n}\n\n\n链路模式中，是对不同来源的两个链路做监控。但是sentinel默认会给进入SpringMVC的所有请求设置同一个root资源，会导致链路模式失效。\n\n我们需要关闭这种对SpringMVC的资源聚合，修改order-service服务的application.yml文件：\n\nspring:\n  cloud:\n    sentinel:\n      web-context-unify: false # 关闭context整合\n\n\n重启服务，访问/order/query和/order/save，可以查看到sentinel的簇点链路规则中，出现了新的资源：\n\n\n\n# 5）添加流控规则\n\n点击goods资源后面的流控按钮，在弹出的表单中填写下面信息：\n\n\n\n只统计从/order/query进入/goods的资源，QPS阈值为2，超出则被限流。\n\n# 6）Jmeter测试\n\n选择《流控模式-链路》：\n\n\n\n可以看到这里200个用户，50秒内发完，QPS为4，超过了我们设定的阈值2\n\n一个http请求是访问/order/save：\n\n\n\n运行的结果：\n\n\n\n完全不受影响。\n\n另一个是访问/order/query：\n\n\n\n运行结果：\n\n\n\n每次只有2个通过。\n\n\n# 总结\n\n流控模式有哪些？\n\n•直接：对当前资源限流\n\n•关联：高优先级资源触发阈值，对低优先级资源限流。\n\n•链路：阈值统计时，只统计从指定资源进入当前资源的请求，是对请求来源的限流\n\n\n# 流控效果\n\n在流控的高级选项中，还有一个流控效果选项：\n\n\n\n流控效果是指请求达到流控阈值时应该采取的措施，包括三种：\n\n * 快速失败：达到阈值后，新的请求会被立即拒绝并抛出FlowException异常。是默认的处理方式。\n\n * warm up：预热模式，对超出阈值的请求同样是拒绝并抛出异常。但这种模式阈值会动态变化，从一个较小值逐渐增加到最大阈值。\n\n * 排队等待：让所有的请求按照先后次序排队执行，两个请求的间隔不能大于指定时长\n\n\n# warm up\n\n阈值一般是一个微服务能承担的最大QPS，但是一个服务刚刚启动时，一切资源尚未初始化（冷启动），如果直接将QPS跑到最大值，可能导致服务瞬间宕机。\n\nwarm up也叫预热模式，是应对服务冷启动的一种方案。请求阈值初始值是 maxThreshold / coldFactor，持续指定时长后，逐渐提高到maxThreshold值。而coldFactor的默认值是3.\n\n例如，我设置QPS的maxThreshold为10，预热时间为5秒，那么初始阈值就是 10 / 3 ，也就是3，然后在5秒后逐渐增长到10.\n\n\n\n案例\n\n需求：给/order/{orderId}这个资源设置限流，最大QPS为10，利用warm up效果，预热时长为5秒\n\n# 1）配置流控规则：\n\n\n\n# 2）Jmeter测试\n\n选择《流控效果，warm up》：\n\n\n\nQPS为10.\n\n刚刚启动时，大部分请求失败，成功的只有3个，说明QPS被限定在3：\n\n\n\n随着时间推移，成功比例越来越高：\n\n\n\n到Sentinel控制台查看实时监控：\n\n\n\n一段时间后：\n\n\n\n\n# 排队等待\n\n当请求超过QPS阈值时，快速失败和warm up 会拒绝新的请求并抛出异常。\n\n而排队等待则是让所有请求进入一个队列中，然后按照阈值允许的时间间隔依次执行。后来的请求必须等待前面执行完成，如果请求预期的等待时间超出最大时长，则会被拒绝。\n\n工作原理\n\n例如：QPS = 5，意味着每200ms处理一个队列中的请求；timeout = 2000，意味着预期等待时长超过2000ms的请求会被拒绝并抛出异常。\n\n那什么叫做预期等待时长呢？\n\n比如现在一下子来了12 个请求，因为每200ms执行一个请求，那么：\n\n * 第6个请求的预期等待时长 = 200 * （6 - 1） = 1000ms\n * 第12个请求的预期等待时长 = 200 * （12-1） = 2200ms\n\n现在，第1秒同时接收到10个请求，但第2秒只有1个请求，此时QPS的曲线这样的：\n\n\n\n如果使用队列模式做流控，所有进入的请求都要排队，以固定的200ms的间隔执行，QPS会变的很平滑：\n\n\n\n平滑的QPS曲线，对于服务器来说是更友好的。\n\n案例\n\n需求：给/order/{orderId}这个资源设置限流，最大QPS为10，利用排队的流控效果，超时时长设置为5s\n\n# 1）添加流控规则\n\n\n\n# 2）Jmeter测试\n\n选择《流控效果，队列》：\n\n\n\nQPS为15，已经超过了我们设定的10。\n\n如果是之前的 快速失败、warmup模式，超出的请求应该会直接报错。\n\n但是我们看看队列模式的运行结果：\n\n\n\n全部都通过了。\n\n再去sentinel查看实时监控的QPS曲线：\n\n\n\nQPS非常平滑，一致保持在10，但是超出的请求没有被拒绝，而是放入队列。因此响应时间（等待时间）会越来越长。\n\n当队列满了以后，才会有部分请求失败：\n\n\n\n\n# 总结\n\n流控效果有哪些？\n\n * 快速失败：QPS超过阈值时，拒绝新的请求\n * warm up： QPS超过阈值时，拒绝新的请求；QPS阈值是逐渐提升的，可以避免冷启动时高并发导致服务宕机。\n * 排队等待：请求会进入队列，按照阈值允许的时间间隔依次执行请求；如果请求预期等待时长大于超时时间，直接拒绝\n   * 流量整形\n\n\n# 热点参数限流\n\n之前的限流是统计访问某个资源的所有请求，判断是否超过QPS阈值。而热点参数限流是分别统计参数值相同的请求，判断是否超过QPS阈值。\n\n更细粒度的限流\n\n\n# 全局参数限流\n\n例如，一个根据id查询商品的接口：\n\n\n\n访问/goods/{id}的请求中，id参数值会有变化，热点参数限流会根据参数值分别统计QPS，统计结果：\n\n\n\n当id=1的请求触发阈值被限流时，id值不为1的请求不受影响。\n\n配置示例：\n\n\n\n代表的含义是：对hot这个资源的0号参数（第一个参数）做统计，每1秒相同参数值的请求数不能超过5\n\n\n# 热点参数限流\n\n刚才的配置中，对查询商品这个接口的所有商品一视同仁，QPS都限定为5.\n\n而在实际开发中，可能部分商品是热点商品，例如秒杀商品，我们希望这部分商品的QPS限制与其它商品不一样，高一些。那就需要配置热点参数限流的高级选项了：\n\n\n\n结合上一个配置，这里的含义是对0号的long类型参数限流，每1秒相同参数的QPS不能超过5，有两个例外：\n\n•如果参数值是100，则每1秒允许的QPS为10\n\n•如果参数值是101，则每1秒允许的QPS为15\n\n\n# 案例\n\n案例需求：给/order/{orderId}这个资源添加热点参数限流，规则如下：\n\n•默认的热点参数规则是每1秒请求量不超过2\n\n•给102这个参数设置例外：每1秒请求量不超过4\n\n•给103这个参数设置例外：每1秒请求量不超过10\n\n注意事项：热点参数限流对默认的SpringMVC资源无效，需要利用@SentinelResource注解标记资源\n\n * 即 只能对使用注解标记的资源进行热点参数限流。\n\n# 1）标记资源\n\n给order-service中的OrderController中的/order/{orderId}资源添加注解：\n\n\n\n# 2）热点参数限流规则\n\n访问该接口，可以看到我们标记的hot资源出现了：\n\n\n\n这里不要点击hot后面的按钮，页面有BUG\n\n点击左侧菜单中热点规则菜单：\n\n\n\n点击新增，填写表单：\n\n\n\n# 3）Jmeter测试\n\n选择《热点参数限流 QPS1》：\n\n\n\n这里发起请求的QPS为5.\n\n包含3个http请求：\n\n普通参数，QPS阈值为2\n\n\n\n运行结果：\n\n\n\n例外项，QPS阈值为4\n\n\n\n运行结果：\n\n\n\n例外项，QPS阈值为10\n\n\n\n运行结果：\n\n',normalizedContent:'雪崩问题虽然有四种方案，但是限流是避免服务因突发的流量而发生故障，是对微服务雪崩问题的预防。我们先学习这种模式。\n\n\n# 簇点链路\n\n当请求进入微服务时，首先会访问dispatcherservlet，然后进入controller、service、mapper，这样的一个调用链就叫做簇点链路。簇点链路中被监控的每一个接口就是一个资源。\n\n默认情况下sentinel会监控springmvc的每一个端点（endpoint，也就是controller中的方法），因此springmvc的每一个端点（endpoint）就是调用链路中的一个资源。\n\n例如，我们刚才访问的order-service中的ordercontroller中的端点：/order/{orderid}\n\n\n\n流控、熔断等都是针对簇点链路中的资源来设置的，因此我们可以点击对应资源后面的按钮来设置规则：\n\n * 流控：流量控制\n * 降级：降级熔断\n * 热点：热点参数限流，是限流的一种\n * 授权：请求的权限控制\n\n\n# 快速入门\n\n\n# 示例\n\n点击资源/order/{orderid}后面的流控按钮，就可以弹出表单。\n\n\n\n表单中可以填写限流规则，如下：\n\n\n\n其含义是限制 /order/{orderid}这个资源的单机qps为1，即每秒只允许1次请求，超出的请求会被拦截并报错。\n\n\n# 练习：\n\n需求：给 /order/{orderid}这个资源设置流控规则，qps不能超过 5，然后测试。\n\n1）首先在sentinel控制台添加限流规则\n\n\n\n2）利用jmeter测试\n\n如果没有用过jmeter，可以参考课前资料提供的文档《jmeter快速入门.md》\n\n课前资料提供了编写好的jmeter测试样例：\n\n\n\n打开jmeter，导入课前资料提供的测试样例：\n\n\n\n选择：\n\n\n\n20个用户，2秒内运行完，qps是10，超过了5.\n\n选中流控入门，qps<5右键运行：\n\n\n\n> 注意，不要点击菜单中的执行按钮来运行。\n\n结果：\n\n\n\n可以看到，成功的请求每次只有5个\n\n\n# 流控模式\n\n在添加限流规则时，点击高级选项，可以选择三种流控模式：\n\n * 直接：统计当前资源的请求，触发阈值时对当前资源直接限流，也是默认的模式\n * 关联：统计与当前资源相关的另一个资源，触发阈值时，对当前资源限流\n * 链路：统计从指定链路访问到本资源的请求，触发阈值时，对指定链路限流\n\n\n\n快速入门测试的就是直接模式。\n\n\n# 关联模式\n\n关联模式：统计与当前资源相关的另一个资源，触发阈值时，对当前资源限流\n\n配置规则：\n\n\n\n语法说明：当/write资源访问量触发阈值时，就会对/read资源限流，避免影响/write资源。\n\n使用场景：比如用户支付时需要修改订单状态，同时用户要查询订单。查询和修改操作会争抢数据库锁，产生竞争。业务需求是优先支付和更新订单的业务，因此当修改订单业务触发阈值时，需要对查询订单业务限流。\n\n需求说明：\n\n * 在ordercontroller新建两个端点：/order/query和/order/update，无需实现业务\n\n * 配置流控规则，当/order/ update资源被访问的qps超过5时，对/order/query请求限流\n\n1）定义/order/query端点，模拟订单查询\n\n@getmapping("/query")\npublic string queryorder() {\n    return "查询订单成功";\n}\n\n\n2）定义/order/update端点，模拟订单更新\n\n@getmapping("/update")\npublic string updateorder() {\n    return "更新订单成功";\n}\n\n\n重启服务，查看sentinel控制台的簇点链路：\n\n\n\n3）配置流控规则\n\n对哪个端点限流，就点击哪个端点后面的按钮。我们是对订单查询/order/query限流，因此点击它后面的按钮：\n\n\n\n在表单中填写流控规则：\n\n\n\n4）在jmeter测试\n\n选择《流控模式-关联》：\n\n\n\n可以看到1000个用户，100秒，因此qps为10，超过了我们设定的阈值：5\n\n查看http请求：\n\n\n\n请求的目标是/order/update，这样这个断点就会触发阈值。\n\n但限流的目标是/order/query，我们在浏览器访问，可以发现：\n\n\n\n确实被限流了。\n\n5）总结\n\n当优先级高的服务触发阈值时，对优先级低的服务进行限制\n\n\n\n\n# 链路模式\n\n链路模式：只针对从指定链路访问到本资源的请求做统计，判断是否超过阈值。\n\n配置示例：\n\n例如有两条请求链路：\n\n * /test1 --\x3e /common\n\n * /test2 --\x3e /common\n\n如果只希望统计从/test2进入到/common的请求，则可以这样配置：\n\n\n\n实战案例\n\n需求：有查询订单和创建订单业务，两者都需要查询商品。针对从查询订单进入到查询商品的请求统计，并设置限流。\n\n步骤：\n\n 1. 在orderservice中添加一个querygoods方法，不用实现业务\n\n 2. 在ordercontroller中，改造/order/query端点，调用orderservice中的querygoods方法\n\n 3. 在ordercontroller中添加一个/order/save的端点，调用orderservice的querygoods方法\n\n 4. 给querygoods设置限流规则，从/order/query进入querygoods的方法限制qps必须小于2\n\n实现：\n\n# 1）添加查询商品方法\n\n在order-service服务中，给orderservice类添加一个querygoods方法：\n\npublic void querygoods(){\n    system.err.println("查询商品");\n}\n\n\n# 2）查询订单时，查询商品\n\n在order-service的ordercontroller中，修改/order/query端点的业务逻辑：\n\n@getmapping("/query")\npublic string queryorder() {\n    // 查询商品\n    orderservice.querygoods();\n    // 查询订单\n    system.out.println("查询订单");\n    return "查询订单成功";\n}\n\n\n# 3）新增订单，查询商品\n\n在order-service的ordercontroller中，修改/order/save端点，模拟新增订单：\n\n@getmapping("/save")\npublic string saveorder() {\n    // 查询商品\n    orderservice.querygoods();\n    // 查询订单\n    system.err.println("新增订单");\n    return "新增订单成功";\n}\n\n\n# 4）给查询商品添加资源标记\n\n默认情况下，orderservice中的方法是不被sentinel监控的，需要我们自己通过注解来标记要监控的方法。\n\n给orderservice的querygoods方法添加@sentinelresource注解：\n\n@sentinelresource("goods")\npublic void querygoods(){\n    system.err.println("查询商品");\n}\n\n\n链路模式中，是对不同来源的两个链路做监控。但是sentinel默认会给进入springmvc的所有请求设置同一个root资源，会导致链路模式失效。\n\n我们需要关闭这种对springmvc的资源聚合，修改order-service服务的application.yml文件：\n\nspring:\n  cloud:\n    sentinel:\n      web-context-unify: false # 关闭context整合\n\n\n重启服务，访问/order/query和/order/save，可以查看到sentinel的簇点链路规则中，出现了新的资源：\n\n\n\n# 5）添加流控规则\n\n点击goods资源后面的流控按钮，在弹出的表单中填写下面信息：\n\n\n\n只统计从/order/query进入/goods的资源，qps阈值为2，超出则被限流。\n\n# 6）jmeter测试\n\n选择《流控模式-链路》：\n\n\n\n可以看到这里200个用户，50秒内发完，qps为4，超过了我们设定的阈值2\n\n一个http请求是访问/order/save：\n\n\n\n运行的结果：\n\n\n\n完全不受影响。\n\n另一个是访问/order/query：\n\n\n\n运行结果：\n\n\n\n每次只有2个通过。\n\n\n# 总结\n\n流控模式有哪些？\n\n•直接：对当前资源限流\n\n•关联：高优先级资源触发阈值，对低优先级资源限流。\n\n•链路：阈值统计时，只统计从指定资源进入当前资源的请求，是对请求来源的限流\n\n\n# 流控效果\n\n在流控的高级选项中，还有一个流控效果选项：\n\n\n\n流控效果是指请求达到流控阈值时应该采取的措施，包括三种：\n\n * 快速失败：达到阈值后，新的请求会被立即拒绝并抛出flowexception异常。是默认的处理方式。\n\n * warm up：预热模式，对超出阈值的请求同样是拒绝并抛出异常。但这种模式阈值会动态变化，从一个较小值逐渐增加到最大阈值。\n\n * 排队等待：让所有的请求按照先后次序排队执行，两个请求的间隔不能大于指定时长\n\n\n# warm up\n\n阈值一般是一个微服务能承担的最大qps，但是一个服务刚刚启动时，一切资源尚未初始化（冷启动），如果直接将qps跑到最大值，可能导致服务瞬间宕机。\n\nwarm up也叫预热模式，是应对服务冷启动的一种方案。请求阈值初始值是 maxthreshold / coldfactor，持续指定时长后，逐渐提高到maxthreshold值。而coldfactor的默认值是3.\n\n例如，我设置qps的maxthreshold为10，预热时间为5秒，那么初始阈值就是 10 / 3 ，也就是3，然后在5秒后逐渐增长到10.\n\n\n\n案例\n\n需求：给/order/{orderid}这个资源设置限流，最大qps为10，利用warm up效果，预热时长为5秒\n\n# 1）配置流控规则：\n\n\n\n# 2）jmeter测试\n\n选择《流控效果，warm up》：\n\n\n\nqps为10.\n\n刚刚启动时，大部分请求失败，成功的只有3个，说明qps被限定在3：\n\n\n\n随着时间推移，成功比例越来越高：\n\n\n\n到sentinel控制台查看实时监控：\n\n\n\n一段时间后：\n\n\n\n\n# 排队等待\n\n当请求超过qps阈值时，快速失败和warm up 会拒绝新的请求并抛出异常。\n\n而排队等待则是让所有请求进入一个队列中，然后按照阈值允许的时间间隔依次执行。后来的请求必须等待前面执行完成，如果请求预期的等待时间超出最大时长，则会被拒绝。\n\n工作原理\n\n例如：qps = 5，意味着每200ms处理一个队列中的请求；timeout = 2000，意味着预期等待时长超过2000ms的请求会被拒绝并抛出异常。\n\n那什么叫做预期等待时长呢？\n\n比如现在一下子来了12 个请求，因为每200ms执行一个请求，那么：\n\n * 第6个请求的预期等待时长 = 200 * （6 - 1） = 1000ms\n * 第12个请求的预期等待时长 = 200 * （12-1） = 2200ms\n\n现在，第1秒同时接收到10个请求，但第2秒只有1个请求，此时qps的曲线这样的：\n\n\n\n如果使用队列模式做流控，所有进入的请求都要排队，以固定的200ms的间隔执行，qps会变的很平滑：\n\n\n\n平滑的qps曲线，对于服务器来说是更友好的。\n\n案例\n\n需求：给/order/{orderid}这个资源设置限流，最大qps为10，利用排队的流控效果，超时时长设置为5s\n\n# 1）添加流控规则\n\n\n\n# 2）jmeter测试\n\n选择《流控效果，队列》：\n\n\n\nqps为15，已经超过了我们设定的10。\n\n如果是之前的 快速失败、warmup模式，超出的请求应该会直接报错。\n\n但是我们看看队列模式的运行结果：\n\n\n\n全部都通过了。\n\n再去sentinel查看实时监控的qps曲线：\n\n\n\nqps非常平滑，一致保持在10，但是超出的请求没有被拒绝，而是放入队列。因此响应时间（等待时间）会越来越长。\n\n当队列满了以后，才会有部分请求失败：\n\n\n\n\n# 总结\n\n流控效果有哪些？\n\n * 快速失败：qps超过阈值时，拒绝新的请求\n * warm up： qps超过阈值时，拒绝新的请求；qps阈值是逐渐提升的，可以避免冷启动时高并发导致服务宕机。\n * 排队等待：请求会进入队列，按照阈值允许的时间间隔依次执行请求；如果请求预期等待时长大于超时时间，直接拒绝\n   * 流量整形\n\n\n# 热点参数限流\n\n之前的限流是统计访问某个资源的所有请求，判断是否超过qps阈值。而热点参数限流是分别统计参数值相同的请求，判断是否超过qps阈值。\n\n更细粒度的限流\n\n\n# 全局参数限流\n\n例如，一个根据id查询商品的接口：\n\n\n\n访问/goods/{id}的请求中，id参数值会有变化，热点参数限流会根据参数值分别统计qps，统计结果：\n\n\n\n当id=1的请求触发阈值被限流时，id值不为1的请求不受影响。\n\n配置示例：\n\n\n\n代表的含义是：对hot这个资源的0号参数（第一个参数）做统计，每1秒相同参数值的请求数不能超过5\n\n\n# 热点参数限流\n\n刚才的配置中，对查询商品这个接口的所有商品一视同仁，qps都限定为5.\n\n而在实际开发中，可能部分商品是热点商品，例如秒杀商品，我们希望这部分商品的qps限制与其它商品不一样，高一些。那就需要配置热点参数限流的高级选项了：\n\n\n\n结合上一个配置，这里的含义是对0号的long类型参数限流，每1秒相同参数的qps不能超过5，有两个例外：\n\n•如果参数值是100，则每1秒允许的qps为10\n\n•如果参数值是101，则每1秒允许的qps为15\n\n\n# 案例\n\n案例需求：给/order/{orderid}这个资源添加热点参数限流，规则如下：\n\n•默认的热点参数规则是每1秒请求量不超过2\n\n•给102这个参数设置例外：每1秒请求量不超过4\n\n•给103这个参数设置例外：每1秒请求量不超过10\n\n注意事项：热点参数限流对默认的springmvc资源无效，需要利用@sentinelresource注解标记资源\n\n * 即 只能对使用注解标记的资源进行热点参数限流。\n\n# 1）标记资源\n\n给order-service中的ordercontroller中的/order/{orderid}资源添加注解：\n\n\n\n# 2）热点参数限流规则\n\n访问该接口，可以看到我们标记的hot资源出现了：\n\n\n\n这里不要点击hot后面的按钮，页面有bug\n\n点击左侧菜单中热点规则菜单：\n\n\n\n点击新增，填写表单：\n\n\n\n# 3）jmeter测试\n\n选择《热点参数限流 qps1》：\n\n\n\n这里发起请求的qps为5.\n\n包含3个http请求：\n\n普通参数，qps阈值为2\n\n\n\n运行结果：\n\n\n\n例外项，qps阈值为4\n\n\n\n运行结果：\n\n\n\n例外项，qps阈值为10\n\n\n\n运行结果：\n\n',charsets:{cjk:!0}},{title:"Sentinel基础",frontmatter:{autoSort:100,title:"Sentinel基础",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/6e3b45/",categories:["后端","微服务","进阶","微服务保护"],tags:["知识","微服务","微服务保护"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/70.%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/05.Sentinel%E5%9F%BA%E7%A1%80.html",relativePath:"01.后端/60.微服务/70.微服务保护/05.Sentinel基础.md",key:"v-11e6d886",path:"/pages/6e3b45/",headers:[{level:2,title:"雪崩问题及解决方案",slug:"雪崩问题及解决方案",normalizedTitle:"雪崩问题及解决方案",charIndex:31},{level:3,title:"雪崩问题",slug:"雪崩问题",normalizedTitle:"雪崩问题",charIndex:21},{level:3,title:"雪崩问题解决",slug:"雪崩问题解决",normalizedTitle:"雪崩问题解决",charIndex:393},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:844},{level:2,title:"服务保护技术对比",slug:"服务保护技术对比",normalizedTitle:"服务保护技术对比",charIndex:1007},{level:2,title:"Sentinel介绍和安装",slug:"sentinel介绍和安装",normalizedTitle:"sentinel介绍和安装",charIndex:1812},{level:3,title:"初识Sentinel",slug:"初识sentinel",normalizedTitle:"初识sentinel",charIndex:1830},{level:3,title:"安装Sentinel",slug:"安装sentinel",normalizedTitle:"安装sentinel",charIndex:2310},{level:2,title:"微服务整合Sentinel",slug:"微服务整合sentinel",normalizedTitle:"微服务整合sentinel",charIndex:2898}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"雪崩问题及解决方案 雪崩问题 雪崩问题解决 总结 服务保护技术对比 Sentinel介绍和安装 初识Sentinel 安装Sentinel 微服务整合Sentinel",content:"在介绍Sentinel之前，我们先来看一下雪崩问题。\n\n\n# 雪崩问题及解决方案\n\n\n# 雪崩问题\n\n微服务调用链路中的某个服务故障，引起整个链路中的所有微服务都不可用，这就是雪崩问题\n\n微服务中，服务间调用关系错综复杂，一个微服务往往依赖于多个其它微服务。\n\n\n\n如图，如果服务提供者I发生了故障，当前的应用的部分业务因为依赖于服务I，因此也会被阻塞。此时，其它不依赖于服务I的业务似乎不受影响。\n\n\n\n但是，依赖服务I的业务请求被阻塞，用户不会得到响应，则tomcat的这个线程不会释放，于是越来越多的用户请求到来，越来越多的线程会阻塞：\n\n\n\n服务器支持的线程和并发数有限，请求一直阻塞，会导致服务器资源耗尽，从而导致所有其它服务都不可用，那么当前服务也就不可用了。\n\n那么，依赖于当前服务的其它服务随着时间的推移，最终也都会变的不可用，形成级联失败，雪崩就发生了：\n\n\n\n\n# 雪崩问题解决\n\n解决雪崩问题的常见方式有四种：\n\n * 超时处理\n * 仓壁模式--线程隔离\n * 熔断降级-断路器\n * 流量控制\n\n# 超时处理\n\n•超时处理：设定超时时间，请求超过一定时间没有响应就返回错误信息，不会无休止等待\n\n\n\n# 仓壁模式\n\n方案2：仓壁模式\n\n仓壁模式来源于船舱的设计：\n\n\n\n船舱都会被隔板分离为多个独立空间，当船体破损时，只会导致部分空间进入，将故障控制在一定范围内，避免整个船体都被淹没。\n\n于此类似，我们可以限定每个业务能使用的线程数，避免耗尽整个tomcat的资源，因此也叫线程隔离。\n\n\n\n# 断路器\n\n断路器模式：由断路器统计业务执行的异常比例，如果超出阈值则会熔断该业务，拦截访问该业务的一切请求。\n\n断路器会统计访问某个服务的请求数量，异常比例：\n\n\n\n当发现访问服务D的请求异常比例过高时，认为服务D有导致雪崩的风险，会拦截访问服务D的一切请求，形成熔断：\n\n\n\n# 限流\n\n流量控制：限制业务访问的QPS，避免服务因流量的突增而故障。\n\n\n\n\n# 总结\n\n什么是雪崩问题？\n\n * 微服务之间相互调用，因为调用链中的一个服务故障，引起整个链路都无法访问的情况。\n\n可以认为：\n\n限流是对服务的保护，避免因瞬间高并发流量而导致服务故障，进而避免雪崩。是一种预防措施。\n\n超时处理、线程隔离、降级熔断是在部分服务故障时，将故障控制在一定范围，避免雪崩。是一种补救措施。\n\n\n# 服务保护技术对比\n\n在SpringCloud当中支持多种服务保护技术：\n\n * Netfix Hystrix\n * Sentinel\n * Resilience4J\n\n早期比较流行的是Hystrix框架，但目前国内实用最广泛的还是阿里巴巴的Sentinel框架，这里我们做下对比：\n\n          SENTINEL                            HYSTRIX\n隔离策略      信号量隔离                               线程池隔离/信号量隔离\n熔断降级策略    基于慢调用比例或异常比例                        基于失败比率\n实时指标实现    滑动窗口                                滑动窗口（基于 RxJava）\n规则配置      支持多种数据源                             支持多种数据源\n扩展性       多个扩展点                               插件的形式\n基于注解的支持   支持                                  支持\n限流        基于 QPS，支持基于调用关系的限流                  有限的支持\n流量整形      支持慢启动、匀速排队模式                        不支持\n系统自适应保护   支持                                  不支持\n控制台       开箱即用，可配置规则、查看秒级监控、机器发现等             不完善\n常见框架的适配   Servlet、Spring Cloud、Dubbo、gRPC 等   Servlet、Spring Cloud Netflix\n\n\n# Sentinel介绍和安装\n\n\n# 初识Sentinel\n\nSentinel是阿里巴巴开源的一款微服务流量控制组件。官网地址：https://sentinelguard.io/zh-cn/index.html\n\nSentinel 具有以下特征:\n\n•丰富的应用场景：Sentinel 承接了阿里巴巴近 10 年的双十一大促流量的核心场景，例如秒杀（即突发流量控制在系统容量可以承受的范围）、消息削峰填谷、集群流量控制、实时熔断下游不可用应用等。\n\n•完备的实时监控：Sentinel 同时提供实时的监控功能。您可以在控制台中看到接入应用的单台机器秒级数据，甚至 500 台以下规模的集群的汇总运行情况。\n\n•广泛的开源生态：Sentinel 提供开箱即用的与其它开源框架/库的整合模块，例如与 Spring Cloud、Dubbo、gRPC 的整合。您只需要引入相应的依赖并进行简单的配置即可快速地接入 Sentinel。\n\n•完善的 SPI 扩展点：Sentinel 提供简单易用、完善的 SPI 扩展接口。您可以通过实现扩展接口来快速地定制逻辑。例如定制规则管理、适配动态数据源等。\n\n\n# 安装Sentinel\n\n1）下载\n\nsentinel官方提供了UI控制台，方便我们对系统做限流设置。大家可以在GitHub下载。\n\n2）运行\n\nD:\\Program\\Sentinel\n\n将jar包放到任意非中文目录，执行命令：\n\njava -jar sentinel-dashboard-1.8.1.jar\n\n\n如果要修改Sentinel的默认端口、账户、密码，可以通过下列配置：\n\n配置项                                默认值        说明\nserver.port                        8080       服务端口\nsentinel.dashboard.auth.username   sentinel   默认用户名\nsentinel.dashboard.auth.password   sentinel   默认密码\n\n例如，修改端口：\n\njava -Dserver.port=8090 -jar sentinel-dashboard-1.8.1.jar\n\n\n3）访问\n\n访问http://localhost:8080页面，就可以看到sentinel的控制台了：\n\n\n\n需要输入账号和密码，默认都是：sentinel\n\n登录后，发现一片空白，什么都没有：\n\n\n\n这是因为我们还没有与微服务整合。\n\n\n# 微服务整合Sentinel\n\n我们在order-service中整合sentinel，并连接sentinel的控制台，步骤如下：\n\n1）引入sentinel依赖\n\n\x3c!--sentinel--\x3e\n<dependency>\n    <groupId>com.alibaba.cloud</groupId> \n    <artifactId>spring-cloud-starter-alibaba-sentinel</artifactId>\n</dependency>\n\n\n2）配置控制台\n\n修改application.yaml文件，添加下面内容：\n\nserver:\n  port: 8088\nspring:\n  cloud: \n    sentinel:\n      transport:\n        dashboard: localhost:8080\n\n\n3）访问order-service的任意端点\n\n打开浏览器，访问http://localhost:8088/order/101，这样才能触发sentinel的监控。\n\n然后再访问sentinel的控制台，查看效果：\n\n",normalizedContent:"在介绍sentinel之前，我们先来看一下雪崩问题。\n\n\n# 雪崩问题及解决方案\n\n\n# 雪崩问题\n\n微服务调用链路中的某个服务故障，引起整个链路中的所有微服务都不可用，这就是雪崩问题\n\n微服务中，服务间调用关系错综复杂，一个微服务往往依赖于多个其它微服务。\n\n\n\n如图，如果服务提供者i发生了故障，当前的应用的部分业务因为依赖于服务i，因此也会被阻塞。此时，其它不依赖于服务i的业务似乎不受影响。\n\n\n\n但是，依赖服务i的业务请求被阻塞，用户不会得到响应，则tomcat的这个线程不会释放，于是越来越多的用户请求到来，越来越多的线程会阻塞：\n\n\n\n服务器支持的线程和并发数有限，请求一直阻塞，会导致服务器资源耗尽，从而导致所有其它服务都不可用，那么当前服务也就不可用了。\n\n那么，依赖于当前服务的其它服务随着时间的推移，最终也都会变的不可用，形成级联失败，雪崩就发生了：\n\n\n\n\n# 雪崩问题解决\n\n解决雪崩问题的常见方式有四种：\n\n * 超时处理\n * 仓壁模式--线程隔离\n * 熔断降级-断路器\n * 流量控制\n\n# 超时处理\n\n•超时处理：设定超时时间，请求超过一定时间没有响应就返回错误信息，不会无休止等待\n\n\n\n# 仓壁模式\n\n方案2：仓壁模式\n\n仓壁模式来源于船舱的设计：\n\n\n\n船舱都会被隔板分离为多个独立空间，当船体破损时，只会导致部分空间进入，将故障控制在一定范围内，避免整个船体都被淹没。\n\n于此类似，我们可以限定每个业务能使用的线程数，避免耗尽整个tomcat的资源，因此也叫线程隔离。\n\n\n\n# 断路器\n\n断路器模式：由断路器统计业务执行的异常比例，如果超出阈值则会熔断该业务，拦截访问该业务的一切请求。\n\n断路器会统计访问某个服务的请求数量，异常比例：\n\n\n\n当发现访问服务d的请求异常比例过高时，认为服务d有导致雪崩的风险，会拦截访问服务d的一切请求，形成熔断：\n\n\n\n# 限流\n\n流量控制：限制业务访问的qps，避免服务因流量的突增而故障。\n\n\n\n\n# 总结\n\n什么是雪崩问题？\n\n * 微服务之间相互调用，因为调用链中的一个服务故障，引起整个链路都无法访问的情况。\n\n可以认为：\n\n限流是对服务的保护，避免因瞬间高并发流量而导致服务故障，进而避免雪崩。是一种预防措施。\n\n超时处理、线程隔离、降级熔断是在部分服务故障时，将故障控制在一定范围，避免雪崩。是一种补救措施。\n\n\n# 服务保护技术对比\n\n在springcloud当中支持多种服务保护技术：\n\n * netfix hystrix\n * sentinel\n * resilience4j\n\n早期比较流行的是hystrix框架，但目前国内实用最广泛的还是阿里巴巴的sentinel框架，这里我们做下对比：\n\n          sentinel                            hystrix\n隔离策略      信号量隔离                               线程池隔离/信号量隔离\n熔断降级策略    基于慢调用比例或异常比例                        基于失败比率\n实时指标实现    滑动窗口                                滑动窗口（基于 rxjava）\n规则配置      支持多种数据源                             支持多种数据源\n扩展性       多个扩展点                               插件的形式\n基于注解的支持   支持                                  支持\n限流        基于 qps，支持基于调用关系的限流                  有限的支持\n流量整形      支持慢启动、匀速排队模式                        不支持\n系统自适应保护   支持                                  不支持\n控制台       开箱即用，可配置规则、查看秒级监控、机器发现等             不完善\n常见框架的适配   servlet、spring cloud、dubbo、grpc 等   servlet、spring cloud netflix\n\n\n# sentinel介绍和安装\n\n\n# 初识sentinel\n\nsentinel是阿里巴巴开源的一款微服务流量控制组件。官网地址：https://sentinelguard.io/zh-cn/index.html\n\nsentinel 具有以下特征:\n\n•丰富的应用场景：sentinel 承接了阿里巴巴近 10 年的双十一大促流量的核心场景，例如秒杀（即突发流量控制在系统容量可以承受的范围）、消息削峰填谷、集群流量控制、实时熔断下游不可用应用等。\n\n•完备的实时监控：sentinel 同时提供实时的监控功能。您可以在控制台中看到接入应用的单台机器秒级数据，甚至 500 台以下规模的集群的汇总运行情况。\n\n•广泛的开源生态：sentinel 提供开箱即用的与其它开源框架/库的整合模块，例如与 spring cloud、dubbo、grpc 的整合。您只需要引入相应的依赖并进行简单的配置即可快速地接入 sentinel。\n\n•完善的 spi 扩展点：sentinel 提供简单易用、完善的 spi 扩展接口。您可以通过实现扩展接口来快速地定制逻辑。例如定制规则管理、适配动态数据源等。\n\n\n# 安装sentinel\n\n1）下载\n\nsentinel官方提供了ui控制台，方便我们对系统做限流设置。大家可以在github下载。\n\n2）运行\n\nd:\\program\\sentinel\n\n将jar包放到任意非中文目录，执行命令：\n\njava -jar sentinel-dashboard-1.8.1.jar\n\n\n如果要修改sentinel的默认端口、账户、密码，可以通过下列配置：\n\n配置项                                默认值        说明\nserver.port                        8080       服务端口\nsentinel.dashboard.auth.username   sentinel   默认用户名\nsentinel.dashboard.auth.password   sentinel   默认密码\n\n例如，修改端口：\n\njava -dserver.port=8090 -jar sentinel-dashboard-1.8.1.jar\n\n\n3）访问\n\n访问http://localhost:8080页面，就可以看到sentinel的控制台了：\n\n\n\n需要输入账号和密码，默认都是：sentinel\n\n登录后，发现一片空白，什么都没有：\n\n\n\n这是因为我们还没有与微服务整合。\n\n\n# 微服务整合sentinel\n\n我们在order-service中整合sentinel，并连接sentinel的控制台，步骤如下：\n\n1）引入sentinel依赖\n\n\x3c!--sentinel--\x3e\n<dependency>\n    <groupid>com.alibaba.cloud</groupid> \n    <artifactid>spring-cloud-starter-alibaba-sentinel</artifactid>\n</dependency>\n\n\n2）配置控制台\n\n修改application.yaml文件，添加下面内容：\n\nserver:\n  port: 8088\nspring:\n  cloud: \n    sentinel:\n      transport:\n        dashboard: localhost:8080\n\n\n3）访问order-service的任意端点\n\n打开浏览器，访问http://localhost:8088/order/101，这样才能触发sentinel的监控。\n\n然后再访问sentinel的控制台，查看效果：\n\n",charsets:{cjk:!0}},{title:"隔离和降级",frontmatter:{autoSort:98,title:"隔离和降级",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/b25c7c/",categories:["后端","微服务","进阶","微服务保护"],tags:["知识","微服务","微服务保护"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/70.%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/30.%E9%9A%94%E7%A6%BB%E5%92%8C%E9%99%8D%E7%BA%A7.html",relativePath:"01.后端/60.微服务/70.微服务保护/30.隔离和降级.md",key:"v-2acc9b91",path:"/pages/b25c7c/",headers:[{level:2,title:"FeignClient整合Sentinel",slug:"feignclient整合sentinel",normalizedTitle:"feignclient整合sentinel",charIndex:374},{level:3,title:"修改配置，开启sentinel功能",slug:"修改配置-开启sentinel功能",normalizedTitle:"修改配置，开启sentinel功能",charIndex:461},{level:3,title:"编写失败降级逻辑",slug:"编写失败降级逻辑",normalizedTitle:"编写失败降级逻辑",charIndex:622},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:2198},{level:2,title:"线程隔离（舱壁模式）",slug:"线程隔离-舱壁模式",normalizedTitle:"线程隔离（舱壁模式）",charIndex:73},{level:3,title:"线程隔离的实现方式",slug:"线程隔离的实现方式",normalizedTitle:"线程隔离的实现方式",charIndex:3020},{level:3,title:"sentinel的线程隔离",slug:"sentinel的线程隔离",normalizedTitle:"sentinel的线程隔离",charIndex:3221},{level:3,title:"总结",slug:"总结-2",normalizedTitle:"总结",charIndex:2198},{level:2,title:"熔断降级",slug:"熔断降级",normalizedTitle:"熔断降级",charIndex:84},{level:3,title:"慢调用",slug:"慢调用",normalizedTitle:"慢调用",charIndex:4075},{level:3,title:"异常比例、异常数",slug:"异常比例、异常数",normalizedTitle:"异常比例、异常数",charIndex:4079}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"FeignClient整合Sentinel 修改配置，开启sentinel功能 编写失败降级逻辑 总结 线程隔离（舱壁模式） 线程隔离的实现方式 sentinel的线程隔离 总结 熔断降级 慢调用 异常比例、异常数",content:'限流是一种预防措施，虽然限流可以尽量避免因高并发而引起的服务故障，但服务还会因为其它原因而故障。\n\n而要将这些故障控制在一定范围，避免雪崩，就要靠线程隔离（舱壁模式）和熔断降级手段了。\n\n线程隔离之前讲到过：调用者在调用服务提供者时，给每个调用的请求分配独立线程池，出现故障时，最多消耗这个线程池内资源，避免把调用者的所有资源耗尽。\n\n\n\n熔断降级：是在调用方这边加入断路器，统计对服务提供者的调用，如果调用的失败比例过高，则熔断该业务，不允许访问该服务的提供者了。\n\n\n\n可以看到，不管是线程隔离还是熔断降级，都是对客户端（调用方）的保护。需要在调用方 发起远程调用时做线程隔离、或者服务熔断。\n\n而我们的微服务远程调用都是基于Feign来完成的，因此我们需要将Feign与Sentinel整合，在Feign里面实现线程隔离和服务熔断。\n\n\n# FeignClient整合Sentinel\n\nSpringCloud中，微服务调用都是通过Feign来实现的，因此做客户端保护必须整合Feign和Sentinel。\n\n\n# 修改配置，开启sentinel功能\n\n修改OrderService的application.yml文件，开启Feign的Sentinel功能：\n\n注意 这里修改 feign中的配置文件没有用\n\nfeign:\n  sentinel:\n    enabled: true # 开启feign对sentinel的支持\n\n\n\n# 编写失败降级逻辑\n\n业务失败后，不能直接报错，而应该返回用户一个友好提示或者默认结果，这个就是失败降级逻辑。\n\n给FeignClient编写失败后的降级逻辑\n\n①方式一：FallbackClass，无法对远程调用的异常做处理\n\n②方式二：FallbackFactory，可以对远程调用的异常做处理，我们选择这种\n\n这里我们演示方式二的失败降级处理。\n\n步骤一：在feing-api项目中定义类，实现FallbackFactory：\n\n\n\n代码：\n\npackage cn.itcast.feign.clients.fallback;\n\nimport cn.itcast.feign.clients.UserClient;\nimport cn.itcast.feign.pojo.User;\nimport feign.hystrix.FallbackFactory;\nimport lombok.extern.slf4j.Slf4j;\n\n@Slf4j\npublic class UserClientFallbackFactory implements FallbackFactory<UserClient> {\n    @Override\n    public UserClient create(Throwable throwable) {\n        return new UserClient() {\n            @Override\n            public User findById(Long id) {\n                log.error("查询用户异常", throwable);\n                return new User();\n            }\n        };\n    }\n}\n\n\n\n步骤二：在feing-api项目中的DefaultFeignConfiguration类中将UserClientFallbackFactory注册为一个Bean：\n\n@Bean\npublic UserClientFallbackFactory userClientFallbackFactory(){\n    return new UserClientFallbackFactory();\n}\n\n\n步骤三：在feing-api项目中的UserClient接口中使用UserClientFallbackFactory：\n\nimport cn.itcast.feign.clients.fallback.UserClientFallbackFactory;\nimport cn.itcast.feign.pojo.User;\nimport org.springframework.cloud.openfeign.FeignClient;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.PathVariable;\n\n@FeignClient(value = "userservice", fallbackFactory = UserClientFallbackFactory.class)\npublic interface UserClient {\n\n    @GetMapping("/user/{id}")\n    User findById(@PathVariable("id") Long id);\n}\n\n\n重启后，访问一次订单查询业务，然后查看sentinel控制台，可以看到新的簇点链路：\n\n\n\n\n# 总结\n\nSentinel支持的雪崩解决方案：\n\n * 线程隔离（仓壁模式）\n * 降级熔断\n\nFeign整合Sentinel的步骤：\n\n * 在orderService中的application.yml中配置：feign.sentienl.enable=true\n * 给FeignClient编写FallbackFactory并注册为Bean\n * 将FallbackFactory配置到FeignClient\n\n问题\n\n * 一定要将Feign的配置类引进来\n   \n   @SpringBootApplication\n   //Feign远程调用开关,并指定配置类\n   //TODO 这里一定要指定配置类，不然 feign-api中的config包下的Bean扫描不到\n   // 在config包下的 @Configuration 起作用\n   @EnableFeignClients(clients = UserClient.class, defaultConfiguration = DefaultFeignConfiguration.class)\n   //使用Feign来取代RestTemplate\n   public class OrderApplication {\n   \n       public static void main(String[] args) {\n           SpringApplication.run(OrderApplication.class, args);\n       }\n   }\n   \n\n * spring-cloud的版本要修改为8\n   \n    \x3c!-- TODO 这里要改成版本8 不然会问题--\x3e\n   <spring-cloud.version>Hoxton.SR8</spring-cloud.version>\n   \n\n\n# 线程隔离（舱壁模式）\n\n\n# 线程隔离的实现方式\n\n线程隔离有两种方式实现：\n\n * 线程池隔离\n\n * 信号量隔离（Sentinel默认采用）\n\n如图：\n\n\n\n线程池隔离：给每个服务调用业务分配一个线程池，利用线程池本身实现隔离效果\n\n信号量隔离：不创建线程池，而是计数器模式，记录业务使用的线程数量，达到信号量上限时，禁止新的请求。\n\n两者的优缺点：\n\n * 扇出：我依赖于n个服务；我执行的时候会调用其他n个服务\n\n\n\n\n# sentinel的线程隔离\n\n用法说明：\n\n在添加限流规则时，可以选择两种阈值类型：\n\n\n\n * QPS：就是每秒的请求数，在快速入门中已经演示过\n\n * 线程数：是该资源能使用用的tomcat线程数的最大值。也就是通过限制线程数量，实现线程隔离（舱壁模式）。\n\n案例需求：给 order-service服务中的UserClient的查询用户接口设置流控规则，线程数不能超过 2。然后利用jemeter测试。\n\n# 1）配置隔离规则\n\n选择feign接口后面的流控按钮：\n\n\n\n填写表单：\n\n\n\n# 2）Jmeter测试\n\n选择《阈值类型-线程数<2》：\n\n\n\n一次发生10个请求，有较大概率并发线程数超过2，而超出的请求会走之前定义的失败降级逻辑。\n\n查看运行结果：\n\n\n\n发现虽然结果都是通过了，不过部分请求得到的响应是降级返回的null信息。\n\n\n# 总结\n\n线程隔离的两种手段是？\n\n * 信号量隔离\n\n * 线程池隔离\n\n信号量隔离的特点是？\n\n * 基于计数器模式，简单，开销小\n\n线程池隔离的特点是？\n\n * 基于线程池模式，有额外开销，但隔离控制更强\n\n\n# 熔断降级\n\n熔断降级是解决雪崩问题的重要手段。其思路是由断路器统计服务调用的异常比例、慢请求比例，如果超出阈值则会熔断该服务。即拦截访问该服务的一切请求；而当服务恢复时，断路器会放行访问该服务的请求。\n\n断路器控制熔断和放行是通过状态机来完成的：\n\n\n\n状态机包括三个状态：\n\n * closed：关闭状态，断路器放行所有请求，并开始统计异常比例、慢请求比例。超过阈值则切换到open状态\n * open：打开状态，服务调用被熔断，访问被熔断服务的请求会被拒绝，快速失败，直接走降级逻辑。Open状态5秒后会进入half-open状态\n * half-open：半开状态，放行一次请求，根据执行结果来判断接下来的操作。\n   * 请求成功：则切换到closed状态\n   * 请求失败：则切换到open状态\n\n断路器熔断策略有三种：慢调用、异常比例、异常数\n\n\n# 慢调用\n\n慢调用：业务的响应时长（RT）大于指定时长的请求认定为慢调用请求。在指定时间内，如果请求数量超过设定的最小数量，慢调用比例大于设定的阈值，则触发熔断。\n\n例如：\n\n\n\n解读：RT超过500ms的调用是慢调用，统计最近10000ms内的请求，如果请求量超过10次，并且慢调用比例不低于0.5，则触发熔断，熔断时长为5秒。然后进入half-open状态，放行一次请求做测试。\n\n案例\n\n需求：给 UserClient的查询用户接口设置降级规则，慢调用的RT阈值为50ms，统计时间为1秒，最小请求数量为5，失败阈值比例为0.4，熔断时长为5\n\n# 1）设置慢调用\n\n修改user-service中的/user/{id}这个接口的业务。通过休眠模拟一个延迟时间：\n\n\n\n此时，orderId=101的订单，关联的是id为1的用户，调用时长为60ms：\n\n\n\norderId=102的订单，关联的是id为2的用户，调用时长为非常短；\n\n\n\n# 2）设置熔断规则\n\n下面，给feign接口设置降级规则：\n\n\n\n规则：\n\n\n\n超过50ms的请求都会被认为是慢请求\n\n# 3）测试\n\n在浏览器访问：http://localhost:8088/order/101，快速刷新5次，可以发现：\n\n\n\n触发了熔断，请求时长缩短至5ms，快速失败了，并且走降级逻辑，返回的null\n\n在浏览器访问：http://localhost:8088/order/102，竟然也被熔断了：\n\n\n\n\n# 异常比例、异常数\n\n异常比例或异常数：统计指定时间内的调用，如果调用次数超过指定请求数，并且出现异常的比例达到设定的比例阈值（或超过指定异常数），则触发熔断。\n\n例如，一个异常比例设置：\n\n\n\n解读：统计最近1000ms内的请求，如果请求量超过10次，并且异常比例不低于0.4，则触发熔断。\n\n一个异常数设置：\n\n\n\n解读：统计最近1000ms内的请求，如果请求量超过10次，并且异常比例不低于2次，则触发熔断。\n\n案例\n\n需求：给 UserClient的查询用户接口设置降级规则，统计时间为1秒，最小请求数量为5，失败阈值比例为0.4，熔断时长为5s\n\n# 1）设置异常请求\n\n首先，修改user-service中的/user/{id}这个接口的业务。手动抛出异常，以触发异常比例的熔断：\n\n\n\n也就是说，id 为 2时，就会触发异常\n\n# 2）设置熔断规则\n\n下面，给feign接口设置降级规则：\n\n\n\n规则：\n\n\n\n在5次请求中，只要异常比例超过0.4，也就是有2次以上的异常，就会触发熔断。\n\n# 3）测试\n\n在浏览器快速访问：http://localhost:8088/order/102，快速刷新5次，触发熔断：\n\n\n\n此时，我们去访问本来应该正常的103：\n\n',normalizedContent:'限流是一种预防措施，虽然限流可以尽量避免因高并发而引起的服务故障，但服务还会因为其它原因而故障。\n\n而要将这些故障控制在一定范围，避免雪崩，就要靠线程隔离（舱壁模式）和熔断降级手段了。\n\n线程隔离之前讲到过：调用者在调用服务提供者时，给每个调用的请求分配独立线程池，出现故障时，最多消耗这个线程池内资源，避免把调用者的所有资源耗尽。\n\n\n\n熔断降级：是在调用方这边加入断路器，统计对服务提供者的调用，如果调用的失败比例过高，则熔断该业务，不允许访问该服务的提供者了。\n\n\n\n可以看到，不管是线程隔离还是熔断降级，都是对客户端（调用方）的保护。需要在调用方 发起远程调用时做线程隔离、或者服务熔断。\n\n而我们的微服务远程调用都是基于feign来完成的，因此我们需要将feign与sentinel整合，在feign里面实现线程隔离和服务熔断。\n\n\n# feignclient整合sentinel\n\nspringcloud中，微服务调用都是通过feign来实现的，因此做客户端保护必须整合feign和sentinel。\n\n\n# 修改配置，开启sentinel功能\n\n修改orderservice的application.yml文件，开启feign的sentinel功能：\n\n注意 这里修改 feign中的配置文件没有用\n\nfeign:\n  sentinel:\n    enabled: true # 开启feign对sentinel的支持\n\n\n\n# 编写失败降级逻辑\n\n业务失败后，不能直接报错，而应该返回用户一个友好提示或者默认结果，这个就是失败降级逻辑。\n\n给feignclient编写失败后的降级逻辑\n\n①方式一：fallbackclass，无法对远程调用的异常做处理\n\n②方式二：fallbackfactory，可以对远程调用的异常做处理，我们选择这种\n\n这里我们演示方式二的失败降级处理。\n\n步骤一：在feing-api项目中定义类，实现fallbackfactory：\n\n\n\n代码：\n\npackage cn.itcast.feign.clients.fallback;\n\nimport cn.itcast.feign.clients.userclient;\nimport cn.itcast.feign.pojo.user;\nimport feign.hystrix.fallbackfactory;\nimport lombok.extern.slf4j.slf4j;\n\n@slf4j\npublic class userclientfallbackfactory implements fallbackfactory<userclient> {\n    @override\n    public userclient create(throwable throwable) {\n        return new userclient() {\n            @override\n            public user findbyid(long id) {\n                log.error("查询用户异常", throwable);\n                return new user();\n            }\n        };\n    }\n}\n\n\n\n步骤二：在feing-api项目中的defaultfeignconfiguration类中将userclientfallbackfactory注册为一个bean：\n\n@bean\npublic userclientfallbackfactory userclientfallbackfactory(){\n    return new userclientfallbackfactory();\n}\n\n\n步骤三：在feing-api项目中的userclient接口中使用userclientfallbackfactory：\n\nimport cn.itcast.feign.clients.fallback.userclientfallbackfactory;\nimport cn.itcast.feign.pojo.user;\nimport org.springframework.cloud.openfeign.feignclient;\nimport org.springframework.web.bind.annotation.getmapping;\nimport org.springframework.web.bind.annotation.pathvariable;\n\n@feignclient(value = "userservice", fallbackfactory = userclientfallbackfactory.class)\npublic interface userclient {\n\n    @getmapping("/user/{id}")\n    user findbyid(@pathvariable("id") long id);\n}\n\n\n重启后，访问一次订单查询业务，然后查看sentinel控制台，可以看到新的簇点链路：\n\n\n\n\n# 总结\n\nsentinel支持的雪崩解决方案：\n\n * 线程隔离（仓壁模式）\n * 降级熔断\n\nfeign整合sentinel的步骤：\n\n * 在orderservice中的application.yml中配置：feign.sentienl.enable=true\n * 给feignclient编写fallbackfactory并注册为bean\n * 将fallbackfactory配置到feignclient\n\n问题\n\n * 一定要将feign的配置类引进来\n   \n   @springbootapplication\n   //feign远程调用开关,并指定配置类\n   //todo 这里一定要指定配置类，不然 feign-api中的config包下的bean扫描不到\n   // 在config包下的 @configuration 起作用\n   @enablefeignclients(clients = userclient.class, defaultconfiguration = defaultfeignconfiguration.class)\n   //使用feign来取代resttemplate\n   public class orderapplication {\n   \n       public static void main(string[] args) {\n           springapplication.run(orderapplication.class, args);\n       }\n   }\n   \n\n * spring-cloud的版本要修改为8\n   \n    \x3c!-- todo 这里要改成版本8 不然会问题--\x3e\n   <spring-cloud.version>hoxton.sr8</spring-cloud.version>\n   \n\n\n# 线程隔离（舱壁模式）\n\n\n# 线程隔离的实现方式\n\n线程隔离有两种方式实现：\n\n * 线程池隔离\n\n * 信号量隔离（sentinel默认采用）\n\n如图：\n\n\n\n线程池隔离：给每个服务调用业务分配一个线程池，利用线程池本身实现隔离效果\n\n信号量隔离：不创建线程池，而是计数器模式，记录业务使用的线程数量，达到信号量上限时，禁止新的请求。\n\n两者的优缺点：\n\n * 扇出：我依赖于n个服务；我执行的时候会调用其他n个服务\n\n\n\n\n# sentinel的线程隔离\n\n用法说明：\n\n在添加限流规则时，可以选择两种阈值类型：\n\n\n\n * qps：就是每秒的请求数，在快速入门中已经演示过\n\n * 线程数：是该资源能使用用的tomcat线程数的最大值。也就是通过限制线程数量，实现线程隔离（舱壁模式）。\n\n案例需求：给 order-service服务中的userclient的查询用户接口设置流控规则，线程数不能超过 2。然后利用jemeter测试。\n\n# 1）配置隔离规则\n\n选择feign接口后面的流控按钮：\n\n\n\n填写表单：\n\n\n\n# 2）jmeter测试\n\n选择《阈值类型-线程数<2》：\n\n\n\n一次发生10个请求，有较大概率并发线程数超过2，而超出的请求会走之前定义的失败降级逻辑。\n\n查看运行结果：\n\n\n\n发现虽然结果都是通过了，不过部分请求得到的响应是降级返回的null信息。\n\n\n# 总结\n\n线程隔离的两种手段是？\n\n * 信号量隔离\n\n * 线程池隔离\n\n信号量隔离的特点是？\n\n * 基于计数器模式，简单，开销小\n\n线程池隔离的特点是？\n\n * 基于线程池模式，有额外开销，但隔离控制更强\n\n\n# 熔断降级\n\n熔断降级是解决雪崩问题的重要手段。其思路是由断路器统计服务调用的异常比例、慢请求比例，如果超出阈值则会熔断该服务。即拦截访问该服务的一切请求；而当服务恢复时，断路器会放行访问该服务的请求。\n\n断路器控制熔断和放行是通过状态机来完成的：\n\n\n\n状态机包括三个状态：\n\n * closed：关闭状态，断路器放行所有请求，并开始统计异常比例、慢请求比例。超过阈值则切换到open状态\n * open：打开状态，服务调用被熔断，访问被熔断服务的请求会被拒绝，快速失败，直接走降级逻辑。open状态5秒后会进入half-open状态\n * half-open：半开状态，放行一次请求，根据执行结果来判断接下来的操作。\n   * 请求成功：则切换到closed状态\n   * 请求失败：则切换到open状态\n\n断路器熔断策略有三种：慢调用、异常比例、异常数\n\n\n# 慢调用\n\n慢调用：业务的响应时长（rt）大于指定时长的请求认定为慢调用请求。在指定时间内，如果请求数量超过设定的最小数量，慢调用比例大于设定的阈值，则触发熔断。\n\n例如：\n\n\n\n解读：rt超过500ms的调用是慢调用，统计最近10000ms内的请求，如果请求量超过10次，并且慢调用比例不低于0.5，则触发熔断，熔断时长为5秒。然后进入half-open状态，放行一次请求做测试。\n\n案例\n\n需求：给 userclient的查询用户接口设置降级规则，慢调用的rt阈值为50ms，统计时间为1秒，最小请求数量为5，失败阈值比例为0.4，熔断时长为5\n\n# 1）设置慢调用\n\n修改user-service中的/user/{id}这个接口的业务。通过休眠模拟一个延迟时间：\n\n\n\n此时，orderid=101的订单，关联的是id为1的用户，调用时长为60ms：\n\n\n\norderid=102的订单，关联的是id为2的用户，调用时长为非常短；\n\n\n\n# 2）设置熔断规则\n\n下面，给feign接口设置降级规则：\n\n\n\n规则：\n\n\n\n超过50ms的请求都会被认为是慢请求\n\n# 3）测试\n\n在浏览器访问：http://localhost:8088/order/101，快速刷新5次，可以发现：\n\n\n\n触发了熔断，请求时长缩短至5ms，快速失败了，并且走降级逻辑，返回的null\n\n在浏览器访问：http://localhost:8088/order/102，竟然也被熔断了：\n\n\n\n\n# 异常比例、异常数\n\n异常比例或异常数：统计指定时间内的调用，如果调用次数超过指定请求数，并且出现异常的比例达到设定的比例阈值（或超过指定异常数），则触发熔断。\n\n例如，一个异常比例设置：\n\n\n\n解读：统计最近1000ms内的请求，如果请求量超过10次，并且异常比例不低于0.4，则触发熔断。\n\n一个异常数设置：\n\n\n\n解读：统计最近1000ms内的请求，如果请求量超过10次，并且异常比例不低于2次，则触发熔断。\n\n案例\n\n需求：给 userclient的查询用户接口设置降级规则，统计时间为1秒，最小请求数量为5，失败阈值比例为0.4，熔断时长为5s\n\n# 1）设置异常请求\n\n首先，修改user-service中的/user/{id}这个接口的业务。手动抛出异常，以触发异常比例的熔断：\n\n\n\n也就是说，id 为 2时，就会触发异常\n\n# 2）设置熔断规则\n\n下面，给feign接口设置降级规则：\n\n\n\n规则：\n\n\n\n在5次请求中，只要异常比例超过0.4，也就是有2次以上的异常，就会触发熔断。\n\n# 3）测试\n\n在浏览器快速访问：http://localhost:8088/order/102，快速刷新5次，触发熔断：\n\n\n\n此时，我们去访问本来应该正常的103：\n\n',charsets:{cjk:!0}},{title:"授权规则",frontmatter:{autoSort:97,title:"授权规则",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/d7aaea/",categories:["后端","微服务","进阶","微服务保护"],tags:["知识","微服务","微服务保护"]},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/70.%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/40.%E6%8E%88%E6%9D%83%E8%A7%84%E5%88%99.html",relativePath:"01.后端/60.微服务/70.微服务保护/40.授权规则.md",key:"v-26d66c54",path:"/pages/d7aaea/",headers:[{level:2,title:"授权规则",slug:"授权规则",normalizedTitle:"授权规则",charIndex:0},{level:3,title:"基本规则",slug:"基本规则",normalizedTitle:"基本规则",charIndex:121},{level:3,title:"如何获取origin",slug:"如何获取origin",normalizedTitle:"如何获取origin",charIndex:450},{level:3,title:"给网关添加请求头",slug:"给网关添加请求头",normalizedTitle:"给网关添加请求头",charIndex:1519},{level:3,title:"配置授权规则",slug:"配置授权规则",normalizedTitle:"配置授权规则",charIndex:1909},{level:2,title:"自定义异常结果",slug:"自定义异常结果",normalizedTitle:"自定义异常结果",charIndex:2014},{level:3,title:"异常类型",slug:"异常类型",normalizedTitle:"异常类型",charIndex:2109},{level:3,title:"自定义异常处理",slug:"自定义异常处理",normalizedTitle:"自定义异常处理",charIndex:2718},{level:2,title:"规则持久化",slug:"规则持久化",normalizedTitle:"规则持久化",charIndex:4246},{level:3,title:"pull模式",slug:"pull模式",normalizedTitle:"pull模式",charIndex:4400},{level:3,title:"push模式",slug:"push模式",normalizedTitle:"push模式",charIndex:4431}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"授权规则 基本规则 如何获取origin 给网关添加请求头 配置授权规则 自定义异常结果 异常类型 自定义异常处理 规则持久化 pull模式 push模式",content:'授权规则可以对请求方来源做判断和控制。\n\n> 网关可以做服务的路由和控制，但是前提是得从网关的入口走。\n> \n> 如果直接走微服务的路口，则在网关配置的拦截信息就不管用了。所以这里需要授权规则，来保护我们的微服务。\n\n\n# 授权规则\n\n\n# 基本规则\n\n授权规则可以对调用方的来源做控制，有白名单和黑名单两种方式。\n\n * 白名单：来源（origin）在白名单内的调用者允许访问\n\n * 黑名单：来源（origin）在黑名单内的调用者不允许访问\n\n点击左侧菜单的授权，可以看到授权规则：\n\n\n\n * 资源名：就是受保护的资源，例如/order/{orderId}\n\n * 流控应用：是来源者的名单，\n   \n   * 如果是勾选白名单，则名单中的来源被许可访问。\n   * 如果是勾选黑名单，则名单中的来源被禁止访问。\n\n比如：\n\n\n\n我们允许请求从gateway到order-service，不允许浏览器访问order-service，那么白名单中就要填写网关的来源名称（origin）。\n\n\n# 如何获取origin\n\nSentinel是通过RequestOriginParser这个接口的parseOrigin来获取请求的来源的。\n\npublic interface RequestOriginParser {\n    /**\n     * 从请求request对象中获取origin，获取方式自定义\n     */\n    String parseOrigin(HttpServletRequest request);\n}\n\n\n这个方法的作用就是从request对象中，获取请求者的origin值并返回。\n\n默认情况下，sentinel不管请求者从哪里来，返回值永远是default，也就是说一切请求的来源都被认为是一样的值default。\n\n因此，我们需要自定义这个接口的实现，让不同的请求，返回不同的origin。\n\n例如order-service服务中，我们定义一个RequestOriginParser的实现类：\n\npackage cn.itcast.order.sentinel;\n\nimport com.alibaba.csp.sentinel.adapter.spring.webmvc.callback.RequestOriginParser;\nimport org.springframework.stereotype.Component;\nimport org.springframework.util.StringUtils;\n\nimport javax.servlet.http.HttpServletRequest;\n\n@Component\npublic class HeaderOriginParser implements RequestOriginParser {\n    @Override\n    public String parseOrigin(HttpServletRequest request) {\n        // 1.获取请求头\n        String origin = request.getHeader("origin");\n        // 2.非空判断\n        if (StringUtils.isEmpty(origin)) {\n            origin = "blank";\n        }\n        return origin;\n    }\n}\n\n\n我们会尝试从request-header中获取origin值。\n\n\n# 给网关添加请求头\n\n既然获取请求origin的方式是从reques-header中获取origin值，我们必须让所有从gateway路由到微服务的请求都带上origin头。\n\n这个需要利用之前学习的一个GatewayFilter来实现，AddRequestHeaderGatewayFilter。\n\n修改gateway服务中的application.yml，添加一个defaultFilter：\n\nspring:\n  cloud:\n    gateway:\n      default-filters:\n        - AddRequestHeader=origin,gateway\n      routes:\n       # ...略\n\n\n这样，从gateway路由的所有请求都会带上origin头，值为gateway。而从其它地方到达微服务的请求则没有这个头。\n\n\n# 配置授权规则\n\n接下来，我们添加一个授权规则，放行origin值为gateway的请求。\n\n\n\n配置如下：\n\n\n\n现在，我们直接跳过网关，访问order-service服务：\n\n\n\n通过网关访问：\n\n\n\n\n# 自定义异常结果\n\n默认情况下，发生限流、降级、授权拦截时，都会抛出异常到调用方。异常结果都是flow limmiting（限流）。这样不够友好，无法得知是限流还是降级还是授权拦截。\n\n\n# 异常类型\n\n而如果要自定义异常时的返回结果，需要实现BlockExceptionHandler接口：\n\npublic interface BlockExceptionHandler {\n    /**\n     * 处理请求被限流、降级、授权拦截时抛出的异常：BlockException\n     */\n    void handle(HttpServletRequest request, HttpServletResponse response, BlockException e) throws Exception;\n}\n\n\n这个方法有三个参数：\n\n * HttpServletRequest request：request对象\n * HttpServletResponse response：response对象\n * BlockException e：被sentinel拦截时抛出的异常\n\n这里的BlockException包含多个不同的子类：\n\n异常                     说明\nFlowException          限流异常\nParamFlowException     热点参数限流的异常\nDegradeException       降级异常\nAuthorityException     授权规则异常\nSystemBlockException   系统规则异常\n\n\n# 自定义异常处理\n\n下面，我们就在order-service定义一个自定义异常处理类：\n\npackage cn.itcast.order.sentinel;\n\nimport com.alibaba.csp.sentinel.adapter.spring.webmvc.callback.BlockExceptionHandler;\nimport com.alibaba.csp.sentinel.slots.block.BlockException;\nimport com.alibaba.csp.sentinel.slots.block.authority.AuthorityException;\nimport com.alibaba.csp.sentinel.slots.block.degrade.DegradeException;\nimport com.alibaba.csp.sentinel.slots.block.flow.FlowException;\nimport com.alibaba.csp.sentinel.slots.block.flow.param.ParamFlowException;\nimport org.springframework.stereotype.Component;\n\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\n\n@Component\npublic class SentinelExceptionHandler implements BlockExceptionHandler {\n    @Override\n    public void handle(HttpServletRequest request, HttpServletResponse response, BlockException e) throws Exception {\n        String msg = "未知异常";\n        int status = 429;\n\n        if (e instanceof FlowException) {\n            msg = "请求被限流了";\n        } else if (e instanceof ParamFlowException) {\n            msg = "请求被热点参数限流";\n        } else if (e instanceof DegradeException) {\n            msg = "请求被降级了";\n        } else if (e instanceof AuthorityException) {\n            msg = "没有权限访问";\n            status = 401;\n        }\n\n        response.setContentType("application/json;charset=utf-8");\n        response.setStatus(status);\n        response.getWriter().println("{\\"msg\\": " + msg + ", \\"status\\": " + status + "}");\n    }\n}\n\n\n重启测试，在不同场景下，会返回不同的异常消息.\n\n限流：\n\n\n\n授权拦截时：\n\n\n\n\n# 规则持久化\n\n现在，sentinel的所有规则都是内存存储，重启后所有规则都会丢失。在生产环境下，我们必须确保这些规则的持久化，避免丢失。\n\n规则是否能持久化，取决于规则管理模式，sentinel支持三种规则管理模式：\n\n * 原始模式：Sentinel的默认模式，将规则保存在内存，重启服务会丢失。\n * pull模式—— 保存在本地文件或数据库中，定时去读取\n * push模式——保存在nacos中，监听变更实时更新——推荐\n\n\n# pull模式\n\npull模式：控制台将配置的规则推送到Sentinel客户端，而客户端会将配置规则保存在本地文件或数据库中。以后会定时去本地文件或数据库中查询，更新本地规则。\n\n\n\n\n# push模式\n\npush模式：控制台将配置规则推送到远程配置中心，例如Nacos。Sentinel客户端监听Nacos，获取配置变更的推送消息，完成本地配置更新。\n\n',normalizedContent:'授权规则可以对请求方来源做判断和控制。\n\n> 网关可以做服务的路由和控制，但是前提是得从网关的入口走。\n> \n> 如果直接走微服务的路口，则在网关配置的拦截信息就不管用了。所以这里需要授权规则，来保护我们的微服务。\n\n\n# 授权规则\n\n\n# 基本规则\n\n授权规则可以对调用方的来源做控制，有白名单和黑名单两种方式。\n\n * 白名单：来源（origin）在白名单内的调用者允许访问\n\n * 黑名单：来源（origin）在黑名单内的调用者不允许访问\n\n点击左侧菜单的授权，可以看到授权规则：\n\n\n\n * 资源名：就是受保护的资源，例如/order/{orderid}\n\n * 流控应用：是来源者的名单，\n   \n   * 如果是勾选白名单，则名单中的来源被许可访问。\n   * 如果是勾选黑名单，则名单中的来源被禁止访问。\n\n比如：\n\n\n\n我们允许请求从gateway到order-service，不允许浏览器访问order-service，那么白名单中就要填写网关的来源名称（origin）。\n\n\n# 如何获取origin\n\nsentinel是通过requestoriginparser这个接口的parseorigin来获取请求的来源的。\n\npublic interface requestoriginparser {\n    /**\n     * 从请求request对象中获取origin，获取方式自定义\n     */\n    string parseorigin(httpservletrequest request);\n}\n\n\n这个方法的作用就是从request对象中，获取请求者的origin值并返回。\n\n默认情况下，sentinel不管请求者从哪里来，返回值永远是default，也就是说一切请求的来源都被认为是一样的值default。\n\n因此，我们需要自定义这个接口的实现，让不同的请求，返回不同的origin。\n\n例如order-service服务中，我们定义一个requestoriginparser的实现类：\n\npackage cn.itcast.order.sentinel;\n\nimport com.alibaba.csp.sentinel.adapter.spring.webmvc.callback.requestoriginparser;\nimport org.springframework.stereotype.component;\nimport org.springframework.util.stringutils;\n\nimport javax.servlet.http.httpservletrequest;\n\n@component\npublic class headeroriginparser implements requestoriginparser {\n    @override\n    public string parseorigin(httpservletrequest request) {\n        // 1.获取请求头\n        string origin = request.getheader("origin");\n        // 2.非空判断\n        if (stringutils.isempty(origin)) {\n            origin = "blank";\n        }\n        return origin;\n    }\n}\n\n\n我们会尝试从request-header中获取origin值。\n\n\n# 给网关添加请求头\n\n既然获取请求origin的方式是从reques-header中获取origin值，我们必须让所有从gateway路由到微服务的请求都带上origin头。\n\n这个需要利用之前学习的一个gatewayfilter来实现，addrequestheadergatewayfilter。\n\n修改gateway服务中的application.yml，添加一个defaultfilter：\n\nspring:\n  cloud:\n    gateway:\n      default-filters:\n        - addrequestheader=origin,gateway\n      routes:\n       # ...略\n\n\n这样，从gateway路由的所有请求都会带上origin头，值为gateway。而从其它地方到达微服务的请求则没有这个头。\n\n\n# 配置授权规则\n\n接下来，我们添加一个授权规则，放行origin值为gateway的请求。\n\n\n\n配置如下：\n\n\n\n现在，我们直接跳过网关，访问order-service服务：\n\n\n\n通过网关访问：\n\n\n\n\n# 自定义异常结果\n\n默认情况下，发生限流、降级、授权拦截时，都会抛出异常到调用方。异常结果都是flow limmiting（限流）。这样不够友好，无法得知是限流还是降级还是授权拦截。\n\n\n# 异常类型\n\n而如果要自定义异常时的返回结果，需要实现blockexceptionhandler接口：\n\npublic interface blockexceptionhandler {\n    /**\n     * 处理请求被限流、降级、授权拦截时抛出的异常：blockexception\n     */\n    void handle(httpservletrequest request, httpservletresponse response, blockexception e) throws exception;\n}\n\n\n这个方法有三个参数：\n\n * httpservletrequest request：request对象\n * httpservletresponse response：response对象\n * blockexception e：被sentinel拦截时抛出的异常\n\n这里的blockexception包含多个不同的子类：\n\n异常                     说明\nflowexception          限流异常\nparamflowexception     热点参数限流的异常\ndegradeexception       降级异常\nauthorityexception     授权规则异常\nsystemblockexception   系统规则异常\n\n\n# 自定义异常处理\n\n下面，我们就在order-service定义一个自定义异常处理类：\n\npackage cn.itcast.order.sentinel;\n\nimport com.alibaba.csp.sentinel.adapter.spring.webmvc.callback.blockexceptionhandler;\nimport com.alibaba.csp.sentinel.slots.block.blockexception;\nimport com.alibaba.csp.sentinel.slots.block.authority.authorityexception;\nimport com.alibaba.csp.sentinel.slots.block.degrade.degradeexception;\nimport com.alibaba.csp.sentinel.slots.block.flow.flowexception;\nimport com.alibaba.csp.sentinel.slots.block.flow.param.paramflowexception;\nimport org.springframework.stereotype.component;\n\nimport javax.servlet.http.httpservletrequest;\nimport javax.servlet.http.httpservletresponse;\n\n@component\npublic class sentinelexceptionhandler implements blockexceptionhandler {\n    @override\n    public void handle(httpservletrequest request, httpservletresponse response, blockexception e) throws exception {\n        string msg = "未知异常";\n        int status = 429;\n\n        if (e instanceof flowexception) {\n            msg = "请求被限流了";\n        } else if (e instanceof paramflowexception) {\n            msg = "请求被热点参数限流";\n        } else if (e instanceof degradeexception) {\n            msg = "请求被降级了";\n        } else if (e instanceof authorityexception) {\n            msg = "没有权限访问";\n            status = 401;\n        }\n\n        response.setcontenttype("application/json;charset=utf-8");\n        response.setstatus(status);\n        response.getwriter().println("{\\"msg\\": " + msg + ", \\"status\\": " + status + "}");\n    }\n}\n\n\n重启测试，在不同场景下，会返回不同的异常消息.\n\n限流：\n\n\n\n授权拦截时：\n\n\n\n\n# 规则持久化\n\n现在，sentinel的所有规则都是内存存储，重启后所有规则都会丢失。在生产环境下，我们必须确保这些规则的持久化，避免丢失。\n\n规则是否能持久化，取决于规则管理模式，sentinel支持三种规则管理模式：\n\n * 原始模式：sentinel的默认模式，将规则保存在内存，重启服务会丢失。\n * pull模式—— 保存在本地文件或数据库中，定时去读取\n * push模式——保存在nacos中，监听变更实时更新——推荐\n\n\n# pull模式\n\npull模式：控制台将配置的规则推送到sentinel客户端，而客户端会将配置规则保存在本地文件或数据库中。以后会定时去本地文件或数据库中查询，更新本地规则。\n\n\n\n\n# push模式\n\npush模式：控制台将配置规则推送到远程配置中心，例如nacos。sentinel客户端监听nacos，获取配置变更的推送消息，完成本地配置更新。\n\n',charsets:{cjk:!0}},{title:"《微服务》",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"01.后端/60.微服务",imgUrl:"/assets/img/Microservice.png",description:"微服务学习笔记--整理自黑马程序员，在原教程基础上添加学习笔记"}},title:"《微服务》",date:"2023-06-30T20:30:40.000Z",permalink:"/back/Microservice/",article:!1,comment:!1,editLink:!1,author:{name:"凉冰",link:"https://github.com/diana-devil"}},regularPath:"/01.%E5%90%8E%E7%AB%AF/60.%E5%BE%AE%E6%9C%8D%E5%8A%A1/",relativePath:"01.后端/60.微服务/README.md",key:"v-03a351e0",path:"/back/Microservice/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"后端",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"01.后端",imgUrl:"/assets/img/java6.webp",description:"Java、SSM、SpringBoot、微服务等后端技术"}},title:"后端",date:"2023-06-20T21:50:53.000Z",permalink:"/Java-C/",sidebar:!1,article:!1,comment:!1,editLink:!1,author:{name:"凉冰",link:"https://github.com/diana-devil"}},regularPath:"/01.%E5%90%8E%E7%AB%AF/",relativePath:"01.后端/README.md",key:"v-2803fd6e",path:"/Java-C/",lastUpdated:"2023 07 4",lastUpdatedTimestamp:1688453793e3,headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"数据结构与算法",frontmatter:{autoSort:100,title:"数据结构与算法",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/17f632/",categories:["算法","算法基础"],tags:["知识","算法"]},regularPath:"/02.%E7%AE%97%E6%B3%95/05.%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/05.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95.html",relativePath:"02.算法/05.算法基础/05.数据结构与算法.md",key:"v-69c2fec1",path:"/pages/17f632/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"学习数据结构最难的不是理解和掌握原理，而是能灵活地将各种场景和问题抽象成对应的数据结构和算法。\n\n\n\n * 数据结构\n   \n   * 数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie 树\n\n * 算法\n   \n   * 递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法\n\n * 什么是数据结构和算法\n   \n   * 数据结构就是指一组数据的存储结构。\n   * 算法就是操作数据的一组方法。\n   * 数据结构和算法是相辅相成的。数据结构是为算法服务的，算法要作用在特定的数据结构之上。\n\n * 为什么要学习数据结构和算法\n   \n   * 提高计算效率\n   * 提高代码能力\n\n * 怎样评价他们\n   \n   * 时间复杂度\n   * 空间复杂度",normalizedContent:"学习数据结构最难的不是理解和掌握原理，而是能灵活地将各种场景和问题抽象成对应的数据结构和算法。\n\n\n\n * 数据结构\n   \n   * 数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、trie 树\n\n * 算法\n   \n   * 递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法\n\n * 什么是数据结构和算法\n   \n   * 数据结构就是指一组数据的存储结构。\n   * 算法就是操作数据的一组方法。\n   * 数据结构和算法是相辅相成的。数据结构是为算法服务的，算法要作用在特定的数据结构之上。\n\n * 为什么要学习数据结构和算法\n   \n   * 提高计算效率\n   * 提高代码能力\n\n * 怎样评价他们\n   \n   * 时间复杂度\n   * 空间复杂度",charsets:{cjk:!0}},{title:"数组",frontmatter:{autoSort:98,title:"数组",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/552b44/",categories:["算法","数组"],tags:["知识","算法"]},regularPath:"/02.%E7%AE%97%E6%B3%95/10.%E6%95%B0%E7%BB%84/05.%E6%95%B0%E7%BB%84.html",relativePath:"02.算法/10.数组/05.数组.md",key:"v-45672518",path:"/pages/552b44/",headers:[{level:2,title:"如何实现随机访问",slug:"如何实现随机访问",normalizedTitle:"如何实现随机访问",charIndex:2},{level:2,title:"低效的插入和删除",slug:"低效的插入和删除",normalizedTitle:"低效的插入和删除",charIndex:178},{level:2,title:"数组越界---c语言",slug:"数组越界-c语言",normalizedTitle:"数组越界---c语言",charIndex:566},{level:2,title:"容器与数组",slug:"容器与数组",normalizedTitle:"容器与数组",charIndex:947},{level:2,title:"LeetCode练习题",slug:"leetcode练习题",normalizedTitle:"leetcode练习题",charIndex:1629}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"如何实现随机访问 低效的插入和删除 数组越界---c语言 容器与数组 LeetCode练习题",content:'# 如何实现随机访问\n\n * 线性表数据结构\n   \n   * 顾名思义，线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。\n\n * 随机访问特性\n   \n   * 数组可以实现根据下标随机访问数组元素\n   * 数组-----连续的内存空间和相同类型的数据\n   * \n\n\n# 低效的插入和删除\n\n> 数组为了保持内存数据的连续性，会导致插入、删除这两个操作比较低效.\n\n * 插入\n   \n   * 数组是有序的时候，需要在第k个位置插入数据，需要将k后面的数据全部后移。\n   \n   * 高效：但是当数组时无序的，没有规律的，需要在第k个位置插入数据时，可以使用搬移操作\n     \n     * 即，将原来第k个位置的元素放到数组最后，将要插入的元素，放到k位置。如下图所示。\n     \n     \n\n * 删除\n   \n   * 如果我们要删除第 k 个位置的数据，为了内存的连续性，也需要搬移数据，不然中间就会出现空洞，内存就不连续了。\n   \n   * 高效：不直接删除元素，而是将要删除的元素进行标记，当达到一定量后，一块删除，这样可以避免多次数据的搬移。----JVM 标记清除垃圾回收算法的核心思想\n     \n     \n\n\n# 数组越界---c语言\n\njava语言在数组越界时，会抛出异常java.lang.ArrayIndexOutOfBoundsException\n\n> 下面这组代码会无限次输出 helloworld，而不是三次。\n> \n> 原因是 a[3]这个内存地址，存放的是 i=0,所以i会一直重复从0到2。\n\nint main(int argc, char* argv[]){\n    int i = 0;\n    int arr[3] = {0};\n    for(; i<=3; i++){\n        arr[i] = 0;\n        printf("hello world\\n");\n    }\n    return 0;\n}\n\n\n * 内存问题\n   \n   * 内存用栈存储，存储结构如下所示\n   * 所以在访问a[3]时，i=0，无限循环\n\n\n# 容器与数组\n\n * ArrayList---优势\n   \n   * 可以将很多数组操作的细节封装起来，前面提到的数组插入、删除数据时需要搬移其他数据。\n   * 支持动态扩容。每次存储空间不够的时候，它都会将空间自动扩容为 1.5 倍大小。我们不需要关心底层的扩容逻辑。\n\n * ArrayList---申请\n   \n   * 因为扩容操作涉及内存申请和数据搬移，是比较耗时的。所以，如果事先能确定需要存储的数据大小，最好在创建 ArrayList 的时候事先指定数据大小。\n\n * 数组与ArrayList\n   \n   * Java ArrayList 无法存储基本类型，比如 int、long，需要封装为 Integer、Long 类，而 Autoboxing、Unboxing 则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组\n   * 如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组。\n   * 当要表示多维数组时，用数组往往会更加直观。比如Object[][] array；而用容器的话则需要这样定义：ArrayList <ArrayList <object>> array\n   \n   总结：\n   \n   > 对于业务开发，直接使用容器就足够了，省时省力。毕竟损耗一丢丢性能，完全不会影响到系统整体的性能。\n   > \n   > 但如果你是做一些非常底层的开发，比如开发网络框架，性能的优化需要做到极致，这个时候数组就会优于容器，成为首选。\n\n\n# LeetCode练习题\n\n * 实现一个支持动态扩容的数组\n * 实现一个大小固定的有序数组，支持动态增删改操作\n * 实现两个有序数组合并为一个有序数组',normalizedContent:'# 如何实现随机访问\n\n * 线性表数据结构\n   \n   * 顾名思义，线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。\n\n * 随机访问特性\n   \n   * 数组可以实现根据下标随机访问数组元素\n   * 数组-----连续的内存空间和相同类型的数据\n   * \n\n\n# 低效的插入和删除\n\n> 数组为了保持内存数据的连续性，会导致插入、删除这两个操作比较低效.\n\n * 插入\n   \n   * 数组是有序的时候，需要在第k个位置插入数据，需要将k后面的数据全部后移。\n   \n   * 高效：但是当数组时无序的，没有规律的，需要在第k个位置插入数据时，可以使用搬移操作\n     \n     * 即，将原来第k个位置的元素放到数组最后，将要插入的元素，放到k位置。如下图所示。\n     \n     \n\n * 删除\n   \n   * 如果我们要删除第 k 个位置的数据，为了内存的连续性，也需要搬移数据，不然中间就会出现空洞，内存就不连续了。\n   \n   * 高效：不直接删除元素，而是将要删除的元素进行标记，当达到一定量后，一块删除，这样可以避免多次数据的搬移。----jvm 标记清除垃圾回收算法的核心思想\n     \n     \n\n\n# 数组越界---c语言\n\njava语言在数组越界时，会抛出异常java.lang.arrayindexoutofboundsexception\n\n> 下面这组代码会无限次输出 helloworld，而不是三次。\n> \n> 原因是 a[3]这个内存地址，存放的是 i=0,所以i会一直重复从0到2。\n\nint main(int argc, char* argv[]){\n    int i = 0;\n    int arr[3] = {0};\n    for(; i<=3; i++){\n        arr[i] = 0;\n        printf("hello world\\n");\n    }\n    return 0;\n}\n\n\n * 内存问题\n   \n   * 内存用栈存储，存储结构如下所示\n   * 所以在访问a[3]时，i=0，无限循环\n\n\n# 容器与数组\n\n * arraylist---优势\n   \n   * 可以将很多数组操作的细节封装起来，前面提到的数组插入、删除数据时需要搬移其他数据。\n   * 支持动态扩容。每次存储空间不够的时候，它都会将空间自动扩容为 1.5 倍大小。我们不需要关心底层的扩容逻辑。\n\n * arraylist---申请\n   \n   * 因为扩容操作涉及内存申请和数据搬移，是比较耗时的。所以，如果事先能确定需要存储的数据大小，最好在创建 arraylist 的时候事先指定数据大小。\n\n * 数组与arraylist\n   \n   * java arraylist 无法存储基本类型，比如 int、long，需要封装为 integer、long 类，而 autoboxing、unboxing 则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组\n   * 如果数据大小事先已知，并且对数据的操作非常简单，用不到 arraylist 提供的大部分方法，也可以直接使用数组。\n   * 当要表示多维数组时，用数组往往会更加直观。比如object[][] array；而用容器的话则需要这样定义：arraylist <arraylist <object>> array\n   \n   总结：\n   \n   > 对于业务开发，直接使用容器就足够了，省时省力。毕竟损耗一丢丢性能，完全不会影响到系统整体的性能。\n   > \n   > 但如果你是做一些非常底层的开发，比如开发网络框架，性能的优化需要做到极致，这个时候数组就会优于容器，成为首选。\n\n\n# leetcode练习题\n\n * 实现一个支持动态扩容的数组\n * 实现一个大小固定的有序数组，支持动态增删改操作\n * 实现两个有序数组合并为一个有序数组',charsets:{cjk:!0}},{title:"时(空)间复杂度",frontmatter:{autoSort:99,title:"时(空)间复杂度",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/f85993/",categories:["算法","算法基础"],tags:["知识","算法"]},regularPath:"/02.%E7%AE%97%E6%B3%95/05.%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/07.%E6%97%B6%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6.html",relativePath:"02.算法/05.算法基础/07.时空间复杂度.md",key:"v-87bd982c",path:"/pages/f85993/",headers:[{level:2,title:"时间复杂度分析--技巧",slug:"时间复杂度分析-技巧",normalizedTitle:"时间复杂度分析--技巧",charIndex:2},{level:2,title:"常见复杂度",slug:"常见复杂度",normalizedTitle:"常见复杂度",charIndex:764},{level:2,title:"最好、最坏、平均、均摊",slug:"最好、最坏、平均、均摊",normalizedTitle:"最好、最坏、平均、均摊",charIndex:1522},{level:2,title:"空间复杂度分析",slug:"空间复杂度分析",normalizedTitle:"空间复杂度分析",charIndex:3138}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"时间复杂度分析--技巧 常见复杂度 最好、最坏、平均、均摊 空间复杂度分析",content:"# 时间复杂度分析--技巧\n\n> 时间复杂度的全称是渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系\n\n * 只关注循环执行次数最多的一段代码\n   \n   > 我们通常会忽略掉公式中的常量、低阶、系数，只需要记录一个最大阶的量级就可以了。所以，我们在分析一个算法、一段代码的时间复杂度的时候，也只关注循环执行次数最多的那一段代码就可以了\n\n * 加法法则：总复杂度等于量级最大的那段代码的复杂度\n   \n   > 这里我要再强调一下，即便这段代码循环 10000 次、100000 次，==只要是一个已知的数，跟 n 无关，照样也是常量级的执行时间==。当 n 无限大的时候，就可以忽略。尽管对代码的执行时间会有很大影响，但是回到时间复杂度的概念来说，它表示的是一个算法执行效率与数据规模增长的变化趋势，所以不管常量的执行时间多大，我们都可以忽略掉。因为它本身对增长趋势并没有影响。\n\n * 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积\n   \n   * 忽略掉f的cal 是O(n),f是O(n)\n   * 加上f的cal是O(n^2^)=O(n*n)\n   \n   int cal(int n) {\n      int ret = 0; \n      int i = 1;\n      for (; i < n; ++i) {\n        ret = ret + f(i);\n      } \n    } \n    \n    int f(int n) {\n     int sum = 0;\n     int i = 1;\n     for (; i < n; ++i) {\n       sum = sum + i;\n     } \n     return sum;\n    }\n   \n\n\n# 常见复杂度\n\n * 非多项式阶\n   * 随着数据规模的增长，算法的执行时间和空间占用暴增，这类算法性能极差。包括， O(2^n)（指数阶）、O(n!)（阶乘阶）\n\n * O(1)\n   * 一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)\n\n int i = 8;\n int j = 6;\n int sum = i + j;\n\n\n * O(logn)\n   * \n   * 所以说，共执行了log~2~n次\n   * 时间复杂度为O(log~2~n)，即O(logn)\n\ni=1;\n while (i <= n)  {\n   i = i * 2;\n }\n\n\n * O(nlogn)\n   \n   * 如果一段代码的时间复杂度是 O(logn)，我们循环执行 n 遍，时间复杂度就是 O(nlogn) 了\n   * 而且，O(nlogn) 也是一种非常常见的算法时间复杂度。比如，归并排序、快速排序的时间复杂度都是 O(nlogn)。\n\n * O(m+n)\n   \n   * 事前不知道m和n的大小，所有总的复杂度是O(m+n)\n   \n   int cal(int m, int n) {\n     int sum_1 = 0;\n     int i = 1;\n     for (; i < m; ++i) {\n       sum_1 = sum_1 + i;\n     }\n   \n     int sum_2 = 0;\n     int j = 1;\n     for (; j < n; ++j) {\n       sum_2 = sum_2 + j;\n     }\n   \n     return sum_1 + sum_2;\n   }\n   \n\n\n# 最好、最坏、平均、均摊\n\n// n表示数组array的长度\nint find(int[] array, int n, int x) {\n  int i = 0;\n  int pos = -1;\n  for (; i < n; ++i) {\n    if (array[i] == x) {\n       pos = i;\n       break;\n    }\n  }\n  return pos;\n}\n\n\n * 最好情况\n   \n   * 当在数组的第一个位置时，时间复杂度为O(1)\n\n * 最坏情况\n   \n   * 当在数组的最后一个位置或者不存在时，时间复杂度为O(n)\n\n * 平均情况\n   \n   > 我们知道，要查找的变量 x，要么在数组里，要么就不在数组里。这两种情况对应的概率统计起来很麻烦，为了方便你理解，我们假设在数组中与不在数组中的概率都为 1/2。另外，要查找的数据出现在 0～n-1 这 n 个位置的概率也是一样的，为 1/n。所以，根据概率乘法法则，要查找的数据出现在 0～n-1 中任意位置的概率就是 1/(2n)\n   \n   \n   \n   > 这个值就是概率论中的加权平均值，也叫作期望值，所以平均时间复杂度的全称应该叫加权平均时间复杂度或者期望时间复杂度。引入概率之后，前面那段代码的加权平均值为 (3n+1)/4。用大 O 表示法来表示，去掉系数和常量，这段代码的加权平均时间复杂度仍然是 O(n)。\n\n * 均摊情况\n   \n   * 当数组有空闲时，插入一个数据——O(1)\n   * 当数组满时，对数组进行求和，将sum放入a[0],继续插入数据---O(n)\n   \n   \n    // array表示一个长度为n的数组\n    // 代码中的array.length就等于n\n    int[] array = new int[n];\n    int count = 0;\n    \n    void insert(int val) {\n       if (count == array.length) {\n          int sum = 0;\n          for (int i = 0; i < array.length; ++i) {\n             sum = sum + array[i];\n          }\n          array[0] = sum;\n          count = 1;\n       }\n   \n       array[count] = val;\n       ++count;\n    }\n   \n   \n   > 对于 insert() 函数来说，O(1) 时间复杂度的插入和 O(n) 时间复杂度的插入，出现的频率是非常有规律的，而且有一定的前后时序关系，一般都是一个 O(n) 插入之后，紧跟着 n-1 个 O(1) 的插入操作，循环往复。\n   > \n   > 我们引入了一种更加简单的分析方法：==摊还分析法==，通过摊还分析得到的时间复杂度我们起了一个名字，叫==均摊时间复杂度==。\n   \n   > 每一次 O(n) 的插入操作，都会跟着 n-1 次 O(1) 的插入操作，所以把耗时多的那次操作均摊到接下来的 n-1 次耗时少的操作上，均摊下来，这一组连续的操作的均摊时间复杂度就是 O(1)。\n   > \n   > 对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块儿分析，看是否能将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上。而且，在能够应用均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好情况时间复杂度。\n\n\n# 空间复杂度分析\n\n * 第 3 行申请了一个大小为 n 的 int 类型数组，除此之外，剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是 O(n)\n\n\nvoid print(int n) {\n  int i = 0;\n    \n  int[] a = new int[n];\n    \n  for (i; i <n; ++i) {\n    a[i] = i * i;\n  }\n\n  for (i = n-1; i >= 0; --i) {\n    print out a[i]\n  }\n}\n",normalizedContent:"# 时间复杂度分析--技巧\n\n> 时间复杂度的全称是渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系\n\n * 只关注循环执行次数最多的一段代码\n   \n   > 我们通常会忽略掉公式中的常量、低阶、系数，只需要记录一个最大阶的量级就可以了。所以，我们在分析一个算法、一段代码的时间复杂度的时候，也只关注循环执行次数最多的那一段代码就可以了\n\n * 加法法则：总复杂度等于量级最大的那段代码的复杂度\n   \n   > 这里我要再强调一下，即便这段代码循环 10000 次、100000 次，==只要是一个已知的数，跟 n 无关，照样也是常量级的执行时间==。当 n 无限大的时候，就可以忽略。尽管对代码的执行时间会有很大影响，但是回到时间复杂度的概念来说，它表示的是一个算法执行效率与数据规模增长的变化趋势，所以不管常量的执行时间多大，我们都可以忽略掉。因为它本身对增长趋势并没有影响。\n\n * 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积\n   \n   * 忽略掉f的cal 是o(n),f是o(n)\n   * 加上f的cal是o(n^2^)=o(n*n)\n   \n   int cal(int n) {\n      int ret = 0; \n      int i = 1;\n      for (; i < n; ++i) {\n        ret = ret + f(i);\n      } \n    } \n    \n    int f(int n) {\n     int sum = 0;\n     int i = 1;\n     for (; i < n; ++i) {\n       sum = sum + i;\n     } \n     return sum;\n    }\n   \n\n\n# 常见复杂度\n\n * 非多项式阶\n   * 随着数据规模的增长，算法的执行时间和空间占用暴增，这类算法性能极差。包括， o(2^n)（指数阶）、o(n!)（阶乘阶）\n\n * o(1)\n   * 一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是ο(1)\n\n int i = 8;\n int j = 6;\n int sum = i + j;\n\n\n * o(logn)\n   * \n   * 所以说，共执行了log~2~n次\n   * 时间复杂度为o(log~2~n)，即o(logn)\n\ni=1;\n while (i <= n)  {\n   i = i * 2;\n }\n\n\n * o(nlogn)\n   \n   * 如果一段代码的时间复杂度是 o(logn)，我们循环执行 n 遍，时间复杂度就是 o(nlogn) 了\n   * 而且，o(nlogn) 也是一种非常常见的算法时间复杂度。比如，归并排序、快速排序的时间复杂度都是 o(nlogn)。\n\n * o(m+n)\n   \n   * 事前不知道m和n的大小，所有总的复杂度是o(m+n)\n   \n   int cal(int m, int n) {\n     int sum_1 = 0;\n     int i = 1;\n     for (; i < m; ++i) {\n       sum_1 = sum_1 + i;\n     }\n   \n     int sum_2 = 0;\n     int j = 1;\n     for (; j < n; ++j) {\n       sum_2 = sum_2 + j;\n     }\n   \n     return sum_1 + sum_2;\n   }\n   \n\n\n# 最好、最坏、平均、均摊\n\n// n表示数组array的长度\nint find(int[] array, int n, int x) {\n  int i = 0;\n  int pos = -1;\n  for (; i < n; ++i) {\n    if (array[i] == x) {\n       pos = i;\n       break;\n    }\n  }\n  return pos;\n}\n\n\n * 最好情况\n   \n   * 当在数组的第一个位置时，时间复杂度为o(1)\n\n * 最坏情况\n   \n   * 当在数组的最后一个位置或者不存在时，时间复杂度为o(n)\n\n * 平均情况\n   \n   > 我们知道，要查找的变量 x，要么在数组里，要么就不在数组里。这两种情况对应的概率统计起来很麻烦，为了方便你理解，我们假设在数组中与不在数组中的概率都为 1/2。另外，要查找的数据出现在 0～n-1 这 n 个位置的概率也是一样的，为 1/n。所以，根据概率乘法法则，要查找的数据出现在 0～n-1 中任意位置的概率就是 1/(2n)\n   \n   \n   \n   > 这个值就是概率论中的加权平均值，也叫作期望值，所以平均时间复杂度的全称应该叫加权平均时间复杂度或者期望时间复杂度。引入概率之后，前面那段代码的加权平均值为 (3n+1)/4。用大 o 表示法来表示，去掉系数和常量，这段代码的加权平均时间复杂度仍然是 o(n)。\n\n * 均摊情况\n   \n   * 当数组有空闲时，插入一个数据——o(1)\n   * 当数组满时，对数组进行求和，将sum放入a[0],继续插入数据---o(n)\n   \n   \n    // array表示一个长度为n的数组\n    // 代码中的array.length就等于n\n    int[] array = new int[n];\n    int count = 0;\n    \n    void insert(int val) {\n       if (count == array.length) {\n          int sum = 0;\n          for (int i = 0; i < array.length; ++i) {\n             sum = sum + array[i];\n          }\n          array[0] = sum;\n          count = 1;\n       }\n   \n       array[count] = val;\n       ++count;\n    }\n   \n   \n   > 对于 insert() 函数来说，o(1) 时间复杂度的插入和 o(n) 时间复杂度的插入，出现的频率是非常有规律的，而且有一定的前后时序关系，一般都是一个 o(n) 插入之后，紧跟着 n-1 个 o(1) 的插入操作，循环往复。\n   > \n   > 我们引入了一种更加简单的分析方法：==摊还分析法==，通过摊还分析得到的时间复杂度我们起了一个名字，叫==均摊时间复杂度==。\n   \n   > 每一次 o(n) 的插入操作，都会跟着 n-1 次 o(1) 的插入操作，所以把耗时多的那次操作均摊到接下来的 n-1 次耗时少的操作上，均摊下来，这一组连续的操作的均摊时间复杂度就是 o(1)。\n   > \n   > 对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块儿分析，看是否能将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上。而且，在能够应用均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好情况时间复杂度。\n\n\n# 空间复杂度分析\n\n * 第 3 行申请了一个大小为 n 的 int 类型数组，除此之外，剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是 o(n)\n\n\nvoid print(int n) {\n  int i = 0;\n    \n  int[] a = new int[n];\n    \n  for (i; i <n; ++i) {\n    a[i] = i * i;\n  }\n\n  for (i = n-1; i >= 0; --i) {\n    print out a[i]\n  }\n}\n",charsets:{cjk:!0}},{title:"链表",frontmatter:{autoSort:97,title:"链表",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/1d3d1a/",categories:["算法","链表"],tags:["知识","算法"]},regularPath:"/02.%E7%AE%97%E6%B3%95/15.%E9%93%BE%E8%A1%A8/05.%E9%93%BE%E8%A1%A8.html",relativePath:"02.算法/15.链表/05.链表.md",key:"v-2e0d1200",path:"/pages/1d3d1a/",headers:[{level:2,title:"链表结构",slug:"链表结构",normalizedTitle:"链表结构",charIndex:2},{level:2,title:"基于链表实现LRU缓存淘汰算法",slug:"基于链表实现lru缓存淘汰算法",normalizedTitle:"基于链表实现lru缓存淘汰算法",charIndex:548},{level:2,title:"判断一个字符串是否是回文串",slug:"判断一个字符串是否是回文串",normalizedTitle:"判断一个字符串是否是回文串",charIndex:1072},{level:2,title:"如何轻松写出正确的链表代码",slug:"如何轻松写出正确的链表代码",normalizedTitle:"如何轻松写出正确的链表代码",charIndex:1390},{level:2,title:"==LeetCode练习题==",slug:"leetcode练习题",normalizedTitle:"==leetcode练习题==",charIndex:2361}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"链表结构 基于链表实现LRU缓存淘汰算法 判断一个字符串是否是回文串 如何轻松写出正确的链表代码 ==LeetCode练习题==",content:"# 链表结构\n\n * 链表与数组\n   \n   * 底层存储\n     \n     * 数组需要一块连续的空间，而链表只需要一些零散的内存快即可。\n     \n     \n   \n   * 性能比拼1\n   \n   \n   \n   * 性能比拼2\n     * 数组简单易用，在实现上使用的是连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对 CPU 缓存不友好，没办法有效预读。\n     * 除此之外，如果你的代码对内存的使用非常苛刻，那数组就更适合你。因为链表中的每个结点都需要消耗额外的存储空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。\n     * 而且，对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片，如果是 Java 语言，就有可能会导致频繁的 GC（Garbage Collection，垃圾回收）\n\n * 单链表\n\n * 双链表\n   * 实际中双向链表应用更广，比单链表效率要高。找到前驱只需要O(1)\n   * Linklist,LinkedHashMap 等都是双向链表。\n   * Linklist——默认删除是从头结点开始删除的。\n\n * 循环链表\n\n\n# 基于链表实现LRU缓存淘汰算法\n\n * 缓存\n\n> 缓存是为了缓解慢速的内存与快速的CPU之间矛盾而产生的，访问速度极快。\n> \n> 缓存的大小有限，当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留？这就需要缓存淘汰策略来决定。常见的策略有三种：先进先出策略 ==FIFO==（First In，First Out）、最少使用策略 ==LFU==（Least Frequently Used）、最近最少使用策略 ==LRU==（Least Recently Used）\n> \n> LRU 策略，使得页面置换次数低，抖动次数少。\n\n * 算法思想\n   \n   > 我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。---头插法\n   \n   * 如果此数据之前被缓存进链表中了，我们遍历得到这个链表的结点，并将原来的位置删除，将该节点插入链表的头部。\n   * 如果此数据没有在缓存链表中，则：\n     * 如果缓存未满，则直接插入到链表的头部。\n     * 如果此时缓存已经满了，则删除链表尾部的内容，然后将新的数据结点插入链表的头部。\n\n * 代码实现\n\n\n# 判断一个字符串是否是回文串\n\n * 算法思想(双向链表)\n   * 用快慢指针找到链表的中点(当快指针走到头时，慢指针就恰好走到中间)\n   * 然后慢指针在往反方向走，走到表头，相当于是反向遍历链表左半部分(双向链表)\n   * 同时，正向遍历链表的右半部分，并且将二者数据进行比较，如果完全一致，则说明是回文串。\n * 算法思想(单链表)\n   * 用快慢指针找到链表的中点(当快指针走到头时，慢指针就恰好走到中间)\n   * 将单链表后半部分逆序，就是慢指针的next部分。一直遍历知道链表尾部\n   * 同时，正向遍历链表的前半部分，并且将二者数据进行比较，如果完全一致，则说明是回文串。\n   * 最后将后半部分复原\n\n\n# 如何轻松写出正确的链表代码\n\n * 理解指针或引用的含义\n   \n   > 将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。\n   > \n   > C语言--指针，Python，Java--引用\n\n * 警惕指针丢失和内存泄漏\n   \n   * 我们插入结点时，一定要注意操作的顺序。(先把next指向存好，在将next指向其他地方)\n   * 删除链表结点时，也一定要记得手动释放内存空间，否则，也会出现内存泄漏的问题。当然，对于像 Java 这种虚拟机自动管理内存的编程语言来说，就不需要考虑这么多了。\n\n * 利用哨兵简化实现难度\n   \n   * 不利用哨兵的话，插入和删除，都需要考虑边界上的特殊结点。第一个和最后一个。\n   \n   %% 插入操作\n   new_node->next = p->next;\n   p->next = new_node;\n   \n   if (head == null) { %%需要额外考虑插入的结点是不是第一个结点\n     head = new_node;\n   }\n   \n   %%删除操作\n   p->next = p->next->next;\n   \n   if (head->next == null) {%%需要额外考虑删除的结点是不是最后一个结点\n      head = null;\n   }\n   \n   \n   * 引入头结点，即哨兵，head指针会一直指向这个头结点。有了这个头结点，插入和删除代码就统一了。\n\n * 重点留意边界条件处理\n   \n   ==健壮性==\n   \n   > 在写任何代码时，也千万不要只是实现业务正常情况下的功能就好了，一定要多想想，你的代码在运行的时候，可能会遇到哪些边界情况或者异常情况。遇到了应该如何应对，这样写出来的代码才够健壮！\n   \n   * 如果链表为空时，代码是否能正常工作？\n   * 如果链表只包含一个结点时，代码是否能正常工作？\n   * 如果链表只包含两个结点时，代码是否能正常工作？\n   * 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？\n\n * 举例画图，辅助思考\n\n * 多写多练，没有捷径\n\n\n# ==LeetCode练习题==\n\n * 实现单链表、循环链表、双向链表，支持增删操作\n * 实现单链表反转\n * 实现两个有序的链表合并为一个有序链表\n * 实现求链表的中间结点\n * 链表中环的检测",normalizedContent:"# 链表结构\n\n * 链表与数组\n   \n   * 底层存储\n     \n     * 数组需要一块连续的空间，而链表只需要一些零散的内存快即可。\n     \n     \n   \n   * 性能比拼1\n   \n   \n   \n   * 性能比拼2\n     * 数组简单易用，在实现上使用的是连续的内存空间，可以借助 cpu 的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对 cpu 缓存不友好，没办法有效预读。\n     * 除此之外，如果你的代码对内存的使用非常苛刻，那数组就更适合你。因为链表中的每个结点都需要消耗额外的存储空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。\n     * 而且，对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片，如果是 java 语言，就有可能会导致频繁的 gc（garbage collection，垃圾回收）\n\n * 单链表\n\n * 双链表\n   * 实际中双向链表应用更广，比单链表效率要高。找到前驱只需要o(1)\n   * linklist,linkedhashmap 等都是双向链表。\n   * linklist——默认删除是从头结点开始删除的。\n\n * 循环链表\n\n\n# 基于链表实现lru缓存淘汰算法\n\n * 缓存\n\n> 缓存是为了缓解慢速的内存与快速的cpu之间矛盾而产生的，访问速度极快。\n> \n> 缓存的大小有限，当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留？这就需要缓存淘汰策略来决定。常见的策略有三种：先进先出策略 ==fifo==（first in，first out）、最少使用策略 ==lfu==（least frequently used）、最近最少使用策略 ==lru==（least recently used）\n> \n> lru 策略，使得页面置换次数低，抖动次数少。\n\n * 算法思想\n   \n   > 我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。---头插法\n   \n   * 如果此数据之前被缓存进链表中了，我们遍历得到这个链表的结点，并将原来的位置删除，将该节点插入链表的头部。\n   * 如果此数据没有在缓存链表中，则：\n     * 如果缓存未满，则直接插入到链表的头部。\n     * 如果此时缓存已经满了，则删除链表尾部的内容，然后将新的数据结点插入链表的头部。\n\n * 代码实现\n\n\n# 判断一个字符串是否是回文串\n\n * 算法思想(双向链表)\n   * 用快慢指针找到链表的中点(当快指针走到头时，慢指针就恰好走到中间)\n   * 然后慢指针在往反方向走，走到表头，相当于是反向遍历链表左半部分(双向链表)\n   * 同时，正向遍历链表的右半部分，并且将二者数据进行比较，如果完全一致，则说明是回文串。\n * 算法思想(单链表)\n   * 用快慢指针找到链表的中点(当快指针走到头时，慢指针就恰好走到中间)\n   * 将单链表后半部分逆序，就是慢指针的next部分。一直遍历知道链表尾部\n   * 同时，正向遍历链表的前半部分，并且将二者数据进行比较，如果完全一致，则说明是回文串。\n   * 最后将后半部分复原\n\n\n# 如何轻松写出正确的链表代码\n\n * 理解指针或引用的含义\n   \n   > 将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。\n   > \n   > c语言--指针，python，java--引用\n\n * 警惕指针丢失和内存泄漏\n   \n   * 我们插入结点时，一定要注意操作的顺序。(先把next指向存好，在将next指向其他地方)\n   * 删除链表结点时，也一定要记得手动释放内存空间，否则，也会出现内存泄漏的问题。当然，对于像 java 这种虚拟机自动管理内存的编程语言来说，就不需要考虑这么多了。\n\n * 利用哨兵简化实现难度\n   \n   * 不利用哨兵的话，插入和删除，都需要考虑边界上的特殊结点。第一个和最后一个。\n   \n   %% 插入操作\n   new_node->next = p->next;\n   p->next = new_node;\n   \n   if (head == null) { %%需要额外考虑插入的结点是不是第一个结点\n     head = new_node;\n   }\n   \n   %%删除操作\n   p->next = p->next->next;\n   \n   if (head->next == null) {%%需要额外考虑删除的结点是不是最后一个结点\n      head = null;\n   }\n   \n   \n   * 引入头结点，即哨兵，head指针会一直指向这个头结点。有了这个头结点，插入和删除代码就统一了。\n\n * 重点留意边界条件处理\n   \n   ==健壮性==\n   \n   > 在写任何代码时，也千万不要只是实现业务正常情况下的功能就好了，一定要多想想，你的代码在运行的时候，可能会遇到哪些边界情况或者异常情况。遇到了应该如何应对，这样写出来的代码才够健壮！\n   \n   * 如果链表为空时，代码是否能正常工作？\n   * 如果链表只包含一个结点时，代码是否能正常工作？\n   * 如果链表只包含两个结点时，代码是否能正常工作？\n   * 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？\n\n * 举例画图，辅助思考\n\n * 多写多练，没有捷径\n\n\n# ==leetcode练习题==\n\n * 实现单链表、循环链表、双向链表，支持增删操作\n * 实现单链表反转\n * 实现两个有序的链表合并为一个有序链表\n * 实现求链表的中间结点\n * 链表中环的检测",charsets:{cjk:!0}},{title:"堆",frontmatter:{autoSort:87,title:"堆",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/abbe04/",categories:["算法","堆"],tags:["知识","算法"]},regularPath:"/02.%E7%AE%97%E6%B3%95/30.%E5%A0%86/05.%E5%A0%86.html",relativePath:"02.算法/30.堆/05.堆.md",key:"v-16ecaa80",path:"/pages/abbe04/",headers:[{level:2,title:"堆基础",slug:"堆基础",normalizedTitle:"堆基础",charIndex:2},{level:2,title:"优先级队列",slug:"优先级队列",normalizedTitle:"优先级队列",charIndex:3952},{level:2,title:"求Topk",slug:"求topk",normalizedTitle:"求topk",charIndex:5573},{level:2,title:"求中位数",slug:"求中位数",normalizedTitle:"求中位数",charIndex:7054}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"堆基础 优先级队列 求Topk 求中位数",content:'# 堆基础\n\n * 堆概念\n   \n   >  1. 堆是一个完全二叉树\n   >  2. 堆中每一个结点的值都必须大于等于(或者小于等于)其子树中每个结点的值。\n   >     * 大于等于是 大堆顶\n   >     * 小于等于是小堆顶\n   >  3. 堆顶大的称为大堆顶，堆顶小的称为小堆顶\n\n * 堆的实现\n   \n   * 元素存储\n     \n     * 完全二叉树，借用数组存储。\n       \n       * ==下标与父子关系==\n         * 设数组中存储下标从1开始\n         * 数组中下标为$i$的节点的左子节点，就是下标为$i2$的节点，右子节点就是下标为$i2+1$的节点，父节点就是下标为 $i/2$ 的节点。\n         * 下标从$n/2$开始，不包括$n/2$,后面的都是叶子结点\n   \n   * 插入一个元素\n     \n     往堆中插入一个元素后，需要调整堆的结构，要让堆符合特性，这一过程称为堆化。\n     \n     * 自下而上的堆化\n       \n       例如，在数组最后插入元素22，然后调整堆的结构\n     \n     * 插入代码\n       \n       /**\n            * 堆中插入元素--自下而上的堆化--大堆顶\n            * 数组下标从1开始计数\n            * @param nums  数组\n            * @param count  数组中已有元素个数\n            * @param value  待插入的值\n            */\n       public void insert(int[] nums, int count, int value) {\n           if (count > nums.length) {\n               System.out.println("数组中元素已满，请扩容数组");\n               return;\n           }\n       \n           //数组最后插入元素\n           int height = count + 1;\n           nums[height] = value;\n           int parent;\n       \n           //自下而上开始堆化\n           while (height > 1) {\n               parent = height / 2;\n               if (nums[height] > nums[parent]) {\n                   swap(nums, height, parent);\n                   height = parent;\n               }else {\n                   break;\n               }\n           }\n       \n       }\n       \n   \n   * 删除堆顶元素\n     \n     将数组最后一个放到堆顶，然后利用同样的父子节点对比方法。对于不满足父子节点大小关系的，互换两个节点，并且重复进行这个过程，直到父子节点之间满足大小关系为止。这就是从上往下的堆化。\n     \n     * 删除代码\n       \n       /**\n            * 删除堆顶元素-- 自上而下的堆化--大堆顶\n            * @param nums 数组\n            * @param count 数组中元素个数\n            */\n       public void deleteHeapTop(int[] nums, int count) {\n           if (count < 1) return;\n           //        将最后一个元素放到堆顶\n           nums[1] = nums[count];\n           nums[count] = 0;\n           heapify(nums, 1, count);\n       }\n       \n       \n       /**\n            * 自上而下的堆化\n            * @param nums 数组\n            * @param height 当前值的下标\n            */\n       public void heapify(int[] nums, int height, int count) {\n           while (true) {\n               int maxPos = height;\n               //当前值不光要与左子树比较还要与右子树比较\n               if (height * 2 <= count && nums[maxPos] < nums[height * 2]) maxPos = height * 2;\n               if (height * 2 + 1 <= count && nums[maxPos] < nums[height * 2 + 1]) maxPos = height * 2 + 1;\n       \n               if (maxPos == height) break;\n               swap(nums, height, maxPos);\n               height = maxPos;\n           }\n       }\n       \n\n * 堆排序\n   \n   * 建堆——原地排序\n     \n     不借助另外一个数组，在自己的数组基础上进行元素调整\n     \n     建堆时间复杂度为O(n)\n     \n     * 从上往下堆化----------自下而上的堆化\n       \n       > 尽管数组中包含 n 个数据，但是我们可以假设，起初堆中只包含一个数据，就是下标为 1 的数据。然后，我们调用前面讲的插入操作，将下标从 2 到 n 的数据依次插入到堆中。这样我们就将包含 n 个数据的数组，组织成了堆。\n     \n     * 从下往上堆化----------自上而下的堆化\n       \n       > 从后往前处理数组，并且每个数据都是从上往下堆化。直接从最后一个非叶子节点开始，依次堆化就行了。\n       \n       * 代码\n         \n         /**\n              * 建堆\n              * @param nums 数组\n              * @param count 数组内元素\n              */\n         public void bulidHeap(int[] nums, int count) {\n             for (int i = count / 2; i >= 1; i--) {\n                 heapify(nums, i, count);\n             }\n         }\n         \n   \n   * 排序\n     \n     > 建堆结束之后，数组中的数据已经是按照大顶堆的特性来组织的。数组中的第一个元素就是堆顶，也就是最大的元素。我们把它跟最后一个元素交换，那最大元素就放到了下标为 n 的位置。 然后对剩余元素，继续堆化，不断的交换元素，直到堆中只剩下一个元素为止。\n     \n     排序过程时间复杂度O(nlogn)\n     \n     所以堆排序整体过程时间复杂度为O(nlogn)\n     \n     * 代码\n       \n       public void sort(int[] nums, int count) {\n           int num = count;\n           while (num > 1) {\n               //建堆\n               bulidHeap(nums, num);\n               //取堆顶元素，放入最后\n               swap(nums, num, 1);\n               num --;\n           }\n       }\n       \n\n * 为什么快速排序要比堆排序性能好？\n   \n   * 堆排序数据访问的方式没有快速排序友好。\n     \n     > 堆排序时，访问元素是跳跃式访问，对cpu缓存不友好，\n     > \n     > 而快排是局部顺序访问。\n   \n   * 对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序。\n     \n     > 建堆的过程，会打乱原先的相对的先后顺序，导致有序度降低。\n     > \n     > 从而堆排序比快速排序交换次数多。\n\n\n# 优先级队列\n\njava-----PriorityQueue\n\n> 一个堆就可以看作一个优先级队列。\n> \n> 往优先级队列中插入一个元素，就相当于往堆中插入一个元素,然后堆化；\n> \n> 从优先级队列中取出优先级最高的元素，就相当于取出堆顶元素,然后堆化。\n> \n> 队列内部无序，但是队首元素，一定是优先级最高的，每次取走队首元素，都会对剩余元素进行堆化\n\n * 代码\n   \n   public static void testPriorityQueue() {\n       //优先级队列(默认是小堆顶)\n       //队列内部无序，但是队首元素，一定是最小的，每次取走队首元素，都会对剩余元素进行堆化\n       //堆化方式是，将最后一个元素放入队首，自上而下的堆化\n       PriorityQueue<Integer> queue = new PriorityQueue<>();\n       //插入的时候是按照完全二叉树的构造方式插入的\n       queue.add(5);\n       queue.add(3);\n       queue.add(1);\n       queue.add(2);\n       queue.add(4);\n       System.out.println(queue);//[1, 2, 3, 5, 4]\n   \n       queue.poll();\n       System.out.println(queue);//[2, 4, 3, 5]\n   \n       queue.poll();\n       System.out.println(queue);//[3, 4, 5]\n   }\n   \n\n * 合并有序小文件\n   \n   * 问题描述\n     \n     > 假设我们有 100 个小文件，每个文件的大小是 100MB，每个文件中存储的都是有序的字符串。我们希望将这些 100 个小文件合并成一个有序的大文件。\n   \n   * 解决办法\n     \n     > 我们取100个小文件的第一个元素，构建一个大小为100的小堆顶。那堆顶的元素，也就是优先级队列队首的元素，就是最小的字符串。\n     > \n     > 我们将这个字符串放入到大文件中，并将其从堆中删除。然后再从小文件中(取走的字符串所属的小文件)取出下一个字符串，放入到堆中。循环这个过程，就可以将 100 个小文件中的数据依次放入到大文件中。\n\n * 高性能定时器\n   \n   * 问题描述\n     \n     > 假设我们有一个定时器，定时器中维护了很多定时任务，每个任务都设定了一个要触发执行的时间点。定时器每过一个很小的单位时间（比如 1 秒），就扫描一遍任务，看是否有任务到达设定的执行时间。如果到达了，就拿出来执行。\n     > \n     > 但是，这样每过 1 秒就扫描一遍任务列表的做法比较低效。\n   \n   * 解决方法\n     \n     > 我们按照任务设定的执行时间，将这些任务存储在优先级队列中，队列首部（也就是小顶堆的堆顶）存储的是最先执行的任务。\n     > \n     > 这样，定时器就不需要每隔 1 秒就扫描一遍任务列表了。它拿队首任务的执行时间点，与当前时间点相减，得到一个时间间隔 T。这样，定时器就可以设定在 T 秒之后，再来执行任务。从当前时间点到（T-1）秒这段时间里，定时器都不需要做任何事情。\n     > \n     > 当 T 秒时间过去之后，定时器取优先级队列中队首的任务执行。**优先级队列取走队首元素后，会对剩余元素堆化，在次将最小的放在队首，保证每次从队首拿的都是最小的时间。**然后再计算新的队首任务的执行时间点与当前时间点的差值，把这个值作为定时器执行下一个任务需要等待的时间。\n\n\n# 求Topk\n\n * 静态数据\n   \n   * 问题描述\n     \n     > 如何在一个包含 n 个数据的数组中，查找==前 K 大数据==呢？\n   \n   * 解决方法\n     \n     >  1. 遍历数组，先取前K个元素，构建==小堆顶==。堆顶元素是K个元素中最小的。\n     >  2. 从第K+1个元素，继续遍历数组，从数组中取出数据与堆顶元素比较。\n     >     * 如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中，然后堆化，构造小堆顶。\n     >       * 堆顶元素是最小的，也就是最危险的，只要来了比它大的，就淘汰它。\n     >     * 如果比堆顶元素小，则不做处理，继续遍历数组。\n     >       * 堆顶元素是最小的，你比我还小，怎么可能进的来？\n     >  3. 这样等数组中的数据都遍历完之后，堆中的数据就是前 K 大数据了。\n     > \n     > 时间复杂度：\n     > \n     > 遍历数组需要 O(n) 的时间复杂度，一次堆化操作需要 O(logK) 的时间复杂度，所以最坏情况下，n 个元素都入堆一次，时间复杂度就是 O(nlogK)。\n   \n   * 代码\n     \n     public static void  topK() {\n         int[] nums = {3,2,4,7,9,0,54,1,435,245,344};\n         int[] top = new int[5 + 1];\n         //建堆\n         int index;\n         for (index = 0; index < 5; index ++) {\n             top[index + 1] = nums[index];\n         }\n         Heap.bulidHeap(top,5, 0);\n         //遍历剩余数组元素，维护堆\n         for (;index < nums.length; index ++) {\n             if (nums[index] > top[1]) {\n                 top[1] = nums[index];\n                 Heap.bulidHeap(top,5, 0);\n             }\n         }\n         System.out.println(Arrays.toString(top));\n     \n     }\n     \n\n * 动态数据\n   \n   * 问题描述\n     \n     > 针对动态数据求得 Top K 就是实时 Top K。一个数据集合中有两个操作，一个是添加数据，另一个询问当前的前 K 大数据。\n     > \n     > 如果每次询问前 K 大数据，我们都基于当前的数据重新计算的话，那时间复杂度就是 O(nlogK)，n 表示当前的数据的大小。\n   \n   * 解决方法\n     \n     > 我们可以一直都维护一个 K 大小的小顶堆，**当有数据被添加到集合中时，我们就拿它与堆顶的元素对比。**如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理。这样，无论任何时候需要查询当前的前 K 大数据，我们都可以立刻返回给他。\n\n\n# 求中位数\n\n * 问题描述\n   \n   > 对于一组静态数据，中位数是固定的，我们可以先排序，第 2n 个数据就是中位数。每次询问中位数的时候，我们直接返回这个固定的值就好了。\n   > \n   > 但是，如果我们面对的是动态数据集合，中位数在不停地变动，如果再用先排序的方法，每次询问中位数的时候，都要先进行排序，那效率就不高了。\n   > \n   > 借助堆这种数据结构，我们不用排序，就可以非常高效地实现求中位数操作。\n\n * 解决方法\n   \n   * 初始化堆\n     \n     > 我们需要维护两个堆，一个大顶堆，一个小顶堆。\n     > \n     > 大顶堆中存储前半部分数据，小顶堆中存储后半部分数据，且小顶堆中的数据都大于大顶堆中的数据。大堆顶元素即为中位数。\n     > \n     > 如果 n 是奇数，情况是类似的，大顶堆就存储 n/2+1 个数据，小顶堆中就存储 n/2 个数据。\n   \n   * 动态变换\n     \n     > 如果新加入的数据小于等于大顶堆的堆顶元素，我们就将这个新数据插入到大顶堆；否则，我们就将这个新数据插入到小顶堆。\n     > \n     > 这个时候就有可能出现，两个堆中的数据个数不符合前面约定的情况，我们可以从一个堆中不停地将堆顶元素移动到另一个堆，通过这样的调整，来让两个堆中的数据满足上面的约定。\n   \n   * 代码\n     \n     对确定数组求中位数\n     \n     public static void getMid() {\n         int[] nums = {3,2,4,7,9,10,54,1,435,245,344};\n         int count = nums.length;\n         int bigCount = 11 / 2 + 1;\n         int littleCount = 11 / 2;\n         int[] bigNum = new int[bigCount + 1];//存储大堆顶\n         int[] littleNum = new int[littleCount + 1];//存储小堆顶\n         int i;\n         Arrays.sort(nums);\n         for (i = 1; i < bigCount + 1; i++) {\n             bigNum[i] = nums[i - 1];\n         }\n         //构建大堆顶\n         Heap.bulidHeap(bigNum, bigCount, 1);\n     \n         for (;i < nums.length + 1; i++) {\n             littleNum[i - bigCount] = nums[i - 1];\n         }\n         //构建小堆顶\n         Heap.bulidHeap(littleNum, littleCount, 0);\n     \n         //中位数即，大堆顶的顶部元素\n         System.out.println(bigNum[1]);\n     \n     }\n     \n   \n   * 对流动数组求中位数，以及堆的实现\n     \n     > 自定义堆的实现，使用一个变量控制大顶堆还是小顶堆 使用动态数组扩容\n     \n     // 对数据流求中位数\n     class MedianFinder {\n         Heap minQueue;\n         Heap maxQueue;\n     \n         public MedianFinder() {\n             // 默认是小堆顶\n             minQueue = new Heap(false);\n             maxQueue = new Heap(true);\n     \n         }\n         // 大顶堆中的元素要小于小顶堆\n         public void addNum(int num) {\n             // 当两个堆中的元素个数不等时，往大顶堆中添加元素；当相等时，往小顶堆中添加元素。\n             // 在往大顶堆添加元素时，先将其放入小顶堆，然后取小堆顶堆顶元素放入大顶堆\n             // 同理，当往小顶堆添加元素时，先将其放入大顶堆，然后取大顶堆堆顶元素，放入小顶堆\n     \n             // 尝试放入大顶堆\n             if (minQueue.size == maxQueue.size) {\n                 if (!minQueue.isEmpty() && num > minQueue.peek()) {\n                     minQueue.offer(num);\n                     maxQueue.offer(minQueue.poll());\n                 } else {\n                     maxQueue.offer(num);\n                 }\n                 return;\n             }\n     \n             // 尝试放入小顶堆\n             if (num < maxQueue.peek()) {\n                 maxQueue.offer(num);\n                 minQueue.offer(maxQueue.poll());\n             } else {\n                 minQueue.offer(num);\n             }     \n     \n         }\n         \n         public double findMedian() {\n             if (maxQueue.size == minQueue.size) {\n                 return (maxQueue.peek() + minQueue.peek()) / 2.0;\n             }\n             // return (double)maxQueue.peek();\n             return maxQueue.peek();\n     \n         }\n     }\n     \n     \n     // 建堆\n     \n     /**\n      * @ClassName heap\n      * @Date 2023/6/21 15:28\n      * @Author diane\n      * @Description 手动建立堆\n      * @Version 1.0\n      */\n     // 自定义堆实现\n     // 上浮，下潜，动态数组\n     class Heap {\n       int[] nums;\n       int size;\n       // true 表示大顶堆；false 表示小顶堆\n       boolean max;\n       public Heap(boolean maxbool) {\n         size = 0;\n         max = maxbool;\n         nums = new int[3];\n       }\n       // 添加元素\n       public void offer(int num) {\n         // 数组满了，扩容\n         if (size == nums.length) {\n           grow();\n         }\n         // 上浮，num,使之满足堆的特性\n         int child = size;\n         while (child > 0) {\n           int parent = (child - 1) / 2;\n           if (max ? nums[parent] < num : nums[parent] > num) {\n             nums[child] = nums[parent];\n           } else {\n             break;\n           }\n           // 更新孩子指针\n           child = parent;\n         }\n         nums[child] = num;\n         size++;\n       }\n       // 移除元素\n       public int poll() {\n         // 不正常移除\n         if (size == 0) {\n           return -1;\n         }\n         // 交换堆顶元素和最末尾元素，然后移除最末尾元素，最后重新堆化-下潜\n         int temp = nums[0];\n         swap(0, size - 1);\n         size--;\n         down(0);\n         return temp;\n       }\n       // 元素下潜\n       public void down(int p) {\n         int left = 2 * p + 1;\n         int right = left + 1;\n         int maxOrMin = p;\n         if (left < size && (max ? (nums[left] > nums[maxOrMin]) : (nums[left] < nums[maxOrMin]))) {\n           maxOrMin = left;\n         }\n         if (right < size && (max ? (nums[right] > nums[maxOrMin]) : (nums[right] < nums[maxOrMin]))) {\n           maxOrMin = right;\n         }\n         // 如果最大或者最小不是原父指针，则递归调用\n         if (maxOrMin != p) {\n           swap(maxOrMin, p);\n           down(maxOrMin);\n         }\n       }\n       // 返回堆顶远古三\n       public int peek() {\n         return nums[0];\n       }\n       // 判空\n       public boolean isEmpty() {\n         return size == 0;\n       }\n       // 交换元素\n       public void swap(int i, int j) {\n         int temp = nums[i];\n         nums[i] = nums[j];\n         nums[j] = temp;\n       }\n       // 数组扩容\n       public void grow() {\n         // 1.5倍扩容\n         int cap = size + (size >> 1);\n         int[] newNums = new int[cap];\n         System.arraycopy(nums, 0, newNums, 0, size);\n         nums = newNums;\n       }\n     \n     \n       // 测试\n       public static void main(String[] args) {\n         Heap heap = new Heap(false);\n         heap.offer(11);\n         heap.offer(5);\n         heap.offer(3);\n         heap.offer(10);\n         heap.offer(1);\n         System.out.println(Arrays.toString(heap.nums));\n         System.out.println(heap.poll());\n         System.out.println(heap.poll());\n         System.out.println(heap.poll());\n         System.out.println(heap.poll());\n         System.out.println(heap.poll());\n         System.out.println(heap.poll());\n       }\n     \n     }\n     \n     \n   \n   * 扩展\n     \n     > 利用两个堆不仅可以快速求出中位数，还可以快速求其他百分位的数据，原理是类似的。\n     > \n     > 例如求99%响应时间\n     > \n     > 一个大顶堆，一个小顶堆。假设当前总数据的个数是 n，大顶堆中保存 n99% 个数据，小顶堆中保存 n1% 个数据。大顶堆堆顶的数据就是我们要找的 99% 响应时间。',normalizedContent:'# 堆基础\n\n * 堆概念\n   \n   >  1. 堆是一个完全二叉树\n   >  2. 堆中每一个结点的值都必须大于等于(或者小于等于)其子树中每个结点的值。\n   >     * 大于等于是 大堆顶\n   >     * 小于等于是小堆顶\n   >  3. 堆顶大的称为大堆顶，堆顶小的称为小堆顶\n\n * 堆的实现\n   \n   * 元素存储\n     \n     * 完全二叉树，借用数组存储。\n       \n       * ==下标与父子关系==\n         * 设数组中存储下标从1开始\n         * 数组中下标为$i$的节点的左子节点，就是下标为$i2$的节点，右子节点就是下标为$i2+1$的节点，父节点就是下标为 $i/2$ 的节点。\n         * 下标从$n/2$开始，不包括$n/2$,后面的都是叶子结点\n   \n   * 插入一个元素\n     \n     往堆中插入一个元素后，需要调整堆的结构，要让堆符合特性，这一过程称为堆化。\n     \n     * 自下而上的堆化\n       \n       例如，在数组最后插入元素22，然后调整堆的结构\n     \n     * 插入代码\n       \n       /**\n            * 堆中插入元素--自下而上的堆化--大堆顶\n            * 数组下标从1开始计数\n            * @param nums  数组\n            * @param count  数组中已有元素个数\n            * @param value  待插入的值\n            */\n       public void insert(int[] nums, int count, int value) {\n           if (count > nums.length) {\n               system.out.println("数组中元素已满，请扩容数组");\n               return;\n           }\n       \n           //数组最后插入元素\n           int height = count + 1;\n           nums[height] = value;\n           int parent;\n       \n           //自下而上开始堆化\n           while (height > 1) {\n               parent = height / 2;\n               if (nums[height] > nums[parent]) {\n                   swap(nums, height, parent);\n                   height = parent;\n               }else {\n                   break;\n               }\n           }\n       \n       }\n       \n   \n   * 删除堆顶元素\n     \n     将数组最后一个放到堆顶，然后利用同样的父子节点对比方法。对于不满足父子节点大小关系的，互换两个节点，并且重复进行这个过程，直到父子节点之间满足大小关系为止。这就是从上往下的堆化。\n     \n     * 删除代码\n       \n       /**\n            * 删除堆顶元素-- 自上而下的堆化--大堆顶\n            * @param nums 数组\n            * @param count 数组中元素个数\n            */\n       public void deleteheaptop(int[] nums, int count) {\n           if (count < 1) return;\n           //        将最后一个元素放到堆顶\n           nums[1] = nums[count];\n           nums[count] = 0;\n           heapify(nums, 1, count);\n       }\n       \n       \n       /**\n            * 自上而下的堆化\n            * @param nums 数组\n            * @param height 当前值的下标\n            */\n       public void heapify(int[] nums, int height, int count) {\n           while (true) {\n               int maxpos = height;\n               //当前值不光要与左子树比较还要与右子树比较\n               if (height * 2 <= count && nums[maxpos] < nums[height * 2]) maxpos = height * 2;\n               if (height * 2 + 1 <= count && nums[maxpos] < nums[height * 2 + 1]) maxpos = height * 2 + 1;\n       \n               if (maxpos == height) break;\n               swap(nums, height, maxpos);\n               height = maxpos;\n           }\n       }\n       \n\n * 堆排序\n   \n   * 建堆——原地排序\n     \n     不借助另外一个数组，在自己的数组基础上进行元素调整\n     \n     建堆时间复杂度为o(n)\n     \n     * 从上往下堆化----------自下而上的堆化\n       \n       > 尽管数组中包含 n 个数据，但是我们可以假设，起初堆中只包含一个数据，就是下标为 1 的数据。然后，我们调用前面讲的插入操作，将下标从 2 到 n 的数据依次插入到堆中。这样我们就将包含 n 个数据的数组，组织成了堆。\n     \n     * 从下往上堆化----------自上而下的堆化\n       \n       > 从后往前处理数组，并且每个数据都是从上往下堆化。直接从最后一个非叶子节点开始，依次堆化就行了。\n       \n       * 代码\n         \n         /**\n              * 建堆\n              * @param nums 数组\n              * @param count 数组内元素\n              */\n         public void bulidheap(int[] nums, int count) {\n             for (int i = count / 2; i >= 1; i--) {\n                 heapify(nums, i, count);\n             }\n         }\n         \n   \n   * 排序\n     \n     > 建堆结束之后，数组中的数据已经是按照大顶堆的特性来组织的。数组中的第一个元素就是堆顶，也就是最大的元素。我们把它跟最后一个元素交换，那最大元素就放到了下标为 n 的位置。 然后对剩余元素，继续堆化，不断的交换元素，直到堆中只剩下一个元素为止。\n     \n     排序过程时间复杂度o(nlogn)\n     \n     所以堆排序整体过程时间复杂度为o(nlogn)\n     \n     * 代码\n       \n       public void sort(int[] nums, int count) {\n           int num = count;\n           while (num > 1) {\n               //建堆\n               bulidheap(nums, num);\n               //取堆顶元素，放入最后\n               swap(nums, num, 1);\n               num --;\n           }\n       }\n       \n\n * 为什么快速排序要比堆排序性能好？\n   \n   * 堆排序数据访问的方式没有快速排序友好。\n     \n     > 堆排序时，访问元素是跳跃式访问，对cpu缓存不友好，\n     > \n     > 而快排是局部顺序访问。\n   \n   * 对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序。\n     \n     > 建堆的过程，会打乱原先的相对的先后顺序，导致有序度降低。\n     > \n     > 从而堆排序比快速排序交换次数多。\n\n\n# 优先级队列\n\njava-----priorityqueue\n\n> 一个堆就可以看作一个优先级队列。\n> \n> 往优先级队列中插入一个元素，就相当于往堆中插入一个元素,然后堆化；\n> \n> 从优先级队列中取出优先级最高的元素，就相当于取出堆顶元素,然后堆化。\n> \n> 队列内部无序，但是队首元素，一定是优先级最高的，每次取走队首元素，都会对剩余元素进行堆化\n\n * 代码\n   \n   public static void testpriorityqueue() {\n       //优先级队列(默认是小堆顶)\n       //队列内部无序，但是队首元素，一定是最小的，每次取走队首元素，都会对剩余元素进行堆化\n       //堆化方式是，将最后一个元素放入队首，自上而下的堆化\n       priorityqueue<integer> queue = new priorityqueue<>();\n       //插入的时候是按照完全二叉树的构造方式插入的\n       queue.add(5);\n       queue.add(3);\n       queue.add(1);\n       queue.add(2);\n       queue.add(4);\n       system.out.println(queue);//[1, 2, 3, 5, 4]\n   \n       queue.poll();\n       system.out.println(queue);//[2, 4, 3, 5]\n   \n       queue.poll();\n       system.out.println(queue);//[3, 4, 5]\n   }\n   \n\n * 合并有序小文件\n   \n   * 问题描述\n     \n     > 假设我们有 100 个小文件，每个文件的大小是 100mb，每个文件中存储的都是有序的字符串。我们希望将这些 100 个小文件合并成一个有序的大文件。\n   \n   * 解决办法\n     \n     > 我们取100个小文件的第一个元素，构建一个大小为100的小堆顶。那堆顶的元素，也就是优先级队列队首的元素，就是最小的字符串。\n     > \n     > 我们将这个字符串放入到大文件中，并将其从堆中删除。然后再从小文件中(取走的字符串所属的小文件)取出下一个字符串，放入到堆中。循环这个过程，就可以将 100 个小文件中的数据依次放入到大文件中。\n\n * 高性能定时器\n   \n   * 问题描述\n     \n     > 假设我们有一个定时器，定时器中维护了很多定时任务，每个任务都设定了一个要触发执行的时间点。定时器每过一个很小的单位时间（比如 1 秒），就扫描一遍任务，看是否有任务到达设定的执行时间。如果到达了，就拿出来执行。\n     > \n     > 但是，这样每过 1 秒就扫描一遍任务列表的做法比较低效。\n   \n   * 解决方法\n     \n     > 我们按照任务设定的执行时间，将这些任务存储在优先级队列中，队列首部（也就是小顶堆的堆顶）存储的是最先执行的任务。\n     > \n     > 这样，定时器就不需要每隔 1 秒就扫描一遍任务列表了。它拿队首任务的执行时间点，与当前时间点相减，得到一个时间间隔 t。这样，定时器就可以设定在 t 秒之后，再来执行任务。从当前时间点到（t-1）秒这段时间里，定时器都不需要做任何事情。\n     > \n     > 当 t 秒时间过去之后，定时器取优先级队列中队首的任务执行。**优先级队列取走队首元素后，会对剩余元素堆化，在次将最小的放在队首，保证每次从队首拿的都是最小的时间。**然后再计算新的队首任务的执行时间点与当前时间点的差值，把这个值作为定时器执行下一个任务需要等待的时间。\n\n\n# 求topk\n\n * 静态数据\n   \n   * 问题描述\n     \n     > 如何在一个包含 n 个数据的数组中，查找==前 k 大数据==呢？\n   \n   * 解决方法\n     \n     >  1. 遍历数组，先取前k个元素，构建==小堆顶==。堆顶元素是k个元素中最小的。\n     >  2. 从第k+1个元素，继续遍历数组，从数组中取出数据与堆顶元素比较。\n     >     * 如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中，然后堆化，构造小堆顶。\n     >       * 堆顶元素是最小的，也就是最危险的，只要来了比它大的，就淘汰它。\n     >     * 如果比堆顶元素小，则不做处理，继续遍历数组。\n     >       * 堆顶元素是最小的，你比我还小，怎么可能进的来？\n     >  3. 这样等数组中的数据都遍历完之后，堆中的数据就是前 k 大数据了。\n     > \n     > 时间复杂度：\n     > \n     > 遍历数组需要 o(n) 的时间复杂度，一次堆化操作需要 o(logk) 的时间复杂度，所以最坏情况下，n 个元素都入堆一次，时间复杂度就是 o(nlogk)。\n   \n   * 代码\n     \n     public static void  topk() {\n         int[] nums = {3,2,4,7,9,0,54,1,435,245,344};\n         int[] top = new int[5 + 1];\n         //建堆\n         int index;\n         for (index = 0; index < 5; index ++) {\n             top[index + 1] = nums[index];\n         }\n         heap.bulidheap(top,5, 0);\n         //遍历剩余数组元素，维护堆\n         for (;index < nums.length; index ++) {\n             if (nums[index] > top[1]) {\n                 top[1] = nums[index];\n                 heap.bulidheap(top,5, 0);\n             }\n         }\n         system.out.println(arrays.tostring(top));\n     \n     }\n     \n\n * 动态数据\n   \n   * 问题描述\n     \n     > 针对动态数据求得 top k 就是实时 top k。一个数据集合中有两个操作，一个是添加数据，另一个询问当前的前 k 大数据。\n     > \n     > 如果每次询问前 k 大数据，我们都基于当前的数据重新计算的话，那时间复杂度就是 o(nlogk)，n 表示当前的数据的大小。\n   \n   * 解决方法\n     \n     > 我们可以一直都维护一个 k 大小的小顶堆，**当有数据被添加到集合中时，我们就拿它与堆顶的元素对比。**如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理。这样，无论任何时候需要查询当前的前 k 大数据，我们都可以立刻返回给他。\n\n\n# 求中位数\n\n * 问题描述\n   \n   > 对于一组静态数据，中位数是固定的，我们可以先排序，第 2n 个数据就是中位数。每次询问中位数的时候，我们直接返回这个固定的值就好了。\n   > \n   > 但是，如果我们面对的是动态数据集合，中位数在不停地变动，如果再用先排序的方法，每次询问中位数的时候，都要先进行排序，那效率就不高了。\n   > \n   > 借助堆这种数据结构，我们不用排序，就可以非常高效地实现求中位数操作。\n\n * 解决方法\n   \n   * 初始化堆\n     \n     > 我们需要维护两个堆，一个大顶堆，一个小顶堆。\n     > \n     > 大顶堆中存储前半部分数据，小顶堆中存储后半部分数据，且小顶堆中的数据都大于大顶堆中的数据。大堆顶元素即为中位数。\n     > \n     > 如果 n 是奇数，情况是类似的，大顶堆就存储 n/2+1 个数据，小顶堆中就存储 n/2 个数据。\n   \n   * 动态变换\n     \n     > 如果新加入的数据小于等于大顶堆的堆顶元素，我们就将这个新数据插入到大顶堆；否则，我们就将这个新数据插入到小顶堆。\n     > \n     > 这个时候就有可能出现，两个堆中的数据个数不符合前面约定的情况，我们可以从一个堆中不停地将堆顶元素移动到另一个堆，通过这样的调整，来让两个堆中的数据满足上面的约定。\n   \n   * 代码\n     \n     对确定数组求中位数\n     \n     public static void getmid() {\n         int[] nums = {3,2,4,7,9,10,54,1,435,245,344};\n         int count = nums.length;\n         int bigcount = 11 / 2 + 1;\n         int littlecount = 11 / 2;\n         int[] bignum = new int[bigcount + 1];//存储大堆顶\n         int[] littlenum = new int[littlecount + 1];//存储小堆顶\n         int i;\n         arrays.sort(nums);\n         for (i = 1; i < bigcount + 1; i++) {\n             bignum[i] = nums[i - 1];\n         }\n         //构建大堆顶\n         heap.bulidheap(bignum, bigcount, 1);\n     \n         for (;i < nums.length + 1; i++) {\n             littlenum[i - bigcount] = nums[i - 1];\n         }\n         //构建小堆顶\n         heap.bulidheap(littlenum, littlecount, 0);\n     \n         //中位数即，大堆顶的顶部元素\n         system.out.println(bignum[1]);\n     \n     }\n     \n   \n   * 对流动数组求中位数，以及堆的实现\n     \n     > 自定义堆的实现，使用一个变量控制大顶堆还是小顶堆 使用动态数组扩容\n     \n     // 对数据流求中位数\n     class medianfinder {\n         heap minqueue;\n         heap maxqueue;\n     \n         public medianfinder() {\n             // 默认是小堆顶\n             minqueue = new heap(false);\n             maxqueue = new heap(true);\n     \n         }\n         // 大顶堆中的元素要小于小顶堆\n         public void addnum(int num) {\n             // 当两个堆中的元素个数不等时，往大顶堆中添加元素；当相等时，往小顶堆中添加元素。\n             // 在往大顶堆添加元素时，先将其放入小顶堆，然后取小堆顶堆顶元素放入大顶堆\n             // 同理，当往小顶堆添加元素时，先将其放入大顶堆，然后取大顶堆堆顶元素，放入小顶堆\n     \n             // 尝试放入大顶堆\n             if (minqueue.size == maxqueue.size) {\n                 if (!minqueue.isempty() && num > minqueue.peek()) {\n                     minqueue.offer(num);\n                     maxqueue.offer(minqueue.poll());\n                 } else {\n                     maxqueue.offer(num);\n                 }\n                 return;\n             }\n     \n             // 尝试放入小顶堆\n             if (num < maxqueue.peek()) {\n                 maxqueue.offer(num);\n                 minqueue.offer(maxqueue.poll());\n             } else {\n                 minqueue.offer(num);\n             }     \n     \n         }\n         \n         public double findmedian() {\n             if (maxqueue.size == minqueue.size) {\n                 return (maxqueue.peek() + minqueue.peek()) / 2.0;\n             }\n             // return (double)maxqueue.peek();\n             return maxqueue.peek();\n     \n         }\n     }\n     \n     \n     // 建堆\n     \n     /**\n      * @classname heap\n      * @date 2023/6/21 15:28\n      * @author diane\n      * @description 手动建立堆\n      * @version 1.0\n      */\n     // 自定义堆实现\n     // 上浮，下潜，动态数组\n     class heap {\n       int[] nums;\n       int size;\n       // true 表示大顶堆；false 表示小顶堆\n       boolean max;\n       public heap(boolean maxbool) {\n         size = 0;\n         max = maxbool;\n         nums = new int[3];\n       }\n       // 添加元素\n       public void offer(int num) {\n         // 数组满了，扩容\n         if (size == nums.length) {\n           grow();\n         }\n         // 上浮，num,使之满足堆的特性\n         int child = size;\n         while (child > 0) {\n           int parent = (child - 1) / 2;\n           if (max ? nums[parent] < num : nums[parent] > num) {\n             nums[child] = nums[parent];\n           } else {\n             break;\n           }\n           // 更新孩子指针\n           child = parent;\n         }\n         nums[child] = num;\n         size++;\n       }\n       // 移除元素\n       public int poll() {\n         // 不正常移除\n         if (size == 0) {\n           return -1;\n         }\n         // 交换堆顶元素和最末尾元素，然后移除最末尾元素，最后重新堆化-下潜\n         int temp = nums[0];\n         swap(0, size - 1);\n         size--;\n         down(0);\n         return temp;\n       }\n       // 元素下潜\n       public void down(int p) {\n         int left = 2 * p + 1;\n         int right = left + 1;\n         int maxormin = p;\n         if (left < size && (max ? (nums[left] > nums[maxormin]) : (nums[left] < nums[maxormin]))) {\n           maxormin = left;\n         }\n         if (right < size && (max ? (nums[right] > nums[maxormin]) : (nums[right] < nums[maxormin]))) {\n           maxormin = right;\n         }\n         // 如果最大或者最小不是原父指针，则递归调用\n         if (maxormin != p) {\n           swap(maxormin, p);\n           down(maxormin);\n         }\n       }\n       // 返回堆顶远古三\n       public int peek() {\n         return nums[0];\n       }\n       // 判空\n       public boolean isempty() {\n         return size == 0;\n       }\n       // 交换元素\n       public void swap(int i, int j) {\n         int temp = nums[i];\n         nums[i] = nums[j];\n         nums[j] = temp;\n       }\n       // 数组扩容\n       public void grow() {\n         // 1.5倍扩容\n         int cap = size + (size >> 1);\n         int[] newnums = new int[cap];\n         system.arraycopy(nums, 0, newnums, 0, size);\n         nums = newnums;\n       }\n     \n     \n       // 测试\n       public static void main(string[] args) {\n         heap heap = new heap(false);\n         heap.offer(11);\n         heap.offer(5);\n         heap.offer(3);\n         heap.offer(10);\n         heap.offer(1);\n         system.out.println(arrays.tostring(heap.nums));\n         system.out.println(heap.poll());\n         system.out.println(heap.poll());\n         system.out.println(heap.poll());\n         system.out.println(heap.poll());\n         system.out.println(heap.poll());\n         system.out.println(heap.poll());\n       }\n     \n     }\n     \n     \n   \n   * 扩展\n     \n     > 利用两个堆不仅可以快速求出中位数，还可以快速求其他百分位的数据，原理是类似的。\n     > \n     > 例如求99%响应时间\n     > \n     > 一个大顶堆，一个小顶堆。假设当前总数据的个数是 n，大顶堆中保存 n99% 个数据，小顶堆中保存 n1% 个数据。大顶堆堆顶的数据就是我们要找的 99% 响应时间。',charsets:{cjk:!0}},{title:"栈",frontmatter:{autoSort:96,title:"栈",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/bfea35/",categories:["算法","栈"],tags:["知识","算法"]},regularPath:"/02.%E7%AE%97%E6%B3%95/20.%E6%A0%88/05.%E6%A0%88.html",relativePath:"02.算法/20.栈/05.栈.md",key:"v-6becdf80",path:"/pages/bfea35/",headers:[{level:2,title:"栈基础",slug:"栈基础",normalizedTitle:"栈基础",charIndex:2},{level:2,title:"栈在函数调用中的应用",slug:"栈在函数调用中的应用",normalizedTitle:"栈在函数调用中的应用",charIndex:284},{level:2,title:"栈在表达式求值中的应用",slug:"栈在表达式求值中的应用",normalizedTitle:"栈在表达式求值中的应用",charIndex:907},{level:2,title:"栈在括号匹配中的应用",slug:"栈在括号匹配中的应用",normalizedTitle:"栈在括号匹配中的应用",charIndex:1200},{level:2,title:"栈：如何实现浏览器的前进和后退功能？",slug:"栈-如何实现浏览器的前进和后退功能",normalizedTitle:"栈：如何实现浏览器的前进和后退功能？",charIndex:1517},{level:2,title:"JVM内存中的栈",slug:"jvm内存中的栈",normalizedTitle:"jvm内存中的栈",charIndex:1834}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"栈基础 栈在函数调用中的应用 栈在表达式求值中的应用 栈在括号匹配中的应用 栈：如何实现浏览器的前进和后退功能？ JVM内存中的栈",content:'# 栈基础\n\n * 理解栈\n   \n   * 栈是一种“操作受限”的线性表，只允许在一端插入和删除数据。\n   * 当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，这时我们就应该首选“栈”这种数据结构\n\n * 实现栈\n   \n   * 顺序栈——用数组实现的栈\n   * 链式栈——用链表实现的栈\n   * 复杂度\n     * 不管是顺序栈还是链式栈，入栈、出栈只涉及栈顶个别数据的操作，所以时间复杂度都是 ==O(1)==。\n     * 在入栈和出栈过程中，只需要一两个临时变量存储空间，所以空间复杂度是 ==O(1)==。\n\n\n# 栈在函数调用中的应用\n\n> 操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成**“栈”**这种结构, 用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。\n\n * 为什么函数调用要用栈来保存临时变量呢？\n   \n   > 其实，我们不一定非要用栈来保存临时变量，只不过如果这个函数调用符合后进先出的特性，用栈这种数据结构来实现，是最顺理成章的选择。\n   > \n   > 从调用函数进入被调用函数，对于数据来说，变化的是什么呢？是作用域。所以根本上，只要能保证每进入一个新的函数，都是一个新的作用域就可以。而要实现这个，用栈就非常方便。\n   > \n   > 在进入被调用函数的时候，分配一段栈空间给这个函数的变量，在函数结束的时候，将栈顶复位，正好回到调用函数的作用域内。\n\n * 程序调用add函数时的栈内 情况\n\nint main() {\n   int a = 1; \n   int ret = 0;\n   int res = 0;\n   ret = add(3, 5);\n   res = a + ret;\n   printf("%d", res);\n   reuturn 0;\n}\n\nint add(int x, int y) {\n   int sum = 0;\n   sum = x + y;\n   return sum;\n}\n\n\n\n# 栈在表达式求值中的应用\n\n3+5*8-6\n\n * 实现流程\n   \n   > 编译器就是通过两个栈来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。\n   > \n   >  1. 我们从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；\n   >  2. 当遇到运算符，就与运算符栈的栈顶元素进行比较。\n   >  3. 如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；\n   >  4. 如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。\n\n * 图示\n\n\n# 栈在括号匹配中的应用\n\n * 背景\n   \n   > 我们假设表达式中只包含三种括号，圆括号 ()、方括号[]和花括号{}，并且它们可以任意嵌套。那我现在给你一个包含三种括号的表达式字符串，如何检查它是否合法呢？\n\n * 实现流程\n   \n   > 1. 我们用栈来保存未匹配的左括号，从左到右依次扫描字符串。\n   > 1. 当扫描到左括号时，则将其压入栈中；\n   > 1. 当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。\n   > 1. **如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。**\n\n\n# 栈：如何实现浏览器的前进和后退功能？\n\n * 实现流程\n   \n   >  1. 我们使用两个栈，X 和 Y，我们把首次浏览的页面依次压入栈 X，当点击后退按钮时，再依次从栈 X 中出栈，并将出栈的数据依次放入栈 Y。\n   > \n   >  2. 当我们点击前进按钮时，我们依次从栈 Y 中取出数据，放入栈 X 中。\n   > \n   >  3. 当栈 X 中没有数据时，那就说明没有页面可以继续后退浏览了。当栈 Y 中没有数据，那就说明没有页面可以点击前进按钮浏览了。\n   > \n   >  4. 当栈Y中有数据c，而此时又访问了新的网页，X中入栈了新成员，则页面c无法在通过前进、后退按钮重复查看了，此时要情空栈Y。\n\n\n# JVM内存中的栈\n\n * 问题\n   \n   > 我们都知道，JVM 内存管理中有个“堆栈”的概念。==栈内存用来存储局部变量和方法调用，堆内存用来存储 Java 中的对象。==\n   > \n   > 那 JVM 里面的“栈”跟我们这里说的“栈”是不是一回事呢？如果不是，那它为什么又叫作“栈”呢？\n\n * 解答\n   \n   **内存中的堆栈和数据结构堆栈不是一个概念，可以说内存中的堆栈是真实存在的物理区，数据结构中的堆栈是抽象的数据存储结构。 **\n\n> 内存空间在逻辑上分为三部分：代码区、静态数据区和动态数据区，动态数据区又分为栈区和堆区。\n> \n> 代码区：存储方法体的二进制代码。高级调度（作业调度）、中级调度（内存调度）、低级调度（进程调度）控制代码区执行代码的切换。\n> \n> 静态数据区：存储全局变量、静态变量、常量，常量包括final修饰的常量和String常量。系统自动分配和回收。\n> \n> 栈区：存储运行方法的形参、局部变量、返回值。由系统自动分配和回收。\n> \n> 堆区：new一个对象的引用或地址存储在栈区，指向该对象存储在堆区中的真实数据。\n\n它们都有“栈”的特性——后进先出，所以都叫“栈”也无可厚非。可以把内存中的栈当中数据结构栈的一种实现',normalizedContent:'# 栈基础\n\n * 理解栈\n   \n   * 栈是一种“操作受限”的线性表，只允许在一端插入和删除数据。\n   * 当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，这时我们就应该首选“栈”这种数据结构\n\n * 实现栈\n   \n   * 顺序栈——用数组实现的栈\n   * 链式栈——用链表实现的栈\n   * 复杂度\n     * 不管是顺序栈还是链式栈，入栈、出栈只涉及栈顶个别数据的操作，所以时间复杂度都是 ==o(1)==。\n     * 在入栈和出栈过程中，只需要一两个临时变量存储空间，所以空间复杂度是 ==o(1)==。\n\n\n# 栈在函数调用中的应用\n\n> 操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成**“栈”**这种结构, 用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。\n\n * 为什么函数调用要用栈来保存临时变量呢？\n   \n   > 其实，我们不一定非要用栈来保存临时变量，只不过如果这个函数调用符合后进先出的特性，用栈这种数据结构来实现，是最顺理成章的选择。\n   > \n   > 从调用函数进入被调用函数，对于数据来说，变化的是什么呢？是作用域。所以根本上，只要能保证每进入一个新的函数，都是一个新的作用域就可以。而要实现这个，用栈就非常方便。\n   > \n   > 在进入被调用函数的时候，分配一段栈空间给这个函数的变量，在函数结束的时候，将栈顶复位，正好回到调用函数的作用域内。\n\n * 程序调用add函数时的栈内 情况\n\nint main() {\n   int a = 1; \n   int ret = 0;\n   int res = 0;\n   ret = add(3, 5);\n   res = a + ret;\n   printf("%d", res);\n   reuturn 0;\n}\n\nint add(int x, int y) {\n   int sum = 0;\n   sum = x + y;\n   return sum;\n}\n\n\n\n# 栈在表达式求值中的应用\n\n3+5*8-6\n\n * 实现流程\n   \n   > 编译器就是通过两个栈来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。\n   > \n   >  1. 我们从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；\n   >  2. 当遇到运算符，就与运算符栈的栈顶元素进行比较。\n   >  3. 如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；\n   >  4. 如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。\n\n * 图示\n\n\n# 栈在括号匹配中的应用\n\n * 背景\n   \n   > 我们假设表达式中只包含三种括号，圆括号 ()、方括号[]和花括号{}，并且它们可以任意嵌套。那我现在给你一个包含三种括号的表达式字符串，如何检查它是否合法呢？\n\n * 实现流程\n   \n   > 1. 我们用栈来保存未匹配的左括号，从左到右依次扫描字符串。\n   > 1. 当扫描到左括号时，则将其压入栈中；\n   > 1. 当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。\n   > 1. **如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。**\n\n\n# 栈：如何实现浏览器的前进和后退功能？\n\n * 实现流程\n   \n   >  1. 我们使用两个栈，x 和 y，我们把首次浏览的页面依次压入栈 x，当点击后退按钮时，再依次从栈 x 中出栈，并将出栈的数据依次放入栈 y。\n   > \n   >  2. 当我们点击前进按钮时，我们依次从栈 y 中取出数据，放入栈 x 中。\n   > \n   >  3. 当栈 x 中没有数据时，那就说明没有页面可以继续后退浏览了。当栈 y 中没有数据，那就说明没有页面可以点击前进按钮浏览了。\n   > \n   >  4. 当栈y中有数据c，而此时又访问了新的网页，x中入栈了新成员，则页面c无法在通过前进、后退按钮重复查看了，此时要情空栈y。\n\n\n# jvm内存中的栈\n\n * 问题\n   \n   > 我们都知道，jvm 内存管理中有个“堆栈”的概念。==栈内存用来存储局部变量和方法调用，堆内存用来存储 java 中的对象。==\n   > \n   > 那 jvm 里面的“栈”跟我们这里说的“栈”是不是一回事呢？如果不是，那它为什么又叫作“栈”呢？\n\n * 解答\n   \n   **内存中的堆栈和数据结构堆栈不是一个概念，可以说内存中的堆栈是真实存在的物理区，数据结构中的堆栈是抽象的数据存储结构。 **\n\n> 内存空间在逻辑上分为三部分：代码区、静态数据区和动态数据区，动态数据区又分为栈区和堆区。\n> \n> 代码区：存储方法体的二进制代码。高级调度（作业调度）、中级调度（内存调度）、低级调度（进程调度）控制代码区执行代码的切换。\n> \n> 静态数据区：存储全局变量、静态变量、常量，常量包括final修饰的常量和string常量。系统自动分配和回收。\n> \n> 栈区：存储运行方法的形参、局部变量、返回值。由系统自动分配和回收。\n> \n> 堆区：new一个对象的引用或地址存储在栈区，指向该对象存储在堆区中的真实数据。\n\n它们都有“栈”的特性——后进先出，所以都叫“栈”也无可厚非。可以把内存中的栈当中数据结构栈的一种实现',charsets:{cjk:!0}},{title:"队列",frontmatter:{autoSort:95,title:"队列",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/7b2a41/",categories:["算法","队列"],tags:["知识","算法"]},regularPath:"/02.%E7%AE%97%E6%B3%95/25.%E9%98%9F%E5%88%97/05.%E9%98%9F%E5%88%97.html",relativePath:"02.算法/25.队列/05.队列.md",key:"v-baa1c044",path:"/pages/7b2a41/",headers:[{level:2,title:"队列基础",slug:"队列基础",normalizedTitle:"队列基础",charIndex:2},{level:2,title:"顺序队列",slug:"顺序队列",normalizedTitle:"顺序队列",charIndex:250},{level:2,title:"循环队列",slug:"循环队列",normalizedTitle:"循环队列",charIndex:99},{level:2,title:"链式队列",slug:"链式队列",normalizedTitle:"链式队列",charIndex:811},{level:2,title:"阻塞队列和并发队列",slug:"阻塞队列和并发队列",normalizedTitle:"阻塞队列和并发队列",charIndex:912},{level:2,title:"队列在线程池等有限资源池中的应用",slug:"队列在线程池等有限资源池中的应用",normalizedTitle:"队列在线程池等有限资源池中的应用",charIndex:1441}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"队列基础 顺序队列 循环队列 链式队列 阻塞队列和并发队列 队列在线程池等有限资源池中的应用",content:"# 队列基础\n\n * 队列\n   \n   队列跟栈一样，也是一种操作受限的线性表数据结构。\n   \n   > 作为一种非常基础的数据结构，队列的应用也非常广泛，特别是一些具有某些额外特性的队列，比如循环队列、阻塞队列、并发队列。\n   > \n   > 它们在很多偏底层系统、框架、中间件的开发中，起着关键性的作用。比如高性能队列 Disruptor、Linux 环形缓存，都用到了循环并发队列；Java concurrent 并发包利用 ArrayBlockingQueue 来实现公平锁等\n\n\n# 顺序队列\n\n用数组实现的队列叫作顺序队列\n\n * 指针\n   \n   * 队头指针——出队，右移，加一\n   * 队尾指针——入队，右移，加一\n   * 队列满与空\n   >  1. 当head==tail时队列空。\n   >  2. 当tail位于数组末尾时，没有空间在提供给入队的元素\n   >     * 若head位于最前面，则是真正的队满\n   >     * 若head位于中间位置，则是虚假的队满，称为假溢出\n\n * 数据搬移——解决假溢出现象\n   \n   随着不停地进行入队、出队操作，head 和 tail 都会持续往后移动。当 tail 移动到最右边，即使数组中还有空闲空间，也无法继续往队列中添加数据了。\n   \n   * 解决办法\n   \n   如果没有空闲空间了，我们只需要在入队时，再集中触发一次数据的搬移操作\n\n\n# 循环队列\n\n循环队列可以避免数据搬移\n\n * 队满判断--size 为数组大小\n   \n   ==$(tail+1)%size=head$==\n\n * 队空判断\n   \n   ==$tail=head$==\n\n * 队头，队尾指针增加\n   \n   ==$tail=(tail+1)%size$==\n   \n   ==$head=(head+1)%size$==\n\n\n# 链式队列\n\n * 特征\n   \n   >  1. 空间无限大，不受约束。\n   >  2. 只允许在队尾，即尾指针处插入(入队)，即尾插法。\n   >  3. 只允许在队头，即头指针处，出队。\n\n\n# 阻塞队列和并发队列\n\n * 阻塞队列\n   \n   阻塞队列其实就是在队列基础上增加了阻塞操作\n   \n   > 就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；\n   > \n   > 如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。\n\n * 并发队列\n   \n   线程安全的队列我们叫作并发队列\n   \n   * 实现方式1——加锁\n     \n     > 最简单直接的实现方式是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。\n   \n   * 实现方式2——CAS原子操作\n     \n     > 实际上，基于数组的循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。\n     > \n     > 考虑使用CAS实现无锁队列，则在入队前，获取tail位置，入队时比较tail是否发生变化，如果否(==这说明刚刚没有其他进程实现了入队操作==)，则允许入队，反之，本次入队失败。出队则是获取head位置，进行cas。\n\n\n# 队列在线程池等有限资源池中的应用\n\n实际上，对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。\n\n * 问题\n   \n   > 当我们向固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是怎么实现的呢？\n\n * 解答\n   \n   * 请求处理\n     \n     > 第一种是非阻塞的处理方式，直接拒绝任务请求；\n     > \n     > 另一种是阻塞的处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理。\n   \n   * 处理排队请求\n     \n     我们希望公平地处理每个排队的请求，先进者先服务，所以==队列==这种数据结构很适合来存储排队请求。\n     \n     >  1. 基于链表的实现方式，可以实现一个支持无限排队的无界队列（unbounded queue），但是可能会导致过多的请求排队等待，请求处理的响应时间过长。所以，针对响应时间比较敏感的系统，基于链表实现的无限排队的线程池是不合适的。\n     >  2. 而基于数组实现的有界队列（bounded queue），队列的大小有限，所以线程池中排队的请求超过队列大小时，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理。",normalizedContent:"# 队列基础\n\n * 队列\n   \n   队列跟栈一样，也是一种操作受限的线性表数据结构。\n   \n   > 作为一种非常基础的数据结构，队列的应用也非常广泛，特别是一些具有某些额外特性的队列，比如循环队列、阻塞队列、并发队列。\n   > \n   > 它们在很多偏底层系统、框架、中间件的开发中，起着关键性的作用。比如高性能队列 disruptor、linux 环形缓存，都用到了循环并发队列；java concurrent 并发包利用 arrayblockingqueue 来实现公平锁等\n\n\n# 顺序队列\n\n用数组实现的队列叫作顺序队列\n\n * 指针\n   \n   * 队头指针——出队，右移，加一\n   * 队尾指针——入队，右移，加一\n   * 队列满与空\n   >  1. 当head==tail时队列空。\n   >  2. 当tail位于数组末尾时，没有空间在提供给入队的元素\n   >     * 若head位于最前面，则是真正的队满\n   >     * 若head位于中间位置，则是虚假的队满，称为假溢出\n\n * 数据搬移——解决假溢出现象\n   \n   随着不停地进行入队、出队操作，head 和 tail 都会持续往后移动。当 tail 移动到最右边，即使数组中还有空闲空间，也无法继续往队列中添加数据了。\n   \n   * 解决办法\n   \n   如果没有空闲空间了，我们只需要在入队时，再集中触发一次数据的搬移操作\n\n\n# 循环队列\n\n循环队列可以避免数据搬移\n\n * 队满判断--size 为数组大小\n   \n   ==$(tail+1)%size=head$==\n\n * 队空判断\n   \n   ==$tail=head$==\n\n * 队头，队尾指针增加\n   \n   ==$tail=(tail+1)%size$==\n   \n   ==$head=(head+1)%size$==\n\n\n# 链式队列\n\n * 特征\n   \n   >  1. 空间无限大，不受约束。\n   >  2. 只允许在队尾，即尾指针处插入(入队)，即尾插法。\n   >  3. 只允许在队头，即头指针处，出队。\n\n\n# 阻塞队列和并发队列\n\n * 阻塞队列\n   \n   阻塞队列其实就是在队列基础上增加了阻塞操作\n   \n   > 就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；\n   > \n   > 如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。\n\n * 并发队列\n   \n   线程安全的队列我们叫作并发队列\n   \n   * 实现方式1——加锁\n     \n     > 最简单直接的实现方式是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。\n   \n   * 实现方式2——cas原子操作\n     \n     > 实际上，基于数组的循环队列，利用 cas 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。\n     > \n     > 考虑使用cas实现无锁队列，则在入队前，获取tail位置，入队时比较tail是否发生变化，如果否(==这说明刚刚没有其他进程实现了入队操作==)，则允许入队，反之，本次入队失败。出队则是获取head位置，进行cas。\n\n\n# 队列在线程池等有限资源池中的应用\n\n实际上，对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。\n\n * 问题\n   \n   > 当我们向固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是怎么实现的呢？\n\n * 解答\n   \n   * 请求处理\n     \n     > 第一种是非阻塞的处理方式，直接拒绝任务请求；\n     > \n     > 另一种是阻塞的处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理。\n   \n   * 处理排队请求\n     \n     我们希望公平地处理每个排队的请求，先进者先服务，所以==队列==这种数据结构很适合来存储排队请求。\n     \n     >  1. 基于链表的实现方式，可以实现一个支持无限排队的无界队列（unbounded queue），但是可能会导致过多的请求排队等待，请求处理的响应时间过长。所以，针对响应时间比较敏感的系统，基于链表实现的无限排队的线程池是不合适的。\n     >  2. 而基于数组实现的有界队列（bounded queue），队列的大小有限，所以线程池中排队的请求超过队列大小时，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理。",charsets:{cjk:!0}},{title:"散列表",frontmatter:{autoSort:90,title:"散列表",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/6215d9/",categories:["算法","散列表"],tags:["知识","算法"]},regularPath:"/02.%E7%AE%97%E6%B3%95/35.%E6%95%A3%E5%88%97%E8%A1%A8/05.%E6%95%A3%E5%88%97%E8%A1%A8.html",relativePath:"02.算法/35.散列表/05.散列表.md",key:"v-6b5af5a0",path:"/pages/6215d9/",headers:[{level:2,title:"散列表基础",slug:"散列表基础",normalizedTitle:"散列表基础",charIndex:2},{level:2,title:"设计工业级散列表",slug:"设计工业级散列表",normalizedTitle:"设计工业级散列表",charIndex:851},{level:2,title:"Java HashMap 分析",slug:"java-hashmap-分析",normalizedTitle:"java hashmap 分析",charIndex:3172},{level:2,title:"散列表和链表的结合",slug:"散列表和链表的结合",normalizedTitle:"散列表和链表的结合",charIndex:6170}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"散列表基础 设计工业级散列表 Java HashMap 分析 散列表和链表的结合",content:"# 散列表基础\n\n散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。\n\n * 散列函数（具体 参照散列表基础 文档）\n   \n   * 设计原则\n     * 计算简单\n     * 散列地址分布均匀\n   * 直接定址法\n   * 数字分析法\n   * 平方取中法\n   * 折叠法\n   * 随机数法\n   * 除留余数法(最常用) ——————HashMap、LinkedHashMap在计算哈希的时候就是用的这个方法\n\n * 散列冲突（具体 参照散列表基础 文档）\n   \n   * 开放定址法(常用)\n   * 二次探测法\n   * 再散列函数法\n   * 公共溢出区法\n   * 链地址法——(常用)\n     * 将所有为同义词（发生冲突的元素）的记录存储在一个单链表中。\n     * ==这里也可以用 双链表、红黑树这种结构==\n\n * 装载因子\n   \n   * 散列表的装载因子=填入表中的元素个数/散列表的长度\n   * 装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。\n\n * 问题思考\n   \n   * Word 文档中单词拼写检查功能是如何实现的？\n     \n     > 常用的英文单词有 20 万个左右，假设单词的平均长度是 10 个字母，平均一个单词占用 10 个字节的内存空间，那 20 万英文单词大约占 2MB 的存储空间，就算放大 10 倍也就是 20MB。\n     > \n     > 对于现在的计算机来说，这个大小完全可以放在内存里面。所以我们可以用散列表来存储整个英文单词词典。\n     > \n     > 当用户输入某个英文单词时，我们拿用户输入的单词去散列表中查找。如果查到，则说明拼写正确；如果没有查到，则说明拼写可能有误，给予提示。\n     > \n     > 借助散列表这种数据结构，我们就可以轻松实现快速判断是否存在拼写错误。\n\n\n# 设计工业级散列表\n\n * 如何设计散列函数\n   \n   * 散列函数的设计不能太复杂\n   * 散列函数生成的值要尽可能随机并且均匀分布\n\n * 装载因子过大怎么办\n   \n   动态扩容\n   \n   > 假设原散列函数是用的除数留余法；\n   > \n   > 当扩容后，原散列表中的元素的位置会发生改变，需要将小散列表中的元素，按计算放到大散列表中。\n   \n   * ==法1==——申请了大数组后，将原来的元素全部按散列结果搬移到大数组中。\n     \n     * 均摊时间复杂度——O(1)\n       \n       > 插入一个数据，最好情况下，不需要扩容，最好时间复杂度是 O(1)。\n       > \n       > 最坏情况下，散列表装载因子过高，启动扩容，我们需要重新申请内存空间，重新计算哈希位置，并且搬移数据，所以时间复杂度是 O(n)。\n       > \n       > 用摊还分析法，均摊情况下，时间复杂度接近最好情况，就是 O(1)。\n\n * 如何避免低效扩容\n   \n   * 法1的做法，太低效了，当数据过多时，十分的耗时。\n     \n     > 如果我们的业务代码直接服务于用户，尽管大部分情况下，插入一个数据的操作都很快，但是，极个别非常慢的插入操作，也会让用户崩溃。这个时候，“一次性”扩容的机制就不合适了。\n   \n   * ==法2==——我们可以将扩容操作穿插在插入操作的过程中，分批完成\n     \n     > 当装载因子触达阈值之后，我们只申请新空间，但并不将老的数据搬移到新散列表中。\n     > \n     > 当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。每次插入一个数据到散列表，我们都重复上面的过程。\n     > \n     > 经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。\n   \n   * 混合散列表如何查询\n     \n     > 对于查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找。\n\n * 如何选择冲突解决方法\n   \n   * 开放定址法——ThreadLocalMap\n     \n     * 优势\n       \n       > 散列表中的数据都存储在数组中，可以有效地利用 CPU 缓存加快查询速度。而且，这种方法实现的散列表，==序列化==起来比较简单。\n     \n     * 劣势\n       \n       > 用开放寻址法解决冲突的散列表，删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据。\n       > \n       > 而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。\n       > \n       > 所以，使用开放寻址法解决冲突的散列表，装载因子的上限不能太大。这也导致这种方法比链表法更浪费内存空间。\n     \n     * 适用场景\n       \n       > 当数据量比较小、装载因子小的时候，适合采用开放寻址法。\n   \n   * 链地址法——LinkedHashMap\n     \n     * 优势\n       \n       >  1. 链表法对内存的利用率比开放寻址法要高。因为链表结点可以在需要的时候再创建，并不需要像开放寻址法那样事先申请好。\n       >  2. **链表法比起开放寻址法，对大装载因子的容忍度更高。**即便装载因子变成 10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多。\n     \n     * 劣势\n       \n       > 1. 链表因为要存储指针，所以对于比较小的对象的存储，是比较消耗内存的，还有可能会让内存的消耗翻倍。(当然，如果我们存储的是大对象，也就是说要存储的对象的大小远远大于一个指针的大小（4 个字节或者 8 个字节），那链表中指针的内存消耗在大对象面前就可以忽略了。)\n       > \n       > 2. 因为链表中的结点是零散分布在内存中的，不是连续的，所以**对 CPU 缓存是不友好**的，这方面对于执行效率也有一定的影响。\n     \n     * 改进\n       \n       > 我们将链表法中的链表改造为其他高效的动态数据结构，比如双链表、跳表、红黑树。这样，即便出现散列冲突，极端情况下，所有的数据都散列到同一个桶内，那最终退化成的散列表的查找时间也只不过是 O(logn)。这样也就有效避免了前面讲到的散列碰撞攻击。\n     \n     * 适用场景\n       \n       > 基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。\n\n * 如果设计一个工业级散列函数\n   \n   * 特性\n     * 支持快速地查询、插入、删除操作；\n     * 内存占用合理，不能浪费过多的内存空间；\n     * 性能稳定，极端情况下，散列表的性能也不会退化到无法接受的情况。\n   * 如何实现——具体业务，具体数据，具体分析\n     * 设计一个合适的散列函数；\n     * 定义装载因子阈值，并且设计动态扩容策略；\n     * 选择合适的散列冲突解决方法\n   * ==没有最好的方法，只有最合适的方法==\n\n\n# Java HashMap 分析\n\n * 初始大小\n   \n   * HashMap 默认的初始大小是 16\n     \n     > 这个默认值是可以设置的，如果事先知道大概的数据量有多大，可以通过修改默认初始大小，减少动态扩容的次数，这样会大大提高 HashMap 的性能。\n   \n   * ==大小设置为 2^n^==,如果你传入的数不是2^n^,在构造函数中，会调整到大于等于它的2^n^\n     \n     //假设传入5=00000101\n     \n     static final int tableSizeFor(int cap) {\n         //这里减一是为了防止传入的就是2^n,处理结果会变成2^(n+1)\n         int n = cap - 1;\n         // >>> 是无符号右移的意思\n         //一直右移，直到n右移过后为0，保证n的后面几位全为1，即达成 (2^n)-1的目的\n         //这里是n和n右移一位取或，并赋值给n\n         n |= n >>> 1;// 00000101 | 00000010 = 00000111==7\n         n |= n >>> 2;// 00000111 | 00000001 = 00000111==7\n         n |= n >>> 4;// 00000111 | 00000000 = 00000111==7\n         n |= n >>> 8;// 00000111 | 00000000 = 00000111==7\n         n |= n >>> 16;// 00000111 | 00000000 = 00000111==7\n         //n<0时返回1,\n         //n>0时  当n不大于最大值时，返回n+1—————— (2^n)-1  + 1 =2^n\n         //n>0时  当n大于最大值时，返回最大值\n         return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;\n     }\n     \n\n * 装载因子和动态扩容\n   \n   * 最大装载因子默认是 0.75\n     \n     > 当 HashMap 中元素个数超过 0.75*capacity（capacity 表示散列表的容量）的时候，就会启动扩容，每次扩容都会扩容为原来的两倍大小。\n\n * 散列冲突解决方法\n   \n   * HashMap 底层采用链表法来解决冲突\n   \n   * 而当链表长度太长（默认超过 8）时，链表就转换为红黑树。\n     \n     >  1. 当链表长度为>=8时，启用红黑树\n     >  2. 当链表长度为<=6时，启用单链表\n\n * 散列函数\n   \n   * 散列函数的设计并不复杂，追求的是简单高效、分布均匀。\n   \n   * 使用的是除数留余法\n   \n   * A % B = A & (B - 1),当B为2^n^时，等式生效。\n     \n     > 设余数为C，商为D，即A%B=C，A=B*D+C 用9和11对4,8求余来说明\n     > \n     >  1. 9=2*4+1===========1001 = 0010 * 0100 + 0001\n     >     \n     >     * 其中1001 的前两位（4-4/2=2）10当作商。\n     >       * 第一个4 是9的2进制有效长度\n     >       * 第二个4是除数4\n     >     * 后两位（4/2=2）01是余数\n     >     * 而当B-1,4-1后，后面两位全是1，A&（B-1) 相当于取A的后两位的值 即01=1\n     > \n     >  2. 11=1*8+3===========1011 = 0001 * 1000 + 0011\n     >     \n     >     * 其中1011 的前1位（4-8/2=1）1当作商。\n     >     * 后两位（8/2=3）011是余数\n     >     * 而当B-1,8-1后，后三位全是1，A&（B-1) 相当于取A的后三位的值 即011=3\n   \n   * 异或补充\n     \n     * 相同为0，相异为1\n     * 与0异或，都保存不变\n     * 与1异或，都取反\n   \n   * h ^ (h>>>16)\n     \n     * int 是32位的\n     \n     * 设 g=h>>>16，即g的高16位为0，低16位为原h的高16位\n     \n     * x=g^h\n       \n       * ==x的高16位 是原h的高16位==，因为g的高16位全为0，与0异或，都保存不变\n       * ==x的低16位，即原h的高16位和低16位的异或值==\n     \n     * 作用\n       \n       * 为后续计算index截取低位，保证低位的随机性。\n       * 保证32位值，每一位都起作用。\n   \n   // 散列函数 ==扰动函数+除数留余法--------- 将  key 与 散列表 位置 联系起来，即存储，或查找\n   // 将哈希值 对散列表(数组)大小 求余， 得到一个下标位置。然后将key值放入该下标位置\n   static final int getindex(Object key) {\n       int h = key.hashCode();//就是获取对象的哈希码\n       return (h ^ (h >>> 16)) & (capacity -1); //capicity表示散列表的大小\n   }\n   \n   \n   \n   //JDK源码实现 分两步走   扰动函数+除数留余\n   // 扰动函数--哈希函数\n   //h = key.hashCode() ————就是获取对象的哈希码\n   //哈希码代表了对象的一种特征，用来区分不同的对象\n   // 返回 键 key 的哈希值 ——————这里是单纯的求哈希值 ，并没有涉及到存储\n   static final int hash(Object key) {\n       int h;\n       //这是是将 h和h右移16位后的值 做异或\n       //作用1--加强随机性\n       //作用2--且保证每一位值的作用\n       return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);\n   }\n   \n   \n   //在插入或查找的时候，计算Key被映射到桶的位置：\n   //相当于  hash(key) % (capacity)\n   //即除数留余法\n   int index = hash(key) & (capacity - 1)\n   \n\n\n# 散列表和链表的结合\n\n * LRU缓存淘汰算法（最近最少使用）\n   \n   * 链表实现\n     \n     * 实现方法\n       \n       我们需要维护一个按照访问时间从大到小有序排列的链表结构。\n       \n       * 从缓存中删除一个数据\n         \n         > 因为缓存大小有限，当缓存空间不够，需要淘汰一个数据的时候，我们就直接将链表头部的结点删除。\n       \n       * 往缓存中添加一个数据(查找)\n         \n         > 当要缓存某个数据的时候，先在链表中查找这个数据。**如果没有找到，则直接将数据放到链表的尾部；**如果找到了，我们就把它移动到链表的尾部。\n       \n       * 在缓存中查找一个数据\n         \n         > 遍历链表，查找数据。\n     \n     * 复杂度\n       \n       > 因为查找数据需要遍历链表，所以单纯用链表实现的 LRU 缓存淘汰算法的时间复杂很高，是 O(n)。\n       > \n       > 总之，就是单链表的查找比较耗费时间。\n   \n   * 链表+散列表实现\n     \n     * 实现方法\n       \n       * 利用散列表实现链表中元素的查找。\n       \n       > 我们使用双向链表存储数据，链表中的每个结点处理存储数据（data）、前驱指针（prev）、后继指针（next）之外，还新增了一个特殊的字段 hnext。这个 hnext 有什么作用呢？\n       > \n       > 因为我们的散列表是通过链表法解决散列冲突的，所以每个结点会在两条链中。一个链是刚刚我们提到的双向链表，另一个链是散列表中的拉链。前驱和后继指针是为了将结点串在双向链表中，hnext 指针是为了将结点串在散列表的拉链中。\n       > \n       > ==说白了，hnext就是将有冲突的链表中的元素，串到一块的==\n     \n     * 复杂度\n       \n       > 散列表的查找时间复杂度为O(1),\n       > \n       > 散列表+双向链表可以使 查找，删除，添加 都为O(1)\n\n * Redis有序集合\n   \n   * 细化一下Redis 有序集合的操作\n     * 1.添加一个成员对象；\n     * 2.按照键值来删除一个成员对象；\n     * 3.按照键值来查找一个成员对象；\n     * 4.按照分值区间查找数据，比如查找积分在[100, 356]之间的成员对象；\n     * 5.按照分值从小到大排序成员变量；\n   * 数据结构实现\n     * 将分值与成员对象组织成跳表，更好的实现操作4\n     * 按键值构建一个散列表,更好实现操作2,3,\n\n * Java LinkedHashMap\n   \n   * 特性\n     \n     > 一般的HashMap，通过散列函数后，原键值在数组(散列表)中会均匀的分布。即散列表中数据是经过散列函数打乱之后无规律存储的。\n     > \n     > 但是LinkedHashMap借助链表+散列表，支持按插入插入顺序遍历数据，支持按插入时间遍历数据。\n   \n   * 按插入顺序访问\n     \n     //LinkedHashMap\n     //输出结果   3 1 5 2\n     HashMap<Integer, Integer> m = new LinkedHashMap<>();\n     m.put(3, 11);\n     m.put(1, 12);\n     m.put(5, 23);\n     m.put(2, 22);\n     \n     for (Map.Entry e : m.entrySet()) {\n       System.out.println(e.getKey());\n     }\n     \n     \n     \n     \n     //HashMap 的 访问\n     //1,17,2,3,5 按照散列值排序的，除数求余吧，除的是16---默认容量\n     HashMap<Integer, Integer> m1 = new HashMap<>();\n     m1.put(3, 11);\n     m1.put(1, 12);\n     m1.put(5, 23);\n     m1.put(2, 22);\n     m1.put(17, 22);\n     //这里是map集合的遍历方式\n     //        Set<Map.Entry<Integer, Integer>> entries = m.entrySet();\n     for (Map.Entry<Integer,Integer> e : m1.entrySet()) {\n         System.out.println(e.getKey());//1,17,2,3,5\n     }\n     \n     \n     \n   \n   * 按插入时间访问\n     \n     * 其实本身就是一个支持 LRU 缓存淘汰策略的缓存系统\n     \n     \n     // 10是初始大小，0.75是装载因子，true是表示按照访问时间排序\n     HashMap<Integer, Integer> m = new LinkedHashMap<>(10, 0.75f, true);\n     m.put(3, 11);\n     m.put(1, 12);\n     m.put(5, 23);\n     m.put(2, 22);\n     \n     m.put(3, 26);\n     m.get(5);\n     \n     for (Map.Entry e : m.entrySet()) {\n       System.out.println(e.getKey());\n     }\n     \n     //输出结果 1  2  3  5\n     \n     \n     * 前4个插入\n       \n       \n     \n     * 第5个插入——m.put(3, 26);\n       \n       * 会先查找这个键值是否已经有了，然后，再将已经存在的 (3,11) 删除，并且将新的 (3,26) 放到链表的尾部。\n       \n       \n     \n     * 第一个取值——m.get(5);\n       \n       * 访问到 key 为 5 的数据的时候，我们将被访问到的数据移动到链表的尾部\n       \n       \n\n * 为什么散列表和链表经常一块使用？\n   \n   > 散列表这种动态数据数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就说，它无法支持按照某种顺序快速地遍历数据。\n   > \n   > 因为散列表是动态数据结构，不停地有数据的插入、删除，所以每当我们希望按顺序遍历散列表中的数据的时候，都需要先排序，那效率势必会很低。\n   > \n   > 而链表则可以解决这个顺序遍历的问题。所以，我们将散列表和链表（或者跳表）结合在一起使用。",normalizedContent:"# 散列表基础\n\n散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。\n\n * 散列函数（具体 参照散列表基础 文档）\n   \n   * 设计原则\n     * 计算简单\n     * 散列地址分布均匀\n   * 直接定址法\n   * 数字分析法\n   * 平方取中法\n   * 折叠法\n   * 随机数法\n   * 除留余数法(最常用) ——————hashmap、linkedhashmap在计算哈希的时候就是用的这个方法\n\n * 散列冲突（具体 参照散列表基础 文档）\n   \n   * 开放定址法(常用)\n   * 二次探测法\n   * 再散列函数法\n   * 公共溢出区法\n   * 链地址法——(常用)\n     * 将所有为同义词（发生冲突的元素）的记录存储在一个单链表中。\n     * ==这里也可以用 双链表、红黑树这种结构==\n\n * 装载因子\n   \n   * 散列表的装载因子=填入表中的元素个数/散列表的长度\n   * 装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。\n\n * 问题思考\n   \n   * word 文档中单词拼写检查功能是如何实现的？\n     \n     > 常用的英文单词有 20 万个左右，假设单词的平均长度是 10 个字母，平均一个单词占用 10 个字节的内存空间，那 20 万英文单词大约占 2mb 的存储空间，就算放大 10 倍也就是 20mb。\n     > \n     > 对于现在的计算机来说，这个大小完全可以放在内存里面。所以我们可以用散列表来存储整个英文单词词典。\n     > \n     > 当用户输入某个英文单词时，我们拿用户输入的单词去散列表中查找。如果查到，则说明拼写正确；如果没有查到，则说明拼写可能有误，给予提示。\n     > \n     > 借助散列表这种数据结构，我们就可以轻松实现快速判断是否存在拼写错误。\n\n\n# 设计工业级散列表\n\n * 如何设计散列函数\n   \n   * 散列函数的设计不能太复杂\n   * 散列函数生成的值要尽可能随机并且均匀分布\n\n * 装载因子过大怎么办\n   \n   动态扩容\n   \n   > 假设原散列函数是用的除数留余法；\n   > \n   > 当扩容后，原散列表中的元素的位置会发生改变，需要将小散列表中的元素，按计算放到大散列表中。\n   \n   * ==法1==——申请了大数组后，将原来的元素全部按散列结果搬移到大数组中。\n     \n     * 均摊时间复杂度——o(1)\n       \n       > 插入一个数据，最好情况下，不需要扩容，最好时间复杂度是 o(1)。\n       > \n       > 最坏情况下，散列表装载因子过高，启动扩容，我们需要重新申请内存空间，重新计算哈希位置，并且搬移数据，所以时间复杂度是 o(n)。\n       > \n       > 用摊还分析法，均摊情况下，时间复杂度接近最好情况，就是 o(1)。\n\n * 如何避免低效扩容\n   \n   * 法1的做法，太低效了，当数据过多时，十分的耗时。\n     \n     > 如果我们的业务代码直接服务于用户，尽管大部分情况下，插入一个数据的操作都很快，但是，极个别非常慢的插入操作，也会让用户崩溃。这个时候，“一次性”扩容的机制就不合适了。\n   \n   * ==法2==——我们可以将扩容操作穿插在插入操作的过程中，分批完成\n     \n     > 当装载因子触达阈值之后，我们只申请新空间，但并不将老的数据搬移到新散列表中。\n     > \n     > 当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。每次插入一个数据到散列表，我们都重复上面的过程。\n     > \n     > 经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。\n   \n   * 混合散列表如何查询\n     \n     > 对于查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找。\n\n * 如何选择冲突解决方法\n   \n   * 开放定址法——threadlocalmap\n     \n     * 优势\n       \n       > 散列表中的数据都存储在数组中，可以有效地利用 cpu 缓存加快查询速度。而且，这种方法实现的散列表，==序列化==起来比较简单。\n     \n     * 劣势\n       \n       > 用开放寻址法解决冲突的散列表，删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据。\n       > \n       > 而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。\n       > \n       > 所以，使用开放寻址法解决冲突的散列表，装载因子的上限不能太大。这也导致这种方法比链表法更浪费内存空间。\n     \n     * 适用场景\n       \n       > 当数据量比较小、装载因子小的时候，适合采用开放寻址法。\n   \n   * 链地址法——linkedhashmap\n     \n     * 优势\n       \n       >  1. 链表法对内存的利用率比开放寻址法要高。因为链表结点可以在需要的时候再创建，并不需要像开放寻址法那样事先申请好。\n       >  2. **链表法比起开放寻址法，对大装载因子的容忍度更高。**即便装载因子变成 10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多。\n     \n     * 劣势\n       \n       > 1. 链表因为要存储指针，所以对于比较小的对象的存储，是比较消耗内存的，还有可能会让内存的消耗翻倍。(当然，如果我们存储的是大对象，也就是说要存储的对象的大小远远大于一个指针的大小（4 个字节或者 8 个字节），那链表中指针的内存消耗在大对象面前就可以忽略了。)\n       > \n       > 2. 因为链表中的结点是零散分布在内存中的，不是连续的，所以**对 cpu 缓存是不友好**的，这方面对于执行效率也有一定的影响。\n     \n     * 改进\n       \n       > 我们将链表法中的链表改造为其他高效的动态数据结构，比如双链表、跳表、红黑树。这样，即便出现散列冲突，极端情况下，所有的数据都散列到同一个桶内，那最终退化成的散列表的查找时间也只不过是 o(logn)。这样也就有效避免了前面讲到的散列碰撞攻击。\n     \n     * 适用场景\n       \n       > 基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。\n\n * 如果设计一个工业级散列函数\n   \n   * 特性\n     * 支持快速地查询、插入、删除操作；\n     * 内存占用合理，不能浪费过多的内存空间；\n     * 性能稳定，极端情况下，散列表的性能也不会退化到无法接受的情况。\n   * 如何实现——具体业务，具体数据，具体分析\n     * 设计一个合适的散列函数；\n     * 定义装载因子阈值，并且设计动态扩容策略；\n     * 选择合适的散列冲突解决方法\n   * ==没有最好的方法，只有最合适的方法==\n\n\n# java hashmap 分析\n\n * 初始大小\n   \n   * hashmap 默认的初始大小是 16\n     \n     > 这个默认值是可以设置的，如果事先知道大概的数据量有多大，可以通过修改默认初始大小，减少动态扩容的次数，这样会大大提高 hashmap 的性能。\n   \n   * ==大小设置为 2^n^==,如果你传入的数不是2^n^,在构造函数中，会调整到大于等于它的2^n^\n     \n     //假设传入5=00000101\n     \n     static final int tablesizefor(int cap) {\n         //这里减一是为了防止传入的就是2^n,处理结果会变成2^(n+1)\n         int n = cap - 1;\n         // >>> 是无符号右移的意思\n         //一直右移，直到n右移过后为0，保证n的后面几位全为1，即达成 (2^n)-1的目的\n         //这里是n和n右移一位取或，并赋值给n\n         n |= n >>> 1;// 00000101 | 00000010 = 00000111==7\n         n |= n >>> 2;// 00000111 | 00000001 = 00000111==7\n         n |= n >>> 4;// 00000111 | 00000000 = 00000111==7\n         n |= n >>> 8;// 00000111 | 00000000 = 00000111==7\n         n |= n >>> 16;// 00000111 | 00000000 = 00000111==7\n         //n<0时返回1,\n         //n>0时  当n不大于最大值时，返回n+1—————— (2^n)-1  + 1 =2^n\n         //n>0时  当n大于最大值时，返回最大值\n         return (n < 0) ? 1 : (n >= maximum_capacity) ? maximum_capacity : n + 1;\n     }\n     \n\n * 装载因子和动态扩容\n   \n   * 最大装载因子默认是 0.75\n     \n     > 当 hashmap 中元素个数超过 0.75*capacity（capacity 表示散列表的容量）的时候，就会启动扩容，每次扩容都会扩容为原来的两倍大小。\n\n * 散列冲突解决方法\n   \n   * hashmap 底层采用链表法来解决冲突\n   \n   * 而当链表长度太长（默认超过 8）时，链表就转换为红黑树。\n     \n     >  1. 当链表长度为>=8时，启用红黑树\n     >  2. 当链表长度为<=6时，启用单链表\n\n * 散列函数\n   \n   * 散列函数的设计并不复杂，追求的是简单高效、分布均匀。\n   \n   * 使用的是除数留余法\n   \n   * a % b = a & (b - 1),当b为2^n^时，等式生效。\n     \n     > 设余数为c，商为d，即a%b=c，a=b*d+c 用9和11对4,8求余来说明\n     > \n     >  1. 9=2*4+1===========1001 = 0010 * 0100 + 0001\n     >     \n     >     * 其中1001 的前两位（4-4/2=2）10当作商。\n     >       * 第一个4 是9的2进制有效长度\n     >       * 第二个4是除数4\n     >     * 后两位（4/2=2）01是余数\n     >     * 而当b-1,4-1后，后面两位全是1，a&（b-1) 相当于取a的后两位的值 即01=1\n     > \n     >  2. 11=1*8+3===========1011 = 0001 * 1000 + 0011\n     >     \n     >     * 其中1011 的前1位（4-8/2=1）1当作商。\n     >     * 后两位（8/2=3）011是余数\n     >     * 而当b-1,8-1后，后三位全是1，a&（b-1) 相当于取a的后三位的值 即011=3\n   \n   * 异或补充\n     \n     * 相同为0，相异为1\n     * 与0异或，都保存不变\n     * 与1异或，都取反\n   \n   * h ^ (h>>>16)\n     \n     * int 是32位的\n     \n     * 设 g=h>>>16，即g的高16位为0，低16位为原h的高16位\n     \n     * x=g^h\n       \n       * ==x的高16位 是原h的高16位==，因为g的高16位全为0，与0异或，都保存不变\n       * ==x的低16位，即原h的高16位和低16位的异或值==\n     \n     * 作用\n       \n       * 为后续计算index截取低位，保证低位的随机性。\n       * 保证32位值，每一位都起作用。\n   \n   // 散列函数 ==扰动函数+除数留余法--------- 将  key 与 散列表 位置 联系起来，即存储，或查找\n   // 将哈希值 对散列表(数组)大小 求余， 得到一个下标位置。然后将key值放入该下标位置\n   static final int getindex(object key) {\n       int h = key.hashcode();//就是获取对象的哈希码\n       return (h ^ (h >>> 16)) & (capacity -1); //capicity表示散列表的大小\n   }\n   \n   \n   \n   //jdk源码实现 分两步走   扰动函数+除数留余\n   // 扰动函数--哈希函数\n   //h = key.hashcode() ————就是获取对象的哈希码\n   //哈希码代表了对象的一种特征，用来区分不同的对象\n   // 返回 键 key 的哈希值 ——————这里是单纯的求哈希值 ，并没有涉及到存储\n   static final int hash(object key) {\n       int h;\n       //这是是将 h和h右移16位后的值 做异或\n       //作用1--加强随机性\n       //作用2--且保证每一位值的作用\n       return (key == null) ? 0 : (h = key.hashcode()) ^ (h >>> 16);\n   }\n   \n   \n   //在插入或查找的时候，计算key被映射到桶的位置：\n   //相当于  hash(key) % (capacity)\n   //即除数留余法\n   int index = hash(key) & (capacity - 1)\n   \n\n\n# 散列表和链表的结合\n\n * lru缓存淘汰算法（最近最少使用）\n   \n   * 链表实现\n     \n     * 实现方法\n       \n       我们需要维护一个按照访问时间从大到小有序排列的链表结构。\n       \n       * 从缓存中删除一个数据\n         \n         > 因为缓存大小有限，当缓存空间不够，需要淘汰一个数据的时候，我们就直接将链表头部的结点删除。\n       \n       * 往缓存中添加一个数据(查找)\n         \n         > 当要缓存某个数据的时候，先在链表中查找这个数据。**如果没有找到，则直接将数据放到链表的尾部；**如果找到了，我们就把它移动到链表的尾部。\n       \n       * 在缓存中查找一个数据\n         \n         > 遍历链表，查找数据。\n     \n     * 复杂度\n       \n       > 因为查找数据需要遍历链表，所以单纯用链表实现的 lru 缓存淘汰算法的时间复杂很高，是 o(n)。\n       > \n       > 总之，就是单链表的查找比较耗费时间。\n   \n   * 链表+散列表实现\n     \n     * 实现方法\n       \n       * 利用散列表实现链表中元素的查找。\n       \n       > 我们使用双向链表存储数据，链表中的每个结点处理存储数据（data）、前驱指针（prev）、后继指针（next）之外，还新增了一个特殊的字段 hnext。这个 hnext 有什么作用呢？\n       > \n       > 因为我们的散列表是通过链表法解决散列冲突的，所以每个结点会在两条链中。一个链是刚刚我们提到的双向链表，另一个链是散列表中的拉链。前驱和后继指针是为了将结点串在双向链表中，hnext 指针是为了将结点串在散列表的拉链中。\n       > \n       > ==说白了，hnext就是将有冲突的链表中的元素，串到一块的==\n     \n     * 复杂度\n       \n       > 散列表的查找时间复杂度为o(1),\n       > \n       > 散列表+双向链表可以使 查找，删除，添加 都为o(1)\n\n * redis有序集合\n   \n   * 细化一下redis 有序集合的操作\n     * 1.添加一个成员对象；\n     * 2.按照键值来删除一个成员对象；\n     * 3.按照键值来查找一个成员对象；\n     * 4.按照分值区间查找数据，比如查找积分在[100, 356]之间的成员对象；\n     * 5.按照分值从小到大排序成员变量；\n   * 数据结构实现\n     * 将分值与成员对象组织成跳表，更好的实现操作4\n     * 按键值构建一个散列表,更好实现操作2,3,\n\n * java linkedhashmap\n   \n   * 特性\n     \n     > 一般的hashmap，通过散列函数后，原键值在数组(散列表)中会均匀的分布。即散列表中数据是经过散列函数打乱之后无规律存储的。\n     > \n     > 但是linkedhashmap借助链表+散列表，支持按插入插入顺序遍历数据，支持按插入时间遍历数据。\n   \n   * 按插入顺序访问\n     \n     //linkedhashmap\n     //输出结果   3 1 5 2\n     hashmap<integer, integer> m = new linkedhashmap<>();\n     m.put(3, 11);\n     m.put(1, 12);\n     m.put(5, 23);\n     m.put(2, 22);\n     \n     for (map.entry e : m.entryset()) {\n       system.out.println(e.getkey());\n     }\n     \n     \n     \n     \n     //hashmap 的 访问\n     //1,17,2,3,5 按照散列值排序的，除数求余吧，除的是16---默认容量\n     hashmap<integer, integer> m1 = new hashmap<>();\n     m1.put(3, 11);\n     m1.put(1, 12);\n     m1.put(5, 23);\n     m1.put(2, 22);\n     m1.put(17, 22);\n     //这里是map集合的遍历方式\n     //        set<map.entry<integer, integer>> entries = m.entryset();\n     for (map.entry<integer,integer> e : m1.entryset()) {\n         system.out.println(e.getkey());//1,17,2,3,5\n     }\n     \n     \n     \n   \n   * 按插入时间访问\n     \n     * 其实本身就是一个支持 lru 缓存淘汰策略的缓存系统\n     \n     \n     // 10是初始大小，0.75是装载因子，true是表示按照访问时间排序\n     hashmap<integer, integer> m = new linkedhashmap<>(10, 0.75f, true);\n     m.put(3, 11);\n     m.put(1, 12);\n     m.put(5, 23);\n     m.put(2, 22);\n     \n     m.put(3, 26);\n     m.get(5);\n     \n     for (map.entry e : m.entryset()) {\n       system.out.println(e.getkey());\n     }\n     \n     //输出结果 1  2  3  5\n     \n     \n     * 前4个插入\n       \n       \n     \n     * 第5个插入——m.put(3, 26);\n       \n       * 会先查找这个键值是否已经有了，然后，再将已经存在的 (3,11) 删除，并且将新的 (3,26) 放到链表的尾部。\n       \n       \n     \n     * 第一个取值——m.get(5);\n       \n       * 访问到 key 为 5 的数据的时候，我们将被访问到的数据移动到链表的尾部\n       \n       \n\n * 为什么散列表和链表经常一块使用？\n   \n   > 散列表这种动态数据数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就说，它无法支持按照某种顺序快速地遍历数据。\n   > \n   > 因为散列表是动态数据结构，不停地有数据的插入、删除，所以每当我们希望按顺序遍历散列表中的数据的时候，都需要先排序，那效率势必会很低。\n   > \n   > 而链表则可以解决这个顺序遍历的问题。所以，我们将散列表和链表（或者跳表）结合在一起使用。",charsets:{cjk:!0}},{title:"哈希算法",frontmatter:{autoSort:89,title:"哈希算法",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/9227a5/",categories:["算法","散列表"],tags:["知识","算法"]},regularPath:"/02.%E7%AE%97%E6%B3%95/35.%E6%95%A3%E5%88%97%E8%A1%A8/10.%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95.html",relativePath:"02.算法/35.散列表/10.哈希算法.md",key:"v-1b852971",path:"/pages/9227a5/",headers:[{level:2,title:"哈希算法基础",slug:"哈希算法基础",normalizedTitle:"哈希算法基础",charIndex:2},{level:2,title:"应用",slug:"应用",normalizedTitle:"应用",charIndex:284},{level:2,title:"问题思考",slug:"问题思考",normalizedTitle:"问题思考",charIndex:4579}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"哈希算法基础 应用 问题思考",content:"# 哈希算法基础\n\n * 定义\n   \n   > 将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是哈希算法，而通过原始数据映射之后得到的二进制值串就是哈希值。\n\n * 算法要求\n   \n   >  1. 从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）\n   >  2. 对输入数据非常敏感，哪怕原始数据只修改了一个 Bit，最后得到的哈希值也大不相同\n   >  3. 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小\n   >  4. 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值\n\n\n# 应用\n\n * 安全加密\n   \n   MD5,SHA,DES,AES\n   \n   > 前面哈希算法四点要求，对用于加密的哈希算法来说，有两点格外重要。第一点是很难根据哈希值反向推导出原始数据，第二点是散列冲突的概率要很小。\n   \n   * 第一点\n     \n     > 第一点很好理解，加密的目的就是防止原始数据泄露，所以很难通过哈希值反向推导原始数据，这是一个最基本的要求。\n   \n   * 第二点\n     \n     > **不管是什么哈希算法，我们只能尽量减少碰撞冲突的概率，理论上是没办法做到完全不冲突的。**为什么这么说呢。\n     > \n     > 这里就基于组合数学中一个非常基础的理论，鸽巢原理（也叫抽屉原理）。这个原理本身很简单，它是说，如果有 10 个鸽巢，有 11 只鸽子，那肯定有 1 个鸽巢中的鸽子数量多于 1 个，换句话说就是，肯定有 2 只鸽子在 1 个鸽巢内。\n     > \n     > 比如前面举的 MD5 的例子，哈希值是固定的 128 位二进制串，能表示的数据是有限的，最多能表示 2^128 个数据，而我们要哈希的数据是无穷的。如果我们对 2^128+1 个数据求哈希值，就必然会存在哈希值相同的情况。\n     > \n     > 一般情况下，哈希值越长的哈希算法，散列冲突的概率越低。\n\n * 唯一标识\n   \n   * 如果要在海量的图库中，怎样搜索一张图是否存在？\n     \n     > 我们不能单纯地用图片的元信息（比如图片名称）来比对，因为有可能存在名称相同但图片内容不同，或者名称不同图片内容相同的情况。\n     > \n     > 我们可以给每一个图片取一个唯一标识，或者说信息摘要。比如，我们可以从图片的二进制码串开头取 100 个字节，从中间取 100 个字节，从最后再取 100 个字节，然后将这 300 个字节放到一块，通过哈希算法（比如 MD5），得到一个哈希字符串，用它作为图片的唯一标识。\n     > \n     > 我们可以把每个图片的唯一标识，和相应的图片文件在图库中的路径信息，都存储在散列表中。当要查看某个图片是不是在图库中的时候，我们先通过哈希算法对这个图片取唯一标识，然后在散列表中查找是否存在这个唯一标识。\n     > \n     > 如果不存在，那就说明这个图片不在图库中；如果存在，我们再通过散列表中存储的文件路径，获取到这个已经存在的图片，跟现在要插入的图片做全量的比对，看是否完全一样。如果一样，就说明已经存在；如果不一样，说明两张图片尽管唯一标识相同，但是并不是相同的图片。\n\n * 数据校验\n   \n   * 并行下载\n     \n     > 我们从多个机器上并行下载一个 2GB 的电影，这个电影文件可能会被分割成很多文件块（比如可以分成 100 块，每块大约 20MB）。等所有的文件块都下载完成之后，再组装成一个完整的电影文件就行了。\n   \n   * 校验\n     \n     > 我们通过哈希算法，对 100 个文件块分别取哈希值，并且保存在种子文件中。我们在前面讲过，哈希算法有一个特点，对数据很敏感。只要文件块的内容有一丁点儿的改变，最后计算出的哈希值就会完全不同。\n     > \n     > 所以，当文件块下载完成之后，我们可以通过相同的哈希算法，对下载好的文件块逐一求哈希值，然后跟种子文件中保存的哈希值比对。如果不同，说明这个文件块不完整或者被篡改了，需要再重新从其他宿主机器上下载这个文件块。\n\n * 散列函数\n   \n   > 散列函数中用到的散列算法，对于散列算法冲突的要求要低很多,是否能反向解密也并不关心,更加关注散列后的值是否能平均分布，也就是，一组数据是否能均匀地散列在各个槽中。除此之外，散列函数执行的快慢，也会影响散列表的性能，所以，散列函数用的散列算法一般都比较简单，比较追求效率。\n\n * 负载均衡\n   \n   * 如何才能实现一个会话粘滞（session sticky）的负载均衡算法呢？\n     \n     > 也就是说，我们需要在同一个客户端上，在一次会话中的所有请求都路由到同一个服务器上。\n   \n   * 方法\n     \n     > 我们可以通过哈希算法，对客户端 IP 地址或者会话 ID 计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号。 这样，我们就可以把同一个 IP 过来的所有请求，都路由到同一个后端服务器上。\n\n * 数据分片\n   \n   ==针对这种海量数据的处理问题，我们都可以采用多机分布式处理。借助这种分片的思路，可以突破单机内存、CPU 等资源的限制。==\n   \n   * 如何统计“搜索关键词”出现的次数？\n     \n     > 假如我们有 1T 的日志文件，这里面记录了用户的搜索关键词，我们想要快速统计出每个关键词被搜索的次数，该怎么做呢？\n     \n     > 我们可以先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度。\n     > \n     >  1. 每个机器负责一片数据，所有机器并行处理。\n     > \n     >  2. 从日志文件中，取出一个关键词，然后通过哈希函数计算哈希值，然后跟n取模，最终得到的值就是该关键词被分配到的机器编号。\n     > \n     >  3. 然后将该关键词分配到对应机器编号上。相当于每个机器同时做两件事，一个是遍历分片的数据，将关键词分配到不同的机器上；另外同时接收从其他机器上传来的关键词。\n     > \n     >  4. 当遍历完所有数据后，哈希值相同的搜索关键词就被分配到了同一个机器上。在没有冲突的情况下，也就是说，同一个搜索关键词会被分配到同一个机器上。\n     > \n     >  5. 每个机器会分别计算它拥有的关键词出现的次数，最后合并起来就是最终的结果。\n     >     \n     >     这里的处理过程也是 MapReduce 的基本设计思想。\n   \n   * 如何快速判断图片是否在图库中？\n     \n     > 假设现在我们的图库中有 1 亿张图片，很显然，在单台机器上构建散列表是行不通的,因为单台机器的内存有限，而 1 亿张图片构建散列表显然远远超过了单台机器的内存上限。\n     \n     * 构建散列表\n       \n       > 我们同样可以对数据进行分片，然后采用多机处理。我们准备 n 台机器，让每台机器只维护某一部分图片对应的散列表。我们每次从图库中读取一个图片，计算唯一标识，然后与机器个数 n 求余取模，得到的值就对应要分配的机器编号，然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表。\n     \n     * 查询是否存在\n       \n       > 当我们要判断一个图片是否在图库中的时候，我们通过同样的哈希算法，计算这个图片的唯一标识，然后与机器个数 n 求余取模。假设得到的值是 k，那就去编号 k 的机器构建的散列表中查找。\n\n * 分布式存储\n   \n   > 我们为了提高数据的读取、写入能力，一般都采用分布式的方式来存储数据，比如分布式缓存。\n   \n   * 一般哈希\n     \n     > 我们可以借用前面数据分片的思想，即通过哈希算法对数据取哈希值，然后对机器个数取模，这个最终值就是应该存储的缓存机器编号。\n     > \n     > 当需要扩容时，所有的数据都要重新计算缓存机器编号。\n     \n     > 这样就相当于，缓存中的数据一下子就都失效了。所有的数据请求都会穿透缓存，直接去请求数据库。这样就可能发生雪崩效应，压垮数据库。\n   \n   * 一致性哈希\n     \n     我们需要一种方法，使得在新加入一个机器后，并不需要做大量的数据搬移——一致性哈希\n     \n     * 一致性哈希映射\n       \n       > 不同于普通哈希，是通过求余的方式来映射的。\n       > \n       > 一致性哈希映射是==环形、顺时针、就近==映射\n       \n       > 整个环，相当于全部的缓存空间，环形空间总被被分为**2^32^**个缓存区。（这里的32是哈希值的二进制位数）\n       > \n       > key值通过某种hash算法转换为一个32位的二进制数\n       > \n       > 每一个缓存结点node(相当于之前提到的缓存机器编号)，用同样的算法，映射到环形空间。\n       > \n       > ==归属原则==\n       > \n       > 每一个key顺时针方向最近的node就是key所归属的存储结点\n     \n     * 增加结点\n       \n       > 当缓存集群的节点有所增加的时候，整个环形空间的映射仍然会保持一致性哈希的顺时针规则，所以有一小部分key的归属会受到影响。\n       \n       原先只有结点1,2,3----现在增加结点4\n       \n       * 影响\n         \n         > 原本归属node2的key2，现在需要归属node4\n         > \n         > 受影响的键只有key2，其他键不受影响\n     \n     * 删除结点\n       \n       > 当缓存集群的节点需要删除的时候（比如节点挂掉），整个环形空间的映射同样会保持一致性哈希的顺时针规则，同样有一小部分key的归属会受到影响。\n       \n       现在假设，结点3挂掉了\n       \n       * 影响\n         \n         > 只有key4受到影响，现在要归属于node1\n     \n     * 虚拟结点\n       \n       > 当缓存在环形空间中分布不均匀时，可以引入虚拟结点。\n       > \n       > 设 原来 只有node1，和node2，分布相当不均匀。现引入，node1-1，node1-2；和node2-1，node2-2.\n       > \n       > 由于虚拟结点数量较多，缓存key与虚拟结点之间的映射关系也变得相对平衡了。\n\n\n# 问题思考\n\n * 你会如何存储用户密码这么重要的数据吗？（MD5+salt）\n   \n   * 字典攻击\n     \n     > 如果用户信息被“脱库”，黑客虽然拿到是加密之后的密文，但可以通过“猜”的方式来破解密码，这是因为，有些用户的密码太简单。比如很多人习惯用 00000、123456 这样的简单数字组合做密码，很容易就被猜中。\n     > \n     > 那我们就需要维护一个常用密码的字典表，把字典中的每个密码用哈希算法计算哈希值，然后拿哈希值跟脱库后的密文比对。如果相同，基本上就可以认为，这个加密之后的密码对应的明文就是字典中的这个密码。\n   \n   * 加盐(salt)\n     \n     > 针对字典攻击，我们可以引入一个盐（salt），跟用户的密码组合在一起，增加密码的复杂度。我们拿组合之后的字符串来做哈希算法加密，将它存储到数据库中，进一步增加破解的难度。\n     > \n     > 不过我这里想多说一句，我认为安全和攻击是一种博弈关系，不存在绝对的安全。所有的安全措施，只是增加攻击的成本而已。\n     > \n     > 比如原密码是123456，不加盐的情况加密后假设是是xyz。 黑客拿到脱机的数据后，通过彩虹表匹配可以轻松破解常用密码。**如果加盐，密码123456加盐后可能是12ng34qq56zz，再对加盐后的密码进行hash后值就与原密码hash后的值完全不同了。**而且加盐的方式有很多种，可以是在头部加，可以在尾部加，还可在内容中间加，甚至加的盐还可以是随机的。这样即使用户使用的是最常用的密码，黑客拿到密文后破解的难度也很高。\n   \n   * 更优秀做法\n     \n     > 除了hash+salt，现在大多公司都采用无论密码长度多少，计算字符串hash时间都固定或者足够慢的算法如PBKDF2WithHmacSHA1，来降低硬件计算hash速度，减少不同长度字符串计算hash所需时间不一样而泄漏字符串长度信息，进一步减少风险。\n\n * 区块链\n   \n   > 区块链是一块块区块组成的，每个区块分为两部分：区块头和区块体。\n   > \n   > 区块头保存着 自己区块体 和 上一个区块头 的哈希值。\n   > \n   > 因为这种链式关系和哈希值的唯一性，只要区块链上任意一个区块被修改过，后面所有区块保存的哈希值就不对了。\n   > \n   > 区块链使用的是 SHA256 哈希算法，计算哈希值非常耗时，如果要篡改一个区块，就必须重新计算该区块后面所有的区块的哈希值，短时间内几乎不可能做到。",normalizedContent:"# 哈希算法基础\n\n * 定义\n   \n   > 将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是哈希算法，而通过原始数据映射之后得到的二进制值串就是哈希值。\n\n * 算法要求\n   \n   >  1. 从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）\n   >  2. 对输入数据非常敏感，哪怕原始数据只修改了一个 bit，最后得到的哈希值也大不相同\n   >  3. 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小\n   >  4. 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值\n\n\n# 应用\n\n * 安全加密\n   \n   md5,sha,des,aes\n   \n   > 前面哈希算法四点要求，对用于加密的哈希算法来说，有两点格外重要。第一点是很难根据哈希值反向推导出原始数据，第二点是散列冲突的概率要很小。\n   \n   * 第一点\n     \n     > 第一点很好理解，加密的目的就是防止原始数据泄露，所以很难通过哈希值反向推导原始数据，这是一个最基本的要求。\n   \n   * 第二点\n     \n     > **不管是什么哈希算法，我们只能尽量减少碰撞冲突的概率，理论上是没办法做到完全不冲突的。**为什么这么说呢。\n     > \n     > 这里就基于组合数学中一个非常基础的理论，鸽巢原理（也叫抽屉原理）。这个原理本身很简单，它是说，如果有 10 个鸽巢，有 11 只鸽子，那肯定有 1 个鸽巢中的鸽子数量多于 1 个，换句话说就是，肯定有 2 只鸽子在 1 个鸽巢内。\n     > \n     > 比如前面举的 md5 的例子，哈希值是固定的 128 位二进制串，能表示的数据是有限的，最多能表示 2^128 个数据，而我们要哈希的数据是无穷的。如果我们对 2^128+1 个数据求哈希值，就必然会存在哈希值相同的情况。\n     > \n     > 一般情况下，哈希值越长的哈希算法，散列冲突的概率越低。\n\n * 唯一标识\n   \n   * 如果要在海量的图库中，怎样搜索一张图是否存在？\n     \n     > 我们不能单纯地用图片的元信息（比如图片名称）来比对，因为有可能存在名称相同但图片内容不同，或者名称不同图片内容相同的情况。\n     > \n     > 我们可以给每一个图片取一个唯一标识，或者说信息摘要。比如，我们可以从图片的二进制码串开头取 100 个字节，从中间取 100 个字节，从最后再取 100 个字节，然后将这 300 个字节放到一块，通过哈希算法（比如 md5），得到一个哈希字符串，用它作为图片的唯一标识。\n     > \n     > 我们可以把每个图片的唯一标识，和相应的图片文件在图库中的路径信息，都存储在散列表中。当要查看某个图片是不是在图库中的时候，我们先通过哈希算法对这个图片取唯一标识，然后在散列表中查找是否存在这个唯一标识。\n     > \n     > 如果不存在，那就说明这个图片不在图库中；如果存在，我们再通过散列表中存储的文件路径，获取到这个已经存在的图片，跟现在要插入的图片做全量的比对，看是否完全一样。如果一样，就说明已经存在；如果不一样，说明两张图片尽管唯一标识相同，但是并不是相同的图片。\n\n * 数据校验\n   \n   * 并行下载\n     \n     > 我们从多个机器上并行下载一个 2gb 的电影，这个电影文件可能会被分割成很多文件块（比如可以分成 100 块，每块大约 20mb）。等所有的文件块都下载完成之后，再组装成一个完整的电影文件就行了。\n   \n   * 校验\n     \n     > 我们通过哈希算法，对 100 个文件块分别取哈希值，并且保存在种子文件中。我们在前面讲过，哈希算法有一个特点，对数据很敏感。只要文件块的内容有一丁点儿的改变，最后计算出的哈希值就会完全不同。\n     > \n     > 所以，当文件块下载完成之后，我们可以通过相同的哈希算法，对下载好的文件块逐一求哈希值，然后跟种子文件中保存的哈希值比对。如果不同，说明这个文件块不完整或者被篡改了，需要再重新从其他宿主机器上下载这个文件块。\n\n * 散列函数\n   \n   > 散列函数中用到的散列算法，对于散列算法冲突的要求要低很多,是否能反向解密也并不关心,更加关注散列后的值是否能平均分布，也就是，一组数据是否能均匀地散列在各个槽中。除此之外，散列函数执行的快慢，也会影响散列表的性能，所以，散列函数用的散列算法一般都比较简单，比较追求效率。\n\n * 负载均衡\n   \n   * 如何才能实现一个会话粘滞（session sticky）的负载均衡算法呢？\n     \n     > 也就是说，我们需要在同一个客户端上，在一次会话中的所有请求都路由到同一个服务器上。\n   \n   * 方法\n     \n     > 我们可以通过哈希算法，对客户端 ip 地址或者会话 id 计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号。 这样，我们就可以把同一个 ip 过来的所有请求，都路由到同一个后端服务器上。\n\n * 数据分片\n   \n   ==针对这种海量数据的处理问题，我们都可以采用多机分布式处理。借助这种分片的思路，可以突破单机内存、cpu 等资源的限制。==\n   \n   * 如何统计“搜索关键词”出现的次数？\n     \n     > 假如我们有 1t 的日志文件，这里面记录了用户的搜索关键词，我们想要快速统计出每个关键词被搜索的次数，该怎么做呢？\n     \n     > 我们可以先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度。\n     > \n     >  1. 每个机器负责一片数据，所有机器并行处理。\n     > \n     >  2. 从日志文件中，取出一个关键词，然后通过哈希函数计算哈希值，然后跟n取模，最终得到的值就是该关键词被分配到的机器编号。\n     > \n     >  3. 然后将该关键词分配到对应机器编号上。相当于每个机器同时做两件事，一个是遍历分片的数据，将关键词分配到不同的机器上；另外同时接收从其他机器上传来的关键词。\n     > \n     >  4. 当遍历完所有数据后，哈希值相同的搜索关键词就被分配到了同一个机器上。在没有冲突的情况下，也就是说，同一个搜索关键词会被分配到同一个机器上。\n     > \n     >  5. 每个机器会分别计算它拥有的关键词出现的次数，最后合并起来就是最终的结果。\n     >     \n     >     这里的处理过程也是 mapreduce 的基本设计思想。\n   \n   * 如何快速判断图片是否在图库中？\n     \n     > 假设现在我们的图库中有 1 亿张图片，很显然，在单台机器上构建散列表是行不通的,因为单台机器的内存有限，而 1 亿张图片构建散列表显然远远超过了单台机器的内存上限。\n     \n     * 构建散列表\n       \n       > 我们同样可以对数据进行分片，然后采用多机处理。我们准备 n 台机器，让每台机器只维护某一部分图片对应的散列表。我们每次从图库中读取一个图片，计算唯一标识，然后与机器个数 n 求余取模，得到的值就对应要分配的机器编号，然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表。\n     \n     * 查询是否存在\n       \n       > 当我们要判断一个图片是否在图库中的时候，我们通过同样的哈希算法，计算这个图片的唯一标识，然后与机器个数 n 求余取模。假设得到的值是 k，那就去编号 k 的机器构建的散列表中查找。\n\n * 分布式存储\n   \n   > 我们为了提高数据的读取、写入能力，一般都采用分布式的方式来存储数据，比如分布式缓存。\n   \n   * 一般哈希\n     \n     > 我们可以借用前面数据分片的思想，即通过哈希算法对数据取哈希值，然后对机器个数取模，这个最终值就是应该存储的缓存机器编号。\n     > \n     > 当需要扩容时，所有的数据都要重新计算缓存机器编号。\n     \n     > 这样就相当于，缓存中的数据一下子就都失效了。所有的数据请求都会穿透缓存，直接去请求数据库。这样就可能发生雪崩效应，压垮数据库。\n   \n   * 一致性哈希\n     \n     我们需要一种方法，使得在新加入一个机器后，并不需要做大量的数据搬移——一致性哈希\n     \n     * 一致性哈希映射\n       \n       > 不同于普通哈希，是通过求余的方式来映射的。\n       > \n       > 一致性哈希映射是==环形、顺时针、就近==映射\n       \n       > 整个环，相当于全部的缓存空间，环形空间总被被分为**2^32^**个缓存区。（这里的32是哈希值的二进制位数）\n       > \n       > key值通过某种hash算法转换为一个32位的二进制数\n       > \n       > 每一个缓存结点node(相当于之前提到的缓存机器编号)，用同样的算法，映射到环形空间。\n       > \n       > ==归属原则==\n       > \n       > 每一个key顺时针方向最近的node就是key所归属的存储结点\n     \n     * 增加结点\n       \n       > 当缓存集群的节点有所增加的时候，整个环形空间的映射仍然会保持一致性哈希的顺时针规则，所以有一小部分key的归属会受到影响。\n       \n       原先只有结点1,2,3----现在增加结点4\n       \n       * 影响\n         \n         > 原本归属node2的key2，现在需要归属node4\n         > \n         > 受影响的键只有key2，其他键不受影响\n     \n     * 删除结点\n       \n       > 当缓存集群的节点需要删除的时候（比如节点挂掉），整个环形空间的映射同样会保持一致性哈希的顺时针规则，同样有一小部分key的归属会受到影响。\n       \n       现在假设，结点3挂掉了\n       \n       * 影响\n         \n         > 只有key4受到影响，现在要归属于node1\n     \n     * 虚拟结点\n       \n       > 当缓存在环形空间中分布不均匀时，可以引入虚拟结点。\n       > \n       > 设 原来 只有node1，和node2，分布相当不均匀。现引入，node1-1，node1-2；和node2-1，node2-2.\n       > \n       > 由于虚拟结点数量较多，缓存key与虚拟结点之间的映射关系也变得相对平衡了。\n\n\n# 问题思考\n\n * 你会如何存储用户密码这么重要的数据吗？（md5+salt）\n   \n   * 字典攻击\n     \n     > 如果用户信息被“脱库”，黑客虽然拿到是加密之后的密文，但可以通过“猜”的方式来破解密码，这是因为，有些用户的密码太简单。比如很多人习惯用 00000、123456 这样的简单数字组合做密码，很容易就被猜中。\n     > \n     > 那我们就需要维护一个常用密码的字典表，把字典中的每个密码用哈希算法计算哈希值，然后拿哈希值跟脱库后的密文比对。如果相同，基本上就可以认为，这个加密之后的密码对应的明文就是字典中的这个密码。\n   \n   * 加盐(salt)\n     \n     > 针对字典攻击，我们可以引入一个盐（salt），跟用户的密码组合在一起，增加密码的复杂度。我们拿组合之后的字符串来做哈希算法加密，将它存储到数据库中，进一步增加破解的难度。\n     > \n     > 不过我这里想多说一句，我认为安全和攻击是一种博弈关系，不存在绝对的安全。所有的安全措施，只是增加攻击的成本而已。\n     > \n     > 比如原密码是123456，不加盐的情况加密后假设是是xyz。 黑客拿到脱机的数据后，通过彩虹表匹配可以轻松破解常用密码。**如果加盐，密码123456加盐后可能是12ng34qq56zz，再对加盐后的密码进行hash后值就与原密码hash后的值完全不同了。**而且加盐的方式有很多种，可以是在头部加，可以在尾部加，还可在内容中间加，甚至加的盐还可以是随机的。这样即使用户使用的是最常用的密码，黑客拿到密文后破解的难度也很高。\n   \n   * 更优秀做法\n     \n     > 除了hash+salt，现在大多公司都采用无论密码长度多少，计算字符串hash时间都固定或者足够慢的算法如pbkdf2withhmacsha1，来降低硬件计算hash速度，减少不同长度字符串计算hash所需时间不一样而泄漏字符串长度信息，进一步减少风险。\n\n * 区块链\n   \n   > 区块链是一块块区块组成的，每个区块分为两部分：区块头和区块体。\n   > \n   > 区块头保存着 自己区块体 和 上一个区块头 的哈希值。\n   > \n   > 因为这种链式关系和哈希值的唯一性，只要区块链上任意一个区块被修改过，后面所有区块保存的哈希值就不对了。\n   > \n   > 区块链使用的是 sha256 哈希算法，计算哈希值非常耗时，如果要篡改一个区块，就必须重新计算该区块后面所有的区块的哈希值，短时间内几乎不可能做到。",charsets:{cjk:!0}},{title:"跳表",frontmatter:{autoSort:91,title:"跳表",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/e28ee4/",categories:["算法","跳表"],tags:["知识","算法"]},regularPath:"/02.%E7%AE%97%E6%B3%95/38.%E8%B7%B3%E8%A1%A8/05.%E8%B7%B3%E8%A1%A8.html",relativePath:"02.算法/38.跳表/05.跳表.md",key:"v-b34a9bcc",path:"/pages/e28ee4/",headers:[{level:2,title:"基础知识",slug:"基础知识",normalizedTitle:"基础知识",charIndex:2},{level:2,title:"时间复杂度分析——O(logn)",slug:"时间复杂度分析-o-logn",normalizedTitle:"时间复杂度分析——o(logn)",charIndex:193},{level:2,title:"空间复杂度分析——O(n)",slug:"空间复杂度分析-o-n",normalizedTitle:"空间复杂度分析——o(n)",charIndex:886},{level:2,title:"高效的动态插入和删除",slug:"高效的动态插入和删除",normalizedTitle:"高效的动态插入和删除",charIndex:1438},{level:2,title:"跳表索引的动态更新",slug:"跳表索引的动态更新",normalizedTitle:"跳表索引的动态更新",charIndex:1784},{level:2,title:"跳表实现",slug:"跳表实现",normalizedTitle:"跳表实现",charIndex:2118}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"基础知识 时间复杂度分析——O(logn) 空间复杂度分析——O(n) 高效的动态插入和删除 跳表索引的动态更新 跳表实现",content:"# 基础知识\n\n * Redis 是用跳表来实现有序集合的\n * 跳表其实是由链表改造而来的，在原链表的基础上加上索引，在索引之上再加索引，通过空间换时间，以实现链表的”二分查找“。这种链表加多级索引的结构，就是跳表。\n * 它确实是一种各方面性能都比较优秀的动态数据结构，可以支持快速地插入、删除、查找操作，写起来也不复杂，甚至可以替代红黑树（Red-black tree）\n\n\n# 时间复杂度分析——O(logn)\n\n> 每两个结点会抽出一个结点作为上一级索引的结点，那第一级索引的结点个数大约就是 n/2，第二级索引的结点个数大约就是 n/4，第三级索引的结点个数大约就是 n/8，依次类推，也就是说，第 k 级索引的结点个数是第 k-1 级索引的结点个数的 1/2，那第 k级索引结点的个数就是 n/(2k)。\n> \n> 假设索引有 h 级，最高级的索引有 2 个结点。通过上面的公式，我们可以得到 n/(2h)=2，从而求得 h=log2n-1。如果包含原始链表这一层，整个跳表的高度就是 ==log2n==。\n> \n> 我们在跳表中查询某个数据的时候，如果每一层都要遍历 m 个结点，那在跳表中查询一个数据的时间复杂度就是 O(m*logn)。那这个 m 的值是多少呢？按照前面这种索引结构，我们每一级索引都最多只需要遍历 3 个结点，也就是说 m=3，为什么是 3 呢？我来解释一下。 ==2+1==——2是结点个数，每两个结点抽一层索引。\n> \n> 假设我们要查找的数据是 x，在第 k 级索引中，我们遍历到 y 结点之后，发现 x 大于 y，小于后面的结点 z，所以我们通过 y 的 down 指针，从第 k 级索引下降到第 k-1 级索引。在第 k-1 级索引中，y 和 z 之间只有 3 个结点（包含 y 和 z），所以，我们在 K-1 级索引中最多只需要遍历 3 个结点，依次类推，每一级索引都最多只需要遍历 3 个结点。\n> \n> 通过上面的分析，我们得到 m=3，所以在跳表中查询任意数据的时间复杂度就是 ==O(logn)==。\n\n\n\n\n# 空间复杂度分析——O(n)\n\n> 假设原始链表大小为 n，那第一级索引大约有 n/2 个结点，第二级索引大约有 n/4 个结点，以此类推，每上升一级就减少一半，直到剩下 2 个结点。如果我们把每层索引的结点数写出来，就是一个等比数列。\n\n\n\n> 这几级索引的结点总和就是 n/2+n/4+n/8…+8+4+2=n-2。 一共是log~2~(n/2)项，n/2算是首项，公比为1/2，计算过程如下所示：\n\n$$ \\frac{\\frac{n}{2}(1-(\\frac{1}{2})^{log_{2}\\frac{n}{2}})}{1-\\frac{1}{2}} =n(1-\\frac{2}{n})=n-2 $$\n\n> 所以跳表的空间复杂度为O(n)。也就是说，如果将包含 n 个结点的单链表构造成跳表，我们需要额外再用接近 n 个结点的存储空间。选择抽取结点个数越大，所需存储空间越小，性能也就越差。\n\n> 实际上，在软件开发中，我们不必太在意索引占用的额外空间。在讲数据结构和算法时，我们习惯性地把要处理的数据看成整数，但是在实际的软件开发中，原始链表中存储的有可能是很大的对象，而索引结点只需要存储关键值和几个指针，并不需要存储对象，所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了。\n\n\n# 高效的动态插入和删除\n\n> 跳表这个动态数据结构，不仅支持查找操作，还支持动态的插入、删除操作，而且插入、删除操作的时间复杂度也是 O(logn)。\n\n * 插入\n   \n   > 在单链表中，一旦定位好要插入的位置，插入结点的时间复杂度是很低的，就是 O(1)。但是，这里为了保证原始链表中数据的有序性，我们需要先找到要插入的位置，这个查找操作就会比较耗时。\n   > \n   > 对于跳表来说，我们讲过查找某个结点的时间复杂度是 O(logn)，所以这里查找某个数据应该插入的位置，方法也是类似的，时间复杂度也是 O(logn)。\n   > \n   > 这里没有考虑索引的更新，动态更新索引在下方说明。\n\n * 删除也是同样的道理，查找花费时间为O(logn),删除为O(1).\n\n\n# 跳表索引的动态更新\n\n> 作为一种==动态数据结构==，我们需要某种手段来维护索引与原始链表大小之间的平衡，也就是说，如果链表中结点多了，索引结点就相应地增加一些，避免复杂度退化，以及查找、插入、删除操作性能下降。\n\n==跳表是通过随机函数来维护前面提到的“平衡性”==\n\n> 当我们往跳表中插入数据的时候，我们可以选择同时将这个数据插入到部分索引层中。\n> \n> 如何选择加入哪些索引层呢？\n> \n> 我们通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值 K，那我们就将这个结点添加到第一级到第 K 级这 K 级索引中。\n> \n> 随机函数的选择很有讲究，从概率上来讲，能够保证跳表的索引大小和数据大小平衡性，不至于性能过度退化。\n\n\n# 跳表实现\n\n为什么 Redis 要用跳表来实现有序集合，而不是红黑树？\n\n * Redis 中的有序集合支持的核心操作主要有下面这几个：\n   \n   * 插入一个数据\n   * 删除一个数据\n   * 查找一个数据\n   * 按照区间查找数据（比如查找值在[100, 356]之间的数据）\n   * 迭代输出有序序列\n   \n   > 插入、删除、查找以及迭代输出有序序列这几个操作，红黑树也可以完成，时间复杂度跟跳表是一样的。\n   > \n   > 但是，按照区间来查找数据这个操作，红黑树的效率没有跳表高。对于按照区间查找数据这个操作，跳表可以做到 O(logn) 的时间复杂度定位区间的起点，然后在原始链表中顺序往后遍历就可以了。这样做非常高效。\n\n * 跳表更容易代码实现。\n\n * 跳表更加灵活，它可以通过改变索引构建策略，有效平衡执行效率和内存消耗。",normalizedContent:"# 基础知识\n\n * redis 是用跳表来实现有序集合的\n * 跳表其实是由链表改造而来的，在原链表的基础上加上索引，在索引之上再加索引，通过空间换时间，以实现链表的”二分查找“。这种链表加多级索引的结构，就是跳表。\n * 它确实是一种各方面性能都比较优秀的动态数据结构，可以支持快速地插入、删除、查找操作，写起来也不复杂，甚至可以替代红黑树（red-black tree）\n\n\n# 时间复杂度分析——o(logn)\n\n> 每两个结点会抽出一个结点作为上一级索引的结点，那第一级索引的结点个数大约就是 n/2，第二级索引的结点个数大约就是 n/4，第三级索引的结点个数大约就是 n/8，依次类推，也就是说，第 k 级索引的结点个数是第 k-1 级索引的结点个数的 1/2，那第 k级索引结点的个数就是 n/(2k)。\n> \n> 假设索引有 h 级，最高级的索引有 2 个结点。通过上面的公式，我们可以得到 n/(2h)=2，从而求得 h=log2n-1。如果包含原始链表这一层，整个跳表的高度就是 ==log2n==。\n> \n> 我们在跳表中查询某个数据的时候，如果每一层都要遍历 m 个结点，那在跳表中查询一个数据的时间复杂度就是 o(m*logn)。那这个 m 的值是多少呢？按照前面这种索引结构，我们每一级索引都最多只需要遍历 3 个结点，也就是说 m=3，为什么是 3 呢？我来解释一下。 ==2+1==——2是结点个数，每两个结点抽一层索引。\n> \n> 假设我们要查找的数据是 x，在第 k 级索引中，我们遍历到 y 结点之后，发现 x 大于 y，小于后面的结点 z，所以我们通过 y 的 down 指针，从第 k 级索引下降到第 k-1 级索引。在第 k-1 级索引中，y 和 z 之间只有 3 个结点（包含 y 和 z），所以，我们在 k-1 级索引中最多只需要遍历 3 个结点，依次类推，每一级索引都最多只需要遍历 3 个结点。\n> \n> 通过上面的分析，我们得到 m=3，所以在跳表中查询任意数据的时间复杂度就是 ==o(logn)==。\n\n\n\n\n# 空间复杂度分析——o(n)\n\n> 假设原始链表大小为 n，那第一级索引大约有 n/2 个结点，第二级索引大约有 n/4 个结点，以此类推，每上升一级就减少一半，直到剩下 2 个结点。如果我们把每层索引的结点数写出来，就是一个等比数列。\n\n\n\n> 这几级索引的结点总和就是 n/2+n/4+n/8…+8+4+2=n-2。 一共是log~2~(n/2)项，n/2算是首项，公比为1/2，计算过程如下所示：\n\n$$ \\frac{\\frac{n}{2}(1-(\\frac{1}{2})^{log_{2}\\frac{n}{2}})}{1-\\frac{1}{2}} =n(1-\\frac{2}{n})=n-2 $$\n\n> 所以跳表的空间复杂度为o(n)。也就是说，如果将包含 n 个结点的单链表构造成跳表，我们需要额外再用接近 n 个结点的存储空间。选择抽取结点个数越大，所需存储空间越小，性能也就越差。\n\n> 实际上，在软件开发中，我们不必太在意索引占用的额外空间。在讲数据结构和算法时，我们习惯性地把要处理的数据看成整数，但是在实际的软件开发中，原始链表中存储的有可能是很大的对象，而索引结点只需要存储关键值和几个指针，并不需要存储对象，所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了。\n\n\n# 高效的动态插入和删除\n\n> 跳表这个动态数据结构，不仅支持查找操作，还支持动态的插入、删除操作，而且插入、删除操作的时间复杂度也是 o(logn)。\n\n * 插入\n   \n   > 在单链表中，一旦定位好要插入的位置，插入结点的时间复杂度是很低的，就是 o(1)。但是，这里为了保证原始链表中数据的有序性，我们需要先找到要插入的位置，这个查找操作就会比较耗时。\n   > \n   > 对于跳表来说，我们讲过查找某个结点的时间复杂度是 o(logn)，所以这里查找某个数据应该插入的位置，方法也是类似的，时间复杂度也是 o(logn)。\n   > \n   > 这里没有考虑索引的更新，动态更新索引在下方说明。\n\n * 删除也是同样的道理，查找花费时间为o(logn),删除为o(1).\n\n\n# 跳表索引的动态更新\n\n> 作为一种==动态数据结构==，我们需要某种手段来维护索引与原始链表大小之间的平衡，也就是说，如果链表中结点多了，索引结点就相应地增加一些，避免复杂度退化，以及查找、插入、删除操作性能下降。\n\n==跳表是通过随机函数来维护前面提到的“平衡性”==\n\n> 当我们往跳表中插入数据的时候，我们可以选择同时将这个数据插入到部分索引层中。\n> \n> 如何选择加入哪些索引层呢？\n> \n> 我们通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值 k，那我们就将这个结点添加到第一级到第 k 级这 k 级索引中。\n> \n> 随机函数的选择很有讲究，从概率上来讲，能够保证跳表的索引大小和数据大小平衡性，不至于性能过度退化。\n\n\n# 跳表实现\n\n为什么 redis 要用跳表来实现有序集合，而不是红黑树？\n\n * redis 中的有序集合支持的核心操作主要有下面这几个：\n   \n   * 插入一个数据\n   * 删除一个数据\n   * 查找一个数据\n   * 按照区间查找数据（比如查找值在[100, 356]之间的数据）\n   * 迭代输出有序序列\n   \n   > 插入、删除、查找以及迭代输出有序序列这几个操作，红黑树也可以完成，时间复杂度跟跳表是一样的。\n   > \n   > 但是，按照区间来查找数据这个操作，红黑树的效率没有跳表高。对于按照区间查找数据这个操作，跳表可以做到 o(logn) 的时间复杂度定位区间的起点，然后在原始链表中顺序往后遍历就可以了。这样做非常高效。\n\n * 跳表更容易代码实现。\n\n * 跳表更加灵活，它可以通过改变索引构建策略，有效平衡执行效率和内存消耗。",charsets:{cjk:!0}},{title:"字符串",frontmatter:{autoSort:85,title:"字符串",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/658fbb/",categories:["算法","字符串"],tags:["知识","算法"]},regularPath:"/02.%E7%AE%97%E6%B3%95/40.%E5%AD%97%E7%AC%A6%E4%B8%B2/05.%E5%AD%97%E7%AC%A6%E4%B8%B2.html",relativePath:"02.算法/40.字符串/05.字符串.md",key:"v-b94aee00",path:"/pages/658fbb/",headers:[{level:2,title:"BF算法",slug:"bf算法",normalizedTitle:"bf算法",charIndex:2},{level:2,title:"RK算法",slug:"rk算法",normalizedTitle:"rk算法",charIndex:583},{level:2,title:"BM算法",slug:"bm算法",normalizedTitle:"bm算法",charIndex:2032},{level:2,title:"KMP算法",slug:"kmp算法",normalizedTitle:"kmp算法",charIndex:4755},{level:2,title:"Trie树",slug:"trie树",normalizedTitle:"trie树",charIndex:7662},{level:2,title:"AC自动机",slug:"ac自动机",normalizedTitle:"ac自动机",charIndex:9232}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"BF算法 RK算法 BM算法 KMP算法 Trie树 AC自动机",content:'# BF算法\n\n暴力匹配算法——朴素匹配算法\n\n * 主串与模式串\n   \n   在主串中查找模式串\n   \n   > 比方说，我们在字符串 A 中查找字符串 B，那字符串 A 就是主串，字符串 B 就是模式串。\n   > \n   > 我们把主串的长度记作 n，模式串的长度记作 m。因为我们是在主串中查找模式串，所以 n>m。\n\n * 算法优劣\n   \n   * 缺点\n     \n     > BF 算法的时间复杂度很高，是 O(n*m)\n     \n     但在实际的开发中，它却是一个比较常用的字符串匹配算法\n   \n   * 优点\n     \n     > 第一，实际的软件开发中，**大部分情况下，模式串和主串的长度都不会太长。**而且每次模式串与主串中的子串匹配的时候，当中途遇到不能匹配的字符的时候，就可以就停止了，不需要把 m 个字符都比对一下。所以，尽管理论上的最坏情况时间复杂度是 O(n*m)，但是，统计意义上，大部分情况下，算法执行效率要比这个高很多。\n     > \n     > 第二，**朴素字符串匹配算法思想简单，代码实现也非常简单。**简单意味着不容易出错，如果有 bug 也容易暴露和修复。在工程中，在满足性能要求的前提下，简单是首选。\n\n * 所以，在实际的软件开发中，绝大部分情况下，朴素的字符串匹配算法就够用了\n\n\n# RK算法\n\nBF算法的优化版，借助了哈希算法来优化比较\n\n * 算法思想\n   \n   > 我们通过哈希算法对主串中的 n-m+1 个子串分别求哈希值，然后逐个与模式串的哈希值比较大小。\n   > \n   > 如果某个子串的哈希值与模式串相等，那就说明对应的子串和模式串匹配了（这里先不考虑哈希冲突的问题，后面我们会讲到）。因为哈希值是一个数字，数字之间比较是否相等是非常快速的，所以模式串和子串比较的效率就提高了。\n\n * 设计哈希算法1\n   \n   * 进制表示法\n     \n     > 我们假设要匹配的字符串的字符集中只包含 K 个字符，我们可以用一个 K 进制数来表示一个子串，这个 K 进制数转化成十进制数，作为子串的哈希值。\n     > \n     > 使用进制表示法，计算哈希值，不会产生哈希冲突，但是会出现值偏大，大到超出数的范围。\n   \n   * 哈希值计算规律\n     \n     > 相邻子串之间的哈希值的计算公式有一定的关系。\n     > \n     >  * 计算技巧\n     >    \n     >    计算26^(m-1)的时候，可以提前计算好，存入数组中，以下标作为次方数，直接查。\n   \n   * 时间复杂度\n     \n     > 整个 RK 算法包含两部分，计算子串哈希值和模式串哈希值与子串哈希值之间的比较。\n     > \n     > 第一部分，我们前面也分析了，可以通过设计特殊的哈希算法，只需要扫描一遍主串就能计算出所有子串的哈希值了，所以这部分的时间复杂度是 O(n)。\n     > \n     > 模式串哈希值与每个子串哈希值之间的比较的时间复杂度是 O(1)，总共需要比较 n-m+1 个子串的哈希值。所以，这部分的时间复杂度也是 O(n)。\n     > \n     > 所以，RK 算法整体的时间复杂度就是 O(n)。\n\n * 设计哈希算法2\n   \n   > 哈希算法的设计方法有很多，我举一个例子说明一下。\n   > \n   > 假设字符串中只包含 a～z 这 26 个英文字母，那我们每个字母对应一个数字，比如 a 对应 1，b 对应 2，以此类推，z 对应 26。我们可以把字符串中每个字母对应的数字相加，最后得到的和作为哈希值。这种哈希算法产生的哈希值的数据范围就相对要小很多了。\n   \n   * 应对哈希冲突\n     \n     > 之前我们只需要比较一下模式串和子串的哈希值，如果两个值相等，那这个子串就一定可以匹配模式串。但是，当存在哈希冲突的时候，有可能存在这样的情况，子串和模式串的哈希值虽然是相同的，但是两者本身并不匹配。\n     > \n     > **当我们发现一个子串的哈希值跟模式串的哈希值相等的时候，我们只需要再对比一下子串和模式串本身就好了。**当然，如果子串的哈希值与模式串的哈希值不相等，那对应的子串和模式串肯定也是不匹配的，就不需要比对子串和模式串本身了。\n\n * RK算法改进\n   \n   > 改进一：先计算模式串的hash值，记录下来，然后计算每一个子串的hash，计算一次，就对比一次，如果hash值匹配，在全量对比字符串。这样做可以不用关心hash冲突问题。不用事先计算出主串中所有子串的哈希值，计算一次，与模式串比较一次即可。\n   > \n   > 改进二：计算子串hash值的时候只要计算到（n-m）处即可，剩下的子串长度小于模式串，不用计算.\n\n\n# BM算法\n\n * 核心思想\n   \n   > BM 算法核心思想是，利用模式串本身的特点，在模式串中某个字符与主串不能匹配的时候，将模式串往后多滑动几位，以此来减少不必要的字符比较，提高匹配的效率。\n\n * 原理分析\n   \n   BM算法的匹配顺序是==按照模式串下标从大到小的顺序，倒着匹配的==\n   \n   * 坏字符规则\n     \n     从模式串的末尾往前倒着匹配，当发现某个字符没法匹配的时候，我们把这个没有匹配的字符叫作坏字符（主串中的字符）。\n     \n     > 我们拿坏字符 c 在模式串中查找，发现模式串中并不存在这个字符，也就是说，字符 c 与模式串中的任何字符都不可能匹配。这个时候，我们可以将模式串直接往后滑动三位(模式串的长度=2-(-1)=3)，将模式串滑动到 c 后面的位置，再从模式串的末尾字符开始比较。\n     \n     * 移动次数\n       \n       >  1. 当发生不匹配的时候，我们把坏字符==对应==的模式串中的字符下标记作 si\n       > \n       >  2. 如果坏字符在模式串中存在，我们把这个坏字符在模式串中的下标记作xi。如果不存在，我们把 xi 记作 -1。\n       > \n       >  3. 那模式串往后移动的位数就等于 si-xi。\n       > \n       >  4. 如果坏字符在模式串里多处出现，那我们在计算 xi 的时候，选择最靠后的那个，因为这样不会让模式串滑动过多，导致本来可能匹配的情况被滑动略过。\n     \n     * 优缺点\n       \n       * 优点\n         \n         > BM 算法在最好情况下的时间复杂度非常低，是 O(n/m)\n         > \n         > 匹配具有类似特点的模式串和主串的时候，BM 算法非常高效。\n       \n       * 缺点\n         \n         > 根据 si-xi 计算出来的移动位数，有可能是==负数==\n         > \n         > 比如主串是 aaaaaaaaaaaaaaaa，模式串是 baaa。不但不会向后滑动模式串，还有可能倒退。\n         > \n         > 所以，BM 算法还需要用到“好后缀规则”。\n   \n   * 好后缀规则\n     \n     在从后往前匹配的时候，已经匹配到的字符串称为好后缀。\n     \n     * 滑动次数\n       \n       > 在好后缀的后缀子串中，查找最长的、能跟模式串前缀子串匹配的后缀子串；\n       > \n       > 简单来说，就是不要一下子移动一整个模式串的距离，看看模式串的开头部分和好后缀的结尾部分有没有重和的，有重和的就少移动几位。\n       > \n       > {u}为好后缀，{v}为能够匹配上的模式串的前缀子串\n       > \n       > 滑动次数=模式串长度-{v}的长度\n   \n   * 方法选用\n     \n     > 我们可以分别计算好后缀和坏字符往后滑动的位数，然后取两个数中最大的，作为模式串往后滑动的位数。这种处理方法还可以避免我们前面提到的，根据坏字符规则，计算得到的往后滑动的位数，有可能是负数的情况。\n\n * 算法实现\n   \n   * 坏字符规则\n     \n     >  1. 可以预先将模式串中的每个字符及其下标都存储到散列表中。\n     >  2. 计算下标xi的时候，可以直接查表获取，而不用遍历整个字符串。\n     \n     * 提供一种散列表的方法\n       \n       > 使用数组，将模式串单个字符的ascii码值作为数组下标，数组中的值为该字符在模式串中的下标。\n     \n     * 移动次数\n     \n     * bc为散列表数组，i为主串的循环大变量，j为模式串的循环小变量\n   \n   * 好后缀规则\n     \n     * 核心内容\n       \n       * 在模式串中，查找跟好后缀匹配的另一个子串；\n       \n       * 在好后缀的后缀子串中，查找最长的、能跟模式串前缀子串匹配的后缀子串；\n     \n     * 实现方法1\n       \n       对模式串进行预处理,解决第一个核心内容\n       \n       > 因为好后缀子串也是模式串的后缀子串，所以我们可以提前将好后缀子串在模式串中再一次出现的起始下标值记录下来。\n       > \n       > 将该值用suffix数组存储，下标为后缀子串的长度。\n       > \n       >  1. 如果模式串中有多个（大于 1 个）子串跟后缀子串{u}匹配，存储模式串中最靠后的那个子串的起始位置，也就是下标最大的那个子串的起始位置。\n       > \n       >  2. 如果该后缀没有再次出现，则将其设为-1\n       > \n       > 比如说，后缀子串为b的时候，b再次出现在下标2处，且自身长度为1，所以suffix[1]=2\n       > \n       > 后缀子串为cab的时候，cab再次出现在下标0处，且自身长度为3，所以suffix[3]=0\n       > \n       > 后缀子串为bcab的时候，bcab没有再次出现，且自身长度为4，所以suffix[4]=-1\n     \n     * 实现方法2\n       \n       除了 suffix 数组之外，我们还需要另外一个 boolean 类型的 prefix 数组，来记录模式串的后缀子串是否能匹配模式串的前缀子串。\n       \n       如果suffix数组的值为0，则说明该后缀自串也是模式串的前缀子串，prefix记为true；\n     \n     * 移动次数\n       \n       * 好后缀再次出现\n       \n       * 好后缀没有再次出现，但是有好后缀的后缀子串为模式串的前缀子串\n       \n       * 什么都没有\n\n * ==优秀思想总结==\n   \n   代码实现具体细节不重要，重要的是处理问题的思想，及更好的优化思路\n   \n   * 使用散列表提高查找效率\n   * 对于重复的，较难的计算，预处理好，需要的时候直接查(26^(m-1))\n   * 善用之前的信息。\n\n\n# KMP算法\n\n * 核心思想\n   \n   > KMP 算法就是在试图寻找一种规律：在模式串和主串匹配的过程中，当遇到坏字符后，对于已经比对过的好前缀，能否找到一种规律，将模式串一次性滑动很多位？\n   > \n   > 这里一次性移动了2位，2=主串虚线框中第一个a的下标-子串虚线框中第一个a的下标\n\n * 最长可匹配前(后)缀\n   \n   把好前缀的所有后缀子串中，最长的可匹配前缀子串的那个后缀子串，叫作最长可匹配后缀子串；对应的前缀子串，叫作最长可匹配前缀子串。\n   \n   最长子串的计算只有模式串有关，可以提前计算。用==next数组(前缀表)==表示。\n   \n   1. 数组的下标是每个前缀结尾字符下标\n   2. 数组的值是这个前缀的 最长可以匹配前缀子串 的结尾字符下标。\n      * 比如，aba可以匹配到的最长前缀子串为a，其下标为0\n      * abab可以匹配到的最长前缀子串为ab，其下标为1\n   3. ==前缀表是用来回退的，它记录了模式串与主串(文本串)不匹配的时候，模式串应该从哪里开始重新匹配。==\n      * ==next[i]=x表示，s[0:i]具有长度为x+1的完全相同的前缀和后缀。==\n      * 从next+1开始匹配,即前一个最长匹配前缀子串末尾的下一位。next中存的是末尾的下标，所有模式串从next+1开始匹配。\n      * j = next[j - 1] + 1\n      * \n\n * KMP算法代码\n   \n   // a, b分别是主串和模式串；n, m分别是主串和模式串的长度。\n   public static int kmp(char[] a, int n, char[] b, int m) {\n       int[] next = getNexts(b, m);\n       int j = 0;\n       for (int i = 0; i < n; ++i) {\n           while (j > 0 && a[i] != b[j]) { // 一直找到a[i]和b[j]\n               j = next[j - 1] + 1;\n           }\n           //j为一次移动的次数，对于模式串来说，不用从0开始比了，一下子滑动到j处\n           //进入循环的j为坏字符的下标\n           //进入后更新j为最长匹配前缀的末尾字符串的下一位，即next[j-1]+1\n           if (a[i] == b[j]) {\n               ++j;\n           }\n           if (j == m) { // 找到匹配模式串的了\n               return i - m + 1;\n           }\n       }\n       return -1;\n   }\n   \n\n * next数组计算方法\n   \n   * 思想\n     \n     在计算next[i]的时候，利用前面已经计算出的next[0],……next[i-1]。\n     \n     * 第一种情况（加字符相等）\n       \n       > 如在计算上文的next[3]和next[4]那样.以next[3]为例\n       > \n       > next[2]时,整个串为aba,最长前缀子串为a,这个a是下标为0的a，k=0,i=2,next[i]=k==>next[2]=0\n       > \n       > 在计算next[3]时，整个串变为abab,新添加的这个b(下标为3)，刚好等于next[0]时的最长前缀子串的下一个字符b(下标为1)\n       > \n       > 此时，next[3]的最长前缀子串为ab，所以next[1]=next[0]+1=1,k=1,i=3,next[i]=k==>next[3]=1\n     \n     * ==第二种情况(加字符不等)==\n       \n       > 设字符串为abxabcabxabx,共12位，下标从0到11\n       > \n       > 在计算next[10]时，串为abxabcabxab,最长前缀子串为abxab,即k=4,i=10,next[10]=4\n       > \n       > 在计算next[11]时，串为abxabcabxabx,新添加了一个x，而next[10]的最长前缀子串的下一个字符为c，二者不相等，即k=4,i=11,k\'=k+1=5,next[5]!=next[11]，则找一个次长前缀子串，即为abxab的的最长前缀子串，即满足情况1的子串。简单来说，我下一个是x，你去找一个最长前缀子串，下一个也是x，符合情况1，这样可以直接用加法来解决。\n       > \n       > k=next[4]=1,此时最长前缀子串为ab，str[k+1]=str[2]====x====str[i]=str[11],正好满足情况1，此时的最长前缀子串为abx，即next[11]=2。\n   \n   * 代码实现\n     \n     // b表示模式串，m表示模式串的长度\n     private static int[] getNexts(char[] b, int m) {\n         int[] next = new int[m];\n         next[0] = -1;\n         int k = -1;\n         for (int i = 1; i < m; ++i) {\n             //去找满足情况1的子串\n             while (k != -1 && b[k + 1] != b[i]) {\n                 k = next[k];//回溯\n             }\n             //找到了，++\n             if (b[k + 1] == b[i]) {\n                 ++k;\n             }\n             //赋值\n             next[i] = k;\n         }\n         return next;\n     }\n     \n\n * 复杂度分析\n   \n   * 空间复杂度——next数组，O(n)\n   * 时间复杂度\n     * 求next数组——O(m)\n       * 就是大循环的次数，小循序次数肯定小于m，设为k，即总体为O(k*m)\n       * k相对于m来说，值很小，所以总的时间复杂度为O(m)\n     * KMP算法——O(n)，同求next数组\n     * ==总的——O(m+n)==\n\n\n# Trie树\n\nTrie 树，也叫“字典树”。它是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。\n\n * 核心思想\n   \n   > Trie 树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起。\n   \n   例如，我们有 6 个字符串，它们分别是：how，hi，her，hello，so，see。构造Trie树，则如下图所示。从根节点到红色节点的一条路径表示一个字符串（注意：红色节点并不都是叶子节点）——例如hi，hil,i和l两个结点都要标红，而i不是叶子结点。\n\n * 查找\n   \n   * her\n     \n     分解为h,e,r最后找到红色结点，说明是一个完整的字符。\n   \n   * he\n     \n     分解为h,e最后没有找到红色结点，说明是一个完整的字符的字符前缀。\n\n * Trie树实现\n   \n   > 假设字符串中只有26个小写字符，**则可以用一个大小为26的数组来存储子节点的指针。**不存在子节点的存null\n   \n   * 数据结构\n     \n     class TrieNode {\n       char data;\n       TrieNode children[26];\n     }\n     \n   \n   * 时间复杂度\n     \n     设总的字符串长度为n，要查找的字符串长度为k，则\n     \n     * 构建Trim树——O(n)\n     * 构建好树，在树中查字符串——O(k)\n\n * Trie树优化\n   \n   * Trie树 很耗内存\n     \n     上述说的，使用数组存子节点指针的方式，需要大量的存储空间，而且很多空间都是闲置的。\n   \n   * 改进\n     \n     * 可以将数组，改成散列表，跳表，红黑树等，稍微牺牲一些查询效率，以节约内存\n     * 缩点优化——将斜树的内容放到一个结点，例如l,l,o可以存储在一起，llo，不影响查询\n\n * Trie树与散列表、红黑树的比较\n   \n   * 查找定长的字符串\n     \n     > 在一组字符串中，查找一个固定的字符串，Trie树虽然效率很高，但是有很多缺点\n     > \n     >  1. 第一，字符串中包含的字符集不能太大。我们前面讲到，如果字符集太大，那存储空间可能就会浪费很多。即便可以优化，但也要付出牺牲查询、插入效率的代价。\n     >  2. 第二，要求字符串的前缀重合比较多，不然空间消耗会变大很多。\n     >  3. 第三，如果要用 Trie 树解决问题，那我们就要自己从零开始实现一个 Trie 树，还要保证没有 bug，这个在工程上是将简单问题复杂化，除非必须，一般不建议这样做。\n     >  4. 第四，我们知道，通过指针串起来的数据块是不连续的，而 Trie 树中用到了指针，所以，对缓存并不友好，性能上会打个折扣。\n     > \n     > 综上，查找固定长度的字符串，Trie树不如使用红黑树或者散列表。\n   \n   * 查找字符串前缀匹配的字符串\n     \n     > Trie 树最有优势的是查找前缀匹配的字符串。对于这个功能，其他数据结构，爱莫能助\n     \n     * 实现搜索关键词提示功能\n     * 实现代码(命令)自动补全功能\n\n * 利用Trie树实现搜索关键词提示功能(最基本的功能实现)\n   \n   深入剖析搜索引擎\n   \n   假设，词库中有hello,her,hi,how,see,so\n   \n   * 输入h，展示hello,her,hi,how\n   * 输入he，展示hello,her\n\n\n# AC自动机\n\nBF 算法、RK 算法、BM 算法、KMP 算法,都是单模式串匹配算法， Trie 树，AC自动机都是多模式串匹配算法。\n\nAC 自动机是基于 Trie 树的一种改进算法，它跟 Trie 树的关系，就像单模式串中，KMP 算法与 BF 算法的关系一样。\n\n * 利用Tire树实现多模式串匹配\n   \n   前面Trie那节介绍的是匹配一个固定长度的字符串问题，很短小，就一个主串匹配一个模式串\n   \n   * 多模式串匹配\n     \n     > 多模式串匹配算法，就是在多个模式串和一个主串之间做匹配，也就是说，在一个主串中查找多个模式串。\n   \n   * 实现流程——借助上图的trie树\n     \n     > 设有超长字符串，作为主串（cheisesa），从第一个字符（c）开始，在 Trie 树中匹配。当匹配到 Trie 树的叶子节点，或者中途遇到不匹配字符的时候，我们将主串的开始匹配位置后移一位，也就是从字符h开始，重新在 Trie 树中匹配(重新从根节点开始搜索)。hei这个字符串可以匹配到，做相应处理。\n     > \n     > 然后继续从s开始匹配,se符合，ses不符合。则继续从e开始匹配，重复此过程。\n   \n   * 缺点\n     \n     每次匹配失败后，从主串当前匹配字符往后移动一位，继续从根节点开始重新匹配，就相当于BF算法在匹配时做法。\n     \n     > 举例：假设匹配abcd\n     > \n     >  * Trie树\n     >    \n     >    1. abc没问题,继续匹配，d和e匹配不上\n     >    \n     >    2. 匹配起始结点从a后移一维，重新开始匹配bcd，匹配完成。\n     >    \n     >    3. 分析：b,c进行了重复匹配.在模式串abcd中已经匹配过了bc，在模式串bcd中再次进行了b和c的匹配。\n     > \n     >  * AC自动机\n     >    \n     >    1. abc没问题,继续匹配，d和e匹配不上\n     >    2. 通过c的失败指针指向模式串bcd的c，这个d刚好可以和abcd中d匹配上，如果到了结尾，可以直接输出匹配到了bcd\n     >    3. 分析:在模式串abcd中已经匹配过了bc，在模式串bcd中只匹配了d\n\n * AC自动机核心思想\n   \n   > AC 自动机实际上就是在 Trie 树之上，加了类似 KMP 的 next 数组，只不过此处的 next 数组是构建在树上罢了。\n   \n   * 数据结构\n     \n     public class AcNode {\n         public char data; \n         public AcNode[] children = new AcNode[26]; // 字符集只包含a~z这26个字符\n         public boolean isEndingChar = false; // 结尾字符为true\n         public int length = -1; // 当isEndingChar=true时，记录模式串长度\n         public AcNode fail; // 失败指针\n         public AcNode(char data) {\n             this.data = data;\n         }\n     }\n     \n   \n   * AC自动机的构建\n     \n     * 将多个模式串构建成 Trie 树；构建一个敏感词的Trie树。——详见Trie树\n     * 在 Trie 树上构建失败指针（相当于 KMP 中的失效函数 next 数组）。\n\n * 构建失败指针\n   \n   核心思想就跟构造KMP的next数组一样\n   \n   * 初始构建失败指针\n     \n     > 假设，我们要匹配的主串为abcd，模式串分别是 c，bc，bcd，abcd；\n     > \n     > 我们沿 Trie 树走到 p 节点，也就是下图中的紫色节点，那 p 的失败指针就是从 root 走到紫色节点形成的字符串 abc的末尾c，指向跟所有模式串前缀匹配的最长可匹配后缀子串，就是箭头指的 bc 模式串的c。\n     > \n     > 如果我们把树中相同深度的节点放到同一层，那么某个节点的失败指针只有可能出现在它所在层的上层。\n     \n     * 可匹配后缀子串\n       \n       > 字符串 abc 的后缀子串有两个 bc，c，我们拿它们与其他模式串匹配，如果某个后缀子串可以匹配某个模式串的前缀，那我们就把这个后缀子串叫作可匹配后缀子串。\n   \n   * 构建子节点失败指针\n     \n     我们假设节点 p 的失败指针指向节点 q，我们看节点 p 的子节点 pc 对应的字符，是否也可以在节点 q 的子节点中找到。\n     \n     * 同加相等====第一种情况\n       \n       > 如果找到了节点 q 的一个子节点 qc，对应的字符跟节点 pc 对应的字符相同，则将节点 pc 的失败指针指向节点 qc。\n     \n     * 同加不等====第二种情况\n       \n       > 如果节点 q 中没有子节点的字符等于节点 pc 包含的字符，则令 q=q->fail（fail 表示失败指针，这里有没有很像 KMP 算法里求 next 的过程？k=next[k]），继续上面的查找，直到 q 是 root 为止，如果还没有找到相同字符的子节点，就让节点 pc 的失败指针指向 root。\n   \n   * 完整的AC自动机构建\n\n * 如何在AC自动机上匹配主串\n   \n   public void match(char[] text) { // text是主串\n       int n = text.length;\n       AcNode p = root;\n       for (int i = 0; i < n; ++i) {\n           int idx = text[i] - \'a\';\n           //前面的条件是，在匹配模式串时，当前模式串的没有text[i]这个字符，即数组部分存储null指针。\n           //一直找到有text[i]这个字符的模式串，由于是通过失败指针跳跃的，\n           //所以该模式串前面虽然没有匹配过,但是重新匹配的话肯定能匹配上，换而言之就是，它的前面的字符串肯定在主串中出现过，且在其他模式串中匹配过。\n           while (p.children[idx] == null && p != root) {\n               p = p.fail; // 失败指针发挥作用的地方\n           }\n           //找到了有text[i]这个字符模式串\n           p = p.children[idx];\n           if (p == null) p = root; // 如果没有匹配的，从root开始重新匹配\n           \n           AcNode tmp = p;\n           while (tmp != root) { // 打印出可以匹配的模式串\n               if (tmp.isEndingChar == true) {\n                   int pos = i-tmp.length+1;\n                   System.out.println("匹配起始下标" + pos + "; 长度" + tmp.length);\n               }\n               tmp = tmp.fail;\n           }\n       }\n   }\n   \n\n * 性能分析——不准\n   \n   设总的字符串(所有模式串的长度总和)长度为n，要查找的字符串(主串)长度为k，则\n   \n   * 构建AC自动机\n     * 构建Trie树——O(n)\n     * 构建失败指针——O(n)\n   * 匹配主串——O(k)',normalizedContent:'# bf算法\n\n暴力匹配算法——朴素匹配算法\n\n * 主串与模式串\n   \n   在主串中查找模式串\n   \n   > 比方说，我们在字符串 a 中查找字符串 b，那字符串 a 就是主串，字符串 b 就是模式串。\n   > \n   > 我们把主串的长度记作 n，模式串的长度记作 m。因为我们是在主串中查找模式串，所以 n>m。\n\n * 算法优劣\n   \n   * 缺点\n     \n     > bf 算法的时间复杂度很高，是 o(n*m)\n     \n     但在实际的开发中，它却是一个比较常用的字符串匹配算法\n   \n   * 优点\n     \n     > 第一，实际的软件开发中，**大部分情况下，模式串和主串的长度都不会太长。**而且每次模式串与主串中的子串匹配的时候，当中途遇到不能匹配的字符的时候，就可以就停止了，不需要把 m 个字符都比对一下。所以，尽管理论上的最坏情况时间复杂度是 o(n*m)，但是，统计意义上，大部分情况下，算法执行效率要比这个高很多。\n     > \n     > 第二，**朴素字符串匹配算法思想简单，代码实现也非常简单。**简单意味着不容易出错，如果有 bug 也容易暴露和修复。在工程中，在满足性能要求的前提下，简单是首选。\n\n * 所以，在实际的软件开发中，绝大部分情况下，朴素的字符串匹配算法就够用了\n\n\n# rk算法\n\nbf算法的优化版，借助了哈希算法来优化比较\n\n * 算法思想\n   \n   > 我们通过哈希算法对主串中的 n-m+1 个子串分别求哈希值，然后逐个与模式串的哈希值比较大小。\n   > \n   > 如果某个子串的哈希值与模式串相等，那就说明对应的子串和模式串匹配了（这里先不考虑哈希冲突的问题，后面我们会讲到）。因为哈希值是一个数字，数字之间比较是否相等是非常快速的，所以模式串和子串比较的效率就提高了。\n\n * 设计哈希算法1\n   \n   * 进制表示法\n     \n     > 我们假设要匹配的字符串的字符集中只包含 k 个字符，我们可以用一个 k 进制数来表示一个子串，这个 k 进制数转化成十进制数，作为子串的哈希值。\n     > \n     > 使用进制表示法，计算哈希值，不会产生哈希冲突，但是会出现值偏大，大到超出数的范围。\n   \n   * 哈希值计算规律\n     \n     > 相邻子串之间的哈希值的计算公式有一定的关系。\n     > \n     >  * 计算技巧\n     >    \n     >    计算26^(m-1)的时候，可以提前计算好，存入数组中，以下标作为次方数，直接查。\n   \n   * 时间复杂度\n     \n     > 整个 rk 算法包含两部分，计算子串哈希值和模式串哈希值与子串哈希值之间的比较。\n     > \n     > 第一部分，我们前面也分析了，可以通过设计特殊的哈希算法，只需要扫描一遍主串就能计算出所有子串的哈希值了，所以这部分的时间复杂度是 o(n)。\n     > \n     > 模式串哈希值与每个子串哈希值之间的比较的时间复杂度是 o(1)，总共需要比较 n-m+1 个子串的哈希值。所以，这部分的时间复杂度也是 o(n)。\n     > \n     > 所以，rk 算法整体的时间复杂度就是 o(n)。\n\n * 设计哈希算法2\n   \n   > 哈希算法的设计方法有很多，我举一个例子说明一下。\n   > \n   > 假设字符串中只包含 a～z 这 26 个英文字母，那我们每个字母对应一个数字，比如 a 对应 1，b 对应 2，以此类推，z 对应 26。我们可以把字符串中每个字母对应的数字相加，最后得到的和作为哈希值。这种哈希算法产生的哈希值的数据范围就相对要小很多了。\n   \n   * 应对哈希冲突\n     \n     > 之前我们只需要比较一下模式串和子串的哈希值，如果两个值相等，那这个子串就一定可以匹配模式串。但是，当存在哈希冲突的时候，有可能存在这样的情况，子串和模式串的哈希值虽然是相同的，但是两者本身并不匹配。\n     > \n     > **当我们发现一个子串的哈希值跟模式串的哈希值相等的时候，我们只需要再对比一下子串和模式串本身就好了。**当然，如果子串的哈希值与模式串的哈希值不相等，那对应的子串和模式串肯定也是不匹配的，就不需要比对子串和模式串本身了。\n\n * rk算法改进\n   \n   > 改进一：先计算模式串的hash值，记录下来，然后计算每一个子串的hash，计算一次，就对比一次，如果hash值匹配，在全量对比字符串。这样做可以不用关心hash冲突问题。不用事先计算出主串中所有子串的哈希值，计算一次，与模式串比较一次即可。\n   > \n   > 改进二：计算子串hash值的时候只要计算到（n-m）处即可，剩下的子串长度小于模式串，不用计算.\n\n\n# bm算法\n\n * 核心思想\n   \n   > bm 算法核心思想是，利用模式串本身的特点，在模式串中某个字符与主串不能匹配的时候，将模式串往后多滑动几位，以此来减少不必要的字符比较，提高匹配的效率。\n\n * 原理分析\n   \n   bm算法的匹配顺序是==按照模式串下标从大到小的顺序，倒着匹配的==\n   \n   * 坏字符规则\n     \n     从模式串的末尾往前倒着匹配，当发现某个字符没法匹配的时候，我们把这个没有匹配的字符叫作坏字符（主串中的字符）。\n     \n     > 我们拿坏字符 c 在模式串中查找，发现模式串中并不存在这个字符，也就是说，字符 c 与模式串中的任何字符都不可能匹配。这个时候，我们可以将模式串直接往后滑动三位(模式串的长度=2-(-1)=3)，将模式串滑动到 c 后面的位置，再从模式串的末尾字符开始比较。\n     \n     * 移动次数\n       \n       >  1. 当发生不匹配的时候，我们把坏字符==对应==的模式串中的字符下标记作 si\n       > \n       >  2. 如果坏字符在模式串中存在，我们把这个坏字符在模式串中的下标记作xi。如果不存在，我们把 xi 记作 -1。\n       > \n       >  3. 那模式串往后移动的位数就等于 si-xi。\n       > \n       >  4. 如果坏字符在模式串里多处出现，那我们在计算 xi 的时候，选择最靠后的那个，因为这样不会让模式串滑动过多，导致本来可能匹配的情况被滑动略过。\n     \n     * 优缺点\n       \n       * 优点\n         \n         > bm 算法在最好情况下的时间复杂度非常低，是 o(n/m)\n         > \n         > 匹配具有类似特点的模式串和主串的时候，bm 算法非常高效。\n       \n       * 缺点\n         \n         > 根据 si-xi 计算出来的移动位数，有可能是==负数==\n         > \n         > 比如主串是 aaaaaaaaaaaaaaaa，模式串是 baaa。不但不会向后滑动模式串，还有可能倒退。\n         > \n         > 所以，bm 算法还需要用到“好后缀规则”。\n   \n   * 好后缀规则\n     \n     在从后往前匹配的时候，已经匹配到的字符串称为好后缀。\n     \n     * 滑动次数\n       \n       > 在好后缀的后缀子串中，查找最长的、能跟模式串前缀子串匹配的后缀子串；\n       > \n       > 简单来说，就是不要一下子移动一整个模式串的距离，看看模式串的开头部分和好后缀的结尾部分有没有重和的，有重和的就少移动几位。\n       > \n       > {u}为好后缀，{v}为能够匹配上的模式串的前缀子串\n       > \n       > 滑动次数=模式串长度-{v}的长度\n   \n   * 方法选用\n     \n     > 我们可以分别计算好后缀和坏字符往后滑动的位数，然后取两个数中最大的，作为模式串往后滑动的位数。这种处理方法还可以避免我们前面提到的，根据坏字符规则，计算得到的往后滑动的位数，有可能是负数的情况。\n\n * 算法实现\n   \n   * 坏字符规则\n     \n     >  1. 可以预先将模式串中的每个字符及其下标都存储到散列表中。\n     >  2. 计算下标xi的时候，可以直接查表获取，而不用遍历整个字符串。\n     \n     * 提供一种散列表的方法\n       \n       > 使用数组，将模式串单个字符的ascii码值作为数组下标，数组中的值为该字符在模式串中的下标。\n     \n     * 移动次数\n     \n     * bc为散列表数组，i为主串的循环大变量，j为模式串的循环小变量\n   \n   * 好后缀规则\n     \n     * 核心内容\n       \n       * 在模式串中，查找跟好后缀匹配的另一个子串；\n       \n       * 在好后缀的后缀子串中，查找最长的、能跟模式串前缀子串匹配的后缀子串；\n     \n     * 实现方法1\n       \n       对模式串进行预处理,解决第一个核心内容\n       \n       > 因为好后缀子串也是模式串的后缀子串，所以我们可以提前将好后缀子串在模式串中再一次出现的起始下标值记录下来。\n       > \n       > 将该值用suffix数组存储，下标为后缀子串的长度。\n       > \n       >  1. 如果模式串中有多个（大于 1 个）子串跟后缀子串{u}匹配，存储模式串中最靠后的那个子串的起始位置，也就是下标最大的那个子串的起始位置。\n       > \n       >  2. 如果该后缀没有再次出现，则将其设为-1\n       > \n       > 比如说，后缀子串为b的时候，b再次出现在下标2处，且自身长度为1，所以suffix[1]=2\n       > \n       > 后缀子串为cab的时候，cab再次出现在下标0处，且自身长度为3，所以suffix[3]=0\n       > \n       > 后缀子串为bcab的时候，bcab没有再次出现，且自身长度为4，所以suffix[4]=-1\n     \n     * 实现方法2\n       \n       除了 suffix 数组之外，我们还需要另外一个 boolean 类型的 prefix 数组，来记录模式串的后缀子串是否能匹配模式串的前缀子串。\n       \n       如果suffix数组的值为0，则说明该后缀自串也是模式串的前缀子串，prefix记为true；\n     \n     * 移动次数\n       \n       * 好后缀再次出现\n       \n       * 好后缀没有再次出现，但是有好后缀的后缀子串为模式串的前缀子串\n       \n       * 什么都没有\n\n * ==优秀思想总结==\n   \n   代码实现具体细节不重要，重要的是处理问题的思想，及更好的优化思路\n   \n   * 使用散列表提高查找效率\n   * 对于重复的，较难的计算，预处理好，需要的时候直接查(26^(m-1))\n   * 善用之前的信息。\n\n\n# kmp算法\n\n * 核心思想\n   \n   > kmp 算法就是在试图寻找一种规律：在模式串和主串匹配的过程中，当遇到坏字符后，对于已经比对过的好前缀，能否找到一种规律，将模式串一次性滑动很多位？\n   > \n   > 这里一次性移动了2位，2=主串虚线框中第一个a的下标-子串虚线框中第一个a的下标\n\n * 最长可匹配前(后)缀\n   \n   把好前缀的所有后缀子串中，最长的可匹配前缀子串的那个后缀子串，叫作最长可匹配后缀子串；对应的前缀子串，叫作最长可匹配前缀子串。\n   \n   最长子串的计算只有模式串有关，可以提前计算。用==next数组(前缀表)==表示。\n   \n   1. 数组的下标是每个前缀结尾字符下标\n   2. 数组的值是这个前缀的 最长可以匹配前缀子串 的结尾字符下标。\n      * 比如，aba可以匹配到的最长前缀子串为a，其下标为0\n      * abab可以匹配到的最长前缀子串为ab，其下标为1\n   3. ==前缀表是用来回退的，它记录了模式串与主串(文本串)不匹配的时候，模式串应该从哪里开始重新匹配。==\n      * ==next[i]=x表示，s[0:i]具有长度为x+1的完全相同的前缀和后缀。==\n      * 从next+1开始匹配,即前一个最长匹配前缀子串末尾的下一位。next中存的是末尾的下标，所有模式串从next+1开始匹配。\n      * j = next[j - 1] + 1\n      * \n\n * kmp算法代码\n   \n   // a, b分别是主串和模式串；n, m分别是主串和模式串的长度。\n   public static int kmp(char[] a, int n, char[] b, int m) {\n       int[] next = getnexts(b, m);\n       int j = 0;\n       for (int i = 0; i < n; ++i) {\n           while (j > 0 && a[i] != b[j]) { // 一直找到a[i]和b[j]\n               j = next[j - 1] + 1;\n           }\n           //j为一次移动的次数，对于模式串来说，不用从0开始比了，一下子滑动到j处\n           //进入循环的j为坏字符的下标\n           //进入后更新j为最长匹配前缀的末尾字符串的下一位，即next[j-1]+1\n           if (a[i] == b[j]) {\n               ++j;\n           }\n           if (j == m) { // 找到匹配模式串的了\n               return i - m + 1;\n           }\n       }\n       return -1;\n   }\n   \n\n * next数组计算方法\n   \n   * 思想\n     \n     在计算next[i]的时候，利用前面已经计算出的next[0],……next[i-1]。\n     \n     * 第一种情况（加字符相等）\n       \n       > 如在计算上文的next[3]和next[4]那样.以next[3]为例\n       > \n       > next[2]时,整个串为aba,最长前缀子串为a,这个a是下标为0的a，k=0,i=2,next[i]=k==>next[2]=0\n       > \n       > 在计算next[3]时，整个串变为abab,新添加的这个b(下标为3)，刚好等于next[0]时的最长前缀子串的下一个字符b(下标为1)\n       > \n       > 此时，next[3]的最长前缀子串为ab，所以next[1]=next[0]+1=1,k=1,i=3,next[i]=k==>next[3]=1\n     \n     * ==第二种情况(加字符不等)==\n       \n       > 设字符串为abxabcabxabx,共12位，下标从0到11\n       > \n       > 在计算next[10]时，串为abxabcabxab,最长前缀子串为abxab,即k=4,i=10,next[10]=4\n       > \n       > 在计算next[11]时，串为abxabcabxabx,新添加了一个x，而next[10]的最长前缀子串的下一个字符为c，二者不相等，即k=4,i=11,k\'=k+1=5,next[5]!=next[11]，则找一个次长前缀子串，即为abxab的的最长前缀子串，即满足情况1的子串。简单来说，我下一个是x，你去找一个最长前缀子串，下一个也是x，符合情况1，这样可以直接用加法来解决。\n       > \n       > k=next[4]=1,此时最长前缀子串为ab，str[k+1]=str[2]====x====str[i]=str[11],正好满足情况1，此时的最长前缀子串为abx，即next[11]=2。\n   \n   * 代码实现\n     \n     // b表示模式串，m表示模式串的长度\n     private static int[] getnexts(char[] b, int m) {\n         int[] next = new int[m];\n         next[0] = -1;\n         int k = -1;\n         for (int i = 1; i < m; ++i) {\n             //去找满足情况1的子串\n             while (k != -1 && b[k + 1] != b[i]) {\n                 k = next[k];//回溯\n             }\n             //找到了，++\n             if (b[k + 1] == b[i]) {\n                 ++k;\n             }\n             //赋值\n             next[i] = k;\n         }\n         return next;\n     }\n     \n\n * 复杂度分析\n   \n   * 空间复杂度——next数组，o(n)\n   * 时间复杂度\n     * 求next数组——o(m)\n       * 就是大循环的次数，小循序次数肯定小于m，设为k，即总体为o(k*m)\n       * k相对于m来说，值很小，所以总的时间复杂度为o(m)\n     * kmp算法——o(n)，同求next数组\n     * ==总的——o(m+n)==\n\n\n# trie树\n\ntrie 树，也叫“字典树”。它是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。\n\n * 核心思想\n   \n   > trie 树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起。\n   \n   例如，我们有 6 个字符串，它们分别是：how，hi，her，hello，so，see。构造trie树，则如下图所示。从根节点到红色节点的一条路径表示一个字符串（注意：红色节点并不都是叶子节点）——例如hi，hil,i和l两个结点都要标红，而i不是叶子结点。\n\n * 查找\n   \n   * her\n     \n     分解为h,e,r最后找到红色结点，说明是一个完整的字符。\n   \n   * he\n     \n     分解为h,e最后没有找到红色结点，说明是一个完整的字符的字符前缀。\n\n * trie树实现\n   \n   > 假设字符串中只有26个小写字符，**则可以用一个大小为26的数组来存储子节点的指针。**不存在子节点的存null\n   \n   * 数据结构\n     \n     class trienode {\n       char data;\n       trienode children[26];\n     }\n     \n   \n   * 时间复杂度\n     \n     设总的字符串长度为n，要查找的字符串长度为k，则\n     \n     * 构建trim树——o(n)\n     * 构建好树，在树中查字符串——o(k)\n\n * trie树优化\n   \n   * trie树 很耗内存\n     \n     上述说的，使用数组存子节点指针的方式，需要大量的存储空间，而且很多空间都是闲置的。\n   \n   * 改进\n     \n     * 可以将数组，改成散列表，跳表，红黑树等，稍微牺牲一些查询效率，以节约内存\n     * 缩点优化——将斜树的内容放到一个结点，例如l,l,o可以存储在一起，llo，不影响查询\n\n * trie树与散列表、红黑树的比较\n   \n   * 查找定长的字符串\n     \n     > 在一组字符串中，查找一个固定的字符串，trie树虽然效率很高，但是有很多缺点\n     > \n     >  1. 第一，字符串中包含的字符集不能太大。我们前面讲到，如果字符集太大，那存储空间可能就会浪费很多。即便可以优化，但也要付出牺牲查询、插入效率的代价。\n     >  2. 第二，要求字符串的前缀重合比较多，不然空间消耗会变大很多。\n     >  3. 第三，如果要用 trie 树解决问题，那我们就要自己从零开始实现一个 trie 树，还要保证没有 bug，这个在工程上是将简单问题复杂化，除非必须，一般不建议这样做。\n     >  4. 第四，我们知道，通过指针串起来的数据块是不连续的，而 trie 树中用到了指针，所以，对缓存并不友好，性能上会打个折扣。\n     > \n     > 综上，查找固定长度的字符串，trie树不如使用红黑树或者散列表。\n   \n   * 查找字符串前缀匹配的字符串\n     \n     > trie 树最有优势的是查找前缀匹配的字符串。对于这个功能，其他数据结构，爱莫能助\n     \n     * 实现搜索关键词提示功能\n     * 实现代码(命令)自动补全功能\n\n * 利用trie树实现搜索关键词提示功能(最基本的功能实现)\n   \n   深入剖析搜索引擎\n   \n   假设，词库中有hello,her,hi,how,see,so\n   \n   * 输入h，展示hello,her,hi,how\n   * 输入he，展示hello,her\n\n\n# ac自动机\n\nbf 算法、rk 算法、bm 算法、kmp 算法,都是单模式串匹配算法， trie 树，ac自动机都是多模式串匹配算法。\n\nac 自动机是基于 trie 树的一种改进算法，它跟 trie 树的关系，就像单模式串中，kmp 算法与 bf 算法的关系一样。\n\n * 利用tire树实现多模式串匹配\n   \n   前面trie那节介绍的是匹配一个固定长度的字符串问题，很短小，就一个主串匹配一个模式串\n   \n   * 多模式串匹配\n     \n     > 多模式串匹配算法，就是在多个模式串和一个主串之间做匹配，也就是说，在一个主串中查找多个模式串。\n   \n   * 实现流程——借助上图的trie树\n     \n     > 设有超长字符串，作为主串（cheisesa），从第一个字符（c）开始，在 trie 树中匹配。当匹配到 trie 树的叶子节点，或者中途遇到不匹配字符的时候，我们将主串的开始匹配位置后移一位，也就是从字符h开始，重新在 trie 树中匹配(重新从根节点开始搜索)。hei这个字符串可以匹配到，做相应处理。\n     > \n     > 然后继续从s开始匹配,se符合，ses不符合。则继续从e开始匹配，重复此过程。\n   \n   * 缺点\n     \n     每次匹配失败后，从主串当前匹配字符往后移动一位，继续从根节点开始重新匹配，就相当于bf算法在匹配时做法。\n     \n     > 举例：假设匹配abcd\n     > \n     >  * trie树\n     >    \n     >    1. abc没问题,继续匹配，d和e匹配不上\n     >    \n     >    2. 匹配起始结点从a后移一维，重新开始匹配bcd，匹配完成。\n     >    \n     >    3. 分析：b,c进行了重复匹配.在模式串abcd中已经匹配过了bc，在模式串bcd中再次进行了b和c的匹配。\n     > \n     >  * ac自动机\n     >    \n     >    1. abc没问题,继续匹配，d和e匹配不上\n     >    2. 通过c的失败指针指向模式串bcd的c，这个d刚好可以和abcd中d匹配上，如果到了结尾，可以直接输出匹配到了bcd\n     >    3. 分析:在模式串abcd中已经匹配过了bc，在模式串bcd中只匹配了d\n\n * ac自动机核心思想\n   \n   > ac 自动机实际上就是在 trie 树之上，加了类似 kmp 的 next 数组，只不过此处的 next 数组是构建在树上罢了。\n   \n   * 数据结构\n     \n     public class acnode {\n         public char data; \n         public acnode[] children = new acnode[26]; // 字符集只包含a~z这26个字符\n         public boolean isendingchar = false; // 结尾字符为true\n         public int length = -1; // 当isendingchar=true时，记录模式串长度\n         public acnode fail; // 失败指针\n         public acnode(char data) {\n             this.data = data;\n         }\n     }\n     \n   \n   * ac自动机的构建\n     \n     * 将多个模式串构建成 trie 树；构建一个敏感词的trie树。——详见trie树\n     * 在 trie 树上构建失败指针（相当于 kmp 中的失效函数 next 数组）。\n\n * 构建失败指针\n   \n   核心思想就跟构造kmp的next数组一样\n   \n   * 初始构建失败指针\n     \n     > 假设，我们要匹配的主串为abcd，模式串分别是 c，bc，bcd，abcd；\n     > \n     > 我们沿 trie 树走到 p 节点，也就是下图中的紫色节点，那 p 的失败指针就是从 root 走到紫色节点形成的字符串 abc的末尾c，指向跟所有模式串前缀匹配的最长可匹配后缀子串，就是箭头指的 bc 模式串的c。\n     > \n     > 如果我们把树中相同深度的节点放到同一层，那么某个节点的失败指针只有可能出现在它所在层的上层。\n     \n     * 可匹配后缀子串\n       \n       > 字符串 abc 的后缀子串有两个 bc，c，我们拿它们与其他模式串匹配，如果某个后缀子串可以匹配某个模式串的前缀，那我们就把这个后缀子串叫作可匹配后缀子串。\n   \n   * 构建子节点失败指针\n     \n     我们假设节点 p 的失败指针指向节点 q，我们看节点 p 的子节点 pc 对应的字符，是否也可以在节点 q 的子节点中找到。\n     \n     * 同加相等====第一种情况\n       \n       > 如果找到了节点 q 的一个子节点 qc，对应的字符跟节点 pc 对应的字符相同，则将节点 pc 的失败指针指向节点 qc。\n     \n     * 同加不等====第二种情况\n       \n       > 如果节点 q 中没有子节点的字符等于节点 pc 包含的字符，则令 q=q->fail（fail 表示失败指针，这里有没有很像 kmp 算法里求 next 的过程？k=next[k]），继续上面的查找，直到 q 是 root 为止，如果还没有找到相同字符的子节点，就让节点 pc 的失败指针指向 root。\n   \n   * 完整的ac自动机构建\n\n * 如何在ac自动机上匹配主串\n   \n   public void match(char[] text) { // text是主串\n       int n = text.length;\n       acnode p = root;\n       for (int i = 0; i < n; ++i) {\n           int idx = text[i] - \'a\';\n           //前面的条件是，在匹配模式串时，当前模式串的没有text[i]这个字符，即数组部分存储null指针。\n           //一直找到有text[i]这个字符的模式串，由于是通过失败指针跳跃的，\n           //所以该模式串前面虽然没有匹配过,但是重新匹配的话肯定能匹配上，换而言之就是，它的前面的字符串肯定在主串中出现过，且在其他模式串中匹配过。\n           while (p.children[idx] == null && p != root) {\n               p = p.fail; // 失败指针发挥作用的地方\n           }\n           //找到了有text[i]这个字符模式串\n           p = p.children[idx];\n           if (p == null) p = root; // 如果没有匹配的，从root开始重新匹配\n           \n           acnode tmp = p;\n           while (tmp != root) { // 打印出可以匹配的模式串\n               if (tmp.isendingchar == true) {\n                   int pos = i-tmp.length+1;\n                   system.out.println("匹配起始下标" + pos + "; 长度" + tmp.length);\n               }\n               tmp = tmp.fail;\n           }\n       }\n   }\n   \n\n * 性能分析——不准\n   \n   设总的字符串(所有模式串的长度总和)长度为n，要查找的字符串(主串)长度为k，则\n   \n   * 构建ac自动机\n     * 构建trie树——o(n)\n     * 构建失败指针——o(n)\n   * 匹配主串——o(k)',charsets:{cjk:!0}},{title:"二叉树",frontmatter:{autoSort:88,title:"二叉树",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/4d7338/",categories:["算法","树"],tags:["知识","算法"]},regularPath:"/02.%E7%AE%97%E6%B3%95/45.%E6%A0%91/05.%E4%BA%8C%E5%8F%89%E6%A0%91.html",relativePath:"02.算法/45.树/05.二叉树.md",key:"v-43f967eb",path:"/pages/4d7338/",headers:[{level:2,title:"遍历",slug:"遍历",normalizedTitle:"遍历",charIndex:2},{level:2,title:"完全二叉树",slug:"完全二叉树",normalizedTitle:"完全二叉树",charIndex:134},{level:2,title:"二叉查找树",slug:"二叉查找树",normalizedTitle:"二叉查找树",charIndex:450},{level:2,title:"AVL树",slug:"avl树",normalizedTitle:"avl树",charIndex:1776},{level:2,title:"==红黑树==",slug:"红黑树",normalizedTitle:"==红黑树==",charIndex:1826},{level:2,title:"递归树",slug:"递归树",normalizedTitle:"递归树",charIndex:3809}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"遍历 完全二叉树 二叉查找树 AVL树 ==红黑树== 递归树",content:"# 遍历\n\n * 前序遍历——中，左，右\n   \n   ABDGHCEIF\n\n * 中序遍历——左，中，右\n   \n   GDHBAEICF\n\n * 后序遍历——左，右，中\n   \n   GHDBIEFCA\n\n * 层序遍历\n   \n   ABCDEFGHI\n\n\n# 完全二叉树\n\n * 具有n个结点的完全二叉树的深度为 $\\left \\lfloor log_2(n)+1 \\right \\rfloor $\n\n * 数组存储\n   \n   * 一般的树存储用链表居多，完全二叉树当然也可以用链表存储。\n   * 但是由于完全二叉树的性质，用数组存储，更方便，也更节省空间。\n   \n   \n\n * 编号\n   \n   * 若 i=1,则结点i为根结点，若i>1，则其双亲结点是$ \\left \\lfloor i/2 \\right \\rfloor$\n   * 如果2i>n,则结点i无左孩子，否则，其左孩子是结点2i\n   * 如果2i+1>n,则结点i无右孩子，否则，其右孩子是结点2i+1\n\n\n# 二叉查找树\n\n> 二叉查找树最大的特点就是，支持动态数据集合的快速插入、删除、查找操作。\n\n * 要求\n   \n   > 二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值。\n\n * 插入、删除、查找\n   \n   ==略==\n\n * 支持重复数据的二叉查找树\n   \n   * 同值扩容法\n     \n     > 二叉查找树中每一个节点不仅会存储一个数据，因此我们通过链表和支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上。\n   \n   * 相同取大法\n     \n     * 插入\n       \n       > 每个节点仍然只存储一个数据。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树，也就是说，把这个新插入的数据当作大于这个节点的值来处理。\n     \n     * 查找\n       \n       > 当要查找数据的时候，遇到值相同的节点，我们并不停止查找操作，而是继续在右子树中查找，直到遇到叶子节点，才停止。这样就可以把键值等于要查找值的所有节点都找出来。\n     \n     * 删除\n       \n       > 对于删除操作，我们也需要先查找到每个要删除的节点，然后再按前面讲的删除操作的方法，依次删除。\n\n * 与散列表的对比\n   \n   * 二叉查找树的劣势\n     \n     > 散列表的插入、删除、查找操作的时间复杂度可以做到常量级的 O(1)，非常高效。\n     > \n     > 而二叉查找树在比较平衡的情况下，插入、删除、查找操作时间复杂度才是 O(logn)。\n   \n   * 二叉查找树的优势\n     \n     >  1. 散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。\n     >     \n     >     而对于二叉查找树来说，我们只需要中序遍历，就可以在 O(n) 的时间复杂度内，输出有序的数据序列。\n     > \n     >  2. 散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定。\n     >     \n     >     但是在工程中，我们最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在 O(logn)。\n     > \n     >  3. 笼统地来说，尽管散列表的查找等操作的时间复杂度是常量级的，**但因为哈希冲突的存在，这个常量不一定比 logn 小，**所以实际的查找速度可能不一定比 O(logn) 快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。\n     > \n     >  4. **散列表的构造比二叉查找树要复杂，需要考虑的东西很多。**比如散列函数的设计、冲突解决办法、扩容、缩容等。平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定。\n     > \n     >  5. 为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法解决冲突的散列表，不然会浪费一定的存储空间。\n\n\n# AVL树\n\n * 高度平衡的二叉查找树\n * 左子树和右子树的高度相参不会超过1，最大是1\n\n\n# ==红黑树==\n\n * 基础\n   \n   > 红黑树的英文是“Red-Black Tree”，简称 R-B Tree。它是一种不严格的平衡二叉查找树，我前面说了，它的定义是不严格符合平衡二叉查找树的定义的。\n   > \n   > 红黑树中的节点，一类被标记为黑色，一类被标记为红色。\n   \n   * ==红黑树的要求==\n     \n     >  1. 根节点是黑色的；\n     > \n     >  2. 每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据；\n     > \n     >  3. 任何相邻(这里值得是父亲和孩子)的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的(上下分隔)；\n     > \n     >  4. 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；\n\n * 近似平衡特性\n   \n   > 平衡二叉查找树的初衷，是为了解决二叉查找树因为动态更新导致的性能退化问题。所以，“平衡”的意思可以等价为性能不退化。“近似平衡”就等价为性能不会退化得太严重。\n   > \n   > **红黑树的高度稳定地趋近 log2n **\n   \n   * 首先，将红色节点从红黑树中去掉，那单纯包含黑色节点的红黑树的高度是多少呢？\n     \n     > 红色节点删除之后，有些节点就没有父节点了，它们会直接拿这些节点的祖父节点（父节点的父节点）作为父节点。所以，之前的二叉树就变成了四叉树。\n     > \n     > 因为从任意节点到可达的叶子节点的每个路径包含相同数目的黑色节点\n     > \n     > 我们从四叉树中取出某些节点，放到叶节点位置，四叉树就变成了完全二叉树。所以，仅包含黑色节点的四叉树的高度，比包含相同节点个数的完全二叉树的高度还要小。\n     > \n     > ==即黑树高度不超过log~2~n==\n   \n   * 把红色节点加回去，高度会变成多少呢？\n     \n     > 在红黑树中，红色节点不能相邻，也就是说，有一个红色节点就要至少有一个黑色节点，将它跟其他红色节点隔开。\n     > \n     > 红黑树中包含最多黑色节点的路径不会超过 log2n，所以加入红色节点之后，最长路径不会超过 2log2n，也就是说，红黑树的高度近似 2log2n。\n     > \n     > 这样推导出来的结果不够精确，实际上红黑树的性能更好。\n\n * 为什么喜欢红黑树\n   \n   > AVL 树是一种高度平衡的二叉树，所以查找的效率非常高，但是，有利就有弊，AVL 树为了维持这种高度的平衡，就要付出更多的代价。每次插入、删除都要做调整，就比较复杂、耗时。所以，对于有频繁的插入、删除操作的数据集合，使用 AVL 树的代价就有点高了。\n   > \n   > **红黑树只是做到了近似平衡，并不是严格的平衡，所以在维护平衡的成本上，要比 AVL 树要低。**所以，红黑树的插入、删除、查找各种操作性能都比较稳定。\n\n * 红黑树和2-3树的关系\n   \n   * 颜色表示\n     \n     > 因为每个结点都只会有一条指向自己的链接（从它的父结点指向它），我们将链接的颜色保存在表示结点的Node数据类型的布尔变量color中（若指向它的链接是红色的，那么该变量为true，黑色则为false）。\n     > \n     > 当我们提到一个结点颜色时，我们指的是指向该结点的链接的颜色\n   \n   * 关系\n     \n     > 红黑树是从2-3树上演变而来的，具体请了解网址内容。\n     > \n     > ==红黑树就是用红链接表示3-结点的2-3树==\n     > \n     > 如果我们将一颗红黑树中的红链接画平，那么所有的空链接到根结点的距离都将是相同的。如果我们将由红链接相连的结点合并，得到的就是一颗2-3树。\n\n * 红黑树实现\n   \n   > 红黑树实现，其实近似于魔方还原，都有固定步骤，具体实现不要求掌握，能看懂过程就行。\n\n * ==关于红黑树==\n   \n   > **红黑树是一种平衡二叉查找树。**它是为了解决普通二叉查找树在数据更新的过程中，复杂度退化的问题而产生的。\n   > \n   > **红黑树的高度近似 log2n，**所以它是近似平衡，插入、删除、查找操作的时间复杂度都是 O(logn)。\n   > \n   > 因为红黑树是一种性能非常稳定的二叉查找树，所以，在工程中，但凡是用到动态插入、删除、查找数据的场景，都可以用到它。\n   > \n   > 不过，它实现起来比较复杂，如果自己写代码实现，难度会有些高，这个时候，我们其实更倾向用==跳表==来替代它。\n\n\n# 递归树\n\n借助递归树来分析递归算法的时间复杂度\n\n还可以借助递推公式来分析时间复杂度\n\n * 归并排序分析\n   \n   > 归并算法中比较耗时的是归并操作，也就是把两个子数组合并为大数组。从图中我们可以看出，每一层归并操作消耗的时间总和是一样的，跟要排序的数据规模有关。我们把每一层归并操作消耗的时间记作 n。\n   > \n   > 我们只需要知道这棵树的高度 h，用高度 h 乘以每一层的时间消耗 n，就可以得到总的时间复杂度 O(n∗h)。\n   > \n   > 归并排序递归树是一棵满二叉树。我们前两节中讲到，满二叉树的高度大约是 log2n，所以，归并排序递归实现的时间复杂度就是 O(nlogn)。\n\n * 快速排序分析\n   \n   > 快速排序在最好情况下，每次分区都能一分为二，这个时候用递推公式 T(n)=2T(2n)+n，很容易就能推导出时间复杂度是 O(nlogn),就跟归并排序相当。\n   > \n   > 我们假设平均情况下，每次分区之后，两个分区的大小比例为 1:k。递归树图绘制如下：\n   \n   > 快速排序的过程中，每次分区都要遍历待分区区间的所有数据，所以，每一层分区操作所遍历的数据的个数之和就是 n。我们现在只要求出递归树的高度 h，这个快排过程遍历的数据个数就是 h∗n ，也就是说，时间复杂度就是 O(h∗n)。\n   > \n   > 因为每次分区并不是均匀地一分为二，所以递归树并不是满二叉树。这样一个递归树的高度是多少呢？\n   > \n   > 我们知道，快速排序结束的条件就是待排序的小区间，大小为 1，也就是说叶子节点里的数据规模是 1。从根节点 n 到叶子节点 1，递归树中最短的一个路径每次都乘以 1/10，最长的一个路径每次都乘以 9/10。通过计算，我们可以得到，从根节点到叶子节点的最短路径是 log~10~n，最长的路径是 log~10/9~n。\n   > \n   > 所以，遍历数据的个数总和就介于 nlog10n 和 nlog910n 之间。根据复杂度的大 O 表示法，对数复杂度的底数不管是多少，我们统一写成 logn，所以，当分区大小比例是 1:9 时，快速排序的时间复杂度仍然是 O(nlogn)。\n\n * 斐波那契数列分析\n   \n   > f(n) 分解为 f(n−1) 和 f(n−2)，每次数据规模都是 −1 或者 −2，叶子节点的数据规模是 1 或者 2。所以，从根节点走到叶子节点，每条路径是长短不一的。如果每次都是 −1，那最长路径大约就是 n；如果每次都是 −2，那最短路径大约就是 n/2。\n   > \n   > 每次分解之后的合并操作只需要一次加法运算，我们把这次加法运算的时间消耗记作 1。所以，从上往下，第一层的总时间消耗是 1，第二层的总时间消耗是 2，第三层的总时间消耗就是 22。**依次类推，第 k 层的时间消耗就是 2k−1，**那整个算法的总的时间消耗就是每一层时间消耗之和。\n   > \n   > 如果路径长度都为 n，那这个总和就是 2^n^-1\n   > \n   > 如果路径长度都是 n/2 ，那整个算法的总的时间消耗就是 2^n/2^−1。\n   > \n   > 这个算法的时间复杂度就介于 O(2^n^) 和 O(2^n/2^) 之间。虽然这样得到的结果还不够精确，只是一个范围，但是我们也基本上知道了上面算法的时间复杂度是指数级的，非常高。",normalizedContent:"# 遍历\n\n * 前序遍历——中，左，右\n   \n   abdghceif\n\n * 中序遍历——左，中，右\n   \n   gdhbaeicf\n\n * 后序遍历——左，右，中\n   \n   ghdbiefca\n\n * 层序遍历\n   \n   abcdefghi\n\n\n# 完全二叉树\n\n * 具有n个结点的完全二叉树的深度为 $\\left \\lfloor log_2(n)+1 \\right \\rfloor $\n\n * 数组存储\n   \n   * 一般的树存储用链表居多，完全二叉树当然也可以用链表存储。\n   * 但是由于完全二叉树的性质，用数组存储，更方便，也更节省空间。\n   \n   \n\n * 编号\n   \n   * 若 i=1,则结点i为根结点，若i>1，则其双亲结点是$ \\left \\lfloor i/2 \\right \\rfloor$\n   * 如果2i>n,则结点i无左孩子，否则，其左孩子是结点2i\n   * 如果2i+1>n,则结点i无右孩子，否则，其右孩子是结点2i+1\n\n\n# 二叉查找树\n\n> 二叉查找树最大的特点就是，支持动态数据集合的快速插入、删除、查找操作。\n\n * 要求\n   \n   > 二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值。\n\n * 插入、删除、查找\n   \n   ==略==\n\n * 支持重复数据的二叉查找树\n   \n   * 同值扩容法\n     \n     > 二叉查找树中每一个节点不仅会存储一个数据，因此我们通过链表和支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上。\n   \n   * 相同取大法\n     \n     * 插入\n       \n       > 每个节点仍然只存储一个数据。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树，也就是说，把这个新插入的数据当作大于这个节点的值来处理。\n     \n     * 查找\n       \n       > 当要查找数据的时候，遇到值相同的节点，我们并不停止查找操作，而是继续在右子树中查找，直到遇到叶子节点，才停止。这样就可以把键值等于要查找值的所有节点都找出来。\n     \n     * 删除\n       \n       > 对于删除操作，我们也需要先查找到每个要删除的节点，然后再按前面讲的删除操作的方法，依次删除。\n\n * 与散列表的对比\n   \n   * 二叉查找树的劣势\n     \n     > 散列表的插入、删除、查找操作的时间复杂度可以做到常量级的 o(1)，非常高效。\n     > \n     > 而二叉查找树在比较平衡的情况下，插入、删除、查找操作时间复杂度才是 o(logn)。\n   \n   * 二叉查找树的优势\n     \n     >  1. 散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。\n     >     \n     >     而对于二叉查找树来说，我们只需要中序遍历，就可以在 o(n) 的时间复杂度内，输出有序的数据序列。\n     > \n     >  2. 散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定。\n     >     \n     >     但是在工程中，我们最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在 o(logn)。\n     > \n     >  3. 笼统地来说，尽管散列表的查找等操作的时间复杂度是常量级的，**但因为哈希冲突的存在，这个常量不一定比 logn 小，**所以实际的查找速度可能不一定比 o(logn) 快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。\n     > \n     >  4. **散列表的构造比二叉查找树要复杂，需要考虑的东西很多。**比如散列函数的设计、冲突解决办法、扩容、缩容等。平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定。\n     > \n     >  5. 为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法解决冲突的散列表，不然会浪费一定的存储空间。\n\n\n# avl树\n\n * 高度平衡的二叉查找树\n * 左子树和右子树的高度相参不会超过1，最大是1\n\n\n# ==红黑树==\n\n * 基础\n   \n   > 红黑树的英文是“red-black tree”，简称 r-b tree。它是一种不严格的平衡二叉查找树，我前面说了，它的定义是不严格符合平衡二叉查找树的定义的。\n   > \n   > 红黑树中的节点，一类被标记为黑色，一类被标记为红色。\n   \n   * ==红黑树的要求==\n     \n     >  1. 根节点是黑色的；\n     > \n     >  2. 每个叶子节点都是黑色的空节点（nil），也就是说，叶子节点不存储数据；\n     > \n     >  3. 任何相邻(这里值得是父亲和孩子)的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的(上下分隔)；\n     > \n     >  4. 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；\n\n * 近似平衡特性\n   \n   > 平衡二叉查找树的初衷，是为了解决二叉查找树因为动态更新导致的性能退化问题。所以，“平衡”的意思可以等价为性能不退化。“近似平衡”就等价为性能不会退化得太严重。\n   > \n   > **红黑树的高度稳定地趋近 log2n **\n   \n   * 首先，将红色节点从红黑树中去掉，那单纯包含黑色节点的红黑树的高度是多少呢？\n     \n     > 红色节点删除之后，有些节点就没有父节点了，它们会直接拿这些节点的祖父节点（父节点的父节点）作为父节点。所以，之前的二叉树就变成了四叉树。\n     > \n     > 因为从任意节点到可达的叶子节点的每个路径包含相同数目的黑色节点\n     > \n     > 我们从四叉树中取出某些节点，放到叶节点位置，四叉树就变成了完全二叉树。所以，仅包含黑色节点的四叉树的高度，比包含相同节点个数的完全二叉树的高度还要小。\n     > \n     > ==即黑树高度不超过log~2~n==\n   \n   * 把红色节点加回去，高度会变成多少呢？\n     \n     > 在红黑树中，红色节点不能相邻，也就是说，有一个红色节点就要至少有一个黑色节点，将它跟其他红色节点隔开。\n     > \n     > 红黑树中包含最多黑色节点的路径不会超过 log2n，所以加入红色节点之后，最长路径不会超过 2log2n，也就是说，红黑树的高度近似 2log2n。\n     > \n     > 这样推导出来的结果不够精确，实际上红黑树的性能更好。\n\n * 为什么喜欢红黑树\n   \n   > avl 树是一种高度平衡的二叉树，所以查找的效率非常高，但是，有利就有弊，avl 树为了维持这种高度的平衡，就要付出更多的代价。每次插入、删除都要做调整，就比较复杂、耗时。所以，对于有频繁的插入、删除操作的数据集合，使用 avl 树的代价就有点高了。\n   > \n   > **红黑树只是做到了近似平衡，并不是严格的平衡，所以在维护平衡的成本上，要比 avl 树要低。**所以，红黑树的插入、删除、查找各种操作性能都比较稳定。\n\n * 红黑树和2-3树的关系\n   \n   * 颜色表示\n     \n     > 因为每个结点都只会有一条指向自己的链接（从它的父结点指向它），我们将链接的颜色保存在表示结点的node数据类型的布尔变量color中（若指向它的链接是红色的，那么该变量为true，黑色则为false）。\n     > \n     > 当我们提到一个结点颜色时，我们指的是指向该结点的链接的颜色\n   \n   * 关系\n     \n     > 红黑树是从2-3树上演变而来的，具体请了解网址内容。\n     > \n     > ==红黑树就是用红链接表示3-结点的2-3树==\n     > \n     > 如果我们将一颗红黑树中的红链接画平，那么所有的空链接到根结点的距离都将是相同的。如果我们将由红链接相连的结点合并，得到的就是一颗2-3树。\n\n * 红黑树实现\n   \n   > 红黑树实现，其实近似于魔方还原，都有固定步骤，具体实现不要求掌握，能看懂过程就行。\n\n * ==关于红黑树==\n   \n   > **红黑树是一种平衡二叉查找树。**它是为了解决普通二叉查找树在数据更新的过程中，复杂度退化的问题而产生的。\n   > \n   > **红黑树的高度近似 log2n，**所以它是近似平衡，插入、删除、查找操作的时间复杂度都是 o(logn)。\n   > \n   > 因为红黑树是一种性能非常稳定的二叉查找树，所以，在工程中，但凡是用到动态插入、删除、查找数据的场景，都可以用到它。\n   > \n   > 不过，它实现起来比较复杂，如果自己写代码实现，难度会有些高，这个时候，我们其实更倾向用==跳表==来替代它。\n\n\n# 递归树\n\n借助递归树来分析递归算法的时间复杂度\n\n还可以借助递推公式来分析时间复杂度\n\n * 归并排序分析\n   \n   > 归并算法中比较耗时的是归并操作，也就是把两个子数组合并为大数组。从图中我们可以看出，每一层归并操作消耗的时间总和是一样的，跟要排序的数据规模有关。我们把每一层归并操作消耗的时间记作 n。\n   > \n   > 我们只需要知道这棵树的高度 h，用高度 h 乘以每一层的时间消耗 n，就可以得到总的时间复杂度 o(n∗h)。\n   > \n   > 归并排序递归树是一棵满二叉树。我们前两节中讲到，满二叉树的高度大约是 log2n，所以，归并排序递归实现的时间复杂度就是 o(nlogn)。\n\n * 快速排序分析\n   \n   > 快速排序在最好情况下，每次分区都能一分为二，这个时候用递推公式 t(n)=2t(2n)+n，很容易就能推导出时间复杂度是 o(nlogn),就跟归并排序相当。\n   > \n   > 我们假设平均情况下，每次分区之后，两个分区的大小比例为 1:k。递归树图绘制如下：\n   \n   > 快速排序的过程中，每次分区都要遍历待分区区间的所有数据，所以，每一层分区操作所遍历的数据的个数之和就是 n。我们现在只要求出递归树的高度 h，这个快排过程遍历的数据个数就是 h∗n ，也就是说，时间复杂度就是 o(h∗n)。\n   > \n   > 因为每次分区并不是均匀地一分为二，所以递归树并不是满二叉树。这样一个递归树的高度是多少呢？\n   > \n   > 我们知道，快速排序结束的条件就是待排序的小区间，大小为 1，也就是说叶子节点里的数据规模是 1。从根节点 n 到叶子节点 1，递归树中最短的一个路径每次都乘以 1/10，最长的一个路径每次都乘以 9/10。通过计算，我们可以得到，从根节点到叶子节点的最短路径是 log~10~n，最长的路径是 log~10/9~n。\n   > \n   > 所以，遍历数据的个数总和就介于 nlog10n 和 nlog910n 之间。根据复杂度的大 o 表示法，对数复杂度的底数不管是多少，我们统一写成 logn，所以，当分区大小比例是 1:9 时，快速排序的时间复杂度仍然是 o(nlogn)。\n\n * 斐波那契数列分析\n   \n   > f(n) 分解为 f(n−1) 和 f(n−2)，每次数据规模都是 −1 或者 −2，叶子节点的数据规模是 1 或者 2。所以，从根节点走到叶子节点，每条路径是长短不一的。如果每次都是 −1，那最长路径大约就是 n；如果每次都是 −2，那最短路径大约就是 n/2。\n   > \n   > 每次分解之后的合并操作只需要一次加法运算，我们把这次加法运算的时间消耗记作 1。所以，从上往下，第一层的总时间消耗是 1，第二层的总时间消耗是 2，第三层的总时间消耗就是 22。**依次类推，第 k 层的时间消耗就是 2k−1，**那整个算法的总的时间消耗就是每一层时间消耗之和。\n   > \n   > 如果路径长度都为 n，那这个总和就是 2^n^-1\n   > \n   > 如果路径长度都是 n/2 ，那整个算法的总的时间消耗就是 2^n/2^−1。\n   > \n   > 这个算法的时间复杂度就介于 o(2^n^) 和 o(2^n/2^) 之间。虽然这样得到的结果还不够精确，只是一个范围，但是我们也基本上知道了上面算法的时间复杂度是指数级的，非常高。",charsets:{cjk:!0}},{title:"排序",frontmatter:{autoSort:93,title:"排序",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/60a4a5/",categories:["算法","排序"],tags:["知识","算法"]},regularPath:"/02.%E7%AE%97%E6%B3%95/60.%E6%8E%92%E5%BA%8F/05.%E6%8E%92%E5%BA%8F.html",relativePath:"02.算法/60.排序/05.排序.md",key:"v-0774325c",path:"/pages/60a4a5/",headers:[{level:2,title:"基础知识",slug:"基础知识",normalizedTitle:"基础知识",charIndex:2},{level:3,title:"分析排序算法",slug:"分析排序算法",normalizedTitle:"分析排序算法",charIndex:11},{level:3,title:"有序度-逆序度",slug:"有序度-逆序度",normalizedTitle:"有序度-逆序度",charIndex:628},{level:2,title:"基于比较的排序",slug:"基于比较的排序",normalizedTitle:"基于比较的排序",charIndex:911},{level:3,title:"冒泡排序-O(n^2^)",slug:"冒泡排序-o-n-2",normalizedTitle:"冒泡排序-o(n^2^)",charIndex:923},{level:3,title:"快速排序-O(nlogn)",slug:"快速排序-o-nlogn",normalizedTitle:"快速排序-o(nlogn)",charIndex:1719},{level:3,title:"插入排序-O(n^2^)",slug:"插入排序-o-n-2",normalizedTitle:"插入排序-o(n^2^)",charIndex:3411},{level:3,title:"希尔排序-O(nlogn--n^2^)",slug:"希尔排序-o-nlogn-n-2",normalizedTitle:"希尔排序-o(nlogn--n^2^)",charIndex:4929},{level:3,title:"选择排序-O(n^2^)",slug:"选择排序-o-n-2",normalizedTitle:"选择排序-o(n^2^)",charIndex:6212},{level:3,title:"归并排序-O(nlogn)",slug:"归并排序-o-nlogn",normalizedTitle:"归并排序-o(nlogn)",charIndex:7107},{level:3,title:"==练习题==",slug:"练习题",normalizedTitle:"==练习题==",charIndex:9013},{level:2,title:"不基于比较的排序",slug:"不基于比较的排序",normalizedTitle:"不基于比较的排序",charIndex:9638},{level:3,title:"桶排序-O(n)",slug:"桶排序-o-n",normalizedTitle:"桶排序-o(n)",charIndex:9651},{level:3,title:"计数排序-O(n)",slug:"计数排序-o-n",normalizedTitle:"计数排序-o(n)",charIndex:12400},{level:3,title:"基数排序-O(n)",slug:"基数排序-o-n",normalizedTitle:"基数排序-o(n)",charIndex:14477}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"基础知识 分析排序算法 有序度-逆序度 基于比较的排序 冒泡排序-O(n^2^) 快速排序-O(nlogn) 插入排序-O(n^2^) 希尔排序-O(nlogn--n^2^) 选择排序-O(n^2^) 归并排序-O(nlogn) ==练习题== 不基于比较的排序 桶排序-O(n) 计数排序-O(n) 基数排序-O(n)",content:"# 基础知识\n\n\n# 分析排序算法\n\n * 排序算法的执行效率\n   \n   * 最好情况、最坏情况、平均情况时间复杂度\n   * 时间复杂度的系数、常数 、低阶\n     * 在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来。\n   * 比较次数和交换（或移动）次数\n     * 虽然冒泡、插入都是O(n^2^)，但是插入排序移动次数是少的，所有性能要优\n\n * 排序算法的内存消耗\n   \n   * 类似于空间复杂度\n   * 新概念--原地排序\n     * 指的是，空间复杂度为O(1)的排序算法\n\n * 排序算法的==稳定性==\n   \n   这个概念是说，如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的==先后顺序不变==\n   \n   * 说明\n     \n     * 平时练习排序的时候都是利用整数来练习，所以稳定与否看不出来利弊，但是当我们对一个对象进行排序时，对象会有多个属性，这时稳定性就有很大作用了。\n   \n   * 订单排序\n     \n     * 要求——我们希望按照金额从小到大对订单数据排序。对于金额相同的订单，我们希望按照下单时间从早到晚有序。\n     * 实现\n       * 先按照下单时间给订单排序\n       * 再按照订单金额重新排序\n     * 这样，如果算法稳定的话，之前按时间排好序的订单在按照金额排序时，顺序不会发生变化。\n\n\n# 有序度-逆序度\n\n * 有序度\n   \n   有序元素对：a[i] <= a[j], 如果i < j。\n   \n   \n   \n   * 1,2,3,4,5,6\n     * 有序度为15，即n*(n-1)/2\n     * 这种完全有序的数组的有序度叫作满有序度\n\n * 逆序度\n   \n   * 逆序度与有序度定义相反\n   * 逆序度 = 满有序度 - 有序度\n\n * 例子——冒泡排序\n   \n   * 排序时，每交换一次，就会增加一个有序度，直到最后完全有序。\n   * 所以总的交换次数是逆序度= 满有序度(n*(n-1)/2) - 初始有序度\n\n\n# 基于比较的排序\n\n\n# 冒泡排序-O(n^2^)\n\n * 算法原理\n   \n   > 从底部开始遍历，依次比较相邻元素的大小，如果下方元素大，则交换。这样一轮过后，最上方的元素肯定是最大的。在继续进行n-1轮就可以啦。\n\n * 算法性能\n   \n   * 空间复杂度——O(1)\n   * 时间复杂度(最好、最坏、平均)——O(n)、O(n^2^)、O(n^2^)\n   * 稳定\n   * 交换次数较多\n\n * 代码\n   \n   //冒泡排序\n   public void bubbleSort(int[] nums) {\n       int len = nums.length;\n       if (len <= 1) {\n           return;\n       }\n       //外层循环表示已经排序好的元素块\n       for (int i = 0; i < len; i++) {\n           boolean flag = false;//设置判断变量\n           //内层循环表示从无序块中选拔一个最大的元素进入有序块中\n           for (int j = 0;j < len - i -1; j++) {\n               if (nums[j] > nums[j+1]) {\n                   swap(nums, j,j+1);\n                   flag = true;\n               }\n           }\n           //当这一轮排序没有元素交换时，表示数组已经有序，不需要在进行排序了\n           if (!flag) {\n               break;\n           }\n       }\n   }\n   \n\n\n# 快速排序-O(nlogn)\n\n * 算法原理\n   \n   > 将大的序列不断的进行分区排序。\n   > \n   > 分区时，找到一个中间值，使其左边都小于它，右边都大于它。不断的减小序列，直到最后剩下一个时，即截止。\n   \n   \n\n * 算法性能\n   \n   * 空间复杂度——O(1)\n   * 时间复杂度(最好、最坏、平均)——O(nlogn)、O(n^2^)、O(nlogn)\n   * 不稳定\n\n * 代码\n   \n   //快速排序\n   public void quickSort(int[] nums) {\n       int len = nums.length;\n       Qsort(nums, 0, len-1);\n   }\n   //递归函数\n   public void Qsort(int[] nums, int low, int high) {\n       //递归终止条件\n       if (low >= high) {\n           return;\n       }\n       int mid = partition(nums, low, high);//找中枢\n       Qsort(nums, low, mid-1);//排中枢左边\n       Qsort(nums, mid+1, high);//排中枢右边\n   \n   }\n   //找中间点-空间复杂度为1，直接在原数组上修改\n   // 双边循环\n   public int partition(int[] nums, int low, int high) {\n       //三数取中，作为中枢值；将中间的放入low位置\n       int mid = low + (high - low) / 2;\n       if (nums[low] > nums[high]) {\n           swap(nums, low, high);\n       }\n       if (nums[mid] > nums[high]) {\n           swap(nums, mid, high);\n       }\n       if (nums[mid] > nums[low]) {\n           swap(nums, mid, low);\n       }\n       int value = nums[low];//默认选取第一个为中枢值--这里用三数取中进行了优化，\n   \n       while (low < high) {\n           //从右往左，大于中枢值的不动，小于的移动到low位\n           while (high > low && nums[high] >= value) {\n               high--;\n           }\n           nums[low] = nums[high];\n           //从左往右，小于中枢值的不动，大于的移动到high位\n           while (low < high && nums[low] <= value) {\n               low ++;\n           }\n           nums[high] = nums[low];\n       }\n       //循环结束后，low和high指向一个位置，将该位置设为中枢值\n       nums[low] = value;\n       return low;\n   }\n   \n\n * 如何在O(n)内查找数组内第K大元素\n   \n   > 利用快速排序中的分区思想，设中枢下标为x，若x+1=k，则说明x指向的值即为第k大值\n   > \n   > 若`k<x+1`，则将x左半部分进行分区；若大于，则将x右半部分进行分区。\n   > \n   > \n   > 知道最后找到 k=x+1 为止。\n\n\n# 插入排序-O(n^2^)\n\n * 算法原理\n   \n   > 在前方构造一个有序元素块，不断的取出后方的无序块中第一个元素，插入到有序块中。\n   > \n   > （通过不断的移动和交换，确保有序块的有序性）\n\n * 算法性能\n   \n   * 空间复杂度——O(1)\n   * 时间复杂度(最好、最坏、平均)——O(n)、O(n^2^)、O(n^2^)\n   * 稳定\n   * 交换次数较少\n\n * 代码\n   \n   //直接插入排序\n   public void insertionSort(int[] nums) {\n       int len = nums.length;\n       if (len <= 1) {\n           return;\n       }\n       //外层循环表示已经排序好的元素块\n       for (int i = 1; i < len; i++) {\n           //内层循环表示 无序元素块中的第一个(它-nums[i]=nums[j]) 在有序块中的排序过程\n           //如果它比前面的小，就交换，否则就退出循环\n           for (int j = i ; j > 0; j--) {\n               if (nums[j] < nums[j-1]) {\n                   swap(nums,j,j-1);\n               }else {\n                   break;\n               }\n           }\n       }\n   }\n   \n   \n   \n   //直接插入排序1----性能更好，使用了移动而非交换，交换相当于多次移动\n   \n       public void insertionSort1(int[] nums) {\n           int len = nums.length;\n           if (len <= 1) {\n               return;\n           }\n           int value;\n           int j;\n           //外层循环表示已经排序好的元素块\n           for (int i = 1; i < len; i++) {\n               value = nums[i];\n   \n               //内层循环表示无序 元素块中的第一个(它-nums[i]=value) 在有序块中的排序过程\n               //如果它比左面小，左后的元素右移，直到不小或者到了数组开头，在将它插入到合适的位置\n               for (j = i-1 ; j >= 0; j--) {\n                   if (value < nums[j]) {\n                       //元素右移，覆盖掉待入队的值\n                       nums[j+1] = nums[j];\n                   }else {\n                       break;\n                   }\n               }\n               nums[j+1] = value;\n           }\n       }\n   \n\n\n# 希尔排序-O(nlogn--n^2^)\n\n * 算法原理\n   \n   > 基于插入排序；将原序列，按照跳跃分割的策略，分成若干个子序列。然后在这些子序列内进行，直接插入排序，可以保证序列基本有序，当增量为1时，最后一轮排序，可保证完全有序。\n   > \n   > 跳跃分割策略：将相距某个增量的记录组成一个子序列，这样能保证子序列内分别进行直接插入排序后得到的结果是基本有序，而不是局部有序。\n\n * 算法性能\n   \n   * 空间复杂度——O(1)\n   * 时间复杂度(最好、最坏、平均)——O()、O(n^2^)、O(nlogn——n^2^)\n   * 不稳定——跳跃式排序\n\n * 代码\n   \n   * 增量为1的时候，就退化成了直接插入排序。\n   * 增量的选取是一个非常难的问题。\n   \n   //希尔排序--- 将数组按照某个增量跳跃式分割\n   public void shellSort(int[] nums) {\n       int len = nums.length;\n       if (len <= 1) {\n           return;\n       }\n       int value;//用来记录无序元素块中的第一个\n       int j;\n       int increment=len; //增量表示,开始时设为数组长度\n   \n       while (increment > 1) {\n           increment = (increment/3) + 1;\n           //外层循环表示已经排序好的元素块\n           for (int i = increment; i < len; i++) {\n               value = nums[i];\n   \n               //内层循环表示 无序元素块中的第一个(它） 在有序块中的排序过程\n               //如果它比左面小，左后的元素右移，直到不小或者到了数组开头，在将它插入到合适的位置\n               for (j = i - increment; j >= 0; j = j - increment) {\n                   if (value < nums[j]) {\n                       //元素右移，覆盖掉待入队的值\n                       nums[j + increment] = nums[j];\n                   }else {\n                       break;\n                   }\n               }\n               nums[j + increment] = value;\n           }\n   \n       }\n   \n   }\n   \n   \n\n\n# 选择排序-O(n^2^)\n\n * 算法原理\n   \n   > 将整个序列分已排序区间和未排序区间。选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。\n   > \n   > 极大减少了交换的次数。\n\n * 算法性能\n   \n   * 空间复杂度——O(1)\n   * 时间复杂度(最好、最坏、平均)——O(n^2^)、O(n^2^)、O(n^2^)\n   * 不稳定\n   * 交换次数极少\n\n * 代码\n   \n   //选择排序\n   public void selectSort(int[] nums) {\n       int len = nums.length;\n       if (len <= 1) {\n           return;\n       }\n       int min;//存储最小值\n       int minIndex;//存储最小值下标\n   \n       //外层循环表示已经排序好的元素块\n       for (int i = 0; i < len; i++) {\n           minIndex = i;\n           min = nums[i];\n           //内层循环表示 从无序元素块中找到一个最小的 ，然后将它放到有序元素块的末尾 的过程\n           for (int j = i + 1; j < len; j++) {\n               if (nums[j] < min) {\n                   min = nums[j];\n                   minIndex = j;\n               }\n           }\n           //相等说明无序块中的第一个就是最小的，不用交换\n           if (minIndex != i) {\n               swap(nums,i,minIndex);\n           }\n       }\n   }\n   \n\n\n# 归并排序-O(nlogn)\n\n * 算法原理\n   \n   > 我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。\n\n * 算法性能\n   \n   * 空间复杂度——O(n)\n   * 时间复杂度——突出一个稳定——(最好、最坏、平均)——O(nlogn)、O(nlogn)、O(nlogn)\n   * 稳定排序\n\n * 程序\n   \n   * int mid = low + (high - low) / 2-----这样写是为了防止二者和过大导致溢出\n   * 这里的等号表示稳定性---让左边的优先 if (nums[p1] <= nums[p2]) {\n   \n   //归并排序\n   public void mergeSort(int[] nums) {\n       //统一函数入口\n       int len = nums.length;\n       Msort(nums,0,len-1);\n   }\n   \n   //递归函数\n   public void Msort(int[] nums, int low, int high) {\n       //先定义递归终止条件\n       if (low >= high) {//这里写成== 无可厚非\n           return;\n       }\n       //计算中值\n       //mid=(low+high)/2-----下面的写法是为了防止二者和过大导致溢出\n       int mid = low + (high - low) / 2;\n   \n       Msort(nums,low,mid); //分左边\n       Msort(nums,mid+1,high);//分右边\n       Merge(nums,low,mid,high);//合并左右两边\n   \n   }\n   \n   //合并\n   public void Merge(int[] nums, int low, int mid, int high){\n       int len = high - low + 1;//要合并的数组长度\n       int[] temps = new int[len];//申请新的数组，用来排序\n       int p1 = low;//保留初始下标，最后将新数组，并到原数组中\n       int p2 = mid + 1;\n       int flag = -1;\n       int i;\n       //开始合并--直到有一边全部放入新数组\n       for (i = 0; i < len; i ++) {\n           //这里的等号表示稳定性---让左边的优先\n           if (nums[p1] <= nums[p2]) {\n               temps[i] = nums[p1++];\n               if (p1 > mid) {\n                   flag = 0;\n                   break;\n               }\n           }else {\n               temps[i] = nums[p2++];\n               if (p2 > high) {\n                   flag = 1;\n                   break;\n               }\n           }\n       }\n       //将剩余数组的所有元素，依次写入即可\n       for (i = i + 1;i < len; i++) {\n           if (flag == 0) {//左边写完了，这次写右边剩余部分\n               temps[i] = nums[p2++];\n           }else {\n               temps[i] = nums[p1++];\n           }\n       }\n       //将排好序的数组 写入到原数组对应位置\n       for (i = 0; i < len; i++) {\n           nums[low + i] = temps[i];\n       }\n   }\n   \n\n\n# ==练习题==\n\n * 问题\n\n> 现在你有 10 个接口访问日志文件，每个日志文件大小约 300MB，每个文件里的日志都是按照时间戳从小到大排序的。你希望将这 10 个较小的日志文件，合并为 1 个日志文件，合并之后的日志仍然按照时间戳从小到大排列。如果处理上述排序任务的机器内存只有 1GB，你有什么好的解决思路，能“快速”地将这 10 个日志文件合并吗？\n\n * 解答\n\n> 1.申请10个40M的数组和一个400M的数组。\n> \n> 2.每个文件都读40M，取各数组中最大时间戳中的最小值。\n> \n> 3.然后利用二分查找，在其他数组中快速定位到小于/等于该时间戳的位置，并做标记。\n> \n> 4.再把各数组中标记位置之前的数据全部放在申请的400M内存中。\n> \n> 5.在原来的40M数组中清除已参加排序的数据。[可优化成不挪动数据，只是用两个索引标记有效数据的起始和截止位置]\n> \n> 6.对400M内存中的有效数据[没装满]做快排。 将排好序的直接写文件。\n> \n> 7.再把每个数组尽量填充满。\n> \n> 从第2步开始继续，直到各个文件都读区完毕。\n> \n> 这么做的好处有： 1.每个文件的内容只读区一次，且是批量读区。比每次只取一条快得多。 2.充分利用了读区到内存中的数据。曹源 同学在文件中查找那个中间数是会比较困难的。 3.每个拷贝到400M大数组中参加快排的数据都被写到了文件中，这样每个数只参加了一次快排。\n\n\n# 不基于比较的排序\n\n\n# 桶排序-O(n)\n\n * 算法\n   \n   > 核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。\n\n * 时间复杂度分析\n   \n   > 如果要排序的数据有 n 个，我们把它们均匀地划分到 m 个桶内，每个桶里就有 k=n/m 个元素。每个桶内部使用快速排序，时间复杂度为 O(k * logk)。\n   > \n   > m 个桶排序的时间复杂度就是 O(m * k * logk)，因为 k=n/m，所以整个桶排序的时间复杂度就是 O(n*log(n/m))。\n   > \n   > 当桶的个数 m 接近数据个数 n 时，log(n/m) 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 O(n)。\n\n * 应用场景\n   \n   > 桶排序对要排序数据的要求是非常苛刻的。\n   > \n   > 首先，要排序的数据需要很容易就能划分成 m 个桶，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。\n   > \n   > 其次，数据在各个桶之间的分布是比较均匀的。在极端情况下，如果数据都被划分到一个桶里，那就退化为 O(nlogn) 的排序算法了。\n   > \n   > 桶排序比较适合用在外部排序中。\n\n * 代码\n   \n   //桶排序\n   public void bucketSort(int[] nums) {\n       int index = 0;\n       //根据数据情况，划分为 5 个桶 ，每个桶用链表实现,将5个桶放到List集合中\n       LinkedList<Integer> bucket1 = new LinkedList<>();\n       LinkedList<Integer> bucket2 = new LinkedList<>();\n       LinkedList<Integer> bucket3 = new LinkedList<>();\n       LinkedList<Integer> bucket4 = new LinkedList<>();\n       LinkedList<Integer> bucket5 = new LinkedList<>();\n   \n       List<LinkedList<Integer>> bucket = new LinkedList<>();\n       bucket.add(bucket1);\n       bucket.add(bucket2);\n       bucket.add(bucket3);\n       bucket.add(bucket4);\n       bucket.add(bucket5);\n   \n       //遍历元素，放入桶中\n       for (int num : nums) {\n           if (num >= 0 && num <=9) {\n               bucket1.add(num);\n           }else if (num >= 10 && num <= 19) {\n               bucket2.add(num);\n           }else if (num >= 20 && num <= 29) {\n               bucket3.add(num);\n           }else if (num >= 30 && num <= 39) {\n               bucket4.add(num);\n           }else if (num >= 40 && num <= 49) {\n               bucket5.add(num);\n           }\n       }\n   \n       //桶内归并排序--稳定\n       for (LinkedList<Integer> list : bucket) {\n           //记录每个桶排序后的元素\n           int [] sortArray = mergeSort(list);\n           //因为桶之间是有序的，所以直接按照桶序号，将桶内排好序的元素放入原数组即可\n           for (int i = 0; i < list.size(); i++) {\n               nums[index++] = sortArray[i];\n           }\n       }\n   }\n   \n\n * 外部排序例子\n   \n   * 问题\n     \n     > 比如说我们有 10GB 的订单数据，我们希望按订单金额（假设金额都是正整数）进行排序，但是我们的内存有限，只有几百 MB，没办法一次性把 10GB 的数据都加载到内存中。这个时候该怎么办呢？\n   \n   * 解法\n     \n     >  1. 我们可以先扫描一遍文件，看订单金额所处的数据范围。\n     >  2. 假设经过扫描之后我们得到，订单金额最小是 1 元，最大是 10 万元。我们将所有订单根据金额划分到 100 个桶里，第一个桶我们存储金额在 1 元到 1000 元之内的订单，第二桶存储金额在 1001 元到 2000 元之内的订单，以此类推。每一个桶对应一个文件，并且按照金额范围的大小顺序编号命名（00，01，02...99）。\n     >  3. 理想的情况下，如果订单金额在 1 到 10 万之间均匀分布，那订单会被均匀划分到 100 个文件中，每个小文件中存储大约 100MB 的订单数据，我们就可以将这 100 个小文件依次放到内存中，用快排来排序。（10GB/100=100MB）\n     >  4. 等所有文件都排好序之后，我们只需要按照文件编号，从小到大依次读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小到大排序的订单数据了。\n     >  5. 针对这些划分之后还是比较大的文件，我们可以继续划分，比如，订单金额在 1 元到 1000 元之间的比较多，我们就将这个区间继续划分为 10 个小区间，1 元到 100 元，101 元到 200 元，201 元到 300 元....901 元到 1000 元。直到所有的文件都能读入内存为止。\n\n\n# 计数排序-O(n)\n\n * 算法原理\n   \n   > 计数排序其实是桶排序的一种特殊情况。当要排序的 n 个数据，所处的范围并不大的时候，比如最大值是 k，我们就可以把数据划分成 k 个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。\n\n * 算法实现\n   \n   * 问题\n     \n     > 假设只有 8 个考生，分数在 0 到 5 分之间。这 8 个考生的成绩我们放在一个数组 A[8]中，它们分别是：2，5，3，0，2，3，0，3。\n   \n   * 解决方案\n     \n     1. 最高分为5分，使用6个桶，0——5,我们使用大小为 6 的数组 C[6]表示桶。\n        * C[6]内存储的并不是考生，而是对应的考生个数。\n     2. 我们对 C[6]数组顺序求和。\n        * C[k]里存储小于等于分数 k 的考生个数\n     3. 我们从后到前依次扫描数组 A（为了稳定）。比如，当扫描到 3 时，我们可以从数组 C 中取出下标为 3 的值 7，也就是说 3 是数组 R 中的第 7 个元素（也就是数组 R 中下标为 6 的位置）。当 3 放入到数组 R 中后，小于等于 3 的元素就只剩下了 6 个了，所以相应的 C[3]要减 1，变成 6。\n     4. 当我们扫描到第 2 个分数为 3 的考生的时候，就会把它放入数组 R 中的第 6 个元素的位置（也就是下标为 5 的位置）。当我们扫描完整个数组 A 后，数组 R 内的数据就是按照分数从小到大有序排列的了。\n\n * 应用场景\n   \n   > 计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。\n   > \n   > 如果考生成绩精确到小数后一位，我们就需要将所有的分数都先乘以 10，转化成整数，然后再放到 9010 个桶内。再比如，如果要排序的数据中有负数，数据的范围是[-1000, 1000]，那我们就需要先对每个数据都加 1000，转化成非负整数。\n\n * 代码\n   \n   //计数排序\n   public void countingSort(int[] nums) {\n       int len = nums.length;\n       if (len < 1) {\n           return;\n       }\n   \n       //查找nums中的最大值\n       final int SIZE = Arrays.stream(nums).max().getAsInt() + 1;\n   \n   \n       //分数是0到5分，分为6个桶，用数组bucket来表示,\n       // 数组下标表示分数，下标对应内容为改分数下学生个数\n       //        final int SIZE = 6;//桶数\n       int[] bucket = new int[SIZE];\n       int[] sortArray = new int[len];// 用来存储排好序的数组\n   \n       //遍历原数组，给bucket赋初始值\n       for (int num : nums) {\n           bucket[num]++;\n       }\n   \n       //将bucket数组求和\n       for (int i = 1; i < SIZE; i++){\n           bucket[i] += bucket[i - 1];\n       }\n   \n       //倒序遍历数组（为了保证稳定），将原数组中元素，排列到新数组中\n       for (int i = len - 1; i >= 0; i--) {\n           //当读入第一个3的时候，bucket中的值为7，即意味着大于等于3的个数有7个，所有这个3要排入sortArray的第7位，即下标为6的位置\n           int index = bucket[nums[i]] - 1;\n           sortArray[index] = nums[i];\n           //这个3 放入对应位置后，bucket中相应的值要减一，表示下一个3来的时候，大于等于这个新3的个数有6个\n           bucket[nums[i]]--;\n       }\n   \n       //将排好序的数组 复制到新数组中\n       int index = 0;\n       for (int num : sortArray) {\n           nums[index++] = num;\n       }\n   }\n   \n\n\n# 基数排序-O(n)\n\n * 算法原理\n   \n   > 将代排序数据按位分割，用稳定排序算法分别对每一位进行排序，从后往前排序，这样第一位的顺序排好后，整个数据的顺序即排列好。\n   > \n   > 如果每位数据范围不是很大，可以采用之前的桶排序或者计数排序。\n   > \n   > **如果数据不等长，可以采用尾部补齐的方式。**尾部不影响最终的顺序\n\n\n\n * 时间复杂度分析\n   \n   > 根据每一位来排序，我们可以用刚讲过的桶排序或者计数排序，它们的时间复杂度可以做到 O(n)。如果要排序的数据有 k 位，那我们就需要 k 次桶排序或者计数排序，总的时间复杂度是 O(k*n)。当 k 不大的时候，比如手机号码排序的例子，k 最大就是 11，所以基数排序的时间复杂度就近似于 O(n)。\n\n * 适用范围\n   \n   > 基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。\n   > \n   > 除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到 O(n) 了。\n\n * 代码\n   \n   * 对电话号码进行排序\n   * 对每一位采用计数排序\n   \n   //基数排序————对11位的手机号排序\n   public void radixSort(String[] nums) {\n       final int SIZE = 11;\n       int len = nums.length;\n       int[] biteArray = new int[len];//用来存储每一位数字\n       int index;//数组下标\n       int[] sub;//原数组元素对应下标\n       String[] newNum = new String[nums.length];//用来排序的新数组\n   \n   \n       //对每一位用计数排序\n       for (int i = SIZE - 1; i >= 0; i--) {\n           index = 0;\n           //读取每一位数字\n           for (String num : nums) {\n               //char 类型 转换成int型，默认是转换成 assic 码值\n               biteArray[index++] = num.charAt(i) - '0';\n           }\n           //对每一位进行 计数排序\n           sub = countingSortIndex(biteArray);\n   \n           //将原数组按照下标调换位置-----空间复杂度O(n)\n           for (int j = 0; j < sub.length; j++) {\n               newNum [j] = nums[sub[j]];\n           }\n           index = 0;\n           for (String num : newNum) {\n               nums[index++] = num;\n           }\n   \n           //将原数组按照下标调换位置-----空间复杂度O(1)---不正确\n           //            for (int j = 0; j < sub.length; j++) {\n           //                if (j < sub[j]) {\n           //                    //这个时候直接交换，不影响什么\n           //                    swap(nums,j,sub[j]);\n           //                }else if (j > sub[j] && j != sub[sub[j]]) {\n           //                    //这个时候，说明位于 sub[j]的元素已经交换过了，再次交换会打乱顺序，\n           //                    //所有要干两件事，第一交换的时候，去和已经交换之后的位置换\n           //                    //第二，要记录下原值被交换的位置\n           //                    swap(nums,j,sub[sub[j]]);\n           //                    sub[j] = sub[sub[j]];\n           //                }\n           //            }\n       }\n   \n   }\n   \n\n==练习题==\n\n * 如何根据年龄给 100 万用户排序？\n   \n   * 解答\n     \n     > 我们假设年龄的范围最小 1 岁，最大不超过 120 岁。我们可以遍历这 100 万用户，根据年龄将其划分到这 120 个桶里，然后依次顺序遍历这 120 个桶中的元素。这样就得到了按照年龄排序的 100 万用户数据。\n\n * 题2\n   \n   * 题目\n     \n     > 假设我们现在需要对D,a,3,F,B,1,c,2,A,z这个字符串进行排序，要将小写字母的放到前面，大写字母放在最后，数字放在中间，不用排序算法，又该怎么解决呢？\n   \n   * 解答\n     \n     > 法1，可以用桶排序，设置三个桶，一个装小写字母，一个装数字，一个装大写字母，最后遍历一下，即可得到排好序的字符串。\n     > \n     > 法2，使用快排的分区思想。第一次分区，认为只有小写和非小写。第二次分区，则将数字和大写字母分开。\n     \n     //法2 \n     //快排思想-分区\n     //参数说明，low，high为低位和高位指针\n     //lowc和highc 为满足条件的 要排在左侧 的 最小值和最大值\n     public int partition(StringBuffer str, int low, int high, char lowc, char highc) {\n         char value = str.charAt(low);\n         while (low < high) {\n             //从右往左遍历，只要不在区间内就停\n             while (high >= low && (str.charAt(high) < lowc || str.charAt(high) > highc)) {\n                 high--;\n             }\n             str.setCharAt(low,str.charAt(high));\n     \n             //从右往左遍历，只要不在区间内就停\n             while (low <= high && str.charAt(low) >= lowc && str.charAt(low) <= highc) {\n                 low++;\n             }\n             str.setCharAt(high,str.charAt(low));\n         }\n         //循环结束，low和high执向同一位置，即为中枢值\n         str.setCharAt(low,value);\n         return low;\n     }\n     //字符串排序\n     public String stringSort(String nums) {\n     \n         //使用StringBuffer 就可以像数组一样操作字符串中某个值\n         StringBuffer str = new StringBuffer(nums);\n     \n         //第一轮分区  区分小写和非小写\n         int partition = partition(str, 0, nums.length() - 1, 'a', 'z');\n         //判断中枢值是否合法\n         char c = str.charAt(partition);\n         if ( c >= 'a' && c <= 'z') {\n             //中枢位置是小写字母\n             partition++;\n         }\n         //第二轮分区  区分大写和数字\n         partition(str, partition, nums.length() - 1, '0', '9');\n         return str.toString();\n     }\n     ",normalizedContent:"# 基础知识\n\n\n# 分析排序算法\n\n * 排序算法的执行效率\n   \n   * 最好情况、最坏情况、平均情况时间复杂度\n   * 时间复杂度的系数、常数 、低阶\n     * 在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来。\n   * 比较次数和交换（或移动）次数\n     * 虽然冒泡、插入都是o(n^2^)，但是插入排序移动次数是少的，所有性能要优\n\n * 排序算法的内存消耗\n   \n   * 类似于空间复杂度\n   * 新概念--原地排序\n     * 指的是，空间复杂度为o(1)的排序算法\n\n * 排序算法的==稳定性==\n   \n   这个概念是说，如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的==先后顺序不变==\n   \n   * 说明\n     \n     * 平时练习排序的时候都是利用整数来练习，所以稳定与否看不出来利弊，但是当我们对一个对象进行排序时，对象会有多个属性，这时稳定性就有很大作用了。\n   \n   * 订单排序\n     \n     * 要求——我们希望按照金额从小到大对订单数据排序。对于金额相同的订单，我们希望按照下单时间从早到晚有序。\n     * 实现\n       * 先按照下单时间给订单排序\n       * 再按照订单金额重新排序\n     * 这样，如果算法稳定的话，之前按时间排好序的订单在按照金额排序时，顺序不会发生变化。\n\n\n# 有序度-逆序度\n\n * 有序度\n   \n   有序元素对：a[i] <= a[j], 如果i < j。\n   \n   \n   \n   * 1,2,3,4,5,6\n     * 有序度为15，即n*(n-1)/2\n     * 这种完全有序的数组的有序度叫作满有序度\n\n * 逆序度\n   \n   * 逆序度与有序度定义相反\n   * 逆序度 = 满有序度 - 有序度\n\n * 例子——冒泡排序\n   \n   * 排序时，每交换一次，就会增加一个有序度，直到最后完全有序。\n   * 所以总的交换次数是逆序度= 满有序度(n*(n-1)/2) - 初始有序度\n\n\n# 基于比较的排序\n\n\n# 冒泡排序-o(n^2^)\n\n * 算法原理\n   \n   > 从底部开始遍历，依次比较相邻元素的大小，如果下方元素大，则交换。这样一轮过后，最上方的元素肯定是最大的。在继续进行n-1轮就可以啦。\n\n * 算法性能\n   \n   * 空间复杂度——o(1)\n   * 时间复杂度(最好、最坏、平均)——o(n)、o(n^2^)、o(n^2^)\n   * 稳定\n   * 交换次数较多\n\n * 代码\n   \n   //冒泡排序\n   public void bubblesort(int[] nums) {\n       int len = nums.length;\n       if (len <= 1) {\n           return;\n       }\n       //外层循环表示已经排序好的元素块\n       for (int i = 0; i < len; i++) {\n           boolean flag = false;//设置判断变量\n           //内层循环表示从无序块中选拔一个最大的元素进入有序块中\n           for (int j = 0;j < len - i -1; j++) {\n               if (nums[j] > nums[j+1]) {\n                   swap(nums, j,j+1);\n                   flag = true;\n               }\n           }\n           //当这一轮排序没有元素交换时，表示数组已经有序，不需要在进行排序了\n           if (!flag) {\n               break;\n           }\n       }\n   }\n   \n\n\n# 快速排序-o(nlogn)\n\n * 算法原理\n   \n   > 将大的序列不断的进行分区排序。\n   > \n   > 分区时，找到一个中间值，使其左边都小于它，右边都大于它。不断的减小序列，直到最后剩下一个时，即截止。\n   \n   \n\n * 算法性能\n   \n   * 空间复杂度——o(1)\n   * 时间复杂度(最好、最坏、平均)——o(nlogn)、o(n^2^)、o(nlogn)\n   * 不稳定\n\n * 代码\n   \n   //快速排序\n   public void quicksort(int[] nums) {\n       int len = nums.length;\n       qsort(nums, 0, len-1);\n   }\n   //递归函数\n   public void qsort(int[] nums, int low, int high) {\n       //递归终止条件\n       if (low >= high) {\n           return;\n       }\n       int mid = partition(nums, low, high);//找中枢\n       qsort(nums, low, mid-1);//排中枢左边\n       qsort(nums, mid+1, high);//排中枢右边\n   \n   }\n   //找中间点-空间复杂度为1，直接在原数组上修改\n   // 双边循环\n   public int partition(int[] nums, int low, int high) {\n       //三数取中，作为中枢值；将中间的放入low位置\n       int mid = low + (high - low) / 2;\n       if (nums[low] > nums[high]) {\n           swap(nums, low, high);\n       }\n       if (nums[mid] > nums[high]) {\n           swap(nums, mid, high);\n       }\n       if (nums[mid] > nums[low]) {\n           swap(nums, mid, low);\n       }\n       int value = nums[low];//默认选取第一个为中枢值--这里用三数取中进行了优化，\n   \n       while (low < high) {\n           //从右往左，大于中枢值的不动，小于的移动到low位\n           while (high > low && nums[high] >= value) {\n               high--;\n           }\n           nums[low] = nums[high];\n           //从左往右，小于中枢值的不动，大于的移动到high位\n           while (low < high && nums[low] <= value) {\n               low ++;\n           }\n           nums[high] = nums[low];\n       }\n       //循环结束后，low和high指向一个位置，将该位置设为中枢值\n       nums[low] = value;\n       return low;\n   }\n   \n\n * 如何在o(n)内查找数组内第k大元素\n   \n   > 利用快速排序中的分区思想，设中枢下标为x，若x+1=k，则说明x指向的值即为第k大值\n   > \n   > 若`k<x+1`，则将x左半部分进行分区；若大于，则将x右半部分进行分区。\n   > \n   > \n   > 知道最后找到 k=x+1 为止。\n\n\n# 插入排序-o(n^2^)\n\n * 算法原理\n   \n   > 在前方构造一个有序元素块，不断的取出后方的无序块中第一个元素，插入到有序块中。\n   > \n   > （通过不断的移动和交换，确保有序块的有序性）\n\n * 算法性能\n   \n   * 空间复杂度——o(1)\n   * 时间复杂度(最好、最坏、平均)——o(n)、o(n^2^)、o(n^2^)\n   * 稳定\n   * 交换次数较少\n\n * 代码\n   \n   //直接插入排序\n   public void insertionsort(int[] nums) {\n       int len = nums.length;\n       if (len <= 1) {\n           return;\n       }\n       //外层循环表示已经排序好的元素块\n       for (int i = 1; i < len; i++) {\n           //内层循环表示 无序元素块中的第一个(它-nums[i]=nums[j]) 在有序块中的排序过程\n           //如果它比前面的小，就交换，否则就退出循环\n           for (int j = i ; j > 0; j--) {\n               if (nums[j] < nums[j-1]) {\n                   swap(nums,j,j-1);\n               }else {\n                   break;\n               }\n           }\n       }\n   }\n   \n   \n   \n   //直接插入排序1----性能更好，使用了移动而非交换，交换相当于多次移动\n   \n       public void insertionsort1(int[] nums) {\n           int len = nums.length;\n           if (len <= 1) {\n               return;\n           }\n           int value;\n           int j;\n           //外层循环表示已经排序好的元素块\n           for (int i = 1; i < len; i++) {\n               value = nums[i];\n   \n               //内层循环表示无序 元素块中的第一个(它-nums[i]=value) 在有序块中的排序过程\n               //如果它比左面小，左后的元素右移，直到不小或者到了数组开头，在将它插入到合适的位置\n               for (j = i-1 ; j >= 0; j--) {\n                   if (value < nums[j]) {\n                       //元素右移，覆盖掉待入队的值\n                       nums[j+1] = nums[j];\n                   }else {\n                       break;\n                   }\n               }\n               nums[j+1] = value;\n           }\n       }\n   \n\n\n# 希尔排序-o(nlogn--n^2^)\n\n * 算法原理\n   \n   > 基于插入排序；将原序列，按照跳跃分割的策略，分成若干个子序列。然后在这些子序列内进行，直接插入排序，可以保证序列基本有序，当增量为1时，最后一轮排序，可保证完全有序。\n   > \n   > 跳跃分割策略：将相距某个增量的记录组成一个子序列，这样能保证子序列内分别进行直接插入排序后得到的结果是基本有序，而不是局部有序。\n\n * 算法性能\n   \n   * 空间复杂度——o(1)\n   * 时间复杂度(最好、最坏、平均)——o()、o(n^2^)、o(nlogn——n^2^)\n   * 不稳定——跳跃式排序\n\n * 代码\n   \n   * 增量为1的时候，就退化成了直接插入排序。\n   * 增量的选取是一个非常难的问题。\n   \n   //希尔排序--- 将数组按照某个增量跳跃式分割\n   public void shellsort(int[] nums) {\n       int len = nums.length;\n       if (len <= 1) {\n           return;\n       }\n       int value;//用来记录无序元素块中的第一个\n       int j;\n       int increment=len; //增量表示,开始时设为数组长度\n   \n       while (increment > 1) {\n           increment = (increment/3) + 1;\n           //外层循环表示已经排序好的元素块\n           for (int i = increment; i < len; i++) {\n               value = nums[i];\n   \n               //内层循环表示 无序元素块中的第一个(它） 在有序块中的排序过程\n               //如果它比左面小，左后的元素右移，直到不小或者到了数组开头，在将它插入到合适的位置\n               for (j = i - increment; j >= 0; j = j - increment) {\n                   if (value < nums[j]) {\n                       //元素右移，覆盖掉待入队的值\n                       nums[j + increment] = nums[j];\n                   }else {\n                       break;\n                   }\n               }\n               nums[j + increment] = value;\n           }\n   \n       }\n   \n   }\n   \n   \n\n\n# 选择排序-o(n^2^)\n\n * 算法原理\n   \n   > 将整个序列分已排序区间和未排序区间。选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。\n   > \n   > 极大减少了交换的次数。\n\n * 算法性能\n   \n   * 空间复杂度——o(1)\n   * 时间复杂度(最好、最坏、平均)——o(n^2^)、o(n^2^)、o(n^2^)\n   * 不稳定\n   * 交换次数极少\n\n * 代码\n   \n   //选择排序\n   public void selectsort(int[] nums) {\n       int len = nums.length;\n       if (len <= 1) {\n           return;\n       }\n       int min;//存储最小值\n       int minindex;//存储最小值下标\n   \n       //外层循环表示已经排序好的元素块\n       for (int i = 0; i < len; i++) {\n           minindex = i;\n           min = nums[i];\n           //内层循环表示 从无序元素块中找到一个最小的 ，然后将它放到有序元素块的末尾 的过程\n           for (int j = i + 1; j < len; j++) {\n               if (nums[j] < min) {\n                   min = nums[j];\n                   minindex = j;\n               }\n           }\n           //相等说明无序块中的第一个就是最小的，不用交换\n           if (minindex != i) {\n               swap(nums,i,minindex);\n           }\n       }\n   }\n   \n\n\n# 归并排序-o(nlogn)\n\n * 算法原理\n   \n   > 我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。\n\n * 算法性能\n   \n   * 空间复杂度——o(n)\n   * 时间复杂度——突出一个稳定——(最好、最坏、平均)——o(nlogn)、o(nlogn)、o(nlogn)\n   * 稳定排序\n\n * 程序\n   \n   * int mid = low + (high - low) / 2-----这样写是为了防止二者和过大导致溢出\n   * 这里的等号表示稳定性---让左边的优先 if (nums[p1] <= nums[p2]) {\n   \n   //归并排序\n   public void mergesort(int[] nums) {\n       //统一函数入口\n       int len = nums.length;\n       msort(nums,0,len-1);\n   }\n   \n   //递归函数\n   public void msort(int[] nums, int low, int high) {\n       //先定义递归终止条件\n       if (low >= high) {//这里写成== 无可厚非\n           return;\n       }\n       //计算中值\n       //mid=(low+high)/2-----下面的写法是为了防止二者和过大导致溢出\n       int mid = low + (high - low) / 2;\n   \n       msort(nums,low,mid); //分左边\n       msort(nums,mid+1,high);//分右边\n       merge(nums,low,mid,high);//合并左右两边\n   \n   }\n   \n   //合并\n   public void merge(int[] nums, int low, int mid, int high){\n       int len = high - low + 1;//要合并的数组长度\n       int[] temps = new int[len];//申请新的数组，用来排序\n       int p1 = low;//保留初始下标，最后将新数组，并到原数组中\n       int p2 = mid + 1;\n       int flag = -1;\n       int i;\n       //开始合并--直到有一边全部放入新数组\n       for (i = 0; i < len; i ++) {\n           //这里的等号表示稳定性---让左边的优先\n           if (nums[p1] <= nums[p2]) {\n               temps[i] = nums[p1++];\n               if (p1 > mid) {\n                   flag = 0;\n                   break;\n               }\n           }else {\n               temps[i] = nums[p2++];\n               if (p2 > high) {\n                   flag = 1;\n                   break;\n               }\n           }\n       }\n       //将剩余数组的所有元素，依次写入即可\n       for (i = i + 1;i < len; i++) {\n           if (flag == 0) {//左边写完了，这次写右边剩余部分\n               temps[i] = nums[p2++];\n           }else {\n               temps[i] = nums[p1++];\n           }\n       }\n       //将排好序的数组 写入到原数组对应位置\n       for (i = 0; i < len; i++) {\n           nums[low + i] = temps[i];\n       }\n   }\n   \n\n\n# ==练习题==\n\n * 问题\n\n> 现在你有 10 个接口访问日志文件，每个日志文件大小约 300mb，每个文件里的日志都是按照时间戳从小到大排序的。你希望将这 10 个较小的日志文件，合并为 1 个日志文件，合并之后的日志仍然按照时间戳从小到大排列。如果处理上述排序任务的机器内存只有 1gb，你有什么好的解决思路，能“快速”地将这 10 个日志文件合并吗？\n\n * 解答\n\n> 1.申请10个40m的数组和一个400m的数组。\n> \n> 2.每个文件都读40m，取各数组中最大时间戳中的最小值。\n> \n> 3.然后利用二分查找，在其他数组中快速定位到小于/等于该时间戳的位置，并做标记。\n> \n> 4.再把各数组中标记位置之前的数据全部放在申请的400m内存中。\n> \n> 5.在原来的40m数组中清除已参加排序的数据。[可优化成不挪动数据，只是用两个索引标记有效数据的起始和截止位置]\n> \n> 6.对400m内存中的有效数据[没装满]做快排。 将排好序的直接写文件。\n> \n> 7.再把每个数组尽量填充满。\n> \n> 从第2步开始继续，直到各个文件都读区完毕。\n> \n> 这么做的好处有： 1.每个文件的内容只读区一次，且是批量读区。比每次只取一条快得多。 2.充分利用了读区到内存中的数据。曹源 同学在文件中查找那个中间数是会比较困难的。 3.每个拷贝到400m大数组中参加快排的数据都被写到了文件中，这样每个数只参加了一次快排。\n\n\n# 不基于比较的排序\n\n\n# 桶排序-o(n)\n\n * 算法\n   \n   > 核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。\n\n * 时间复杂度分析\n   \n   > 如果要排序的数据有 n 个，我们把它们均匀地划分到 m 个桶内，每个桶里就有 k=n/m 个元素。每个桶内部使用快速排序，时间复杂度为 o(k * logk)。\n   > \n   > m 个桶排序的时间复杂度就是 o(m * k * logk)，因为 k=n/m，所以整个桶排序的时间复杂度就是 o(n*log(n/m))。\n   > \n   > 当桶的个数 m 接近数据个数 n 时，log(n/m) 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 o(n)。\n\n * 应用场景\n   \n   > 桶排序对要排序数据的要求是非常苛刻的。\n   > \n   > 首先，要排序的数据需要很容易就能划分成 m 个桶，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。\n   > \n   > 其次，数据在各个桶之间的分布是比较均匀的。在极端情况下，如果数据都被划分到一个桶里，那就退化为 o(nlogn) 的排序算法了。\n   > \n   > 桶排序比较适合用在外部排序中。\n\n * 代码\n   \n   //桶排序\n   public void bucketsort(int[] nums) {\n       int index = 0;\n       //根据数据情况，划分为 5 个桶 ，每个桶用链表实现,将5个桶放到list集合中\n       linkedlist<integer> bucket1 = new linkedlist<>();\n       linkedlist<integer> bucket2 = new linkedlist<>();\n       linkedlist<integer> bucket3 = new linkedlist<>();\n       linkedlist<integer> bucket4 = new linkedlist<>();\n       linkedlist<integer> bucket5 = new linkedlist<>();\n   \n       list<linkedlist<integer>> bucket = new linkedlist<>();\n       bucket.add(bucket1);\n       bucket.add(bucket2);\n       bucket.add(bucket3);\n       bucket.add(bucket4);\n       bucket.add(bucket5);\n   \n       //遍历元素，放入桶中\n       for (int num : nums) {\n           if (num >= 0 && num <=9) {\n               bucket1.add(num);\n           }else if (num >= 10 && num <= 19) {\n               bucket2.add(num);\n           }else if (num >= 20 && num <= 29) {\n               bucket3.add(num);\n           }else if (num >= 30 && num <= 39) {\n               bucket4.add(num);\n           }else if (num >= 40 && num <= 49) {\n               bucket5.add(num);\n           }\n       }\n   \n       //桶内归并排序--稳定\n       for (linkedlist<integer> list : bucket) {\n           //记录每个桶排序后的元素\n           int [] sortarray = mergesort(list);\n           //因为桶之间是有序的，所以直接按照桶序号，将桶内排好序的元素放入原数组即可\n           for (int i = 0; i < list.size(); i++) {\n               nums[index++] = sortarray[i];\n           }\n       }\n   }\n   \n\n * 外部排序例子\n   \n   * 问题\n     \n     > 比如说我们有 10gb 的订单数据，我们希望按订单金额（假设金额都是正整数）进行排序，但是我们的内存有限，只有几百 mb，没办法一次性把 10gb 的数据都加载到内存中。这个时候该怎么办呢？\n   \n   * 解法\n     \n     >  1. 我们可以先扫描一遍文件，看订单金额所处的数据范围。\n     >  2. 假设经过扫描之后我们得到，订单金额最小是 1 元，最大是 10 万元。我们将所有订单根据金额划分到 100 个桶里，第一个桶我们存储金额在 1 元到 1000 元之内的订单，第二桶存储金额在 1001 元到 2000 元之内的订单，以此类推。每一个桶对应一个文件，并且按照金额范围的大小顺序编号命名（00，01，02...99）。\n     >  3. 理想的情况下，如果订单金额在 1 到 10 万之间均匀分布，那订单会被均匀划分到 100 个文件中，每个小文件中存储大约 100mb 的订单数据，我们就可以将这 100 个小文件依次放到内存中，用快排来排序。（10gb/100=100mb）\n     >  4. 等所有文件都排好序之后，我们只需要按照文件编号，从小到大依次读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小到大排序的订单数据了。\n     >  5. 针对这些划分之后还是比较大的文件，我们可以继续划分，比如，订单金额在 1 元到 1000 元之间的比较多，我们就将这个区间继续划分为 10 个小区间，1 元到 100 元，101 元到 200 元，201 元到 300 元....901 元到 1000 元。直到所有的文件都能读入内存为止。\n\n\n# 计数排序-o(n)\n\n * 算法原理\n   \n   > 计数排序其实是桶排序的一种特殊情况。当要排序的 n 个数据，所处的范围并不大的时候，比如最大值是 k，我们就可以把数据划分成 k 个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。\n\n * 算法实现\n   \n   * 问题\n     \n     > 假设只有 8 个考生，分数在 0 到 5 分之间。这 8 个考生的成绩我们放在一个数组 a[8]中，它们分别是：2，5，3，0，2，3，0，3。\n   \n   * 解决方案\n     \n     1. 最高分为5分，使用6个桶，0——5,我们使用大小为 6 的数组 c[6]表示桶。\n        * c[6]内存储的并不是考生，而是对应的考生个数。\n     2. 我们对 c[6]数组顺序求和。\n        * c[k]里存储小于等于分数 k 的考生个数\n     3. 我们从后到前依次扫描数组 a（为了稳定）。比如，当扫描到 3 时，我们可以从数组 c 中取出下标为 3 的值 7，也就是说 3 是数组 r 中的第 7 个元素（也就是数组 r 中下标为 6 的位置）。当 3 放入到数组 r 中后，小于等于 3 的元素就只剩下了 6 个了，所以相应的 c[3]要减 1，变成 6。\n     4. 当我们扫描到第 2 个分数为 3 的考生的时候，就会把它放入数组 r 中的第 6 个元素的位置（也就是下标为 5 的位置）。当我们扫描完整个数组 a 后，数组 r 内的数据就是按照分数从小到大有序排列的了。\n\n * 应用场景\n   \n   > 计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。\n   > \n   > 如果考生成绩精确到小数后一位，我们就需要将所有的分数都先乘以 10，转化成整数，然后再放到 9010 个桶内。再比如，如果要排序的数据中有负数，数据的范围是[-1000, 1000]，那我们就需要先对每个数据都加 1000，转化成非负整数。\n\n * 代码\n   \n   //计数排序\n   public void countingsort(int[] nums) {\n       int len = nums.length;\n       if (len < 1) {\n           return;\n       }\n   \n       //查找nums中的最大值\n       final int size = arrays.stream(nums).max().getasint() + 1;\n   \n   \n       //分数是0到5分，分为6个桶，用数组bucket来表示,\n       // 数组下标表示分数，下标对应内容为改分数下学生个数\n       //        final int size = 6;//桶数\n       int[] bucket = new int[size];\n       int[] sortarray = new int[len];// 用来存储排好序的数组\n   \n       //遍历原数组，给bucket赋初始值\n       for (int num : nums) {\n           bucket[num]++;\n       }\n   \n       //将bucket数组求和\n       for (int i = 1; i < size; i++){\n           bucket[i] += bucket[i - 1];\n       }\n   \n       //倒序遍历数组（为了保证稳定），将原数组中元素，排列到新数组中\n       for (int i = len - 1; i >= 0; i--) {\n           //当读入第一个3的时候，bucket中的值为7，即意味着大于等于3的个数有7个，所有这个3要排入sortarray的第7位，即下标为6的位置\n           int index = bucket[nums[i]] - 1;\n           sortarray[index] = nums[i];\n           //这个3 放入对应位置后，bucket中相应的值要减一，表示下一个3来的时候，大于等于这个新3的个数有6个\n           bucket[nums[i]]--;\n       }\n   \n       //将排好序的数组 复制到新数组中\n       int index = 0;\n       for (int num : sortarray) {\n           nums[index++] = num;\n       }\n   }\n   \n\n\n# 基数排序-o(n)\n\n * 算法原理\n   \n   > 将代排序数据按位分割，用稳定排序算法分别对每一位进行排序，从后往前排序，这样第一位的顺序排好后，整个数据的顺序即排列好。\n   > \n   > 如果每位数据范围不是很大，可以采用之前的桶排序或者计数排序。\n   > \n   > **如果数据不等长，可以采用尾部补齐的方式。**尾部不影响最终的顺序\n\n\n\n * 时间复杂度分析\n   \n   > 根据每一位来排序，我们可以用刚讲过的桶排序或者计数排序，它们的时间复杂度可以做到 o(n)。如果要排序的数据有 k 位，那我们就需要 k 次桶排序或者计数排序，总的时间复杂度是 o(k*n)。当 k 不大的时候，比如手机号码排序的例子，k 最大就是 11，所以基数排序的时间复杂度就近似于 o(n)。\n\n * 适用范围\n   \n   > 基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。\n   > \n   > 除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到 o(n) 了。\n\n * 代码\n   \n   * 对电话号码进行排序\n   * 对每一位采用计数排序\n   \n   //基数排序————对11位的手机号排序\n   public void radixsort(string[] nums) {\n       final int size = 11;\n       int len = nums.length;\n       int[] bitearray = new int[len];//用来存储每一位数字\n       int index;//数组下标\n       int[] sub;//原数组元素对应下标\n       string[] newnum = new string[nums.length];//用来排序的新数组\n   \n   \n       //对每一位用计数排序\n       for (int i = size - 1; i >= 0; i--) {\n           index = 0;\n           //读取每一位数字\n           for (string num : nums) {\n               //char 类型 转换成int型，默认是转换成 assic 码值\n               bitearray[index++] = num.charat(i) - '0';\n           }\n           //对每一位进行 计数排序\n           sub = countingsortindex(bitearray);\n   \n           //将原数组按照下标调换位置-----空间复杂度o(n)\n           for (int j = 0; j < sub.length; j++) {\n               newnum [j] = nums[sub[j]];\n           }\n           index = 0;\n           for (string num : newnum) {\n               nums[index++] = num;\n           }\n   \n           //将原数组按照下标调换位置-----空间复杂度o(1)---不正确\n           //            for (int j = 0; j < sub.length; j++) {\n           //                if (j < sub[j]) {\n           //                    //这个时候直接交换，不影响什么\n           //                    swap(nums,j,sub[j]);\n           //                }else if (j > sub[j] && j != sub[sub[j]]) {\n           //                    //这个时候，说明位于 sub[j]的元素已经交换过了，再次交换会打乱顺序，\n           //                    //所有要干两件事，第一交换的时候，去和已经交换之后的位置换\n           //                    //第二，要记录下原值被交换的位置\n           //                    swap(nums,j,sub[sub[j]]);\n           //                    sub[j] = sub[sub[j]];\n           //                }\n           //            }\n       }\n   \n   }\n   \n\n==练习题==\n\n * 如何根据年龄给 100 万用户排序？\n   \n   * 解答\n     \n     > 我们假设年龄的范围最小 1 岁，最大不超过 120 岁。我们可以遍历这 100 万用户，根据年龄将其划分到这 120 个桶里，然后依次顺序遍历这 120 个桶中的元素。这样就得到了按照年龄排序的 100 万用户数据。\n\n * 题2\n   \n   * 题目\n     \n     > 假设我们现在需要对d,a,3,f,b,1,c,2,a,z这个字符串进行排序，要将小写字母的放到前面，大写字母放在最后，数字放在中间，不用排序算法，又该怎么解决呢？\n   \n   * 解答\n     \n     > 法1，可以用桶排序，设置三个桶，一个装小写字母，一个装数字，一个装大写字母，最后遍历一下，即可得到排好序的字符串。\n     > \n     > 法2，使用快排的分区思想。第一次分区，认为只有小写和非小写。第二次分区，则将数字和大写字母分开。\n     \n     //法2 \n     //快排思想-分区\n     //参数说明，low，high为低位和高位指针\n     //lowc和highc 为满足条件的 要排在左侧 的 最小值和最大值\n     public int partition(stringbuffer str, int low, int high, char lowc, char highc) {\n         char value = str.charat(low);\n         while (low < high) {\n             //从右往左遍历，只要不在区间内就停\n             while (high >= low && (str.charat(high) < lowc || str.charat(high) > highc)) {\n                 high--;\n             }\n             str.setcharat(low,str.charat(high));\n     \n             //从右往左遍历，只要不在区间内就停\n             while (low <= high && str.charat(low) >= lowc && str.charat(low) <= highc) {\n                 low++;\n             }\n             str.setcharat(high,str.charat(low));\n         }\n         //循环结束，low和high执向同一位置，即为中枢值\n         str.setcharat(low,value);\n         return low;\n     }\n     //字符串排序\n     public string stringsort(string nums) {\n     \n         //使用stringbuffer 就可以像数组一样操作字符串中某个值\n         stringbuffer str = new stringbuffer(nums);\n     \n         //第一轮分区  区分小写和非小写\n         int partition = partition(str, 0, nums.length() - 1, 'a', 'z');\n         //判断中枢值是否合法\n         char c = str.charat(partition);\n         if ( c >= 'a' && c <= 'z') {\n             //中枢位置是小写字母\n             partition++;\n         }\n         //第二轮分区  区分大写和数字\n         partition(str, partition, nums.length() - 1, '0', '9');\n         return str.tostring();\n     }\n     ",charsets:{cjk:!0}},{title:"贪心算法",frontmatter:{autoSort:84,title:"贪心算法",date:"2023-07-01T16:09:52.000Z",permalink:"/pages/1b322d/",categories:["算法","贪心"],tags:["知识","算法"]},regularPath:"/02.%E7%AE%97%E6%B3%95/75.%E8%B4%AA%E5%BF%83/01.%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95.html",relativePath:"02.算法/75.贪心/01.贪心算法.md",key:"v-383faa3a",path:"/pages/1b322d/",headers:[{level:2,title:"贪心算法",slug:"贪心算法",normalizedTitle:"贪心算法",charIndex:2}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"贪心算法",content:"# 贪心算法\n\n * 经典应用\n   \n   * 霍夫曼编码\n   * Prim 和 Kruskal 最小生成树算法\n   * Dijkstra 单源最短路径算法\n\n * 贪心算法步骤\n   \n   * 当我们看到这类问题的时候，首先要联想到贪心算法\n     \n     * 针对一组数据，我们定义了限制值和期望值，希望从中选出几个数据，在满足限制值的情况下，期望值最大。\n     * 当前一步的选择不会影响下一步的选择的时候，考虑贪心算法。\n       * 例如背包问题，就一直找最大，填满背包即可。可以用贪心算法\n       * 例如0-1背包问题，一直找最大的话，会限制背包空间，从而影响下一件物品的选择，不适合用贪心\n       * 例如最短路径问题，一直找最小的话，一般都不是最小，因为选择了最小路径之后，接下来要选择的路会受到影响。\n   \n   * 我们尝试看下这个问题是否可以用贪心算法解决\n   \n   * 我们举几个例子看下贪心算法产生的结果是否是最优的\n     \n     * 大部分情况下，举几个例子验证一下就可以了。",normalizedContent:"# 贪心算法\n\n * 经典应用\n   \n   * 霍夫曼编码\n   * prim 和 kruskal 最小生成树算法\n   * dijkstra 单源最短路径算法\n\n * 贪心算法步骤\n   \n   * 当我们看到这类问题的时候，首先要联想到贪心算法\n     \n     * 针对一组数据，我们定义了限制值和期望值，希望从中选出几个数据，在满足限制值的情况下，期望值最大。\n     * 当前一步的选择不会影响下一步的选择的时候，考虑贪心算法。\n       * 例如背包问题，就一直找最大，填满背包即可。可以用贪心算法\n       * 例如0-1背包问题，一直找最大的话，会限制背包空间，从而影响下一件物品的选择，不适合用贪心\n       * 例如最短路径问题，一直找最小的话，一般都不是最小，因为选择了最小路径之后，接下来要选择的路会受到影响。\n   \n   * 我们尝试看下这个问题是否可以用贪心算法解决\n   \n   * 我们举几个例子看下贪心算法产生的结果是否是最优的\n     \n     * 大部分情况下，举几个例子验证一下就可以了。",charsets:{cjk:!0}},{title:"二分查找",frontmatter:{autoSort:92,title:"二分查找",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/4fe23f/",categories:["算法","二分查找"],tags:["知识","算法"]},regularPath:"/02.%E7%AE%97%E6%B3%95/70.%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/05.%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE.html",relativePath:"02.算法/70.二分查找/05.二分查找.md",key:"v-611bc590",path:"/pages/4fe23f/",headers:[{level:2,title:"一般二分查找",slug:"一般二分查找",normalizedTitle:"一般二分查找",charIndex:616},{level:2,title:"变体二分查找",slug:"变体二分查找",normalizedTitle:"变体二分查找",charIndex:1051}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"一般二分查找 变体二分查找",content:"二分查找是一种时间复杂度为O(logn)的查找算法\n\n * 使用场景\n   \n   * 首先，二分查找依赖的是顺序表结构，简单点说就是数组。\n   \n   * 其次，二分查找针对的是有序数据。\n     \n     * 所以，二分查找只能用在插入、删除操作不频繁，一次排序多次查找的场景中。\n   \n   * 再次，数据量太小不适合二分查找。\n     \n     * 如果要处理的数据量很小，完全没有必要用二分查找，顺序遍历就足够了\n   \n   * 最后，数据量太大也不适合二分查找。\n     \n     > 二分查找的底层需要依赖数组这种数据结构，而数组为了支持随机访问的特性，要求内存空间连续，对内存的要求比较苛刻。比如，我们有 1GB 大小的数据，如果希望用数组来存储，那就需要 1GB 的连续内存空间。\n     > \n     > 而我们的二分查找是作用在数组这种数据结构之上的，所以太大的数据用数组存储就比较吃力了，也就不能用二分查找了。\n   \n   * 更适用于近似查找\n     \n     > **“值等于给定值”的二分查找 **------凡是用二分查找能解决的，绝大部分我们更倾向于用散列表或者二叉查找树。\n     > \n     > 二分查找更适合用在“近似”查找问题，在这类问题上，二分查找的优势更加明显。比如今天讲的这几种变体问题，用其他数据结构，比如散列表、二叉树，就比较难实现了。\n\n\n# 一般二分查找\n\n==数组中的数组单调递增==——{1,2,3,4,5,6}\n\npublic int search(int[] nums, int target) {\n    int high=nums.length-1;\n    int low=0;\n\n    while(low<=high){ //这里要带等号\n        //防止溢出，右移一位表示除2\n        int mid=low+((high-low)>>1);\n        if(target>nums[mid]){ //右边查\n            low=mid+1;\n        }else if(target<nums[mid]){ //左边查\n            high=mid-1;\n        }\n        else{//查到了\n            return mid;\n        }\n    }\n    return -1;\n}\n\n\n\n# 变体二分查找\n\n==数组中数据的单调不减==——{1,3,4,5,6,6,6,7,8}\n\n * 查找第一个等于给定值的元素\n\n//*查找第一个等于给定值的元素\npublic int findFirstEqual(int[] nums,int target) {\n    int high=nums.length-1;\n    int low=0;\n\n    while(low<=high){ //这里要带等号\n        //            int mid=(low+high)/2;\n        int mid=low+((high-low)>>1);\n        if(target>nums[mid]){ //右边查\n            low=mid+1;\n        }else if(target<nums[mid]){ //左边查\n            high=mid-1;\n        }else {//--相等继续查\n            //mid 在最左侧 或者 mid左侧不等于target即停止，，否则一直查\n            if (mid == 0 || nums[mid - 1] !=target) return mid;\n            else high = mid - 1;\n        }\n    }\n    return -1;\n}\n\n\n * 查找最后一个等于给定值的元素\n\n//* 查找最后一个等于给定值的元素\npublic int findLastEqual(int[] nums,int target) {\n    int high=nums.length-1;\n    int low=0;\n\n    while(low<=high){ //这里要带等号\n        //            int mid=(low+high)/2;\n        int mid=low+((high-low)>>1);\n        if(target>nums[mid]){ //右边查\n            low=mid+1;\n        }else if(target<nums[mid]){ //左边查\n            high=mid-1;\n        }else {//--相等继续查\n            //mid 在最右侧 或者 mid右侧不等于target即停止，，否则一直查\n            if (mid == nums.length-1 || nums[mid + 1] !=target) return mid;\n            else low=mid+1;\n        }\n    }\n    return -1;\n}\n\n\n * 查找第一个大于等于给定值的元素\n\n//* 查找第一个大于等于给定值的元素\npublic int findFirstHigher(int[] nums,int target) {\n    int high=nums.length-1;\n    int low=0;\n\n    while(low<=high){ //这里要带等号\n        //            int mid=(low+high)/2;\n        int mid=low+((high-low)>>1);\n        if(target>nums[mid]){ //右边查\n            low=mid+1;\n        }else if(target<=nums[mid]){ //左边查\n            if (mid == 0 || nums[mid - 1] < target) return mid;\n            else high=mid-1;\n        }\n    }\n    return -1;\n}\n\n\n * 查找最后一个小于等于给定值的元素\n\n//* 查找最后一个小于等于给定值的元素\npublic int findLastLower(int[] nums,int target) {\n    int high=nums.length-1;\n    int low=0;\n\n    while(low<=high){ //这里要带等号\n        //            int mid=(low+high)/2;\n        int mid=low+((high-low)>>1);\n        if(target>=nums[mid]){ //右边查\n            if (mid == nums.length-1 || nums[mid + 1] > target) return mid;\n            else low=mid+1;\n        }else if(target<nums[mid]){ //左边查\n            high=mid-1;\n        }\n    }\n    return -1;\n}\n",normalizedContent:"二分查找是一种时间复杂度为o(logn)的查找算法\n\n * 使用场景\n   \n   * 首先，二分查找依赖的是顺序表结构，简单点说就是数组。\n   \n   * 其次，二分查找针对的是有序数据。\n     \n     * 所以，二分查找只能用在插入、删除操作不频繁，一次排序多次查找的场景中。\n   \n   * 再次，数据量太小不适合二分查找。\n     \n     * 如果要处理的数据量很小，完全没有必要用二分查找，顺序遍历就足够了\n   \n   * 最后，数据量太大也不适合二分查找。\n     \n     > 二分查找的底层需要依赖数组这种数据结构，而数组为了支持随机访问的特性，要求内存空间连续，对内存的要求比较苛刻。比如，我们有 1gb 大小的数据，如果希望用数组来存储，那就需要 1gb 的连续内存空间。\n     > \n     > 而我们的二分查找是作用在数组这种数据结构之上的，所以太大的数据用数组存储就比较吃力了，也就不能用二分查找了。\n   \n   * 更适用于近似查找\n     \n     > **“值等于给定值”的二分查找 **------凡是用二分查找能解决的，绝大部分我们更倾向于用散列表或者二叉查找树。\n     > \n     > 二分查找更适合用在“近似”查找问题，在这类问题上，二分查找的优势更加明显。比如今天讲的这几种变体问题，用其他数据结构，比如散列表、二叉树，就比较难实现了。\n\n\n# 一般二分查找\n\n==数组中的数组单调递增==——{1,2,3,4,5,6}\n\npublic int search(int[] nums, int target) {\n    int high=nums.length-1;\n    int low=0;\n\n    while(low<=high){ //这里要带等号\n        //防止溢出，右移一位表示除2\n        int mid=low+((high-low)>>1);\n        if(target>nums[mid]){ //右边查\n            low=mid+1;\n        }else if(target<nums[mid]){ //左边查\n            high=mid-1;\n        }\n        else{//查到了\n            return mid;\n        }\n    }\n    return -1;\n}\n\n\n\n# 变体二分查找\n\n==数组中数据的单调不减==——{1,3,4,5,6,6,6,7,8}\n\n * 查找第一个等于给定值的元素\n\n//*查找第一个等于给定值的元素\npublic int findfirstequal(int[] nums,int target) {\n    int high=nums.length-1;\n    int low=0;\n\n    while(low<=high){ //这里要带等号\n        //            int mid=(low+high)/2;\n        int mid=low+((high-low)>>1);\n        if(target>nums[mid]){ //右边查\n            low=mid+1;\n        }else if(target<nums[mid]){ //左边查\n            high=mid-1;\n        }else {//--相等继续查\n            //mid 在最左侧 或者 mid左侧不等于target即停止，，否则一直查\n            if (mid == 0 || nums[mid - 1] !=target) return mid;\n            else high = mid - 1;\n        }\n    }\n    return -1;\n}\n\n\n * 查找最后一个等于给定值的元素\n\n//* 查找最后一个等于给定值的元素\npublic int findlastequal(int[] nums,int target) {\n    int high=nums.length-1;\n    int low=0;\n\n    while(low<=high){ //这里要带等号\n        //            int mid=(low+high)/2;\n        int mid=low+((high-low)>>1);\n        if(target>nums[mid]){ //右边查\n            low=mid+1;\n        }else if(target<nums[mid]){ //左边查\n            high=mid-1;\n        }else {//--相等继续查\n            //mid 在最右侧 或者 mid右侧不等于target即停止，，否则一直查\n            if (mid == nums.length-1 || nums[mid + 1] !=target) return mid;\n            else low=mid+1;\n        }\n    }\n    return -1;\n}\n\n\n * 查找第一个大于等于给定值的元素\n\n//* 查找第一个大于等于给定值的元素\npublic int findfirsthigher(int[] nums,int target) {\n    int high=nums.length-1;\n    int low=0;\n\n    while(low<=high){ //这里要带等号\n        //            int mid=(low+high)/2;\n        int mid=low+((high-low)>>1);\n        if(target>nums[mid]){ //右边查\n            low=mid+1;\n        }else if(target<=nums[mid]){ //左边查\n            if (mid == 0 || nums[mid - 1] < target) return mid;\n            else high=mid-1;\n        }\n    }\n    return -1;\n}\n\n\n * 查找最后一个小于等于给定值的元素\n\n//* 查找最后一个小于等于给定值的元素\npublic int findlastlower(int[] nums,int target) {\n    int high=nums.length-1;\n    int low=0;\n\n    while(low<=high){ //这里要带等号\n        //            int mid=(low+high)/2;\n        int mid=low+((high-low)>>1);\n        if(target>=nums[mid]){ //右边查\n            if (mid == nums.length-1 || nums[mid + 1] > target) return mid;\n            else low=mid+1;\n        }else if(target<nums[mid]){ //左边查\n            high=mid-1;\n        }\n    }\n    return -1;\n}\n",charsets:{cjk:!0}},{title:"图",frontmatter:{autoSort:86,title:"图",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/dacecb/",categories:["算法","图"],tags:["知识","算法"]},regularPath:"/02.%E7%AE%97%E6%B3%95/50.%E5%9B%BE/05.%E5%9B%BE.html",relativePath:"02.算法/50.图/05.图.md",key:"v-22994320",path:"/pages/dacecb/",headers:[{level:2,title:"图的定义",slug:"图的定义",normalizedTitle:"图的定义",charIndex:2},{level:2,title:"图的存储结构",slug:"图的存储结构",normalizedTitle:"图的存储结构",charIndex:628},{level:2,title:"图的遍历",slug:"图的遍历",normalizedTitle:"图的遍历",charIndex:3860},{level:2,title:"最小生成树",slug:"最小生成树",normalizedTitle:"最小生成树",charIndex:7376},{level:2,title:"最短路径",slug:"最短路径",normalizedTitle:"最短路径",charIndex:9481},{level:2,title:"拓扑排序",slug:"拓扑排序",normalizedTitle:"拓扑排序",charIndex:11121},{level:2,title:"关键路径",slug:"关键路径",normalizedTitle:"关键路径",charIndex:13048}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"图的定义 图的存储结构 图的遍历 最小生成树 最短路径 拓扑排序 关键路径",content:'# 图的定义\n\n * 图中的数据元素称之为顶点，顶点之间的逻辑关系用边来表示。\n\n * 按方向分\n   \n   * 有向图——边有方向\n     * A到D的有向边为弧，A为弧尾，D为弧头\n     * <A,D>表示此弧\n     * 如果任意两个顶点之间都存在方向相反的弧，则称为 有向完全图\n   * 无向图——边无方向\n     * (A,D)或者(D,A)表示此边\n     * 若任意两个顶点之间都存在边，则称为 无向完全图\n\n * 权与网\n   \n   * 这种与图的边或弧相关的数字，叫做权\n   * 带权的图称为网\n\n * 度\n   \n   * 无向图任一顶点的度是与其相关联的边的数目，总边数为总度数的一半\n   * 有向图\n     * 以v为头的弧称为v的入度——箭头指向v\n     * 以v为尾的弧称为v的出度——箭头从v指出\n\n * 连通\n   \n   * 若顶点间存在路径，则说明顶点间是连通的 。\n   * 如果路径最终回到起始点则称为环，顶点不出现重复的路径称为简单路径。\n   * 若任意两点都是连通的，则图就是连通图，有向则称为强连通图。\n   * 图中有子图，若子图极大连通，则称为连通分量，有向称为称为强连通分量\n   * 无向图连通，且n个顶点n-1条边叫做生成树。有向图中一顶点入度为0，其余顶点出度为1，则称为有向树。\n   * 图1为普通的连通图，图2和图3为图1的生成树，图4不是（不连通）。\n\n\n# 图的存储结构\n\n# 邻接矩阵\n\n * 定义\n   \n   > 邻接矩阵是一种顺序存储结构。\n   > \n   > 邻接矩阵存储方式，是用两个数组来表示图。一个一维数组存储图中的顶点信息，一个二维数组存储图中的边或弧的信息。\n\n * 无向图\n   \n   * 有边连通记为1，否则为0\n   * 无向图的连接矩阵为对称阵\n\n\n\n * 有向图\n   * 横着看，行，表示指出（第一行，为v0指向v3），行和为出度\n   * 竖着看，列，表示指向(第一列，v1指向v0，v2指向v0)，列和为入度\n\n\n\n * 网\n   * 原邻接矩阵为1的地方改为权\n   * 原邻接矩阵为0的地方,除了对角线保持不变，其他改为无穷大(int——>65536)\n\n\n\n * 代码实现\n   \n   * 构造无向网图\n   \n   #实现无向网图\n   #定义邻接矩阵类\n   class adjacent_matrix:\n   \n       def __init__(self,peak,side): #初始化\n           self.peak=peak\n           self.side=side\n   \n       def printPic(self): #输出\n           print(self.peak)\n           print(self.side)\n   \n   #构建无向网图\n   def CreatPic():\n       str=input("请输入顶点(用空格分离)：")\n       peak=str.split(\' \')#用空格切片，存放顶点\n       n=len(peak)\n       num=input("请输入边数")\n       side=np.zeros([n,n])\n       for i in range(int(num)):\n           str=input("请输入连通顶点，权")\n           power=str.split(\' \')\n           x=peak.index(power[0])\n           y=peak.index(power[1])\n           side[x][y]=power[2]\n           side[y][x] = power[2]  #对称矩阵\n       pic=adjacent_matrix(peak,side)\n       return pic\n   \n   \n   if __name__=="__main__":\n       pic=CreatPic() #构建无向网图\n       pic.printPic() #输出\n   \n\n# 邻接表\n\n邻接表中的链表，可以换成别的数据结构\n\n比如，更适合动态数据的动态数据结构 红黑树，跳表，散列表等等\n\n * 区别\n\n> 邻接矩阵对于边数较少的图来说，有些浪费存储空间，就像顺序存储一样。\n> \n> 链接表是一种数组+单链表的形式。\n\n * 定义\n\n> 图的顶点用一个一维数组存储，其中每个数据元素还需要存储指向第一个邻接点的指针。\n> \n> 图中每个顶点v的所有邻接点构成一个线性表，无向图称为顶点v的边表，有向图称为顶点v作为弧尾的出边表。\n\n * 无向图\n\n\n\n * 有向图\n   * 顶点v作为弧尾的出边表——邻接表\n   * 顶点v作为弧头的入边表——逆邻接表\n\n\n\n * 网图\n   * 增加权值信息\n\n\n\n * 代码实现\n\nimport numpy as np\nfrom 链表.LinkedList_d import *\n\n\n#定义邻接表\nclass adjacent_list:\n    #初始化\n    def __init__(self,allpeaks):\n        #allpeaks 为图的所有顶点\n        self.peaks=allpeaks #一维数组，存放所有顶点\n        self.node={}#一个字典，键为顶点，值为链表，链表的头为该顶点，其他结点为与其连通的结点\n\n    #创建每个顶点的出边表\n    def creat_outlist(self,peak0,peaks):\n\n        link=LinkedList()#初始化一个链表\n        link.add(peak0)\n\n        #传入顶点数组，第一个存放，出边表的主顶点；剩余存放其他顶点\n        if peaks!=[\'None\']:\n            for peak in peaks:\n                link.add(peak)\n\n        # 顶点的一个单链表\n        self.node[peak0] = link  # 新增一个键值对\n\n\n    #打印图\n    def printPic(self):\n        print(self.peaks)\n        #print(self.node)\n        for (k,v) in self.node.items(): #k是键(k是一个字符串)，v是值(v是一个单链表)\n            print("{}结点的边表为:".format(k))\n            v.printLinked()#输出链表\n\n\n\n#构建图(有向图，无向图)\ndef CreatPic():\n    str=input("请输入顶点(用空格分离)：")\n    allpeaks=str.split(\' \')#用空格切片，存放顶点\n    pic = adjacent_list(allpeaks)#创建邻接表\n\n    num=len(allpeaks)\n    for i in range(int(num)):\n        str=input("请输入与顶点{}连通(或者其指向)的顶点,若无输入None".format(allpeaks[i]))\n        peaks=str.split(\' \')\n        pic.creat_outlist(allpeaks[i],peaks)\n\n    return pic\n\nif __name__=="__main__":\n    pic=CreatPic() #构建无向网图\n    pic.printPic() #输出\n\n\n\n# 十字链表\n\n> 邻接表对于有向图来说，不是很友好，只能关心出度，而逆邻接表也只能关心入度，所以将二者结合，形成十字链表\n\n * 数据结构\n   * firstin为入边表头指针\n   * firstout为出边表头指针\n   * tailvex值弧起点在顶点表的下标\n   * headvex值弧终点在顶点表的下标\n   * headlink为入边表指针域，指向终点相同的下条边\n   * taillink为出边表指针域，指向起点相同的下条边\n\n\n\n * 十字链表\n   \n   * 虚线1，指的是边v1—>v0\n   * 虚线2，指的是边v2—>v0\n   * 虚线3，指的是边v2—>v1\n   * 虚线4，指的是边v1—>v2\n   * 虚线5，指的是边v0—>v3\n   \n   \n\n# 邻接多重表?\n\n> 对于无向图的邻接表，更好的是关注顶点的操作；若要是关注边的操作，则可以使用邻接多重表\n> \n> 邻接表一条边，用两个结点表示；而邻接多重表的一条边，用一个结点表示\n\n * 边表结点结构 *\n\n\n\n * 邻接多重表\n\n\n\n# 边集数组\n\n> 边集数组由一个一维数组和一个二维数组组成，一维数组存放顶点，二维数组存放边的信息\n\n\n\n\n# 图的遍历\n\n> 从图中某一顶点出发仿遍图中其余顶点，且使每一个顶点仅被访问一次，这一过程叫做图的遍历。\n\n# 深度优先遍历\n\n> 从上到下，先搜一侧的最深处，然后在返回搜索其他地方的最深处，直到遍历结束。\n> \n> 类似于树的中序遍历，是一个递归的过程。\n\n * 代码\n   \n   * 邻接矩阵\n   \n   #深度优先递归算法-邻接矩阵\n   def DFS_matrix(pic,i):\n       visited1[i]=True #访问过的结点置为 true\n       print(pic.peak[i]) #打印结点\n       for j in range(length):\n           if(pic.side[i][j]==1 and visited1[j]==False):\n               DFS_matrix(pic,j)\n   \n   #邻接矩阵版深度优先遍历\n   def traverse_deep_matrix(pic):\n       for i in range(length):#执行递归算法，如果图是连通的，则只执行一次\n           if not visited1[i]:\n               print(111)\n               DFS_matrix(pic,i)\n   \n   \n   \n   * 邻接表\n   \n   #深度优先递归算法-邻接表\n   def DFS_list(pic,i):\n       visited2[i]=True #访问过的结点置为 true\n       peak=pic.peaks[i]#当前下标的结点\n       link=pic.node[peak]#找到以其作为下标的链表\n   \n       print(link.head.data)  # 打印头结点的数据\n       #print(2,peak)\n   \n       p=link.head.getNext()  #首个结点 算是 边表的开头\n   \n       #print(2223)\n       #如果链表不空，一直遍历\n       while(p!=None):\n           #print(333)\n           #print(p.getData())\n           j = pic.peaks.index(p.getData())  # 获取结点数据在结点表的位置\n           #print(j)\n           if(visited2[j]==False): #p不为空 或者 该节点没被访问\n               DFS_list(pic,j)#递归调用\n           p=p.getNext()#指针后移\n   \n   \n   #邻接表版深度优先遍历\n   def traverse_deep_list(pic):\n       for i in range(length1):#执行递归算法，如果图是连通的，则只执行一次\n           if not visited2[i]:\n               #print(222)\n               DFS_list(pic,i)#传入链表 node是字典类型，键为peak，值为该点对应的边表\n   \n\n# 广度优先遍历\n\n> 从上到下，从左到右，一层一层的搜索，直到遍历结束。\n> \n> 类似于树的层序遍历。\n\n * 代码\n   \n   * 邻接矩阵\n   \n   # 广度优先递归算法-邻接矩阵\n   def BFS_matrix(pic):\n       q=Queue() #初始化队列\n       length=len(pic.peak)\n       vistied=[False]*length #初始化参观矩阵\n       for i in range(length):\n           #如果是连通图，只循环一次即可\n           if(not vistied[i]):\n               print(111)\n               #如果没被访问过\n               vistied[i]=True\n               peak=pic.peak[i] #得到顶点\n               print(peak) #打印连通图的起点\n               q.EnQueue(peak)#入队列\n               q.printQ()#打印队列\n               while(not q.IsEmpty()):\n                   peak=q.DnQueue()\n                   i=pic.peak.index(peak)\n                   for j in range(length):\n                       if (pic.side[i][j]==1 and vistied[j]==False):\n                           vistied[j]=True\n                           peak=pic.peak[j]\n                           print(peak)\n                           q.EnQueue(peak)\n                   q.printQ()  # 打印队列\n   \n   \n   * 邻接表\n   \n   # 深度优先递归算法-邻接表\n   def BFS_list(pic):\n       q=Queue() #初始化队列\n       length=len(pic.peaks)\n       vistied=[False]*length #初始化参观矩阵\n       for i in range(length):\n           #如果是连通图，只循环一次即可\n           if(not vistied[i]):\n               print(111)\n               #如果没被访问过\n               vistied[i]=True\n               peak=pic.peaks[i] #得到顶点\n               print(peak) #打印连通图的起点\n               q.EnQueue(peak)#入队列\n               q.printQ()#打印队列\n               while(not q.IsEmpty()):\n                   peak=q.DnQueue()\n                   i=pic.peaks.index(peak)\n                   link=pic.node[peak] #得到对应结点的列表\n                   p = link.head.getNext()  # 首个结点 算是 边表的开头\n   \n                   # 如果链表不空，一直遍历\n                   while (p != None):\n                       peak=p.getData()\n                       j=pic.peaks.index(peak) #得到结点对应的下标\n                       if (vistied[j]==False):\n                           vistied[j]=True\n                           print(peak)\n                           q.EnQueue(peak)\n                       p=p.getNext()\n                   q.printQ()  # 打印队列\n   \n\n\n# 最小生成树\n\n> 把构造连通网的最小代价生成树称为最小生成树。\n> \n> n个结点，n-1条边\n> \n> Prim算法对于稠密图，边多的情况有优势。\n> \n> Kruskal算法对于稀疏图，边少的情况有优势。（针对于边开展的）\n\n# 普里姆(Prim)算法\n\n> 设V为所有结点的合集，U为已加入最小生成树边所依附的结点的合集，TE为最小生成树的边的合集。\n> \n> 则在所有u属于U,v属于(V-U)，的边（u,v）中找到一个代价最小的边（u0，v0），加入TE，同时将v0加入U，直到V等于U为止。\n\ndef MinTree_prim(pic):\n    length=len(pic.peak)\n    peak=pic.peak[0]\n    U=[peak]#初始化 最小生成树结点--U\n    V=pic.peak.copy()\n    V.remove(peak)#剩余结点合集\n    while(V): #如果V不是空，就一直循环\n        min=65536\n        min_list = []#每次循环都重置\n        for u in U:\n            #print(u)\n            #print(pic.peak)\n            i=pic.peak.index(u)\n            for j in range(length):\n                peak=pic.side[i][j]\n                #print(peak)\n                if peak>0 and peak<min and (pic.peak[j] in V):\n                    min=peak\n                    min_list = [i, j, pic.side[i][j]]\n\n                    # min_list=[begin,end,pic.side[i][j],j]\n                    #min_side.append(list)#将所有的边存入\n        if(min_list):\n            jnum=min_list[1]\n            inum=min_list[0]\n            pic.side[inum][jnum]=0#将找出的最小的边权值设为0\n            begin=pic.peak[inum]\n            end=pic.peak[jnum]\n            print("({},{})-{}".format(begin,end,min_list[2])) #打印最小边\n            U.append(end)\n            V.remove(end)\n\n\n# 克鲁斯卡尔(Kruskal)算法\n\n> 将边集数组按照 权值排好序，优先选择权值小且不构成回路的边加入最小生成树，直到遍历结束。\n\n#Kruskal算法 生成最小生成树\ndef MinTree_Kruskal(pic):\n    length=pic.getNodeNum()#结点个数\n    parents=[0]*length #初始化数组\n    for list in pic.elist:\n        n=Find(parents,list[0])#传入begin\n        m=Find(parents,list[1])#传入end\n        if(n!=m): #此边与现有树还没有生成环\n            parents[n]=m\n            print("({},{})-{}".format(list[0],list[1],list[-1]))\n    print(parents)\n\n#查找连线顶点的尾端\ndef Find(parents,f):\n    while(parents[f]>0):\n        f=parents[f]\n    return f\n\n\nif __name__=="__main__":\n    # 初始化边集数组\n    edges=[[4,7,7],[2,8,8],[0,1,10],[0,5,11],[1,8,12],[3,7,16],[1,6,16],[5,6,17],[1,2,18],[6,7,19],[3,4,20],[3,8,21],[2,3,22],[3,6,24],[4,5,26]]\n    pic=Edges()\n    for edge in edges:\n        #print(edge)\n        pic.add(edge[0],edge[1],edge[-1])\n    #print(e.elist)\n\n    MinTree_Kruskal(pic)\n\n\n\n# 最短路径\n\n> 对于网图，最短路径是指两顶点之间经过的边上权值之和最少的路径，并且我们称路径上的第一个顶点为源点，最后一个顶点为终点。\n\n# 迪杰斯特拉(Dijkstra)算法\n\n> 每次循环都找源点到下一个结点的最短距离，直到终点。\n> \n> 大循环中的两个小循环，第一个小循环找最小的D[j],第二个小循环修正D[j]，并给P[j]赋值\n\n#最短路径问题\ndef ShortPath_Dijkstra(pic):\n    length=len(pic.peak)\n    final=[0]*length\n    k=length\n    final[0]=1 #初始化final 置1表示已经访问到，置0表示还没有访问\n    D=pic.side[0]#初始化 D  表示源点到任意点的距离，默认为0号到其他点的距离\n    P=[0]*length #初始化路径数组，表示源点到该点的最短路径的前驱结点\n\n    #主循环，每次求得v0到某个点的最短路径\n    for i in range(1,length):\n        min=65535\n        #寻找距离v0最近的点-D最小的\n        for j in range(length):\n            if final[j]==0 and D[j]<min and D[j]!=0:\n               k=j\n               min=D[j]\n\n        final[k]=1 #表示第k个结点已经被寻得\n\n        #修正当前最短路径及距离-找到最小值所在点k点与其相邻的点j的和 与D[j]做比较\n        for j in range(length):\n            if final[j]==0 and (min+pic.side[k][j]<D[j] or D[j]==0) and pic.side[k][j]!=0:\n                D[j]=min+pic.side[k][j] #修改当前路径长度\n                P[j]=k #修改访问前驱\n\n    return P,D\n\n\n# 弗洛伊德(Floyd)算法\n\n> 寻找v到w的最小距离，比较原始距离和经过k点中转的距离，哪个小取哪个\n> \n> D[v][w]=min{D[v][w],D[v][k]+D[k][w]}\n> \n> 可以取得所有点到其他点的最短路径，O(n^3)\n\n#最短路径问题\ndef ShortPath_Floyd(pic):\n    #初始化 P D\n    length=len(pic.peak)\n    D=pic.side.copy()\n    P=[]\n    for i in range(length):\n        p = []\n        for j in range(length):\n            p.append(j)\n            if D[i][j]==0 and i!=j:\n                D[i][j]=65536\n        P.append(p)\n\n    #寻找最小D[v][w]\n    for k in range(length):#k为中转点\n        for v in range(length):\n            for w in range(length):\n                if(D[v][w]>D[v][k]+D[k][w]):\n                    D[v][w]=D[v][k]+D[k][w] #修正最小路径\n                    P[v][w]=P[v][k] #修正路径前驱结点\n\n\n    return P,D\n\n\n\n# 拓扑排序\n\n * AOV网\n   \n   > 在一个表示工程的有向图中，我们用顶点表示活动，用弧表示活动之间的关系，这样的有向图为顶点表示活动的网，称为AOV网，AOV网中不能存在回路。\n\n\n\n * 拓扑排序\n   \n   \n   \n   > 所谓拓扑排序，就是对一个有向图构造拓扑序列的过程。拓扑序列不唯一。\n   > \n   > 如果网的结点全部被输出，即是一个拓扑序列，即此网不存在回路，是AOV网。\n   > \n   > 如果有结点没有被输出，则这个网存在回路，不是AOV网。\n\n * 排序算法\n   \n   > 基本思路是：从AOV网中选择一个入度为0的顶点输出，然后删除此结点，并删除以该结点为尾的弧，重复此步骤，直到不存在入度为0的点或者所有结点全部输出为止。\n   > \n   > 算法关键：\n   > \n   > 1. 使用邻接表和栈实现\n   > \n   > 2. 用栈来存储入度为0的结点，在原邻接表的基础上增加一个入度统计表。\n   \n   def topo(pic):\n       stack=Stack()#创建一个栈，用来存储入度为0的点\n       length=len(pic.peaks)\n       count=0 #出栈的顶点个数\n       peaklist=[]\n   \n       # #找入度值，构造入度数组---复杂\n       #法1-O(n^2)\n       #in_du = []\n       # for peak in pic.peaks:\n       #     num = 0  # 用来统计入度数\n       #     for v in pic.node.values():\n       #         if v.search(int(peak)):\n       #             num=num+1\n       #     in_du.append(num-1)\n   \n       #法2-O(n)\n       in_du = [-1]*length\n       for v in pic.node.values():\n           current=v.head\n           while current:\n               data=current.getData()\n               index=pic.peaks.index(data)\n               in_du[index]=in_du[index]+1\n               current=current.getNext()\n   \n       #对每个结点进行遍历，将度为0的点入栈\n       for i in range(length):\n           if in_du[i]==0:\n               stack.push(pic.peaks[i])\n   \n       #如果栈不空\n       while(not stack.isEmpty()):\n           peak=stack.pop()\n           peaklist.append(peak) #将入度为0的点存储，最后一起输出\n           count+=1#个数加一\n           link=pic.node[peak]#弹出结点的边表\n           current=link.head.getNext()\n           while(current):\n               data=current.getData()\n               #print(data)\n               index=pic.peaks.index(data)\n               #print(index)\n               in_du[index]-=1\n               if(in_du[index]==0):\n                   stack.push(data)\n               current=current.getNext()\n       print(peaklist)\n   \n       if count<length:\n           return False\n       else:\n           return True\n   \n\n\n# 关键路径\n\n * AOE网\n   \n   > 用顶点表示事件，用有向边表示活动，用边上的权值表示活动的持续时间，这种有向图的边表示活动的网，我们称之为AOE网。\n   > \n   > 与AOV网对比,AOE网侧重工程需要的时间。\n\n\n\n * 关键路径\n\n> 路径上各个活动所持续的时间之和称为路径长度。从源点到终点具有最大长度的路径叫做关键路径，在关键路径上的活动叫做关键活动。\n> \n> 如果要提高效率，则需要改善关键路径上的关键活动。\n\n * 算法\n   \n   * etv——正拓扑序列，取大路径\n   * ltv——反拓扑序列，取小路径\n   * ete——<vk,vj>--etv[k]--最早不能早过k\n     * 只有事件k发生了，ete活动才开始\n     * ete=etv[k]\n   * lte——<vk,vj>--最晚不能晚过j，而且还有留出工作时间\n     * lte=ltv[j]-len<vk,vj>\n\n * 程序\n\n#求拓扑序列及 etv-各个结点的最早发生时间\ndef topo_key(pic):\n    stack=Stack()#创建一个栈，用来存储入度为0的点\n    stack2=Stack()#这个栈用来存储从上个栈中弹出的结点\n    length=len(pic.peaks)\n    count=0 #出栈的顶点个数\n    peaklist=[]\n    etv=[0]*length #事件最早发生时间\n\n    #法2-O(n)\n    in_du = [-1]*length\n    for v in pic.node.values():\n        current=v.head\n        while current:\n            data=current.getData()\n            peak=data.split(\',\')[0]#第一个是数据结点\n            index=pic.peaks.index(peak)\n            in_du[index]=in_du[index]+1\n            current=current.getNext()\n\n    #对每个结点进行遍历，将度为0的点入栈\n    for i in range(length):\n        if in_du[i]==0:\n            stack.push(i)\n\n    #如果栈不空\n    while(not stack.isEmpty()):\n        peak_index=stack.pop() #将入度为0的点的下标出栈\n        peak=pic.peaks[peak_index]#下标对应的结点\n        stack2.push(peak_index)#将上一个栈出的点入栈\n        peaklist.append(peak) #将入度为0的点存储，最后一起输出\n        count+=1#个数加一\n        link=pic.node[peak]#弹出结点的边表\n        current=link.head.getNext()\n        while(current):\n            data=current.getData()\n            peak = data.split(\',\')[0]  # 第一个是数据结点\n            weight=int(data.split(\',\')[1])  # 第二个是权重\n            #print(peak)\n            index=pic.peaks.index(peak)\n            #print(index)\n            in_du[index]-=1\n            if(in_du[index]==0):#如果该结点入度为0，则入栈\n                stack.push(index)\n\n            if(etv[index]<etv[peak_index]+weight):#取大值\n                etv[index]=etv[peak_index]+weight\n\n            current=current.getNext()\n    print("拓扑序列为： ",peaklist)\n    print("拓扑序列各顶点对应最早时间为: ",etv)\n\n    if count<length:\n        return False\n    else:\n        return etv,stack2\n\n\n#打印关键路径\ndef key_path(pic):\n    #调用拓扑排序，得到输出结果\n    s=topo_key(pic)\n    if(s):\n        etv=s[0]\n        stank=s[1]\n    else:\n        print("拓扑序列错误！")\n        return False\n\n    length=len(etv)\n    ltv=[etv[length-1]]*length#初始化ltv 全为etv的最后一个值\n\n    #出栈，计算ltv\n    while(not stank.isEmpty()):\n        peak_index=stank.pop()# 将入度为0的点的下标出栈\n        peak = pic.peaks[peak_index]  # 下标对应的结点\n        link = pic.node[peak]  # 弹出结点的边表\n        current = link.head.getNext()\n\n        while (current): #如果链表不为空\n            data = current.getData()\n            peak = data.split(\',\')[0]  # 第一个是数据结点\n            weight = int(data.split(\',\')[1])  # 第二个是权重\n            # print(peak)\n            index = pic.peaks.index(peak)#栈中弹出结点指向的结点的下标\n\n            if (ltv[peak_index] > ltv[index] - weight):  # 取小值\n                ltv[peak_index] = ltv[index] - weight\n\n            current = current.getNext()\n\n    #计算关键路径\n    key_list=[] #存放关键路径\n    for i in range(length): #遍历每个结点\n        peak=pic.peaks[i]\n        link=pic.node[peak]\n        current=link.head.getNext()\n        while(current):#遍历每个结点指向的边，即结点的边表\n            data = current.getData()\n            peak = data.split(\',\')[0]  # 第一个是数据结点\n            weight = int(data.split(\',\')[1])  # 第二个是权重\n            index = pic.peaks.index(peak)  # 指向结点的下标\n            ete=etv[i] #活动最早开始时间不能早过 其弧尾的时间\n            lte=ltv[index]-weight#活动最晚开始的时间不能晚过其弧头的时间，其中还要预留出活动的时间\n            if(ete==lte): #如果两者相等，则说明是关键路径\n                path="<{},{}>-{}".format(pic.peaks[i],peak,weight)\n                key_list.append(path)\n\n            current=current.getNext()\n\n    print("关键路径为： ",key_list)\n',normalizedContent:'# 图的定义\n\n * 图中的数据元素称之为顶点，顶点之间的逻辑关系用边来表示。\n\n * 按方向分\n   \n   * 有向图——边有方向\n     * a到d的有向边为弧，a为弧尾，d为弧头\n     * <a,d>表示此弧\n     * 如果任意两个顶点之间都存在方向相反的弧，则称为 有向完全图\n   * 无向图——边无方向\n     * (a,d)或者(d,a)表示此边\n     * 若任意两个顶点之间都存在边，则称为 无向完全图\n\n * 权与网\n   \n   * 这种与图的边或弧相关的数字，叫做权\n   * 带权的图称为网\n\n * 度\n   \n   * 无向图任一顶点的度是与其相关联的边的数目，总边数为总度数的一半\n   * 有向图\n     * 以v为头的弧称为v的入度——箭头指向v\n     * 以v为尾的弧称为v的出度——箭头从v指出\n\n * 连通\n   \n   * 若顶点间存在路径，则说明顶点间是连通的 。\n   * 如果路径最终回到起始点则称为环，顶点不出现重复的路径称为简单路径。\n   * 若任意两点都是连通的，则图就是连通图，有向则称为强连通图。\n   * 图中有子图，若子图极大连通，则称为连通分量，有向称为称为强连通分量\n   * 无向图连通，且n个顶点n-1条边叫做生成树。有向图中一顶点入度为0，其余顶点出度为1，则称为有向树。\n   * 图1为普通的连通图，图2和图3为图1的生成树，图4不是（不连通）。\n\n\n# 图的存储结构\n\n# 邻接矩阵\n\n * 定义\n   \n   > 邻接矩阵是一种顺序存储结构。\n   > \n   > 邻接矩阵存储方式，是用两个数组来表示图。一个一维数组存储图中的顶点信息，一个二维数组存储图中的边或弧的信息。\n\n * 无向图\n   \n   * 有边连通记为1，否则为0\n   * 无向图的连接矩阵为对称阵\n\n\n\n * 有向图\n   * 横着看，行，表示指出（第一行，为v0指向v3），行和为出度\n   * 竖着看，列，表示指向(第一列，v1指向v0，v2指向v0)，列和为入度\n\n\n\n * 网\n   * 原邻接矩阵为1的地方改为权\n   * 原邻接矩阵为0的地方,除了对角线保持不变，其他改为无穷大(int——>65536)\n\n\n\n * 代码实现\n   \n   * 构造无向网图\n   \n   #实现无向网图\n   #定义邻接矩阵类\n   class adjacent_matrix:\n   \n       def __init__(self,peak,side): #初始化\n           self.peak=peak\n           self.side=side\n   \n       def printpic(self): #输出\n           print(self.peak)\n           print(self.side)\n   \n   #构建无向网图\n   def creatpic():\n       str=input("请输入顶点(用空格分离)：")\n       peak=str.split(\' \')#用空格切片，存放顶点\n       n=len(peak)\n       num=input("请输入边数")\n       side=np.zeros([n,n])\n       for i in range(int(num)):\n           str=input("请输入连通顶点，权")\n           power=str.split(\' \')\n           x=peak.index(power[0])\n           y=peak.index(power[1])\n           side[x][y]=power[2]\n           side[y][x] = power[2]  #对称矩阵\n       pic=adjacent_matrix(peak,side)\n       return pic\n   \n   \n   if __name__=="__main__":\n       pic=creatpic() #构建无向网图\n       pic.printpic() #输出\n   \n\n# 邻接表\n\n邻接表中的链表，可以换成别的数据结构\n\n比如，更适合动态数据的动态数据结构 红黑树，跳表，散列表等等\n\n * 区别\n\n> 邻接矩阵对于边数较少的图来说，有些浪费存储空间，就像顺序存储一样。\n> \n> 链接表是一种数组+单链表的形式。\n\n * 定义\n\n> 图的顶点用一个一维数组存储，其中每个数据元素还需要存储指向第一个邻接点的指针。\n> \n> 图中每个顶点v的所有邻接点构成一个线性表，无向图称为顶点v的边表，有向图称为顶点v作为弧尾的出边表。\n\n * 无向图\n\n\n\n * 有向图\n   * 顶点v作为弧尾的出边表——邻接表\n   * 顶点v作为弧头的入边表——逆邻接表\n\n\n\n * 网图\n   * 增加权值信息\n\n\n\n * 代码实现\n\nimport numpy as np\nfrom 链表.linkedlist_d import *\n\n\n#定义邻接表\nclass adjacent_list:\n    #初始化\n    def __init__(self,allpeaks):\n        #allpeaks 为图的所有顶点\n        self.peaks=allpeaks #一维数组，存放所有顶点\n        self.node={}#一个字典，键为顶点，值为链表，链表的头为该顶点，其他结点为与其连通的结点\n\n    #创建每个顶点的出边表\n    def creat_outlist(self,peak0,peaks):\n\n        link=linkedlist()#初始化一个链表\n        link.add(peak0)\n\n        #传入顶点数组，第一个存放，出边表的主顶点；剩余存放其他顶点\n        if peaks!=[\'none\']:\n            for peak in peaks:\n                link.add(peak)\n\n        # 顶点的一个单链表\n        self.node[peak0] = link  # 新增一个键值对\n\n\n    #打印图\n    def printpic(self):\n        print(self.peaks)\n        #print(self.node)\n        for (k,v) in self.node.items(): #k是键(k是一个字符串)，v是值(v是一个单链表)\n            print("{}结点的边表为:".format(k))\n            v.printlinked()#输出链表\n\n\n\n#构建图(有向图，无向图)\ndef creatpic():\n    str=input("请输入顶点(用空格分离)：")\n    allpeaks=str.split(\' \')#用空格切片，存放顶点\n    pic = adjacent_list(allpeaks)#创建邻接表\n\n    num=len(allpeaks)\n    for i in range(int(num)):\n        str=input("请输入与顶点{}连通(或者其指向)的顶点,若无输入none".format(allpeaks[i]))\n        peaks=str.split(\' \')\n        pic.creat_outlist(allpeaks[i],peaks)\n\n    return pic\n\nif __name__=="__main__":\n    pic=creatpic() #构建无向网图\n    pic.printpic() #输出\n\n\n\n# 十字链表\n\n> 邻接表对于有向图来说，不是很友好，只能关心出度，而逆邻接表也只能关心入度，所以将二者结合，形成十字链表\n\n * 数据结构\n   * firstin为入边表头指针\n   * firstout为出边表头指针\n   * tailvex值弧起点在顶点表的下标\n   * headvex值弧终点在顶点表的下标\n   * headlink为入边表指针域，指向终点相同的下条边\n   * taillink为出边表指针域，指向起点相同的下条边\n\n\n\n * 十字链表\n   \n   * 虚线1，指的是边v1—>v0\n   * 虚线2，指的是边v2—>v0\n   * 虚线3，指的是边v2—>v1\n   * 虚线4，指的是边v1—>v2\n   * 虚线5，指的是边v0—>v3\n   \n   \n\n# 邻接多重表?\n\n> 对于无向图的邻接表，更好的是关注顶点的操作；若要是关注边的操作，则可以使用邻接多重表\n> \n> 邻接表一条边，用两个结点表示；而邻接多重表的一条边，用一个结点表示\n\n * 边表结点结构 *\n\n\n\n * 邻接多重表\n\n\n\n# 边集数组\n\n> 边集数组由一个一维数组和一个二维数组组成，一维数组存放顶点，二维数组存放边的信息\n\n\n\n\n# 图的遍历\n\n> 从图中某一顶点出发仿遍图中其余顶点，且使每一个顶点仅被访问一次，这一过程叫做图的遍历。\n\n# 深度优先遍历\n\n> 从上到下，先搜一侧的最深处，然后在返回搜索其他地方的最深处，直到遍历结束。\n> \n> 类似于树的中序遍历，是一个递归的过程。\n\n * 代码\n   \n   * 邻接矩阵\n   \n   #深度优先递归算法-邻接矩阵\n   def dfs_matrix(pic,i):\n       visited1[i]=true #访问过的结点置为 true\n       print(pic.peak[i]) #打印结点\n       for j in range(length):\n           if(pic.side[i][j]==1 and visited1[j]==false):\n               dfs_matrix(pic,j)\n   \n   #邻接矩阵版深度优先遍历\n   def traverse_deep_matrix(pic):\n       for i in range(length):#执行递归算法，如果图是连通的，则只执行一次\n           if not visited1[i]:\n               print(111)\n               dfs_matrix(pic,i)\n   \n   \n   \n   * 邻接表\n   \n   #深度优先递归算法-邻接表\n   def dfs_list(pic,i):\n       visited2[i]=true #访问过的结点置为 true\n       peak=pic.peaks[i]#当前下标的结点\n       link=pic.node[peak]#找到以其作为下标的链表\n   \n       print(link.head.data)  # 打印头结点的数据\n       #print(2,peak)\n   \n       p=link.head.getnext()  #首个结点 算是 边表的开头\n   \n       #print(2223)\n       #如果链表不空，一直遍历\n       while(p!=none):\n           #print(333)\n           #print(p.getdata())\n           j = pic.peaks.index(p.getdata())  # 获取结点数据在结点表的位置\n           #print(j)\n           if(visited2[j]==false): #p不为空 或者 该节点没被访问\n               dfs_list(pic,j)#递归调用\n           p=p.getnext()#指针后移\n   \n   \n   #邻接表版深度优先遍历\n   def traverse_deep_list(pic):\n       for i in range(length1):#执行递归算法，如果图是连通的，则只执行一次\n           if not visited2[i]:\n               #print(222)\n               dfs_list(pic,i)#传入链表 node是字典类型，键为peak，值为该点对应的边表\n   \n\n# 广度优先遍历\n\n> 从上到下，从左到右，一层一层的搜索，直到遍历结束。\n> \n> 类似于树的层序遍历。\n\n * 代码\n   \n   * 邻接矩阵\n   \n   # 广度优先递归算法-邻接矩阵\n   def bfs_matrix(pic):\n       q=queue() #初始化队列\n       length=len(pic.peak)\n       vistied=[false]*length #初始化参观矩阵\n       for i in range(length):\n           #如果是连通图，只循环一次即可\n           if(not vistied[i]):\n               print(111)\n               #如果没被访问过\n               vistied[i]=true\n               peak=pic.peak[i] #得到顶点\n               print(peak) #打印连通图的起点\n               q.enqueue(peak)#入队列\n               q.printq()#打印队列\n               while(not q.isempty()):\n                   peak=q.dnqueue()\n                   i=pic.peak.index(peak)\n                   for j in range(length):\n                       if (pic.side[i][j]==1 and vistied[j]==false):\n                           vistied[j]=true\n                           peak=pic.peak[j]\n                           print(peak)\n                           q.enqueue(peak)\n                   q.printq()  # 打印队列\n   \n   \n   * 邻接表\n   \n   # 深度优先递归算法-邻接表\n   def bfs_list(pic):\n       q=queue() #初始化队列\n       length=len(pic.peaks)\n       vistied=[false]*length #初始化参观矩阵\n       for i in range(length):\n           #如果是连通图，只循环一次即可\n           if(not vistied[i]):\n               print(111)\n               #如果没被访问过\n               vistied[i]=true\n               peak=pic.peaks[i] #得到顶点\n               print(peak) #打印连通图的起点\n               q.enqueue(peak)#入队列\n               q.printq()#打印队列\n               while(not q.isempty()):\n                   peak=q.dnqueue()\n                   i=pic.peaks.index(peak)\n                   link=pic.node[peak] #得到对应结点的列表\n                   p = link.head.getnext()  # 首个结点 算是 边表的开头\n   \n                   # 如果链表不空，一直遍历\n                   while (p != none):\n                       peak=p.getdata()\n                       j=pic.peaks.index(peak) #得到结点对应的下标\n                       if (vistied[j]==false):\n                           vistied[j]=true\n                           print(peak)\n                           q.enqueue(peak)\n                       p=p.getnext()\n                   q.printq()  # 打印队列\n   \n\n\n# 最小生成树\n\n> 把构造连通网的最小代价生成树称为最小生成树。\n> \n> n个结点，n-1条边\n> \n> prim算法对于稠密图，边多的情况有优势。\n> \n> kruskal算法对于稀疏图，边少的情况有优势。（针对于边开展的）\n\n# 普里姆(prim)算法\n\n> 设v为所有结点的合集，u为已加入最小生成树边所依附的结点的合集，te为最小生成树的边的合集。\n> \n> 则在所有u属于u,v属于(v-u)，的边（u,v）中找到一个代价最小的边（u0，v0），加入te，同时将v0加入u，直到v等于u为止。\n\ndef mintree_prim(pic):\n    length=len(pic.peak)\n    peak=pic.peak[0]\n    u=[peak]#初始化 最小生成树结点--u\n    v=pic.peak.copy()\n    v.remove(peak)#剩余结点合集\n    while(v): #如果v不是空，就一直循环\n        min=65536\n        min_list = []#每次循环都重置\n        for u in u:\n            #print(u)\n            #print(pic.peak)\n            i=pic.peak.index(u)\n            for j in range(length):\n                peak=pic.side[i][j]\n                #print(peak)\n                if peak>0 and peak<min and (pic.peak[j] in v):\n                    min=peak\n                    min_list = [i, j, pic.side[i][j]]\n\n                    # min_list=[begin,end,pic.side[i][j],j]\n                    #min_side.append(list)#将所有的边存入\n        if(min_list):\n            jnum=min_list[1]\n            inum=min_list[0]\n            pic.side[inum][jnum]=0#将找出的最小的边权值设为0\n            begin=pic.peak[inum]\n            end=pic.peak[jnum]\n            print("({},{})-{}".format(begin,end,min_list[2])) #打印最小边\n            u.append(end)\n            v.remove(end)\n\n\n# 克鲁斯卡尔(kruskal)算法\n\n> 将边集数组按照 权值排好序，优先选择权值小且不构成回路的边加入最小生成树，直到遍历结束。\n\n#kruskal算法 生成最小生成树\ndef mintree_kruskal(pic):\n    length=pic.getnodenum()#结点个数\n    parents=[0]*length #初始化数组\n    for list in pic.elist:\n        n=find(parents,list[0])#传入begin\n        m=find(parents,list[1])#传入end\n        if(n!=m): #此边与现有树还没有生成环\n            parents[n]=m\n            print("({},{})-{}".format(list[0],list[1],list[-1]))\n    print(parents)\n\n#查找连线顶点的尾端\ndef find(parents,f):\n    while(parents[f]>0):\n        f=parents[f]\n    return f\n\n\nif __name__=="__main__":\n    # 初始化边集数组\n    edges=[[4,7,7],[2,8,8],[0,1,10],[0,5,11],[1,8,12],[3,7,16],[1,6,16],[5,6,17],[1,2,18],[6,7,19],[3,4,20],[3,8,21],[2,3,22],[3,6,24],[4,5,26]]\n    pic=edges()\n    for edge in edges:\n        #print(edge)\n        pic.add(edge[0],edge[1],edge[-1])\n    #print(e.elist)\n\n    mintree_kruskal(pic)\n\n\n\n# 最短路径\n\n> 对于网图，最短路径是指两顶点之间经过的边上权值之和最少的路径，并且我们称路径上的第一个顶点为源点，最后一个顶点为终点。\n\n# 迪杰斯特拉(dijkstra)算法\n\n> 每次循环都找源点到下一个结点的最短距离，直到终点。\n> \n> 大循环中的两个小循环，第一个小循环找最小的d[j],第二个小循环修正d[j]，并给p[j]赋值\n\n#最短路径问题\ndef shortpath_dijkstra(pic):\n    length=len(pic.peak)\n    final=[0]*length\n    k=length\n    final[0]=1 #初始化final 置1表示已经访问到，置0表示还没有访问\n    d=pic.side[0]#初始化 d  表示源点到任意点的距离，默认为0号到其他点的距离\n    p=[0]*length #初始化路径数组，表示源点到该点的最短路径的前驱结点\n\n    #主循环，每次求得v0到某个点的最短路径\n    for i in range(1,length):\n        min=65535\n        #寻找距离v0最近的点-d最小的\n        for j in range(length):\n            if final[j]==0 and d[j]<min and d[j]!=0:\n               k=j\n               min=d[j]\n\n        final[k]=1 #表示第k个结点已经被寻得\n\n        #修正当前最短路径及距离-找到最小值所在点k点与其相邻的点j的和 与d[j]做比较\n        for j in range(length):\n            if final[j]==0 and (min+pic.side[k][j]<d[j] or d[j]==0) and pic.side[k][j]!=0:\n                d[j]=min+pic.side[k][j] #修改当前路径长度\n                p[j]=k #修改访问前驱\n\n    return p,d\n\n\n# 弗洛伊德(floyd)算法\n\n> 寻找v到w的最小距离，比较原始距离和经过k点中转的距离，哪个小取哪个\n> \n> d[v][w]=min{d[v][w],d[v][k]+d[k][w]}\n> \n> 可以取得所有点到其他点的最短路径，o(n^3)\n\n#最短路径问题\ndef shortpath_floyd(pic):\n    #初始化 p d\n    length=len(pic.peak)\n    d=pic.side.copy()\n    p=[]\n    for i in range(length):\n        p = []\n        for j in range(length):\n            p.append(j)\n            if d[i][j]==0 and i!=j:\n                d[i][j]=65536\n        p.append(p)\n\n    #寻找最小d[v][w]\n    for k in range(length):#k为中转点\n        for v in range(length):\n            for w in range(length):\n                if(d[v][w]>d[v][k]+d[k][w]):\n                    d[v][w]=d[v][k]+d[k][w] #修正最小路径\n                    p[v][w]=p[v][k] #修正路径前驱结点\n\n\n    return p,d\n\n\n\n# 拓扑排序\n\n * aov网\n   \n   > 在一个表示工程的有向图中，我们用顶点表示活动，用弧表示活动之间的关系，这样的有向图为顶点表示活动的网，称为aov网，aov网中不能存在回路。\n\n\n\n * 拓扑排序\n   \n   \n   \n   > 所谓拓扑排序，就是对一个有向图构造拓扑序列的过程。拓扑序列不唯一。\n   > \n   > 如果网的结点全部被输出，即是一个拓扑序列，即此网不存在回路，是aov网。\n   > \n   > 如果有结点没有被输出，则这个网存在回路，不是aov网。\n\n * 排序算法\n   \n   > 基本思路是：从aov网中选择一个入度为0的顶点输出，然后删除此结点，并删除以该结点为尾的弧，重复此步骤，直到不存在入度为0的点或者所有结点全部输出为止。\n   > \n   > 算法关键：\n   > \n   > 1. 使用邻接表和栈实现\n   > \n   > 2. 用栈来存储入度为0的结点，在原邻接表的基础上增加一个入度统计表。\n   \n   def topo(pic):\n       stack=stack()#创建一个栈，用来存储入度为0的点\n       length=len(pic.peaks)\n       count=0 #出栈的顶点个数\n       peaklist=[]\n   \n       # #找入度值，构造入度数组---复杂\n       #法1-o(n^2)\n       #in_du = []\n       # for peak in pic.peaks:\n       #     num = 0  # 用来统计入度数\n       #     for v in pic.node.values():\n       #         if v.search(int(peak)):\n       #             num=num+1\n       #     in_du.append(num-1)\n   \n       #法2-o(n)\n       in_du = [-1]*length\n       for v in pic.node.values():\n           current=v.head\n           while current:\n               data=current.getdata()\n               index=pic.peaks.index(data)\n               in_du[index]=in_du[index]+1\n               current=current.getnext()\n   \n       #对每个结点进行遍历，将度为0的点入栈\n       for i in range(length):\n           if in_du[i]==0:\n               stack.push(pic.peaks[i])\n   \n       #如果栈不空\n       while(not stack.isempty()):\n           peak=stack.pop()\n           peaklist.append(peak) #将入度为0的点存储，最后一起输出\n           count+=1#个数加一\n           link=pic.node[peak]#弹出结点的边表\n           current=link.head.getnext()\n           while(current):\n               data=current.getdata()\n               #print(data)\n               index=pic.peaks.index(data)\n               #print(index)\n               in_du[index]-=1\n               if(in_du[index]==0):\n                   stack.push(data)\n               current=current.getnext()\n       print(peaklist)\n   \n       if count<length:\n           return false\n       else:\n           return true\n   \n\n\n# 关键路径\n\n * aoe网\n   \n   > 用顶点表示事件，用有向边表示活动，用边上的权值表示活动的持续时间，这种有向图的边表示活动的网，我们称之为aoe网。\n   > \n   > 与aov网对比,aoe网侧重工程需要的时间。\n\n\n\n * 关键路径\n\n> 路径上各个活动所持续的时间之和称为路径长度。从源点到终点具有最大长度的路径叫做关键路径，在关键路径上的活动叫做关键活动。\n> \n> 如果要提高效率，则需要改善关键路径上的关键活动。\n\n * 算法\n   \n   * etv——正拓扑序列，取大路径\n   * ltv——反拓扑序列，取小路径\n   * ete——<vk,vj>--etv[k]--最早不能早过k\n     * 只有事件k发生了，ete活动才开始\n     * ete=etv[k]\n   * lte——<vk,vj>--最晚不能晚过j，而且还有留出工作时间\n     * lte=ltv[j]-len<vk,vj>\n\n * 程序\n\n#求拓扑序列及 etv-各个结点的最早发生时间\ndef topo_key(pic):\n    stack=stack()#创建一个栈，用来存储入度为0的点\n    stack2=stack()#这个栈用来存储从上个栈中弹出的结点\n    length=len(pic.peaks)\n    count=0 #出栈的顶点个数\n    peaklist=[]\n    etv=[0]*length #事件最早发生时间\n\n    #法2-o(n)\n    in_du = [-1]*length\n    for v in pic.node.values():\n        current=v.head\n        while current:\n            data=current.getdata()\n            peak=data.split(\',\')[0]#第一个是数据结点\n            index=pic.peaks.index(peak)\n            in_du[index]=in_du[index]+1\n            current=current.getnext()\n\n    #对每个结点进行遍历，将度为0的点入栈\n    for i in range(length):\n        if in_du[i]==0:\n            stack.push(i)\n\n    #如果栈不空\n    while(not stack.isempty()):\n        peak_index=stack.pop() #将入度为0的点的下标出栈\n        peak=pic.peaks[peak_index]#下标对应的结点\n        stack2.push(peak_index)#将上一个栈出的点入栈\n        peaklist.append(peak) #将入度为0的点存储，最后一起输出\n        count+=1#个数加一\n        link=pic.node[peak]#弹出结点的边表\n        current=link.head.getnext()\n        while(current):\n            data=current.getdata()\n            peak = data.split(\',\')[0]  # 第一个是数据结点\n            weight=int(data.split(\',\')[1])  # 第二个是权重\n            #print(peak)\n            index=pic.peaks.index(peak)\n            #print(index)\n            in_du[index]-=1\n            if(in_du[index]==0):#如果该结点入度为0，则入栈\n                stack.push(index)\n\n            if(etv[index]<etv[peak_index]+weight):#取大值\n                etv[index]=etv[peak_index]+weight\n\n            current=current.getnext()\n    print("拓扑序列为： ",peaklist)\n    print("拓扑序列各顶点对应最早时间为: ",etv)\n\n    if count<length:\n        return false\n    else:\n        return etv,stack2\n\n\n#打印关键路径\ndef key_path(pic):\n    #调用拓扑排序，得到输出结果\n    s=topo_key(pic)\n    if(s):\n        etv=s[0]\n        stank=s[1]\n    else:\n        print("拓扑序列错误！")\n        return false\n\n    length=len(etv)\n    ltv=[etv[length-1]]*length#初始化ltv 全为etv的最后一个值\n\n    #出栈，计算ltv\n    while(not stank.isempty()):\n        peak_index=stank.pop()# 将入度为0的点的下标出栈\n        peak = pic.peaks[peak_index]  # 下标对应的结点\n        link = pic.node[peak]  # 弹出结点的边表\n        current = link.head.getnext()\n\n        while (current): #如果链表不为空\n            data = current.getdata()\n            peak = data.split(\',\')[0]  # 第一个是数据结点\n            weight = int(data.split(\',\')[1])  # 第二个是权重\n            # print(peak)\n            index = pic.peaks.index(peak)#栈中弹出结点指向的结点的下标\n\n            if (ltv[peak_index] > ltv[index] - weight):  # 取小值\n                ltv[peak_index] = ltv[index] - weight\n\n            current = current.getnext()\n\n    #计算关键路径\n    key_list=[] #存放关键路径\n    for i in range(length): #遍历每个结点\n        peak=pic.peaks[i]\n        link=pic.node[peak]\n        current=link.head.getnext()\n        while(current):#遍历每个结点指向的边，即结点的边表\n            data = current.getdata()\n            peak = data.split(\',\')[0]  # 第一个是数据结点\n            weight = int(data.split(\',\')[1])  # 第二个是权重\n            index = pic.peaks.index(peak)  # 指向结点的下标\n            ete=etv[i] #活动最早开始时间不能早过 其弧尾的时间\n            lte=ltv[index]-weight#活动最晚开始的时间不能晚过其弧头的时间，其中还要预留出活动的时间\n            if(ete==lte): #如果两者相等，则说明是关键路径\n                path="<{},{}>-{}".format(pic.peaks[i],peak,weight)\n                key_list.append(path)\n\n            current=current.getnext()\n\n    print("关键路径为： ",key_list)\n',charsets:{cjk:!0}},{title:"回溯算法",frontmatter:{title:"回溯算法",date:"2023-07-01T16:09:46.000Z",permalink:"/pages/ec0ce8/",categories:["算法","回溯法"],tags:["知识","算法"]},regularPath:"/02.%E7%AE%97%E6%B3%95/80.%E5%9B%9E%E6%BA%AF%E6%B3%95/01.%E5%9B%9E%E6%BA%AF.html",relativePath:"02.算法/80.回溯法/01.回溯.md",key:"v-6b145c20",path:"/pages/ec0ce8/",headers:[{level:2,title:"回溯算法",slug:"回溯算法",normalizedTitle:"回溯算法",charIndex:2}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"回溯算法",content:"# 回溯算法\n\n * 定义\n   \n   > 回溯法也可以叫做回溯搜索法，它是一种搜索的方式。回溯是递归的副产品，只要有递归就会有回溯。\n   > \n   > 所以以下讲解中，回溯函数也就是递归函数，指的都是一个函数。\n\n * 效率\n   \n   > ** 虽然回溯法很难，很不好理解，但是回溯法并不是什么高效的算法**。\n   > \n   > 因为回溯的本质是穷举，穷举所有可能，然后选出我们想要的答案，如果想让回溯法高效一些，可以加一些剪枝的操作，但也改不了回溯法就是穷举的本质。\n   \n   * 效率不高为什么用？\n     \n     > 因为没得选，一些问题能暴力搜出来就不错了，撑死了再剪枝一下，还没有更高效的解法。\n\n * 解决问题\n   \n   * 组合问题：N个数里面按一定规则找出k个数的集合\n   * 切割问题：一个字符串按一定规则有几种切割方式\n   * 子集问题：一个N个数的集合里有多少符合条件的子集\n   * 排列问题：N个数按一定规则全排列，有几种排列方式\n   * 棋盘问题：N皇后，解数独等等\n\n * 如何理解回溯法\n   \n   回溯法解决的问题都可以抽象为树形结构\n   \n   > 因为回溯法解决的都是在集合中递归查找子集，集合的大小就构成了树的宽度，递归的深度，都构成的树的深度。\n   > \n   > 递归就要有终止条件，所以必然是一棵高度有限的树（N叉树）。\n\n * 回溯法模板\n   \n   * 回溯函数模板返回值以及参数\n     \n     * 返回值一般为void，更新一个全局变量\n     * 参数不好确定，一般是先写逻辑，然后需要什么参数，就填什么参数。\n   \n   * 回溯函数终止条件\n     \n     * 对树来说就是叶子结点\n     * 一般来说，就是找到了满足要求的一种情况，就可以保存答案，并结束递归\n   \n   * 回溯搜索的遍历过程\n     \n     * 回溯法一般是在集合中递归搜索，集合的大小构成了树的宽度，递归的深度构成的树的深度。\n     \n     * for循环可以理解是横向遍历，backtracking（递归）就是纵向遍历\n       \n       for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）) {\n           处理节点;\n           backtracking(路径，选择列表); // 递归,自己调用自己\n           回溯，撤销处理结果\n       }\n       \n       \n       \n   \n   * 回溯算法模板框架\n     \n     void backtracking(参数) {\n         if (终止条件) {\n             存放结果;\n             return;\n         }\n     \n         for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）) {\n             处理节点;\n             backtracking(路径，选择列表); // 递归\n             回溯，撤销处理结果\n         }\n     }\n     ",normalizedContent:"# 回溯算法\n\n * 定义\n   \n   > 回溯法也可以叫做回溯搜索法，它是一种搜索的方式。回溯是递归的副产品，只要有递归就会有回溯。\n   > \n   > 所以以下讲解中，回溯函数也就是递归函数，指的都是一个函数。\n\n * 效率\n   \n   > ** 虽然回溯法很难，很不好理解，但是回溯法并不是什么高效的算法**。\n   > \n   > 因为回溯的本质是穷举，穷举所有可能，然后选出我们想要的答案，如果想让回溯法高效一些，可以加一些剪枝的操作，但也改不了回溯法就是穷举的本质。\n   \n   * 效率不高为什么用？\n     \n     > 因为没得选，一些问题能暴力搜出来就不错了，撑死了再剪枝一下，还没有更高效的解法。\n\n * 解决问题\n   \n   * 组合问题：n个数里面按一定规则找出k个数的集合\n   * 切割问题：一个字符串按一定规则有几种切割方式\n   * 子集问题：一个n个数的集合里有多少符合条件的子集\n   * 排列问题：n个数按一定规则全排列，有几种排列方式\n   * 棋盘问题：n皇后，解数独等等\n\n * 如何理解回溯法\n   \n   回溯法解决的问题都可以抽象为树形结构\n   \n   > 因为回溯法解决的都是在集合中递归查找子集，集合的大小就构成了树的宽度，递归的深度，都构成的树的深度。\n   > \n   > 递归就要有终止条件，所以必然是一棵高度有限的树（n叉树）。\n\n * 回溯法模板\n   \n   * 回溯函数模板返回值以及参数\n     \n     * 返回值一般为void，更新一个全局变量\n     * 参数不好确定，一般是先写逻辑，然后需要什么参数，就填什么参数。\n   \n   * 回溯函数终止条件\n     \n     * 对树来说就是叶子结点\n     * 一般来说，就是找到了满足要求的一种情况，就可以保存答案，并结束递归\n   \n   * 回溯搜索的遍历过程\n     \n     * 回溯法一般是在集合中递归搜索，集合的大小构成了树的宽度，递归的深度构成的树的深度。\n     \n     * for循环可以理解是横向遍历，backtracking（递归）就是纵向遍历\n       \n       for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）) {\n           处理节点;\n           backtracking(路径，选择列表); // 递归,自己调用自己\n           回溯，撤销处理结果\n       }\n       \n       \n       \n   \n   * 回溯算法模板框架\n     \n     void backtracking(参数) {\n         if (终止条件) {\n             存放结果;\n             return;\n         }\n     \n         for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）) {\n             处理节点;\n             backtracking(路径，选择列表); // 递归\n             回溯，撤销处理结果\n         }\n     }\n     ",charsets:{cjk:!0}},{title:"递归",frontmatter:{autoSort:94,title:"递归",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/1ee87b/",categories:["算法","回溯法"],tags:["知识","算法"]},regularPath:"/02.%E7%AE%97%E6%B3%95/80.%E5%9B%9E%E6%BA%AF%E6%B3%95/50.%E9%80%92%E5%BD%92.html",relativePath:"02.算法/80.回溯法/50.递归.md",key:"v-2275b9f7",path:"/pages/1ee87b/",headers:[{level:2,title:"递归需要满足的三个条件",slug:"递归需要满足的三个条件",normalizedTitle:"递归需要满足的三个条件",charIndex:23},{level:2,title:"如何编写递归代码",slug:"如何编写递归代码",normalizedTitle:"如何编写递归代码",charIndex:112},{level:2,title:"==警惕堆栈溢出==",slug:"警惕堆栈溢出",normalizedTitle:"==警惕堆栈溢出==",charIndex:317},{level:2,title:"==警惕重复计算==",slug:"警惕重复计算",normalizedTitle:"==警惕重复计算==",charIndex:711},{level:2,title:"递归代码改为非递归代码",slug:"递归代码改为非递归代码",normalizedTitle:"递归代码改为非递归代码",charIndex:978}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"递归需要满足的三个条件 如何编写递归代码 ==警惕堆栈溢出== ==警惕重复计算== 递归代码改为非递归代码",content:"去的过程叫“递”，回来的过程叫“归”\n\n\n# 递归需要满足的三个条件\n\n * 一个问题的解可以分解为一个或几个子问题的解\n * 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样\n * 存在递归终止条件\n\n\n# 如何编写递归代码\n\n * 写出递推公式，找到终止条件\n   \n   > 写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。\n\n * ==不要试图去分解递归的每个步骤==\n   \n   > 编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。\n\n\n# ==警惕堆栈溢出==\n\n * 溢出原因\n   \n   > 函数调用会使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈。\n   > \n   > 系统栈或者虚拟机栈空间一般都不大。如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会有堆栈溢出的风险。\n\n * 如何避免\n   \n   > 我们可以通过在代码中限制递归调用的最大深度的方式来解决这个问题。\n   > \n   > 但这种做法并不能完全解决问题，因为最大允许的递归深度跟当前线程剩余的栈空间大小有关，事先无法计算。如果实时计算，代码过于复杂，就会影响代码的可读性。\n   > \n   > 所以，如果最大深度比较小，比如 10、50，就可以用这种方法，否则这种方法并不是很实用。\n   > \n   > 最好的方法 是如果遇到深层次的递归调用，就将其转换成迭代来实现\n\n\n# ==警惕重复计算==\n\n> 从图中，我们可以直观地看到，想要计算 f(5)，需要先计算 f(4) 和 f(3)，而计算 f(4) 还需要计算 f(3)，因此，f(3) 就被计算了很多次，这就是重复计算问题。\n\n * 解决办法\n   \n   * 我们可以通过一个数据结构（比如散列表）来保存已经求解过的 f(k)\n     \n     > 当递归调用到 f(k) 时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回，不需要重复计算，这样就能避免刚讲的问题了。\n   \n   * ==像这种避免重复计算的思想很重要==\n\n\n# 递归代码改为非递归代码\n\n * 递归优点\n   \n   * 代码的表达力很强，写起来简洁。\n\n * 递归缺点\n   \n   * 容易造成堆栈溢出\n   * 容易造成重复计算\n   * 空间复杂度搞——因为递归调用一次就会在内存栈中保存一次现场数据\n   * 时间复杂度较高——递归代码里多了很多函数调用，当这些函数调用的数量较大时，就会积聚成一个可观的时间成本\n\n * 用迭代循环代替递归\n   \n   * 基本上所有的递归代码都可以借助迭代循环来实现",normalizedContent:"去的过程叫“递”，回来的过程叫“归”\n\n\n# 递归需要满足的三个条件\n\n * 一个问题的解可以分解为一个或几个子问题的解\n * 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样\n * 存在递归终止条件\n\n\n# 如何编写递归代码\n\n * 写出递推公式，找到终止条件\n   \n   > 写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。\n\n * ==不要试图去分解递归的每个步骤==\n   \n   > 编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。\n\n\n# ==警惕堆栈溢出==\n\n * 溢出原因\n   \n   > 函数调用会使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈。\n   > \n   > 系统栈或者虚拟机栈空间一般都不大。如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会有堆栈溢出的风险。\n\n * 如何避免\n   \n   > 我们可以通过在代码中限制递归调用的最大深度的方式来解决这个问题。\n   > \n   > 但这种做法并不能完全解决问题，因为最大允许的递归深度跟当前线程剩余的栈空间大小有关，事先无法计算。如果实时计算，代码过于复杂，就会影响代码的可读性。\n   > \n   > 所以，如果最大深度比较小，比如 10、50，就可以用这种方法，否则这种方法并不是很实用。\n   > \n   > 最好的方法 是如果遇到深层次的递归调用，就将其转换成迭代来实现\n\n\n# ==警惕重复计算==\n\n> 从图中，我们可以直观地看到，想要计算 f(5)，需要先计算 f(4) 和 f(3)，而计算 f(4) 还需要计算 f(3)，因此，f(3) 就被计算了很多次，这就是重复计算问题。\n\n * 解决办法\n   \n   * 我们可以通过一个数据结构（比如散列表）来保存已经求解过的 f(k)\n     \n     > 当递归调用到 f(k) 时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回，不需要重复计算，这样就能避免刚讲的问题了。\n   \n   * ==像这种避免重复计算的思想很重要==\n\n\n# 递归代码改为非递归代码\n\n * 递归优点\n   \n   * 代码的表达力很强，写起来简洁。\n\n * 递归缺点\n   \n   * 容易造成堆栈溢出\n   * 容易造成重复计算\n   * 空间复杂度搞——因为递归调用一次就会在内存栈中保存一次现场数据\n   * 时间复杂度较高——递归代码里多了很多函数调用，当这些函数调用的数量较大时，就会积聚成一个可观的时间成本\n\n * 用迭代循环代替递归\n   \n   * 基本上所有的递归代码都可以借助迭代循环来实现",charsets:{cjk:!0}},{title:"动态规划",frontmatter:{autoSort:84,title:"动态规划",date:"2023-07-01T16:09:49.000Z",permalink:"/pages/f7cec8/",categories:["算法","动态规划"],tags:["知识","算法"]},regularPath:"/02.%E7%AE%97%E6%B3%95/90.%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/01.%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92.html",relativePath:"02.算法/90.动态规划/01.动态规划.md",key:"v-5aae938c",path:"/pages/f7cec8/",headers:[{level:2,title:"动态规划",slug:"动态规划",normalizedTitle:"动态规划",charIndex:2},{level:3,title:"一个模型三个特征",slug:"一个模型三个特征",normalizedTitle:"一个模型三个特征",charIndex:11},{level:3,title:"动态规划解题思路",slug:"动态规划解题思路",normalizedTitle:"动态规划解题思路",charIndex:591},{level:3,title:"如何实现搜索引擎中的拼写纠错功能？",slug:"如何实现搜索引擎中的拼写纠错功能",normalizedTitle:"如何实现搜索引擎中的拼写纠错功能？",charIndex:1039},{level:2,title:"三种算法思想比较分析",slug:"三种算法思想比较分析",normalizedTitle:"三种算法思想比较分析",charIndex:2346}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"动态规划 一个模型三个特征 动态规划解题思路 如何实现搜索引擎中的拼写纠错功能？ 三种算法思想比较分析",content:"# 动态规划\n\n\n# 一个模型三个特征\n\n * 一个模型\n   \n   它指的是动态规划适合解决的问题的模型。我把这个模型定义为“多阶段决策最优解模型”。\n   \n   > 我们一般是用动态规划来解决最优问题。而解决问题的过程，需要经历多个决策阶段。每个决策阶段都对应着一组状态。然后我们寻找一组决策序列，经过这组决策序列，能够产生最终期望求解的最优值。\n\n * 三个特征\n   \n   * 最优子结构\n     \n     > 最优子结构指的是，问题的最优解包含子问题的最优解。反过来说就是，我们可以通过子问题的最优解，推导出问题的最优解。如果我们把最优子结构，对应到我们前面定义的动态规划问题模型上，那我们也可以理解为，==后面阶段的状态可以通过前面阶段的状态推导出来==。\n   \n   * 无后效性\n     \n     > 无后效性有两层含义，第一层含义是，在推导后面阶段的状态的时候，我们只关心前面阶段的状态值，不关心这个状态是怎么一步一步推导出来的。\n     > \n     > 第二层含义是，某阶段状态一旦确定，就不受之后阶段的决策影响。无后效性是一个非常“宽松”的要求。只要满足前面提到的动态规划问题模型，其实基本上都会满足无后效性。\n   \n   * 重复子问题\n     \n     > 不同的决策序列，到达某个相同的阶段时，可能会产生重复的状态。\n\n\n# 动态规划解题思路\n\n * 状态转移表\n   \n   一般能用动态规划解决的问题，都可以使用回溯算法的暴力搜索解决。\n   \n   > 所以，当我们拿到问题的时候，我们可以先用简单的回溯算法解决，然后定义状态，每个状态表示一个节点，然后对应画出递归树。从递归树中，我们很容易可以看出来，是否存在重复子问题，以及重复子问题是如何产生的。以此来寻找规律，看是否能用动态规划解决。\n   \n   * 解决重复子问题\n     * 回溯+忘备录法\n       * 就是将遍历的子问题保存下来，在遍历的时候先查询下，如果有了就直接拿来用。\n       * 效率和动态规划差不多了。\n     * 状态转移表法\n       * dp数组，2维\n       * dp数组，1维，滚动数组\n\n * 状态转移方程\n   \n   状态转移方程是解决动态规划的关键\n   \n   * 写出状态转移方程\n   * 根据状态转移方程，填充dp数组\n   * 从dp数组中找到问题的解（一般来说最后一个是解）\n\n\n# 如何实现搜索引擎中的拼写纠错功能？\n\n程序实现参考代码随想录动态规划编辑距离\n\n * 如何量化两个字符串的相似度？——==编辑距离==\n   \n   > 顾名思义，编辑距离指的就是，将一个字符串转化成另一个字符串，需要的最少编辑操作次数（比如增加一个字符、删除一个字符、替换一个字符）。编辑距离越大，说明两个字符串的相似程度越小；相反，编辑距离就越小，说明两个字符串的相似程度越大。对于两个完全相同的字符串来说，编辑距离就是 0。\n   \n   * 莱文斯坦距离\n     \n     > 莱文斯坦距离允许增加、删除、替换字符这三个编辑操作。\n     > \n     > 将两个字符串相互转换。\n     > \n     > 莱文斯坦距离的大小，表示两个字符串差异的大小\n   \n   * 最长公共子序列\n     \n     > 最长公共子串的大小，表示两个字符串相似程度的大小。\n     \n     \n\n * 纠错优化\n   \n   > 当用户在搜索框内，输入一个拼写错误的单词时，我们就拿这个单词跟词库中的单词一一进行比较，计算编辑距离，将编辑距离最小的单词，作为纠正之后的单词，提示给用户。\n   \n   * 针对纠错效果不好的问题，我们有很多种优化思路，我这里介绍几种。\n     \n     >  1. 我们并不仅仅取出编辑距离最小的那个单词，而是**取出编辑距离最小的 TOP 10，然后根据其他参数，决策选择哪个单词作为拼写纠错单词。**比如使用搜索热门程度来决定哪个单词作为拼写纠错单词。\n     > \n     >  2. 我们还可以用多种编辑距离计算方法，比如今天讲到的两种，然后分别编辑距离最小的 TOP 10，然后求交集，用交集的结果，再继续优化处理。\n     > \n     >  3. 我们还可以通过统计用户的搜索日志，**得到最常被拼错的单词列表，以及对应的拼写正确的单词。**搜索引擎在拼写纠错的时候，首先在这个最常被拼错单词列表中查找。如果一旦找到，直接返回对应的正确的单词。这样纠错的效果非常好。\n     > \n     >  4. 我们还有更加高级一点的做法，引入个性化因素。针对每个用户，维护这个用户特有的搜索喜好，也就是常用的搜索关键词。当用户输入错误的单词的时候，我们首先在这个用户常用的搜索关键词中，计算编辑距离，查找编辑距离最小的单词。\n   \n   * 针对纠错性能方面，我们也有相应的优化方式。我讲两种分治的优化思路。\n     \n     >  1. 如果纠错功能的 TPS 不高，**我们可以部署多台机器，每台机器运行一个独立的纠错功能。**当有一个纠错请求的时候，我们通过负载均衡，分配到其中一台机器，来计算编辑距离，得到纠错单词。\n     >  2. 如果纠错系统的响应时间太长，也就是，**每个纠错请求处理时间过长，我们可以将纠错的词库，分割到很多台机器。**当有一个纠错请求的时候，我们就将这个拼写错误的单词，同时发送到这多台机器，让多台机器并行处理，分别得到编辑距离最小的单词，然后再比对合并，最终决定出一个最优的纠错单词。\n\n\n# 三种算法思想比较分析\n\n贪心，回溯，动态规划\n\n * 总结\n   \n   * 贪心：一条路走到黑，就一次机会，只能哪边看着顺眼走哪边\n   * 回溯：一条路走到黑，无数次重来的机会，还怕我走不出来 (Snapshot View)\n   * 动态规划：拥有上帝视角，手握无数平行宇宙的历史存档， 同时发展出无数个未来 (Versioned Archive View)\n\n * 贪心\n   \n   **贪心算法实际上是动态规划算法的一种特殊情况。**它解决问题起来更加高效，代码实现也更加简洁。不过，它可以解决的问题也更加有限。它能解决的问题需要满足三个条件，最优子结构、无后效性和贪心选择性（这里我们不怎么强调重复子问题）。\n   \n   **“贪心选择性”的意思是，通过局部最优的选择，能产生全局的最优选择。**每一个阶段，我们都选择当前看起来最优的决策，所有阶段的决策完成之后，最终由这些局部最优解构成全局最优解。\n\n * 回溯\n   \n   回溯算法是个“万金油”。**基本上能用的动态规划、贪心解决的问题，我们都可以用回溯算法解决。**回溯算法相当于==穷举搜索==。穷举所有的情况，然后对比得到最优解。不过，回溯算法的时间复杂度非常高，是指数级别的，只能用来解决小规模数据的问题。==对于大规模数据的问题，用回溯算法解决的执行效率就很低了。==\n\n * 动态规划\n   \n   尽管动态规划比回溯算法高效，但是，并不是所有问题，都可以用动态规划来解决。能用动态规划解决的问题，需要满足三个特征，最优子结构、无后效性和重复子问题。在重复子问题这一点上，动态规划和分治算法的区分非常明显。==分治算法要求分割成的子问题，不能有重复子问题，而动态规划正好相反，动态规划之所以高效，就是因为回溯算法实现中存在大量的重复子问题==。",normalizedContent:"# 动态规划\n\n\n# 一个模型三个特征\n\n * 一个模型\n   \n   它指的是动态规划适合解决的问题的模型。我把这个模型定义为“多阶段决策最优解模型”。\n   \n   > 我们一般是用动态规划来解决最优问题。而解决问题的过程，需要经历多个决策阶段。每个决策阶段都对应着一组状态。然后我们寻找一组决策序列，经过这组决策序列，能够产生最终期望求解的最优值。\n\n * 三个特征\n   \n   * 最优子结构\n     \n     > 最优子结构指的是，问题的最优解包含子问题的最优解。反过来说就是，我们可以通过子问题的最优解，推导出问题的最优解。如果我们把最优子结构，对应到我们前面定义的动态规划问题模型上，那我们也可以理解为，==后面阶段的状态可以通过前面阶段的状态推导出来==。\n   \n   * 无后效性\n     \n     > 无后效性有两层含义，第一层含义是，在推导后面阶段的状态的时候，我们只关心前面阶段的状态值，不关心这个状态是怎么一步一步推导出来的。\n     > \n     > 第二层含义是，某阶段状态一旦确定，就不受之后阶段的决策影响。无后效性是一个非常“宽松”的要求。只要满足前面提到的动态规划问题模型，其实基本上都会满足无后效性。\n   \n   * 重复子问题\n     \n     > 不同的决策序列，到达某个相同的阶段时，可能会产生重复的状态。\n\n\n# 动态规划解题思路\n\n * 状态转移表\n   \n   一般能用动态规划解决的问题，都可以使用回溯算法的暴力搜索解决。\n   \n   > 所以，当我们拿到问题的时候，我们可以先用简单的回溯算法解决，然后定义状态，每个状态表示一个节点，然后对应画出递归树。从递归树中，我们很容易可以看出来，是否存在重复子问题，以及重复子问题是如何产生的。以此来寻找规律，看是否能用动态规划解决。\n   \n   * 解决重复子问题\n     * 回溯+忘备录法\n       * 就是将遍历的子问题保存下来，在遍历的时候先查询下，如果有了就直接拿来用。\n       * 效率和动态规划差不多了。\n     * 状态转移表法\n       * dp数组，2维\n       * dp数组，1维，滚动数组\n\n * 状态转移方程\n   \n   状态转移方程是解决动态规划的关键\n   \n   * 写出状态转移方程\n   * 根据状态转移方程，填充dp数组\n   * 从dp数组中找到问题的解（一般来说最后一个是解）\n\n\n# 如何实现搜索引擎中的拼写纠错功能？\n\n程序实现参考代码随想录动态规划编辑距离\n\n * 如何量化两个字符串的相似度？——==编辑距离==\n   \n   > 顾名思义，编辑距离指的就是，将一个字符串转化成另一个字符串，需要的最少编辑操作次数（比如增加一个字符、删除一个字符、替换一个字符）。编辑距离越大，说明两个字符串的相似程度越小；相反，编辑距离就越小，说明两个字符串的相似程度越大。对于两个完全相同的字符串来说，编辑距离就是 0。\n   \n   * 莱文斯坦距离\n     \n     > 莱文斯坦距离允许增加、删除、替换字符这三个编辑操作。\n     > \n     > 将两个字符串相互转换。\n     > \n     > 莱文斯坦距离的大小，表示两个字符串差异的大小\n   \n   * 最长公共子序列\n     \n     > 最长公共子串的大小，表示两个字符串相似程度的大小。\n     \n     \n\n * 纠错优化\n   \n   > 当用户在搜索框内，输入一个拼写错误的单词时，我们就拿这个单词跟词库中的单词一一进行比较，计算编辑距离，将编辑距离最小的单词，作为纠正之后的单词，提示给用户。\n   \n   * 针对纠错效果不好的问题，我们有很多种优化思路，我这里介绍几种。\n     \n     >  1. 我们并不仅仅取出编辑距离最小的那个单词，而是**取出编辑距离最小的 top 10，然后根据其他参数，决策选择哪个单词作为拼写纠错单词。**比如使用搜索热门程度来决定哪个单词作为拼写纠错单词。\n     > \n     >  2. 我们还可以用多种编辑距离计算方法，比如今天讲到的两种，然后分别编辑距离最小的 top 10，然后求交集，用交集的结果，再继续优化处理。\n     > \n     >  3. 我们还可以通过统计用户的搜索日志，**得到最常被拼错的单词列表，以及对应的拼写正确的单词。**搜索引擎在拼写纠错的时候，首先在这个最常被拼错单词列表中查找。如果一旦找到，直接返回对应的正确的单词。这样纠错的效果非常好。\n     > \n     >  4. 我们还有更加高级一点的做法，引入个性化因素。针对每个用户，维护这个用户特有的搜索喜好，也就是常用的搜索关键词。当用户输入错误的单词的时候，我们首先在这个用户常用的搜索关键词中，计算编辑距离，查找编辑距离最小的单词。\n   \n   * 针对纠错性能方面，我们也有相应的优化方式。我讲两种分治的优化思路。\n     \n     >  1. 如果纠错功能的 tps 不高，**我们可以部署多台机器，每台机器运行一个独立的纠错功能。**当有一个纠错请求的时候，我们通过负载均衡，分配到其中一台机器，来计算编辑距离，得到纠错单词。\n     >  2. 如果纠错系统的响应时间太长，也就是，**每个纠错请求处理时间过长，我们可以将纠错的词库，分割到很多台机器。**当有一个纠错请求的时候，我们就将这个拼写错误的单词，同时发送到这多台机器，让多台机器并行处理，分别得到编辑距离最小的单词，然后再比对合并，最终决定出一个最优的纠错单词。\n\n\n# 三种算法思想比较分析\n\n贪心，回溯，动态规划\n\n * 总结\n   \n   * 贪心：一条路走到黑，就一次机会，只能哪边看着顺眼走哪边\n   * 回溯：一条路走到黑，无数次重来的机会，还怕我走不出来 (snapshot view)\n   * 动态规划：拥有上帝视角，手握无数平行宇宙的历史存档， 同时发展出无数个未来 (versioned archive view)\n\n * 贪心\n   \n   **贪心算法实际上是动态规划算法的一种特殊情况。**它解决问题起来更加高效，代码实现也更加简洁。不过，它可以解决的问题也更加有限。它能解决的问题需要满足三个条件，最优子结构、无后效性和贪心选择性（这里我们不怎么强调重复子问题）。\n   \n   **“贪心选择性”的意思是，通过局部最优的选择，能产生全局的最优选择。**每一个阶段，我们都选择当前看起来最优的决策，所有阶段的决策完成之后，最终由这些局部最优解构成全局最优解。\n\n * 回溯\n   \n   回溯算法是个“万金油”。**基本上能用的动态规划、贪心解决的问题，我们都可以用回溯算法解决。**回溯算法相当于==穷举搜索==。穷举所有的情况，然后对比得到最优解。不过，回溯算法的时间复杂度非常高，是指数级别的，只能用来解决小规模数据的问题。==对于大规模数据的问题，用回溯算法解决的执行效率就很低了。==\n\n * 动态规划\n   \n   尽管动态规划比回溯算法高效，但是，并不是所有问题，都可以用动态规划来解决。能用动态规划解决的问题，需要满足三个特征，最优子结构、无后效性和重复子问题。在重复子问题这一点上，动态规划和分治算法的区分非常明显。==分治算法要求分割成的子问题，不能有重复子问题，而动态规划正好相反，动态规划之所以高效，就是因为回溯算法实现中存在大量的重复子问题==。",charsets:{cjk:!0}},{title:"《数据结构与算法》",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"02.算法",imgUrl:"/assets/img/算法.jpg",description:"算法学习笔记--算法原理 + LeetCode真题"}},title:"《数据结构与算法》",date:"2023-06-30T20:30:40.000Z",permalink:"/Algorithm/",article:!1,comment:!1,editLink:!1,author:{name:"凉冰",link:"https://github.com/diana-devil"}},regularPath:"/02.%E7%AE%97%E6%B3%95/",relativePath:"02.算法/README.md",key:"v-c5de5724",path:"/Algorithm/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"分治算法",frontmatter:{title:"分治算法",date:"2023-07-01T16:09:50.000Z",permalink:"/pages/5b1763/",categories:["算法","分治"],tags:["知识","算法"]},regularPath:"/02.%E7%AE%97%E6%B3%95/95.%E5%88%86%E6%B2%BB/01.%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95.html",relativePath:"02.算法/95.分治/01.分治算法.md",key:"v-bdb21a50",path:"/pages/5b1763/",headers:[{level:2,title:"分治算法",slug:"分治算法",normalizedTitle:"分治算法",charIndex:2}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"分治算法",content:"# 分治算法\n\n * 核心思想\n   \n   > 分治算法（divide and conquer）的核心思想其实就是四个字，分而治之 ，也就是将原问题划分成 n 个规模较小，并且结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。\n   > \n   > 分治算法是一种处理问题的思想，递归是一种编程技巧。实际上，分治算法一般都比较适合用递归来实现。\n\n * 算法步骤\n   \n   * 分治算法的递归实现中，每一层递归都会涉及这样三个操作：\n   >  * 分解：将原问题分解成一系列子问题；\n   >  * 解决：递归地求解各个子问题，若子问题足够小，则直接求解；\n   >  * 合并：将子问题的结果合并成原问题。\n   \n   * 分治算法能解决的问题，一般需要满足下面这几个条件：\n   >  * 原问题与分解成的小问题具有相同的模式；\n   >  * ==原问题分解成的子问题可以独立求解，子问题之间没有相关性，这一点是分治算法跟动态规划的明显区别==，等我们讲到动态规划的时候，会详细对比这两种算法；\n   >  * 具有分解终止条件，也就是说，当问题足够小时，可以直接求解；\n   >  * 可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了。\n\n * 分治算法求解逆序对\n   \n   如何编程求出一组数据的有序对个数或者逆序对个数呢？\n   \n   * 求解1——普通双循环\n     \n     > 最笨的方法是，拿每个数字跟它后面的数字比较，看有几个比它小的。我们把比它小的数字个数记作 k，通过这样的方式，把每个数字都考察一遍之后，然后对每个数字对应的 k 值求和，最后得到的总和就是逆序对个数。不过，这样操作的时间复杂度是 O(n^2)。\n   \n   * 求解2——分治+归并\n     \n     > 我们用分治算法来试试。我们套用分治的思想来求数组 A 的逆序对个数。我们可以将数组分成前后两半 A1 和 A2，分别计算 A1 和 A2 的逆序对个数 K1 和 K2，然后再计算 A1 与 A2 之间的逆序对个数 K3。那数组 A 的逆序对个数就等于 K1+K2+K3。\n     > \n     > K1和K2好计算，怎样计算K3呢？可以借助归并排序，合并数组的思想。在合并的时候计算逆序对\n\n * 分治算法处理海量数据\n   \n   ==MapReduce 的本质就是分治思想==\n   \n   * 问题所在\n     \n     > 比如，给 10GB 的订单文件按照金额排序这样一个需求，看似是一个简单的排序问题，但是因为数据量大，有 10GB，而我们的机器的内存可能只有 2、3GB 这样子，无法一次性加载到内存，也就无法通过单纯地使用快排、归并等基础算法来解决了。\n   \n   * 处理办法\n     \n     > 要解决这种数据量大到内存装不下的问题，我们就可以利用分治的思想。**我们可以将海量的数据集合根据某种方法，划分为几个小的数据集合，每个小的数据集合单独加载到内存来解决，然后再将小数据集合合并成大数据集合。**实际上，利用这种分治的处理思路，不仅仅能克服内存的限制，还能利用多线程或者多机处理，加快处理的速度。",normalizedContent:"# 分治算法\n\n * 核心思想\n   \n   > 分治算法（divide and conquer）的核心思想其实就是四个字，分而治之 ，也就是将原问题划分成 n 个规模较小，并且结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。\n   > \n   > 分治算法是一种处理问题的思想，递归是一种编程技巧。实际上，分治算法一般都比较适合用递归来实现。\n\n * 算法步骤\n   \n   * 分治算法的递归实现中，每一层递归都会涉及这样三个操作：\n   >  * 分解：将原问题分解成一系列子问题；\n   >  * 解决：递归地求解各个子问题，若子问题足够小，则直接求解；\n   >  * 合并：将子问题的结果合并成原问题。\n   \n   * 分治算法能解决的问题，一般需要满足下面这几个条件：\n   >  * 原问题与分解成的小问题具有相同的模式；\n   >  * ==原问题分解成的子问题可以独立求解，子问题之间没有相关性，这一点是分治算法跟动态规划的明显区别==，等我们讲到动态规划的时候，会详细对比这两种算法；\n   >  * 具有分解终止条件，也就是说，当问题足够小时，可以直接求解；\n   >  * 可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了。\n\n * 分治算法求解逆序对\n   \n   如何编程求出一组数据的有序对个数或者逆序对个数呢？\n   \n   * 求解1——普通双循环\n     \n     > 最笨的方法是，拿每个数字跟它后面的数字比较，看有几个比它小的。我们把比它小的数字个数记作 k，通过这样的方式，把每个数字都考察一遍之后，然后对每个数字对应的 k 值求和，最后得到的总和就是逆序对个数。不过，这样操作的时间复杂度是 o(n^2)。\n   \n   * 求解2——分治+归并\n     \n     > 我们用分治算法来试试。我们套用分治的思想来求数组 a 的逆序对个数。我们可以将数组分成前后两半 a1 和 a2，分别计算 a1 和 a2 的逆序对个数 k1 和 k2，然后再计算 a1 与 a2 之间的逆序对个数 k3。那数组 a 的逆序对个数就等于 k1+k2+k3。\n     > \n     > k1和k2好计算，怎样计算k3呢？可以借助归并排序，合并数组的思想。在合并的时候计算逆序对\n\n * 分治算法处理海量数据\n   \n   ==mapreduce 的本质就是分治思想==\n   \n   * 问题所在\n     \n     > 比如，给 10gb 的订单文件按照金额排序这样一个需求，看似是一个简单的排序问题，但是因为数据量大，有 10gb，而我们的机器的内存可能只有 2、3gb 这样子，无法一次性加载到内存，也就无法通过单纯地使用快排、归并等基础算法来解决了。\n   \n   * 处理办法\n     \n     > 要解决这种数据量大到内存装不下的问题，我们就可以利用分治的思想。**我们可以将海量的数据集合根据某种方法，划分为几个小的数据集合，每个小的数据集合单独加载到内存来解决，然后再将小数据集合合并成大数据集合。**实际上，利用这种分治的处理思路，不仅仅能克服内存的限制，还能利用多线程或者多机处理，加快处理的速度。",charsets:{cjk:!0}},{title:"README",frontmatter:{title:"README",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/b02b7f/",categories:["技术","Git"],tags:["知识","技术","Git"]},regularPath:"/03.%E6%8A%80%E6%9C%AF/01.Git/05.Git%20%E6%A6%82%E8%BF%B0.html",relativePath:"03.技术/01.Git/05.Git 概述.md",key:"v-423b419f",path:"/pages/b02b7f/",headers:[{level:2,title:"Git 概述",slug:"git-概述",normalizedTitle:"git 概述",charIndex:246},{level:2,title:"Git代码托管服务",slug:"git代码托管服务",normalizedTitle:"git代码托管服务",charIndex:259},{level:3,title:"常用的Git代码托管服务",slug:"常用的git代码托管服务",normalizedTitle:"常用的git代码托管服务",charIndex:273}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"Git 概述 Git代码托管服务 常用的Git代码托管服务",content:"# Git\n\nGit是一个分布式版本控制系统，常用于版本管理和协同开发项目。它最初由Linux开源社区的Linus Torvalds创建，并因其速度、简单性和强大的分支和合并功能而受到广泛喜爱。\n\n使用Git，开发者可以在本地创建代码仓库，记录并跟踪文件的不同版本，同时可以将代码仓库与远程服务器进行同步，以便多个开发者之间共享和合作开发代码。\n\n它具有许多其他的高级特性，如标签管理、远程分支跟踪、代码回滚等。通过使用Git，开发者可以更好地组织和管理代码，并与团队成员协作开发。\n\n\n# Git 概述\n\n\n\n\n# Git代码托管服务\n\n\n# 常用的Git代码托管服务\n\nGitHub\n\n码云",normalizedContent:"# git\n\ngit是一个分布式版本控制系统，常用于版本管理和协同开发项目。它最初由linux开源社区的linus torvalds创建，并因其速度、简单性和强大的分支和合并功能而受到广泛喜爱。\n\n使用git，开发者可以在本地创建代码仓库，记录并跟踪文件的不同版本，同时可以将代码仓库与远程服务器进行同步，以便多个开发者之间共享和合作开发代码。\n\n它具有许多其他的高级特性，如标签管理、远程分支跟踪、代码回滚等。通过使用git，开发者可以更好地组织和管理代码，并与团队成员协作开发。\n\n\n# git 概述\n\n\n\n\n# git代码托管服务\n\n\n# 常用的git代码托管服务\n\ngithub\n\n码云",charsets:{cjk:!0}},{title:"《Git》",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"03.技术/01.Git",imgUrl:"/assets/img/git.jpg",description:"Git学习笔记--分布式版本控制系统-git-神中神！"}},title:"《Git》",date:"2023-06-30T20:30:40.000Z",permalink:"/technology/Git/",article:!1,comment:!1,editLink:!1,author:{name:"凉冰",link:"https://github.com/diana-devil"}},regularPath:"/03.%E6%8A%80%E6%9C%AF/01.Git/",relativePath:"03.技术/01.Git/README.md",key:"v-3bea15f6",path:"/technology/Git/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"Git基础",frontmatter:{title:"Git基础",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/91f9c0/",categories:["技术","Git"],tags:["知识","技术","Git"]},regularPath:"/03.%E6%8A%80%E6%9C%AF/01.Git/10.Git%E5%9F%BA%E7%A1%80.html",relativePath:"03.技术/01.Git/10.Git基础.md",key:"v-4946107a",path:"/pages/91f9c0/",headers:[{level:2,title:"Git常用命令",slug:"git常用命令",normalizedTitle:"git常用命令",charIndex:2},{level:3,title:"Git全局设置",slug:"git全局设置",normalizedTitle:"git全局设置",charIndex:14},{level:3,title:"获取Git仓库",slug:"获取git仓库",normalizedTitle:"获取git仓库",charIndex:201},{level:3,title:"工作区、暂存区、版本库 概念",slug:"工作区、暂存区、版本库-概念",normalizedTitle:"工作区、暂存区、版本库 概念",charIndex:378},{level:3,title:"Git工作区中文件的两种状态",slug:"git工作区中文件的两种状态",normalizedTitle:"git工作区中文件的两种状态",charIndex:399},{level:3,title:"本地仓库操作",slug:"本地仓库操作",normalizedTitle:"本地仓库操作",charIndex:691},{level:3,title:"远程仓库操作",slug:"远程仓库操作",normalizedTitle:"远程仓库操作",charIndex:1578},{level:3,title:"本地仓库同时关联多个远程仓库",slug:"本地仓库同时关联多个远程仓库",normalizedTitle:"本地仓库同时关联多个远程仓库",charIndex:2819},{level:3,title:"分支操作",slug:"分支操作",normalizedTitle:"分支操作",charIndex:3762},{level:3,title:"标签操作",slug:"标签操作",normalizedTitle:"标签操作",charIndex:5512},{level:2,title:"Idea 集合Git",slug:"idea-集合git",normalizedTitle:"idea 集合git",charIndex:5856},{level:3,title:"在IDEA中配置Git",slug:"在idea中配置git",normalizedTitle:"在idea中配置git",charIndex:5871},{level:3,title:"获取Git仓库",slug:"获取git仓库-2",normalizedTitle:"获取git仓库",charIndex:201},{level:3,title:"本地仓库操作",slug:"本地仓库操作-2",normalizedTitle:"本地仓库操作",charIndex:691},{level:3,title:"远程仓库操作",slug:"远程仓库操作-2",normalizedTitle:"远程仓库操作",charIndex:1578},{level:3,title:"分支操作",slug:"分支操作-2",normalizedTitle:"分支操作",charIndex:3762}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688392125e3,headersStr:"Git常用命令 Git全局设置 获取Git仓库 工作区、暂存区、版本库 概念 Git工作区中文件的两种状态 本地仓库操作 远程仓库操作 本地仓库同时关联多个远程仓库 分支操作 标签操作 Idea 集合Git 在IDEA中配置Git 获取Git仓库 本地仓库操作 远程仓库操作 分支操作",content:'# Git常用命令\n\n\n# Git全局设置\n\n * 设置用户信息\n   \n   > git config -- global user.name "xxx"————设置用户名，随便设\n   > \n   > git config -- global user.email "xxx.com"————设置用户邮箱，随便设\n\n * 查看配置信息\n   \n   > git config --list\n\n\n# 获取Git仓库\n\n * 从本地初始化一个Git仓库（不常用）\n   \n   >  1. 任意创建一个空目录，作为本地git仓库。\n   >  2. 在 Git bash 中输入 git init ,可以生成一个本地仓库\n\n * 从远程仓库克隆\n   \n   >  * git clone+地址\n   > \n   >  * 注意，仓库之间不能嵌套\n\n\n# 工作区、暂存区、版本库 概念\n\n\n\n\n# Git工作区中文件的两种状态\n\n * untracked未跟踪（未被纳入版本控制）\n   * 新创建的文件，未执行git add+(file/*)(可以指定当前文件 file，也可以指定*,表示提交所有)属于未跟踪状态\n * tracked已跟踪（被纳入版本控制）\n   * Unmodified 未修改状态——执行git commit命令后，被提交的文件，是未修改状态\n   * Modified 已修改状态——提交文件后，对文件进行了修改，就是已修改状态\n   * Staged已暂存状态———— 执行git add *命令后，会将信息存入index文件中，属于暂存状态\n\n\n# 本地仓库操作\n\n * git status——查看文件状态\n\n * git add——将文件的修改加入暂存区\n   \n   * git add *——对工作区所有文件执行操作\n   * git add user.txt——对工作区指定文件进行操作\n   * 执行后，信息被存入index文件下,文件状态由untracked变为Staged\n\n * git reset——将暂存区的文件取消暂存或者是切换到指定版本\n   \n   * git reset user.txt——对工作区指定文件进行操作\n     * 取消暂存状态，执行后文件状态更改为untracked\n   * git reset --hard be17bf08e857f0ad8aff68df5f988dc7e20eab50\n     * 会回到指定版本号的版本状态，版本号由git log 提供\n\n * git commit——将暂存区的文件修改提交到版本库\n   \n   * git commit -m "init" user.txt\n     * -m "init" 表示msg,本次提交要说明的信息,日志显示的时候，会显示该信息\n     * 双引号内可以跟中文\n   * 执行后，状态由Staged变为Unmodified\n\n * git log——查看日志\n   \n   * 执行后，会显示之前的git commit信息\n   \n   $ git log\n   commit be17bf08e857f0ad8aff68df5f988dc7e20eab50 (HEAD -> master)\n   Date:   Mon May 16 15:09:16 2022 +0800\n       \n       update user.txt\n       \n   commit 04a05f597aa85ca85c7cc0767828e497f0b320b3\n   Date:   Mon May 16 14:56:33 2022 +0800\n       \n       init\n   \n\n\n# 远程仓库操作\n\n * git remote——查看远程仓库\n   \n   * git remote -v——查看详细信息\n   \n   $ git remote -v\n   origin  https://gitee.com/diana-liangbing/hellogit.git (fetch)\n   origin  https://gitee.com/diana-liangbing/hellogit.git (push)\n   \n\n * git remote add——添加远程仓库(将本地仓库和远程仓库关联起来)\n   \n   * git remote add origin https://gitee.com/diana-liangbing/repo1.git\n     * origin,不是默认，可改，但是推荐这个\n     * https://gitee.com/diana-liangbing/repo1.git,对应的远程仓库的链接\n\n * git clone——从远程仓库克隆\n   \n   * git clone https://gitee.com/diana-liangbing/hellogit.git,将远程仓库克隆下来\n\n * git pull——从远程仓库拉取\n   \n   * git pull origin master\n   * git pull [remote-name] [branch-name]\n   >  * 作用是从远程仓库获取最先版本并合并到本地仓库\n   >  * 如果当前本地仓库不是从远程仓库克隆，而是本地创建的仓库，并且仓库中存在文件，在拉取的时候会报错\n   >    * 在git pull后加参数 --allow-unrelated-histories\n   >    * 按i进入插入模式，可以输入信息\n   >    * 输入完成，按 esc，退出编辑\n   >    * 输入：wq 保存并退出\n\n * git push——推送到远程仓库\n   \n   * git push origin master\n   * git push [remote-name] [branch-name]\n   >  * 将文件推送到远程仓库之前，要先提交到本地仓库\n   > \n   > git add *\n   > git commit -m "init" 111.txt\n   > git push origin master\n   > \n   > \n   >  * origin 表示远程仓库的名称，可以用 git remote得到\n   >  * master 表示当前分支，master是主分支，默认分支\n\n * git rm -r --cached .idea\n   \n   * 删除掉git文件列表里的文件，保留本地文件 ———— --cached\n   * 在云端删除idea文件，但是本地保留着\n\n\n# 本地仓库同时关联多个远程仓库\n\n * 分开推送\n   \n   首先查看当前本地仓库所关联的远程仓库\n   \n   $ git remote -v\n   origin  https://gitee.com/diana-liangbing/hellogit.git (fetch)\n   origin  https://gitee.com/diana-liangbing/hellogit.git (push)\n   \n   \n   然后添加额外的远程仓库\n   \n   git remote add gitee https://gitee.com/xxx/xxx.git\n   \n   \n   这里的重点在于gitee这个，相当于额外命名了一个远程仓库；此时再去运行git remote -v\n   \n   $ git remote -v\n   origin  https://gitee.com/diana-liangbing/hellogit.git (fetch)\n   origin  https://gitee.com/diana-liangbing/hellogit.git (push)\n   gitee  https://gitee.com/xxx/xxx.git (fetch)\n   gitee  https://gitee.com/xxx/xxx.git (push)\n   \n   \n   如果想要推送的话，需要分开推送\n   \n   $ git push  gitee master // 到gitee\n   $ git push  origin master // 到origin\n   \n   \n   这种情况，适用于仓库较少，且需要区别管理的情况。\n\n * 同步推送\n   \n   如果仓库数量多，且需要同步进行推送，可以采用这个方法\n   \n   $ git remote set-url --add origin https://gitee.com/xxx/xxx.git\n   \n   \n   这样，多个仓库就都关联到了origin上，同步推送的话，\n   \n   $ git push  origin master // 同步推送\n   \n\n\n# 分支操作\n\n * 同一个仓库可以有多个分支，各个分支相互独立，互不干扰，git init时，会默认创建一个master分支\n\n * git branch ——查看分支\n   \n   * git branch——列出所有本地分支\n     * 当前分支是绿色的，且前加了※\n   * git branch -r——列出所有远程分支\n   * git branch -a——列出所有本地分支和远程分支\n   * git branch -d b1——删除b1对应分支\n\n * git branch [name]——创建分支\n   \n   * git branch b1——创建分支b1 --名字随便取\n\n * git checkout [name]——切换分支\n   \n   * 每个分支相互独立，本地仓库的文件显示的是当前分支的文件，可以看到b1分支有一个b1.txt，而master分支下没有\n   \n   * git checkout b1——切换到b1分支\n   \n   \n   \n   * git checkout master——切换到master分支\n   \n   \n\n * git push [remote-name] [branch-name]——推送至远程仓库分支\n   \n   $ git push origin b1-- 将b1分支推送到远程仓库\n   \n   Total 0 (delta 0), reused 0 (delta 0), pack-reused 0\n   remote: Powered by GITEE.COM [GNK-6.3]\n   remote: Create a pull request for \'b1\' on Gitee by visiting:\n   remote:     https://gitee.com/diana-liangbing/repo1/pull/new/diana-liangbing:b1...diana-liangbing:master\n   To https://gitee.com/diana-liangbing/repo1.git\n    * [new branch]      b1 -> b1\n   \n\n * git merge [name]——合并分支\n   \n   * git merge b1——现处在master分支，要合并b1分支下的文件\n   \n   * 合并冲突 (两个分支下都有这个文件，且文件内容不同，就会出现冲突)\n     \n     $ git merge b1\n     Auto-merging b1.txt\n     CONFLICT (content): Merge conflict in b1.txt\n     Automatic merge failed; fix conflicts and then commit the result.\n     \n     --------------------------------------------------冲突时  master分支下  b1.txt内部文件-------------------------------------------\n     b1 分支独有\n     <<<<<<< HEAD\n     master修改了\n     =======\n     b1分支修改了\n     \n     >>>>>>> b1\n     \n   \n   * 冲突解决 （手动修改文件……）\n     \n     * 手动修改文件，达到想要的结果\n     * git add\n     * git commit -m "手动修改了融合错误" *\n       * 报错：fatal: cannot do a partial commit during a merge.\n       * 解决方法: 在原命令后面加 -i\n       * git commit -m "手动修改了融合错误" * -i---——错误解决\n     * git push origin master\n     \n     \n\n\n# 标签操作\n\n * Git中的标签，指的是某个分支某个特定时间点的状态。通过标签，可以很方便的切换到标记时的状态\n   * 比较有代表性的是 人们会使用这个功能来标记发布节点（v1.0 v1.2）\n * git tag ——列出已有标签\n * git tag [name]——创建标签\n   * git tag v1\n * git push [remote-name] [tag-name]——将标签推送至远程仓库\n   * git push origin v1\n   * 记录当前工作区状态，并完成上传\n * git checkout -b [branch] [tag-name]——检出标签（将当时标签标记的代码clone下来）\n   * 检出标签时需要新建一个分支来指向标签\n\n\n# Idea 集合Git\n\n\n# 在IDEA中配置Git\n\n设置——>git——>git可执行文件目录——>git.exe\n\n\n# 获取Git仓库\n\n * 新建本地仓库\n   \n   > vcs——>创建Git仓库\n\n * 链接到远程仓库\n   \n   > VCS——>从版本控制中获取——>输入远程仓库的url\n\n\n# 本地仓库操作\n\n * 文件未加入暂存区(红色）\n\n * 将文件加入暂存区（绿色）\n   \n   * 创建文件时，自动提示，点击确定，即可加入暂存区\n   * 右键单击Git——>添加\n\n * 将暂存区文件提交到版本库（黑色）\n   \n   * 右键单击Git——>添加文件，前提是 必须在缓存区里面\n   * 简单方式 右上角 对号——>提交 ，不用先存暂存区，直接点√就可以\n\n * 查看日志\n   \n   * 简单方式 右上角 小表符号 可以查看日志信息\n\n\n# 远程仓库操作\n\n * 查看远程仓库\n   \n   * 右键单击文件夹目录，Git——>管理远程\n\n * 添加远程仓库\n   \n   * 右键单击文件夹目录，Git——>管理远程——>＋ 加号，— 减号，修改\n\n * 推送至远程仓库\n   \n   * 右上角 对号——>提交并推送 ，适用用还没有提交到版本库的文件\n   * 右上角 上箭头——>推送，适用于已经提交到版本库的文件\n\n * 从远程仓库拉取\n   \n   * 右上角 下箭头——>更新\n\n\n# 分支操作\n\n * 查看分支\n   \n   * 右下角\n   \n   \n\n * 创建分支\n   \n   * 点开有个新分支\n\n * 切换分支\n   \n   * 点开，点击要切换的分支，最上面有个签出\n\n * 将分支推送到远程仓库\n   \n   * 点开，点击要切换的分支，点击推送\n\n * 合并分支\n   \n   * 点开，点击要切换的分支，点击合并或者基变',normalizedContent:'# git常用命令\n\n\n# git全局设置\n\n * 设置用户信息\n   \n   > git config -- global user.name "xxx"————设置用户名，随便设\n   > \n   > git config -- global user.email "xxx.com"————设置用户邮箱，随便设\n\n * 查看配置信息\n   \n   > git config --list\n\n\n# 获取git仓库\n\n * 从本地初始化一个git仓库（不常用）\n   \n   >  1. 任意创建一个空目录，作为本地git仓库。\n   >  2. 在 git bash 中输入 git init ,可以生成一个本地仓库\n\n * 从远程仓库克隆\n   \n   >  * git clone+地址\n   > \n   >  * 注意，仓库之间不能嵌套\n\n\n# 工作区、暂存区、版本库 概念\n\n\n\n\n# git工作区中文件的两种状态\n\n * untracked未跟踪（未被纳入版本控制）\n   * 新创建的文件，未执行git add+(file/*)(可以指定当前文件 file，也可以指定*,表示提交所有)属于未跟踪状态\n * tracked已跟踪（被纳入版本控制）\n   * unmodified 未修改状态——执行git commit命令后，被提交的文件，是未修改状态\n   * modified 已修改状态——提交文件后，对文件进行了修改，就是已修改状态\n   * staged已暂存状态———— 执行git add *命令后，会将信息存入index文件中，属于暂存状态\n\n\n# 本地仓库操作\n\n * git status——查看文件状态\n\n * git add——将文件的修改加入暂存区\n   \n   * git add *——对工作区所有文件执行操作\n   * git add user.txt——对工作区指定文件进行操作\n   * 执行后，信息被存入index文件下,文件状态由untracked变为staged\n\n * git reset——将暂存区的文件取消暂存或者是切换到指定版本\n   \n   * git reset user.txt——对工作区指定文件进行操作\n     * 取消暂存状态，执行后文件状态更改为untracked\n   * git reset --hard be17bf08e857f0ad8aff68df5f988dc7e20eab50\n     * 会回到指定版本号的版本状态，版本号由git log 提供\n\n * git commit——将暂存区的文件修改提交到版本库\n   \n   * git commit -m "init" user.txt\n     * -m "init" 表示msg,本次提交要说明的信息,日志显示的时候，会显示该信息\n     * 双引号内可以跟中文\n   * 执行后，状态由staged变为unmodified\n\n * git log——查看日志\n   \n   * 执行后，会显示之前的git commit信息\n   \n   $ git log\n   commit be17bf08e857f0ad8aff68df5f988dc7e20eab50 (head -> master)\n   date:   mon may 16 15:09:16 2022 +0800\n       \n       update user.txt\n       \n   commit 04a05f597aa85ca85c7cc0767828e497f0b320b3\n   date:   mon may 16 14:56:33 2022 +0800\n       \n       init\n   \n\n\n# 远程仓库操作\n\n * git remote——查看远程仓库\n   \n   * git remote -v——查看详细信息\n   \n   $ git remote -v\n   origin  https://gitee.com/diana-liangbing/hellogit.git (fetch)\n   origin  https://gitee.com/diana-liangbing/hellogit.git (push)\n   \n\n * git remote add——添加远程仓库(将本地仓库和远程仓库关联起来)\n   \n   * git remote add origin https://gitee.com/diana-liangbing/repo1.git\n     * origin,不是默认，可改，但是推荐这个\n     * https://gitee.com/diana-liangbing/repo1.git,对应的远程仓库的链接\n\n * git clone——从远程仓库克隆\n   \n   * git clone https://gitee.com/diana-liangbing/hellogit.git,将远程仓库克隆下来\n\n * git pull——从远程仓库拉取\n   \n   * git pull origin master\n   * git pull [remote-name] [branch-name]\n   >  * 作用是从远程仓库获取最先版本并合并到本地仓库\n   >  * 如果当前本地仓库不是从远程仓库克隆，而是本地创建的仓库，并且仓库中存在文件，在拉取的时候会报错\n   >    * 在git pull后加参数 --allow-unrelated-histories\n   >    * 按i进入插入模式，可以输入信息\n   >    * 输入完成，按 esc，退出编辑\n   >    * 输入：wq 保存并退出\n\n * git push——推送到远程仓库\n   \n   * git push origin master\n   * git push [remote-name] [branch-name]\n   >  * 将文件推送到远程仓库之前，要先提交到本地仓库\n   > \n   > git add *\n   > git commit -m "init" 111.txt\n   > git push origin master\n   > \n   > \n   >  * origin 表示远程仓库的名称，可以用 git remote得到\n   >  * master 表示当前分支，master是主分支，默认分支\n\n * git rm -r --cached .idea\n   \n   * 删除掉git文件列表里的文件，保留本地文件 ———— --cached\n   * 在云端删除idea文件，但是本地保留着\n\n\n# 本地仓库同时关联多个远程仓库\n\n * 分开推送\n   \n   首先查看当前本地仓库所关联的远程仓库\n   \n   $ git remote -v\n   origin  https://gitee.com/diana-liangbing/hellogit.git (fetch)\n   origin  https://gitee.com/diana-liangbing/hellogit.git (push)\n   \n   \n   然后添加额外的远程仓库\n   \n   git remote add gitee https://gitee.com/xxx/xxx.git\n   \n   \n   这里的重点在于gitee这个，相当于额外命名了一个远程仓库；此时再去运行git remote -v\n   \n   $ git remote -v\n   origin  https://gitee.com/diana-liangbing/hellogit.git (fetch)\n   origin  https://gitee.com/diana-liangbing/hellogit.git (push)\n   gitee  https://gitee.com/xxx/xxx.git (fetch)\n   gitee  https://gitee.com/xxx/xxx.git (push)\n   \n   \n   如果想要推送的话，需要分开推送\n   \n   $ git push  gitee master // 到gitee\n   $ git push  origin master // 到origin\n   \n   \n   这种情况，适用于仓库较少，且需要区别管理的情况。\n\n * 同步推送\n   \n   如果仓库数量多，且需要同步进行推送，可以采用这个方法\n   \n   $ git remote set-url --add origin https://gitee.com/xxx/xxx.git\n   \n   \n   这样，多个仓库就都关联到了origin上，同步推送的话，\n   \n   $ git push  origin master // 同步推送\n   \n\n\n# 分支操作\n\n * 同一个仓库可以有多个分支，各个分支相互独立，互不干扰，git init时，会默认创建一个master分支\n\n * git branch ——查看分支\n   \n   * git branch——列出所有本地分支\n     * 当前分支是绿色的，且前加了※\n   * git branch -r——列出所有远程分支\n   * git branch -a——列出所有本地分支和远程分支\n   * git branch -d b1——删除b1对应分支\n\n * git branch [name]——创建分支\n   \n   * git branch b1——创建分支b1 --名字随便取\n\n * git checkout [name]——切换分支\n   \n   * 每个分支相互独立，本地仓库的文件显示的是当前分支的文件，可以看到b1分支有一个b1.txt，而master分支下没有\n   \n   * git checkout b1——切换到b1分支\n   \n   \n   \n   * git checkout master——切换到master分支\n   \n   \n\n * git push [remote-name] [branch-name]——推送至远程仓库分支\n   \n   $ git push origin b1-- 将b1分支推送到远程仓库\n   \n   total 0 (delta 0), reused 0 (delta 0), pack-reused 0\n   remote: powered by gitee.com [gnk-6.3]\n   remote: create a pull request for \'b1\' on gitee by visiting:\n   remote:     https://gitee.com/diana-liangbing/repo1/pull/new/diana-liangbing:b1...diana-liangbing:master\n   to https://gitee.com/diana-liangbing/repo1.git\n    * [new branch]      b1 -> b1\n   \n\n * git merge [name]——合并分支\n   \n   * git merge b1——现处在master分支，要合并b1分支下的文件\n   \n   * 合并冲突 (两个分支下都有这个文件，且文件内容不同，就会出现冲突)\n     \n     $ git merge b1\n     auto-merging b1.txt\n     conflict (content): merge conflict in b1.txt\n     automatic merge failed; fix conflicts and then commit the result.\n     \n     --------------------------------------------------冲突时  master分支下  b1.txt内部文件-------------------------------------------\n     b1 分支独有\n     <<<<<<< head\n     master修改了\n     =======\n     b1分支修改了\n     \n     >>>>>>> b1\n     \n   \n   * 冲突解决 （手动修改文件……）\n     \n     * 手动修改文件，达到想要的结果\n     * git add\n     * git commit -m "手动修改了融合错误" *\n       * 报错：fatal: cannot do a partial commit during a merge.\n       * 解决方法: 在原命令后面加 -i\n       * git commit -m "手动修改了融合错误" * -i---——错误解决\n     * git push origin master\n     \n     \n\n\n# 标签操作\n\n * git中的标签，指的是某个分支某个特定时间点的状态。通过标签，可以很方便的切换到标记时的状态\n   * 比较有代表性的是 人们会使用这个功能来标记发布节点（v1.0 v1.2）\n * git tag ——列出已有标签\n * git tag [name]——创建标签\n   * git tag v1\n * git push [remote-name] [tag-name]——将标签推送至远程仓库\n   * git push origin v1\n   * 记录当前工作区状态，并完成上传\n * git checkout -b [branch] [tag-name]——检出标签（将当时标签标记的代码clone下来）\n   * 检出标签时需要新建一个分支来指向标签\n\n\n# idea 集合git\n\n\n# 在idea中配置git\n\n设置——>git——>git可执行文件目录——>git.exe\n\n\n# 获取git仓库\n\n * 新建本地仓库\n   \n   > vcs——>创建git仓库\n\n * 链接到远程仓库\n   \n   > vcs——>从版本控制中获取——>输入远程仓库的url\n\n\n# 本地仓库操作\n\n * 文件未加入暂存区(红色）\n\n * 将文件加入暂存区（绿色）\n   \n   * 创建文件时，自动提示，点击确定，即可加入暂存区\n   * 右键单击git——>添加\n\n * 将暂存区文件提交到版本库（黑色）\n   \n   * 右键单击git——>添加文件，前提是 必须在缓存区里面\n   * 简单方式 右上角 对号——>提交 ，不用先存暂存区，直接点√就可以\n\n * 查看日志\n   \n   * 简单方式 右上角 小表符号 可以查看日志信息\n\n\n# 远程仓库操作\n\n * 查看远程仓库\n   \n   * 右键单击文件夹目录，git——>管理远程\n\n * 添加远程仓库\n   \n   * 右键单击文件夹目录，git——>管理远程——>＋ 加号，— 减号，修改\n\n * 推送至远程仓库\n   \n   * 右上角 对号——>提交并推送 ，适用用还没有提交到版本库的文件\n   * 右上角 上箭头——>推送，适用于已经提交到版本库的文件\n\n * 从远程仓库拉取\n   \n   * 右上角 下箭头——>更新\n\n\n# 分支操作\n\n * 查看分支\n   \n   * 右下角\n   \n   \n\n * 创建分支\n   \n   * 点开有个新分支\n\n * 切换分支\n   \n   * 点开，点击要切换的分支，最上面有个签出\n\n * 将分支推送到远程仓库\n   \n   * 点开，点击要切换的分支，点击推送\n\n * 合并分支\n   \n   * 点开，点击要切换的分支，点击合并或者基变',charsets:{cjk:!0}},{title:"《Linux》",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"03.技术/02.Linux",imgUrl:"/assets/img/Linux.webp",description:"Linux学习笔记--性能卓越操作系统-Linux！"}},title:"《Linux》",date:"2023-06-30T20:30:40.000Z",permalink:"/technology/Linux/",article:!1,comment:!1,editLink:!1,author:{name:"凉冰",link:"https://github.com/diana-devil"}},regularPath:"/03.%E6%8A%80%E6%9C%AF/02.Linux/",relativePath:"03.技术/02.Linux/README.md",key:"v-4e60c2d6",path:"/technology/Linux/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"vim编辑器",frontmatter:{autoSort:99,title:"vim编辑器",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/549714/",categories:["技术","Linux"],tags:["知识","技术","Linux"]},regularPath:"/03.%E6%8A%80%E6%9C%AF/02.Linux/20.vim.html",relativePath:"03.技术/02.Linux/20.vim.md",key:"v-3c2c77d5",path:"/pages/549714/",headers:[{level:2,title:"重点掌握",slug:"重点掌握",normalizedTitle:"重点掌握",charIndex:43},{level:2,title:"命令模式",slug:"命令模式",normalizedTitle:"命令模式",charIndex:165},{level:3,title:"光标移动",slug:"光标移动",normalizedTitle:"光标移动",charIndex:174},{level:3,title:"复制",slug:"复制",normalizedTitle:"复制",charIndex:30},{level:3,title:"剪切",slug:"剪切",normalizedTitle:"剪切",charIndex:654},{level:3,title:"撤销",slug:"撤销",normalizedTitle:"撤销",charIndex:36},{level:2,title:"插入模式",slug:"插入模式",normalizedTitle:"插入模式",charIndex:858},{level:2,title:"底行模式",slug:"底行模式",normalizedTitle:"底行模式",charIndex:453},{level:3,title:"保存文件",slug:"保存文件",normalizedTitle:"保存文件",charIndex:1090},{level:3,title:"退出",slug:"退出",normalizedTitle:"退出",charIndex:1059},{level:3,title:"保存并退出",slug:"保存并退出",normalizedTitle:"保存并退出",charIndex:1166},{level:3,title:"调用外部命令",slug:"调用外部命令",normalizedTitle:"调用外部命令",charIndex:1191},{level:3,title:"查找",slug:"查找",normalizedTitle:"查找",charIndex:24},{level:3,title:"替换",slug:"替换",normalizedTitle:"替换",charIndex:27},{level:3,title:"显示行号",slug:"显示行号",normalizedTitle:"显示行号",charIndex:1473},{level:3,title:"切换文件",slug:"切换文件",normalizedTitle:"切换文件",charIndex:1519},{level:2,title:"实用功能",slug:"实用功能",normalizedTitle:"实用功能",charIndex:1836},{level:2,title:"扩展内容",slug:"扩展内容",normalizedTitle:"扩展内容",charIndex:1979},{level:3,title:"vim配置",slug:"vim配置",normalizedTitle:"vim配置",charIndex:1988},{level:3,title:"异常退出",slug:"异常退出",normalizedTitle:"异常退出",charIndex:2105},{level:3,title:"别名机制",slug:"别名机制",normalizedTitle:"别名机制",charIndex:2165},{level:3,title:"退出方式",slug:"退出方式",normalizedTitle:"退出方式",charIndex:1059},{level:3,title:"加密",slug:"加密",normalizedTitle:"加密",charIndex:2401}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"重点掌握 命令模式 光标移动 复制 剪切 撤销 插入模式 底行模式 保存文件 退出 保存并退出 调用外部命令 查找 替换 显示行号 切换文件 实用功能 扩展内容 vim配置 异常退出 别名机制 退出方式 加密",content:'# vim编辑器\n\n光标的移动，模式切换，删除，查找，替换，复制，粘贴，撤销\n\n\n# 重点掌握\n\n * yy—— 拷贝\n * dd——删除\n * :/ 关键字——查找\n * set nu——设置行号\n * G——末行\n * gg——首行\n * u——撤销\n * 数字+ shift +g——快速定位到某一个 （数字+G）\n\n\n# 命令模式\n\n\n# 光标移动\n\n * 光标移动到行首\n   \n   * shift +6（^）（T上面的数字6）\n\n * 光标移动到行尾\n   \n   * shift+4（$）(R上面的数字4)\n\n * 光标移动到首行\n   \n   * gg\n\n * 光标移动到末行\n   \n   * G\n\n * 向上翻屏\n   \n   * ctrl+b\n   * PgUp\n\n * 向下翻屏\n   \n   * ctrl+f\n   * PgDn\n\n * 快速将光标移动到指定行\n   \n   * 数字 G\n\n * 以当前光标为准向上/下，左/右移动几行\n   \n   * 数字 方向键\n\n * 底行模式下的快速移动：移动到指定行\n   \n   * : 行数+回车\n\n\n# 复制\n\n * 复制光标所在行\n   \n   * yy\n   * p ——粘贴\n\n * 以光标所在行为准（包含所在行），向下复制指定的行数\n   \n   * 数字 yy\n   * p——粘贴\n\n * 可视化复制\n   \n   * ctrl+v,然后按下 方向键来选中需要复制的区块，按下yy键进行复制，按p键进行粘贴\n\n\n# 剪切\n\n * 剪切/删除 当前行\n   \n   * dd（删除之后下行上移）\n   * 严格来说，dd是剪切，但是如果不粘贴，就相当于删除\n\n * 以光标所在行为准（包含所在行），向下剪切/删除指定的行数\n   \n   * 数字 yy\n\n * 剪切/删除当前行，但是删除以后下一行不上移\n   \n   * D\n\n\n# 撤销\n\n * 撤销\n   * u\n * 恢复之前的撤销\n   * ctrl +R\n\n\n# 插入模式\n\n * 进入方式(重点记前两个)\n   \n   * i——在光标所在字符前开始插入\n   * a——在光标所在后开始插入\n   * o——在光标所在行的下面另起一新行插入\n   * I——在光标所在行的行首开始插入，如果行首有空格则在空格后插入\n   * A——在光标所在行的行尾开始插入\n   * O——在光标所在行的上面另起一行开始插入\n   * S——删除光标所在行并开始插入\n\n * 退出方式\n   \n   * esc\n\n\n# 底行模式\n\n\n# 保存文件\n\n * :w——保存文件\n * :w 路径(中间有一个空格)——另存为\n\n\n# 退出\n\n * :q——退出\n * :q!——强制退出\n\n\n# 保存并退出\n\n * :wq——保存并退出\n\n\n# 调用外部命令\n\n * :!外部命令(无空格)——在vim编辑器中执行外部命令\n\n\n# 查找\n\n（/也可以进入底行模式，专门用于搜索）\n\n * /关键词\n * 在搜索结果中切换上/下一个结果\n   * N/n\n * 取消高亮\n   * :nohl\n\n\n# 替换\n\n * :s/原词/新词——替换光标所在行的第一处符合条件的内容\n * :s/原词/新词/g——替换光标所在行全部符合条件的内容\n * :%s/原词/新词——替换整个文档中每行的第一处符合条件的内容\n * :%s/原词/新词/g——替换整个文档的符合条件的内容\n * %表示整个文档 g表示全局\n\n\n# 显示行号\n\n * :set nu——显示行号\n * set nonu——不显示行号\n\n\n# 切换文件\n\n * :files——显示打开的文件\n   \n   :files\n     1      "a.txt"                        第 41 行\n     2 %a   "b"                            第 31 行\n     3 #    "a"                            第 1 行                         \n   \n   \n   * %a——正在活跃的文件\n   * #——上一个打开的文件\n\n * :open 文件名——打开指定文件\n\n * :bn——切换到下一个文件\n\n * :bp——切换到上一个文件\n\n\n# 实用功能\n\n * 代码着色\n   * syntax on——着色\n   * syntax off——取消着色\n * vim中计算器的使用\n   1. 进入编辑模式\n   2. 按下快捷键ctrl+r,然后输入=，此时光标会变到最后一行\n   3. 输入需要计算的内容，按下回车\n\n\n# 扩展内容\n\n\n# vim配置\n\n * 在文件打开的时候在底行模式下输入的配置(临时有效)\n * 个人配置文件(~/.vimrc),没有可以新建(优先级较高)\n   * 配置好之后，会自动执行配置文件\n * 全局配置文件(/etc/vimrc)\n\n\n# 异常退出\n\n在编辑文件之后并没有正常的退出\n\n * 解决方法\n   * 删除交换文件（.file.swp）即可\n\n\n# 别名机制\n\n相当于创建一些属于自己的命令\n\n * 别名映射文件\n   * ~/.bashrc\n     * alias cls=\'clear\'\n     * 前面是你想输入的命令，后面是要替换的命令\n\n\n# 退出方式\n\n * :x\n   \n   * 文件不修改时，相当于:q\n   \n   * 文件修改时，相当于:wq\n   \n   * 如果文件没有被修改，:x退出，不会修改 文件修改时间\n     \n     :wq退出，内容没有影响，但是文件修改时间会改变\n\n\n# 加密\n\n * :X\n   * 对文件进行加密，需要进行保存',normalizedContent:'# vim编辑器\n\n光标的移动，模式切换，删除，查找，替换，复制，粘贴，撤销\n\n\n# 重点掌握\n\n * yy—— 拷贝\n * dd——删除\n * :/ 关键字——查找\n * set nu——设置行号\n * g——末行\n * gg——首行\n * u——撤销\n * 数字+ shift +g——快速定位到某一个 （数字+g）\n\n\n# 命令模式\n\n\n# 光标移动\n\n * 光标移动到行首\n   \n   * shift +6（^）（t上面的数字6）\n\n * 光标移动到行尾\n   \n   * shift+4（$）(r上面的数字4)\n\n * 光标移动到首行\n   \n   * gg\n\n * 光标移动到末行\n   \n   * g\n\n * 向上翻屏\n   \n   * ctrl+b\n   * pgup\n\n * 向下翻屏\n   \n   * ctrl+f\n   * pgdn\n\n * 快速将光标移动到指定行\n   \n   * 数字 g\n\n * 以当前光标为准向上/下，左/右移动几行\n   \n   * 数字 方向键\n\n * 底行模式下的快速移动：移动到指定行\n   \n   * : 行数+回车\n\n\n# 复制\n\n * 复制光标所在行\n   \n   * yy\n   * p ——粘贴\n\n * 以光标所在行为准（包含所在行），向下复制指定的行数\n   \n   * 数字 yy\n   * p——粘贴\n\n * 可视化复制\n   \n   * ctrl+v,然后按下 方向键来选中需要复制的区块，按下yy键进行复制，按p键进行粘贴\n\n\n# 剪切\n\n * 剪切/删除 当前行\n   \n   * dd（删除之后下行上移）\n   * 严格来说，dd是剪切，但是如果不粘贴，就相当于删除\n\n * 以光标所在行为准（包含所在行），向下剪切/删除指定的行数\n   \n   * 数字 yy\n\n * 剪切/删除当前行，但是删除以后下一行不上移\n   \n   * d\n\n\n# 撤销\n\n * 撤销\n   * u\n * 恢复之前的撤销\n   * ctrl +r\n\n\n# 插入模式\n\n * 进入方式(重点记前两个)\n   \n   * i——在光标所在字符前开始插入\n   * a——在光标所在后开始插入\n   * o——在光标所在行的下面另起一新行插入\n   * i——在光标所在行的行首开始插入，如果行首有空格则在空格后插入\n   * a——在光标所在行的行尾开始插入\n   * o——在光标所在行的上面另起一行开始插入\n   * s——删除光标所在行并开始插入\n\n * 退出方式\n   \n   * esc\n\n\n# 底行模式\n\n\n# 保存文件\n\n * :w——保存文件\n * :w 路径(中间有一个空格)——另存为\n\n\n# 退出\n\n * :q——退出\n * :q!——强制退出\n\n\n# 保存并退出\n\n * :wq——保存并退出\n\n\n# 调用外部命令\n\n * :!外部命令(无空格)——在vim编辑器中执行外部命令\n\n\n# 查找\n\n（/也可以进入底行模式，专门用于搜索）\n\n * /关键词\n * 在搜索结果中切换上/下一个结果\n   * n/n\n * 取消高亮\n   * :nohl\n\n\n# 替换\n\n * :s/原词/新词——替换光标所在行的第一处符合条件的内容\n * :s/原词/新词/g——替换光标所在行全部符合条件的内容\n * :%s/原词/新词——替换整个文档中每行的第一处符合条件的内容\n * :%s/原词/新词/g——替换整个文档的符合条件的内容\n * %表示整个文档 g表示全局\n\n\n# 显示行号\n\n * :set nu——显示行号\n * set nonu——不显示行号\n\n\n# 切换文件\n\n * :files——显示打开的文件\n   \n   :files\n     1      "a.txt"                        第 41 行\n     2 %a   "b"                            第 31 行\n     3 #    "a"                            第 1 行                         \n   \n   \n   * %a——正在活跃的文件\n   * #——上一个打开的文件\n\n * :open 文件名——打开指定文件\n\n * :bn——切换到下一个文件\n\n * :bp——切换到上一个文件\n\n\n# 实用功能\n\n * 代码着色\n   * syntax on——着色\n   * syntax off——取消着色\n * vim中计算器的使用\n   1. 进入编辑模式\n   2. 按下快捷键ctrl+r,然后输入=，此时光标会变到最后一行\n   3. 输入需要计算的内容，按下回车\n\n\n# 扩展内容\n\n\n# vim配置\n\n * 在文件打开的时候在底行模式下输入的配置(临时有效)\n * 个人配置文件(~/.vimrc),没有可以新建(优先级较高)\n   * 配置好之后，会自动执行配置文件\n * 全局配置文件(/etc/vimrc)\n\n\n# 异常退出\n\n在编辑文件之后并没有正常的退出\n\n * 解决方法\n   * 删除交换文件（.file.swp）即可\n\n\n# 别名机制\n\n相当于创建一些属于自己的命令\n\n * 别名映射文件\n   * ~/.bashrc\n     * alias cls=\'clear\'\n     * 前面是你想输入的命令，后面是要替换的命令\n\n\n# 退出方式\n\n * :x\n   \n   * 文件不修改时，相当于:q\n   \n   * 文件修改时，相当于:wq\n   \n   * 如果文件没有被修改，:x退出，不会修改 文件修改时间\n     \n     :wq退出，内容没有影响，但是文件修改时间会改变\n\n\n# 加密\n\n * :x\n   * 对文件进行加密，需要进行保存',charsets:{cjk:!0}},{title:"Linux面试题",frontmatter:{autoSort:10,title:"Linux面试题",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/343c9a/",categories:["技术","Linux"],tags:["知识","技术","Linux"]},regularPath:"/03.%E6%8A%80%E6%9C%AF/02.Linux/50.Linux%E9%9D%A2%E8%AF%95%E9%A2%98.html",relativePath:"03.技术/02.Linux/50.Linux面试题.md",key:"v-51430cb6",path:"/pages/343c9a/",headers:[{level:2,title:"如何找回root密码",slug:"如何找回root密码",normalizedTitle:"如何找回root密码",charIndex:10}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"如何找回root密码",content:"# 面试题\n\n\n# 如何找回root密码\n\n 1. 开机页面 按e进入编辑页面。\n 2. 找到Linux16开头内容所在的行数，在末尾输入 init=/bin/sh\n 3. 按ctrl + x进入单用户模式\n 4. 输入mount -o remount,rw /\n 5. 输入passwd,输入密码，再次确认密码\n 6. 输入touch /.autorelabel\n 7. 输入exec /sbin/init\n 8. 完成后，系统自动重启，密码已经更换",normalizedContent:"# 面试题\n\n\n# 如何找回root密码\n\n 1. 开机页面 按e进入编辑页面。\n 2. 找到linux16开头内容所在的行数，在末尾输入 init=/bin/sh\n 3. 按ctrl + x进入单用户模式\n 4. 输入mount -o remount,rw /\n 5. 输入passwd,输入密码，再次确认密码\n 6. 输入touch /.autorelabel\n 7. 输入exec /sbin/init\n 8. 完成后，系统自动重启，密码已经更换",charsets:{cjk:!0}},{title:"技术",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"03.技术",imgUrl:"/assets/img/other.png",description:"技术文档、教程、技巧、实用工具"}},title:"技术",date:"2023-06-20T21:50:53.000Z",permalink:"/technology/",sidebar:!1,article:!1,comment:!1,editLink:!1,author:{name:"凉冰",link:"https://github.com/diana-devil"}},regularPath:"/03.%E6%8A%80%E6%9C%AF/",relativePath:"03.技术/README.md",key:"v-2b2717ba",path:"/technology/",lastUpdated:"2023 07 4",lastUpdatedTimestamp:1688453793e3,headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"README",frontmatter:{title:"README",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/b5d182/",categories:["技术","正则表达式"],tags:["知识","技术","正则表达式"]},regularPath:"/03.%E6%8A%80%E6%9C%AF/10.%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/05.%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F.html",relativePath:"03.技术/10.正则表达式/05.正则表达式.md",key:"v-9044c656",path:"/pages/b5d182/",headers:[{level:2,title:"克制",slug:"克制",normalizedTitle:"克制",charIndex:12},{level:2,title:"元字符的分类",slug:"元字符的分类",normalizedTitle:"元字符的分类",charIndex:544},{level:2,title:"正则三种模式",slug:"正则三种模式",normalizedTitle:"正则三种模式",charIndex:2618},{level:2,title:"分组与引用",slug:"分组与引用",normalizedTitle:"分组与引用",charIndex:3333},{level:2,title:"匹配模式",slug:"匹配模式",normalizedTitle:"匹配模式",charIndex:5279},{level:2,title:"转义",slug:"转义",normalizedTitle:"转义",charIndex:2035},{level:2,title:"正则流派",slug:"正则流派",normalizedTitle:"正则流派",charIndex:9200},{level:2,title:"正则应用",slug:"正则应用",normalizedTitle:"正则应用",charIndex:9650},{level:2,title:"原理",slug:"原理",normalizedTitle:"原理",charIndex:17409},{level:2,title:"正则解决常见问题",slug:"正则解决常见问题",normalizedTitle:"正则解决常见问题",charIndex:21130},{level:2,title:"从编程语言的角度来理解正则表达式",slug:"从编程语言的角度来理解正则表达式",normalizedTitle:"从编程语言的角度来理解正则表达式",charIndex:24320}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"克制 元字符的分类 正则三种模式 分组与引用 匹配模式 转义 正则流派 正则应用 原理 正则解决常见问题 从编程语言的角度来理解正则表达式",content:"# 正则表达式\n\n\n# 克制\n\n那到底该怎么“克制”呢？我的经验有以下三点。\n\n> **第⼀，能用普通字符串处理的，坚决⽤普通字符串处理。**字符串处理的速度不见得差，可读性却好上很多。如果要在大段文本中定位所有的 today 或者 tomorrow，用最简单的字符串查找，直接找两遍，明显比 to(day|morrow) 看起来更清楚。\n\n> **第⼆，能写注释的正则表达式，⼀定要写注释。**正则表达式的语法非常古老，不够直观，为了便于阅读和维护，如今大部分语言里都可以通过 x 打开注释模式。有了注释，复杂正则表达式的结构也能一目了然。\n\n> **第三，能用多个简单正则表达式解决的，⼀定不要苛求用一个复杂的正则表达式。**这里最明显的例子就是输入条件的验证。比如说，常见的密码要求“必须包含数字、小写字母、大写字母、特殊符号中的至少两种，且长度在 8 到 16 之间”。\n> \n> 你当然可以绞尽脑汁用一个正则表达式来验证，但如果放下执念，⽤多个正则表达式分别验证“包含数字”“包含小写字母”“包含大写字母”“包含特殊符号”这四个条件，要求验证成功结果数大于等于 2，再配合一个正则表达式验证长度，这样做也是可行的。虽然看起来繁琐，但可维护性绝对远远强于单个正则表达式。\n\n\n# 元字符的分类\n\n所谓元字符就是指 那些 在正则表达式中具有特殊意义的专用字符，正则就是由一系列的元字符组成\n\n# 特殊单字符\n\n * .——任意字符(换行除外)\n * \\d——任意数字 \\D——任意非数字\n * \\w—— 任意字母，数字，下划线 \\W——任意非字母，数字，下划线\n * \\s——任意空白符 \\S——任意非空白字符\n\n\n\n# 空白符\n\n * \\n——换行符（常用）\n * \\——里面有一个空格，用来匹配空格\n * \\s——可以匹配各种空白字符——（常用）\n\n\n\n# 量词\n\n * 英文的星号（*）——0 到多次\n\n * 加号（+）——1 到多次\n\n * 问号（?）——0 到 1 次\n\n * {m,n}——m 到 n 次\n\n * {m}——出现m次\n\n * {m,}——至少出现m次(注意后面的逗号)\n\n * .*——会匹配出长度最长的结果——默认贪婪匹配\n\n * .*?——会匹配出最短的结果——非贪婪匹配\n\n\n\n# 范围\n\n * |——用来隔开多个正则，表示满足其中任意一个就行（类似于或）\n * []——代表多选一，可以表示里面的任意单个元素\n   * [aeiou]——任意元音字符\n   * [a-z]——所有小写字符\n   * [^a-z]——^表示非，表达的是不能是里面的任何单个字符\n\n\n\n# 断言\n\n正则中提供了一些结构，只用于匹配位置，而不是文本内容本身，这种结构就是断言\n\n\n\n * 单词边界(Word Bounday)\n   \n   * \\b——表示单词边界\n   \n   \n   \n   * \\btom\\b——保证只有一个tom\n\n * 行的开始/结束(^,$)\n   \n   回车(\\r)和换行(\\n)是两个概念\n   \n   * 回车&换行\n     \n     \n   \n   * 开始和结束\n     \n     * 要求6位数字——^\\d{6}$\n   \n   \n   \n   * 多行模式下，^ $可以匹配每一行的开头或者结尾\n     \n     * 所以对于输入数据的开头或者结尾使用\\A和\\z(Python)\n   \n   * 解决这个问题还有一种做法，我们可以在使用正则校验前，先判断一下字符串的长度，如果不满足长度要求，那就不需要再用正则去判断了。\n     \n     * 相当于你用正则解决主要的问题，而不是所有问题，这也是前面说的使用正则要克制.\n\n\n\n * 环视(Look Around)\n   \n   * 环视就是要求匹配部分的前面或后面要满足（或不满足）某种规则，有些地方也称环视为零宽断言\n   * 左尖括号代表看左边，没有尖括号是看右边，感叹号是非的意思。\n   \n   \n   \n   * 环视与子组\n     * 环视中虽然也有括号，但不会保存成子组\n     * 保存成子组的一般是匹配到的文本内容，后续用于替换等操作，而环视是表示对文本左右环境的要求，即环视只匹配位置，不匹配文本内容\n   * 六位邮编\n     * 中间6位数字，左边不是数字，右边不是数字\n     * (?<!\\d)\\d{6}(?!\\d)\n   * 单词边界\n     * \\b\\w+\\b——一个带边界的单词\n     * (?<!w)\\w+(?!\\w)或者(?<W)\\w+(?!\\w)\n\n# 示例\n\n * 某个资源，https://，http://，ftp://\n   \n   * (https?|ftp):\\/\\/\n     * s?——表示有一个或者没有s——匹配 http和https\n     * \\/——\\表示转义字符，\\/——表示正常的/\n\n * 正则——手机号——第一版\n   \n   * 第 1 位固定为数字 1；\n   * 第 2 位可能是 3，4，5，6，7，8，9；\n   * 第 3 位到第 11 位我们认为可能是 0-9 任意数字\n   * 1[3-9][0-9]{9}\n\n * 替换重复出现的单词\n   \n   * 以前的做法\n     \n     * the little cat cat is in the hat hat, we like it.\n     * 查找——(\\w+)\\s\\1\n     * 替换——\\1\n     * 这种做法无法满足\n   \n   * 现在的做法\n     \n     * the little cat cat2 is in the hat hat2, we like it.\n     \n     * cat与cat2属于不同的单词，但是(\\w+)\\s\\1达不到预计的效果\n       \n       \n     \n     * 查找——(\\w+)( \\1\\b)+\n       \n       * 严谨点——(\\w+)(?:\\s+\\1\\b)+\n       * ?:——不保存子组\n       * \\s+——可以匹配多个空白字符串\n     \n     * 替换——\\1\n       \n       \n\n\n# 正则三种模式\n\n贪婪匹配，非贪婪匹配，独占模式\n\n# 贪婪匹配\n\n> 在正则中，表示次数的量词默认是贪婪的，在贪婪模式下，会尝试尽可能最大长度去匹配\n\n * 在字符串 aaabb 中使用正则 a* 的匹配过程\n\n\n\n * a* 在匹配开头的 a 时，会尝试尽量匹配更多的 a，直到第一个字母 b 不满足要求为止，匹配上三个 a，后面每次匹配时都得到了空字符串。\n * 贪婪模式的特点就是尽可能进行最大长度匹配\n\n# 非贪婪匹配\n\n> 那么如何将贪婪模式变成非贪婪模式呢？我们可以在量词后面加上英文的问号 (?)——a？*\n\n\n\n * 这次匹配到的结果都是单个的 a，就连每个 a 左边的空字符串也匹配上了。\n * 非贪婪模式会尽可能短地去匹配\n * 两种模式对比——查找引号字符\n\n\n\n# 独占模式\n\n> 不管是贪婪模式，还是非贪婪模式，都需要发生回溯才能完成相应的功能。\n> \n> 独占模式，它类似贪婪匹配，但匹配过程不会发生回溯。\n\n * 贪婪模式（默认）\n\n\n\n * 非贪婪模式（量词？）\n\n\n\n * 独占模式（量词+）\n\n\n\n * 对比\n   \n   \n\n * 案例\n   \n   * we found “the little cat” is in the hat, we like “the little cat”\n   * 要求，提取出所有单词，引号中的单词（the little cat）看做一个\n   * \\w+|“.+?”——双引号内部 非贪婪模式\n   * \\w+|“[\\w\\s]+”——双引号内部 单词，空白符，一个或多个\n   * \\w+|“[^”]+”——双引号内部，不包含引号的一个或多个字符\n\n\n# 分组与引用\n\n * 案例1\n   \n   * \\d{18}|\\d{15}\n     \n     * 能识别18位或者15位数字\n     * 但是当每行数字超过18位后，只能识别18位的\n   \n   * \\d{15}|\\d{18}\n     \n     * 只能识别15位\n     * 因为18位中包含15位，且在大多数正则实现中，多分支选择都是左边的优先\n   \n   * \\d{15}(\\d{3})?\n     \n     * 前面表示15位，后面括号作为一个整体，加问号表示0个或者1个\n     * 由多个元字符组成某个部分，应该被看成一个整体的时候，可以用括号括起来表示一个整体，这是括号的一个重要功能\n     * 其实用括号括起来还有另外一个作用，那就是“复用”\n\n# 分组与编号\n\n * 子组\n   \n   * 保存子组(默认)\n     \n     * 用括号括起来的部分 子表达式 会被保存成一个子组\n     * 默认情况下，子组会被保存，并分配编号\n     \n     \n   \n   * 不保存子组\n     \n     * 你可能只想用括号将某些部分看成一个整体，后续不用再用它。这时我们可以使用(?:) 不保存子组。\n     \n     * 不保存子组可以理解为，括号只用于归组，把某个部分当成“单个元素”，不分配编号，后面不会再进行这部分的引用。\n     \n     * 不保存子组，性能会比较高。\n   \n   * 区别\n     \n     \n\n----------------------------------------\n\n * 括号嵌套查找编号\n   \n   * 对嵌套的括号查询-编号——数左边开括号的个数即可\n\n * 命名分组\n   \n   * 一些编程语言提供了命名分组（named grouping），这样和数字相比更容易辨识，不容易出错。命名分组的格式为(?P<分组名>正则)。\n\n# 分组引用\n\n * 各编程语言\n   \n   * \\num——\\1(1为分组编号)——\n   \n   * $1\n   \n   \n   \n   * 分组引用在查找和替换中使用\n     \n     * (\\w+) \\1——（中间有空格）\\w+表示一个单词，(\\w+)\\1表示引用分组的第一个\n     \n     * (\\w+)\\s\\1——空格尽量用 \\s\n       \n       \n     \n     * \\1表示第一个分组，即 ((\\d{4})-(\\d{2})-(\\d{2}))=2020-05-10\n     \n     * \\2表示第二个分组，即 (\\d{4})=2020\n     \n     * 在替换的时候，引用\\1,表示引用2020-05-10\n       \n       \n\n# 子组引用(*重要*)\n\n * \\d(\\d){2}——\\d(\\d)+——120\n   * 此时正则保存了一个子组，即\\1=0\n   * 这种()量词的情况 只保留最后一个作为子组\n * \\d(\\d)(\\d)——120\n   * 此时正则保存了两个子组，即\\1=2 && \\2=0\n * \\d(\\d{2})——\\d(\\d+)——120\n   * 此时正则保存了一个子组，即\\1=20\n   * 这种(量词)，量词在括号里面的情况，整个括号表示一个整体，即一个子组\n * (\\w+)\\s\\1——cat cat\n   * 第一个cat为子组1,后面的\\1表示引用之前的子组cat\n   * 这样即可表示 前后两个单词相同，即可表示 重复单词\n * ((\\w+)\\s\\2)——cat cat\n   * 第一个子组为 cat cat ,第二个子组为第一个 cat\n   * \\2 表示引用 第二个子组即第一个cat，这样也可以匹配上 重复单词\n * ([a-z]+)(\\d)\\s\\2——cat2 2\n   * 第一个子组为 cat， 因为+在括号里面\n   * 第二个子组是2， \\2表示引用第二个子组，即2,所有可以匹配上重复的字符2\n * (\\w+)(\\s\\1)+——cat cat或者cat cat cat或者cat cat cat cat\n   \n   * 第一个子组为第一个 cat\n   \n   * \\1表示引用第一个子组，即引用 cat\n   \n   * +表示一个或多个，再次引用第一个子组 cat，这样就可以表示 匹配到多个重复的单词\n   \n   * 示例\n     \n     * 查找\n       \n       * (\\w+)(\\s\\1)+\n     \n     * 替换\n       \n       * \\1\n\n\n# 匹配模式\n\n模式--mode\n\n * 模式修饰符——(?模式标识)\n   \n   * 不区分大小写——(?i)——Insensitive首字母的小写\n   * 点号通配模式——(?s)——Single\n   * 多行匹配模式——(?m)——Multiline\n   * 注释模式——(?#)\n   \n   \n\n# 不区分大小写模式（case-Insensitive）\n\n * 找出所有cat，不区分大小写\n   \n   * CAT=CAt=Cat=cat=cAt=cAT=cAt=caT\n   * [Cc][Aa][Tt]——笨方法\n   * (?i)cat\n\n * 匹配两个连续出现的cat，不区分大小写\n   \n   * (?i)(cat)\\s\\1\n   \n   \n\n * 不区分大小写，匹配两个连续出现的cat，且第一次和第二次大小写一致\n   \n   * ((?i)cat) \\1\n   * 用一个大括号包裹起来，表示不区分大小写只作用于这个括号里的内容\n   \n   \n\n * 部分区分大小写——the cat 中 the不区分，cat区分\n   \n   * ((?i)the) cat\n   * 让模式修饰符(?i)只作用于the,而不作用于cat\n   * 如果不加括号，默认是两个都会被作用（即作用于整个正则），加了括号，相当于加了限定条件，只作用于the\n\n * 总结\n   \n   * 不区分大小写模式的指定方式，使用模式修饰符 (?i)；\n   * 修饰符如果在括号内，作用范围是这个括号内的正则，而不是整个正则；\n   * 使用编程语言时可以使用预定义好的常量来指定匹配模式。\n\n# 点号通配模式（Single）\n\n也叫单行匹配模式，但是和多行匹配模式没有关系\n\n * .——可以匹配除了换行之外的任何字符\n * (?s).——可以匹配包括换行在内的任何字符\n\n# 多行匹配模式（Multiline）\n\n * 默认\n   \n   * ^——匹配整个字符串的开头\n   \n   * $——匹配整个字符串的结尾\n   \n   \n\n * (?m)——多行匹配模式\n   \n   * ^——匹配每行的开头\n   * $——匹配每行的结尾\n   \n   \n\n# 注释模式（Comment）\n\n * 正则表达式很复杂，可以在正则表达式内部加入注释\n * (?#)\n * (\\w+)(?#word)\\s\\1(?#word repeat again)\n\n# 示例\n\n * HTML 标签是不区分大小写的，比如我们要提取网页中的 head 标签中的内容，用正则如何实现呢？\n\n * >  <meta charset=\"utf-8\">\n\n * (?si)<head.*?>.*<\\/head>\n\n\n# 转义\n\n\n\n * 常用转义字符\n   \n   \n\n# 字符串转义和正则转义\n\n * 正则中：\n   \n   * \\d——表示单个数字\n   \n   * \\\\d——表示\\d——\\\\表示真正的反斜杠\n   \n   * \\\\|d——表示\\或者d\n\n * 程序中：\n   \n   * \\\\\\\\——\\\n     \n     * 四个反斜杠表示一个反斜杠\n     \n     * 因为先经过字符串转义\\\\\\\\——\\\\,真正被读入正则的只有\\\\\n     \n     * 然后，正则转义\\\\——\\\n       \n       \n   \n   * python中，可以使用原生字符串的方式来避免出现上面的情况\n     \n     \n     >>> import re\n     >>> re.findall('\\\\\\\\', 'a*b+c?\\\\d123d\\\\')\n     ['\\\\', '\\\\']// 第一个\\表示转义字符，第二个反斜杠表示真正的反斜杠\n     \n     \n     >>> import re\n     >>> re.findall(r'\\\\', 'a*b+c?\\\\d123d\\\\') //r表示读入原生字符\n     ['\\\\', '\\\\']\n     \n     \n     \n\n# 元字符的转义\n\n * 一般转义(直接在前面加\\即可)\n   \n   * \\*——*\n   * \\+——+\n   * \\?——?\n   * \\-——-``\n   * \\^——^\n   * \\$——$\n   * \\|——|\n\n * 括号的转义\n   \n   * 方括号 [] 和 花括号 {} 只需转义开括号\n   * 圆括号 () 两个都要转义\n   \n   >>> import re\n   >>> re.findall('\\(\\)\\[]\\{}', '()[]{}')\n   ['()[]{}']\n   >>> re.findall('\\(\\)\\[\\]\\{\\}', '()[]{}')  # 方括号和花括号都转义也可以\n   ['()[]{}']\n   \n\n * 转义函数\n   \n   * 使用转义函数可以将整个文本转义，将整个文本看做是一个正常的字符串，将其中的特殊字符加上转义。\n   \n   >>> import re\n   >>> re.escape('\\d')  # 反斜杠和字母d转义\n   '\\\\\\\\d'\n   >>> re.findall(re.escape('\\d'), '\\d')\n   ['\\\\d']\n   \n   >>> re.escape('[+]')  # 中括号和加号\n   '\\\\[\\\\+\\\\]'\n   >>> re.findall(re.escape('[+]'), '[+]')\n   ['[+]']\n   \n   \n   * 其他编程语言的函数\n   \n   \n\n# 字符组中的转义\n\n在字符组里只有三种情况需要转义\n\n * 脱字符^在中括号中，且在第一个位置需要转义\n   * 转义前代表非\n   * 转义后代表普通字符\n\n>>> import re\n>>> re.findall(r'[^ab]', '^ab')  # 转义前代表\"非\"\n['^']\n>>> re.findall(r'[\\^ab]', '^ab')  # 转义后代表普通字符\n['^', 'a', 'b']\n\n\n * 中划线-在中括号中，且不在首尾位置\n   * 在开头或者结尾不需要转义\n   * 在中间\n     * 转义前表示范围\n     * 转义后表示普通字符\n\n>>> import re\n>>> re.findall(r'[a-c]', 'abc-')  # 中划线在中间，代表\"范围\"\n['a', 'b', 'c']\n>>> re.findall(r'[a\\-c]', 'abc-')  # 中划线在中间，转义后的\n['a', 'c', '-']\n>>> re.findall(r'[-ac]', 'abc-')  # 在开头，不需要转义\n['a', 'c', '-']\n>>> re.findall(r'[ac-]', 'abc-')  # 在结尾，不需要转义\n['a', 'c', '-']\n\n\n\n * 右括号]在中括号中，且不在首位\n   * 在首位不需要转义，表示普通字符\n   * 不在首位，需要转义，不然[]会被认为提前结束\n\n>>> import re\n>>> re.findall(r'[]ab]', ']ab')  # 右括号不转义，在首位\n[']', 'a', 'b']\n>>> re.findall(r'[a]b]', ']ab')  # 右括号不转义，不在首位\n[]  # 匹配不上，因为含义是 a后面跟上b]\n>>> re.findall(r'[a\\]b]', ']ab')  # 转义后代表普通字符\n[']', 'a', 'b']\n\n\n其他字符，无需转义\n\n * []内部，+,*,?,.,()等不需要转义，就代表原来的字符.\n * 但如果在中括号中出现\\d或\\w 等符号时，他们还是元字符本身的含义。\n   * \\d=数字——\\w=字符\n\n>>> import re\n>>> re.findall(r'[.*+?()]', '[.*+?()]')  # 单个长度的元字符 \n['.', '*', '+', '?', '(', ')']\n>>> re.findall(r'[\\d]', 'd12\\\\')  # \\w，\\d等在中括号中还是元字符的功能\n['1', '2']  # 匹配上了数字，而不是反斜杠\\和字母d\n\n\n# 示例\n\n * \\\\n\\n\\\\——换行符\\n,用△表示\n * 即\\\\ n \\n \\\\——>字符串转义\\n△\\\n * 输入字符串——字符串转义——正则转义\n   * \\n——>△——>△=换行符\n   * \\\\n——>\\n——>△=换行符\n   * \\\\\\n——>\\△——>△=换行符\n   * \\\\\\\\n——>\\\\n——>\\n=反斜杠+n\n\n\n>>> import re\n>>> re.findall('\\n', '\\\\n\\n\\\\')\n['\\n']  # 找到了换行符\n\n\n>>> re.findall('\\\\n', '\\\\n\\n\\\\')  //输入`\\n`,匹配到`\\n`\n['\\n']  # 找到了换行符\n\n>>> re.findall('\\\\\\n', '\\\\n\\n\\\\') //输入`\\△`，\n['\\n']  # 找到了换行符\n\n>>> re.findall('\\\\\\\\n', '\\\\n\\n\\\\')\n['\\\\n'] # 找到了反斜杠和字母n\n\n\n\n# 正则流派\n\n\n\n# POSIX 流派\n\n * BRE\n   * GNU BRE 只有一个 E，使用时“花圆问管加”({}()?|+)时都要转义\n   * 早期标准，BRE不支持 ? + |\n   * GNU BRE 支持，但是需要加转义字符，即,\\? \\+ \\|\n * ERE\n   * GNU ERE 名称中有两个 E，不需要再转义\n\n\n\n# PCRE 流派\n\n * 来源于 Prel分支\n   * 这个流派显著特征是有\\d、\\w、\\s 这类字符fin组简记方式\n * 现在大部分编程语言支持的都是这个流派\n\n# Linux中使用正则\n\n * 按照 BRE 标准 实现的有 grep、sed 和 vi/vim 等\n * 按照 ERE 标准 实现的有 egrep、awk 等\n\n\n\n * 可以使用 man grep来查看支持那些标准\n   \n   \n\n * grep默认是 BRE流派\n\n * egrep=grep -E,是ERE流派\n\n * grep -P是PCRE 流派\n   \n   \n\n\n# 正则应用\n\n# 正则处理Unicode编码\n\n\n\n * \\w——不能匹配汉字\n\n * 量词正常使用\n\n * '极客{3}'\n   \n   * 匹配极客客客\n   * 表示的是客这个汉字重复3次，而不是客这个汉字对应的编码最后一个字节重复3次\n   * 如果重复是最后一个字节，应该极(?:客){3}——分组的形式\n\n# 在编辑器中使用正则（Sublime Text）\n\n * Sublime 的一些快捷键\n   * shift+alt+1-9(非小键盘)\n     * 使屏幕显示相等数字的小窗口\n   * ctrl+h——替换\n   * Ctrl+L——选择整行\n\n\n\n\n\n * 光标移动和文本选择\n   \n   * 按住shift键选中，然后按左右键可以左右选择文本块\n   * 按住shift+alt，光标可以按块移动，快速移动到下一个单词。\n\n * 多焦点编辑\n   \n   * 查找——>快速查找全部\n   \n   * 例子：提取JSON中的姓名和手机号\n     \n     {\n       \"error_code\": 0,\n       \"result\": {\n         \"data\": [\n           {\n             \"name\": \"朱小明\",\n             \"tel\": \"138xx138000\"\n           },\n           {\n             \"name\": \"王五\",\n             \"tel\": \"139xx139000\"\n           }\n         ]\n       }\n     }\n     \n     朱小明\n     138xx138000\n     王五\n     139xx139000\n     \n     \n     \n     * 选中\": \",快速查找全部\n     * 按右方向键，将光标移动到引号右边\n     * 按住shift+alt，快速选择整个引号内的内容\n     * 复制，粘贴即可\n\n * 竖向编辑\n   \n   * shift+鼠标右键,可同时操作处于同一列的文本，同时编辑\n   * ctrl+鼠标左键，可以选中多处文本，同时编辑。\n\n * 内容提取\n   \n   * \\S+@\\S+\\.\\S+(?=;)——简单邮箱提取\n     \n     * \\S——任意非空白字符\n     * (?=;)——环视;\n   \n   * 小李： jkmkqhvrc@265.com;\n     小王： atvl@sogou@.com;\n     小红： vtoispm@tom.com;\n     小真： olncckkerlikb@citiz.com;\n     小爱： mddbatlosa@msn.com;\n     \n     jkmkqhvrc@265.com\n     atvl@sogou@.com\n     vtoispm@tom.com\n     olncckkerlikb@citiz.com\n     mddbatlosa@msn.com\n     \n\n * 内容替换\n   \n   * (\\S+@(\\S+)\\.\\S+)==查找\n     \n     * 两个括号，两个分组，均可以用来引用\n       * \\1=jkmkqhvrc@265.com\n       * \\2=265\n   \n   * \\2邮箱 ===\\1==替换\n   \n   * jkmkqhvrc@265.com\n     atvl@sogou@.com\n     vtoispm@tom.com\n     olncckkerlikb@citiz.com\n     mddbatlosa@msn.com\n     \n     265邮箱 ===jkmkqhvrc@265.com\n     sogou@邮箱 ===atvl@sogou@.com\n     tom邮箱 ===vtoispm@tom.com\n     citiz邮箱 ===olncckkerlikb@citiz.com\n     msn邮箱 ===mddbatlosa@msn.com\n     \n\n * 统计一篇英文文章中每个单词出现的次数\n   \n   * 处理成一行一个单词\n     \n     * \\W——\\n\n     \n     * 将非字符转换成空格\n     \n     * (?<!\\w)\\s——``\n       \n       * 将左边不是字符的空格去掉\n       \n       I have a cat, cat is a dog, dog is I cat.\n       \n       \n       I\n       have\n       a\n       cat\n       \n       cat\n       is\n       a\n       dog\n       \n       dog\n       is\n       I\n       cat\n       \n       \n       I\n       have\n       a\n       cat\n       cat\n       is\n       a\n       dog\n       dog\n       is\n       I\n       cat\n       \n   \n   * 使用sort 命令排序，uniq -c统计次数\n     \n     uniq -c：   统计每行出现次数\n     sort :\n     \t-n  按数字排序\n     \t-r\t逆序排序\n     \t-k1\t根据-t的分割，分成几域，取第1个域排序\n     \t-t  指定分隔符，默认的分隔符为空白字符和非空白字符之间的空字符\n     head -n10： 取前10行数据\n     \n     \n     $ sort word.txt | uniq -c\n           2 I\n           2 a\n           3 cat\n           2 dog\n           1 have\n           2 is\n     \n     $ sort word.txt | uniq -c | sort -nr\n           3 cat\n           2 is\n           2 dog\n           2 a\n           2 I\n           1 have\n     \n     \n\n# 在语言中用正则\n\n\n\n * 校验\n   \n   验证日期 2022-06-01\n   \n   * python\n     \n     # 测试环境 Python3\n     >>> import re\n     >>> re.match(r'\\d{4}-\\d{2}-\\d{2}', '2020-06-01')\n     <re.Match object; span=(0, 10), match='2020-06-01'>\n     # 这个输出是匹配到了，范围是从下标0到下标10，匹配结果是2020-06-01\n     # re.search 输出结果也是类似的\n     \n     \n     * \\A \\Z表示文本的开头和结尾\n     * 不建议使用^和$,因为在多行模式下，可以表示每行的开头和结尾\n     \n     # 测试环境 Python3\n     >>> import re\n     >>> reg = re.compile(r'\\A\\d{4}-\\d{2}-\\d{2}\\Z')  # 建议先编译，提高效率\n     >>> reg.search('2020-06-01') is not None\n     True\n     >>> reg.match('2020-06-01') is not None  # 使用match时\\A可省略\n     True\n     \n     \n   \n   * java\n     \n     * \\A \\z表示文本的开头和结尾\n     \n     \n     import java.util.regex.Matcher;\n     import java.util.regex.Pattern;\n     \n     class Main {\n       public static void main(String[] args) {\n         //方法1，可以不加 \\A 和 \\z\n         System.out.println(Pattern.matches(\"\\\\d{4}-\\\\d{2}-\\\\d{2}\", \"2020-06-01\")); // true\n     \n         //方法2，可以不加 \\A 和 \\z\n         System.out.println(\"2020-06-01\".matches(\"\\\\d{4}-\\\\d{2}-\\\\d{2}\")); // true\n         \n         //方法3，必须加上 \\A 和 \\z\n         Pattern pattern = Pattern.compile(\"\\\\A\\\\d{4}-\\\\d{2}-\\\\d{2}\\\\z\");\n         System.out.println(pattern.matcher(\"2020-06-01\").find()); // true\n       }\n     }\n     \n\n * 提取\n   \n   日志时间提取\n   \n   * python\n   \n   # 没有子组时\n   >>> import re\n   >>> reg = re.compile(r'\\d{4}-\\d{2}')\n   >>> reg.findall('2020-05 2020-06')\n   ['2020-05', '2020-06']\n   \n   # 有子组时\n   >>> reg = re.compile(r'(\\d{4})-(\\d{2})')\n   >>> reg.findall('2020-05 2020-06')\n   [('2020', '05'), ('2020', '06')]\n   \n   \n   * 节约内存，使用迭代器\n   \n   \n   >>> import re\n   >>> reg = re.compile(r'(\\d{4})-(\\d{2})')\n   >>> for match in reg.finditer('2020-05 2020-06'):\n   ...     print('date: ', match[0])  # 整个正则匹配到的内容\n   ...     print('year: ', match[1])  # 第一个子组\n   ...     print('month:', match[2])  # 第二个子组\n   ...\n   date:  2020-05\n   year:  2020\n   month: 05\n   date:  2020-06\n   year:  2020\n   month: 06\n   \n   \n   * java\n     \n     \n     import java.util.regex.Matcher;\n     import java.util.regex.Pattern;\n     \n     class Main {\n       public static void main(String[] args) {    \n         Pattern pattern = Pattern.compile(\"\\\\d{4}-\\\\d{2}\");\n         Matcher match = pattern.matcher(\"2020-06 2020-07\");\n         while (match.find()) {\n           System.out.println(match.group());      \n         }\n       }\n     }\n     \n\n * 替换\n   \n   (02-20-2022)月日年——年月日(2022年02月20日)\n   \n   * python\n   \n   >>> import re\n   >>> reg = re.compile(r'(\\d{2})-(\\d{2})-(\\d{4})')\n   \n   >>> reg.sub(r'\\3年\\1月\\2日', '02-20-2020 05-21-2020')\n   '2020年02月20日 2020年05月21日'\n   \n   # 可以在替换中使用 \\g<数字>，如果分组多于10个时避免歧义\n   >>> reg.sub(r'\\g<3>年\\g<1>月\\g<2>日', '02-20-2020 05-21-2020')\n   '2020年02月20日 2020年05月21日'\n   \n   # 返回替换次数\n   >>> reg.subn(r'\\3年\\1月\\2日', '02-20-2020 05-21-2020')\n   ('2020年02月20日 2020年05月21日', 2)\n   \n   \n   \n   * java\n     * 替换时 引用子组要用 $\n   \n   import java.util.regex.Matcher;\n   import java.util.regex.Pattern;\n   \n   class Main {\n     public static void main(String[] args) {\n       //方法1，输出 2020年02月20日 2020年05月21日\n       System.out.println(\"02-20-2020 05-21-2020\".replaceAll(\"(\\\\d{2})-(\\\\d{2})-(\\\\d{4})\", \"$3年$1月$2日\"));\n       \n       //方法2，输出 2020年02月20日 2020年05月21日\n       final Pattern pattern = Pattern.compile(\"(\\\\d{2})-(\\\\d{2})-(\\\\d{4})\");\n       Matcher match = pattern.matcher(\"02-20-2020 05-21-2020\");\n       System.out.println(match.replaceAll(\"$3年$1月$2日\"));\n     }\n   }\n   \n\n * 切割\n   \n   切割得到单词\n   \n   * python\n   \n   >>> import re\n   >>> reg = re.compile(r'\\W+')\n   >>> reg.split(\"apple, pear! orange; tea\")\n   ['apple', 'pear', 'orange', 'tea']\n   \n   # 限制切割次数，比如切一刀，变成两部分\n   >>> reg.split(\"apple, pear! orange; tea\", 1)\n   ['apple', 'pear! orange; tea']\n   \n   \n   * java\n   \n   import java.util.regex.Matcher;\n   import java.util.regex.Pattern;\n   \n   class Main {\n     public static void main(String[] args) {\n       Pattern pattern = Pattern.compile(\"\\\\W+\");\n       for(String s : pattern.split(\"apple, pear! orange; tea\")) {\n         System.out.println(s);\n       }\n     }\n   }\n   \n\n * 示例\n   \n   xxx#163.com (请把#换成@)\n   \n   * python\n   \n   // 替换——在用正则提取邮箱\n   >>> reg=re.compile(r'#')\n   >>> reg.sub(r'@','xxx#163.com')\n   'xxx@163.com'\n   \n   //提取\n   >>> reg=re.compile(r'\\w+[#@]\\w+.\\w+')\n   >>> reg.findall('联系邮箱：xxx#163.com (请把#换成@) 联系邮箱：xxx@163.com')\n   ['xxx#163.com', 'xxx@163.com']\n   \n   \n   \n   * java\n\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\n//替换\nclass Main {\n  public static void main(String[] args) {\n   \t\tPattern compile = Pattern.compile(\"#\");\n\t\tMatcher matcher = compile.matcher(\"xxx#163.com\");\n\t\tSystem.out.println(matcher.replaceAll(\"@\"));\n    }\n}\n\n//提取\nclass Main {\n  public static void main(String[] args){\n        //提取 带#或者带@的邮箱\n        Pattern pattern=Pattern.compile(\"\\\\w+[@#]\\\\w+.\\\\w+\");\n        Matcher matcher = pattern.matcher(\"联系邮箱：xxx#163.com (请把#换成@) 联系邮箱：xxx@163.com\");\n        while (matcher.find())\n        {\n            System.out.println(matcher.group());\n        }\n\n    }\n}\n\n\n\n# 原理\n\n\n\n# 正则匹配原理\n\n * 有穷状态自动机\n   \n   * 正则之所以能够处理复杂文本，就是因为采用了有穷状态自动机\n   * 有穷自动机的具体实现称为正则引擎，主要有 DFA 和 NFA 两种\n     * DFA：确定性有穷自动机（Deterministic finite automaton） NFA：非确定性有穷自动机（Non-deterministic finite automaton）\n\n * 正则匹配过程\n   \n   * 编译**(compile)**的过程，其实就是生成自动机的过程\n   \n   * NFA工作机制\n     \n     NFA 引擎的工作方式是，先看正则，再看文本，而且以正则为主导\n     \n     > 字符串：we study on jikeshijian app 正则：jike(zhushou|shijian|shixi)\n     \n     * 正则中的第一个字符是 j，NFA 引擎在字符串中查找 j，接着匹配其后是否为 i ，如果是 i 则继续，这样一直找到 jike\n       \n       > regex: jike(zhushou|shijian|shixi) ^ text: we study on jikeshijian app ^\n     \n     * 我们再根据正则看文本后面是不是 z，发现不是，此时 zhushou 分支淘汰\n       \n       > regex: jike(zhushou|shijian|shixi) ^ 淘汰此分支(zhushou) text: we study on jikeshijian app ^\n     \n     * 我们接着看其它的分支，看文本部分是不是 s，直到 shijian 整个匹配上。\n     \n     * shijian 在匹配过程中如果不失败，就不会看后面的 shixi 分支。\n     \n     * 假设这里文本改一下，把 jikeshijian 变成 jikeshixi，正则 shijian 的 j 匹配不上时 shixi 的 x，会接着使用正则 shixi 来进行匹配，重新从 s 开始（NFA 引擎会记住这里）。\n       \n       > 第二个分支匹配失败 regex: jike(zhushou|shijian|shixi) ^ 淘汰此分支(正则j匹配不上文本x) text: we study on jikeshixi app ^\n       > \n       > 再次尝试第三个分支 regex: jike(zhushou|shijian|shixi) ^ text: we study on jikeshixi app ^\n   \n   * DFA工作机制\n     \n     DFA 会先看文本，再看正则表达式，是以文本为主导的\n     \n     * DFA 会从 we 中的 w 开始依次查找 j，定位到 j ，这个字符后面是 i。所以我们接着看正则部分是否有 i ，如果正则后面是个 i ，那就以同样的方式，匹配到后面的 ke\n       \n       > text: we study on jikeshijian app ^ regex: jike(zhushou|shijian|shixi) ^\n     \n     * 继续进行匹配，文本 e 后面是字符 s ，DFA 接着看正则表达式部分，此时 zhushou 分支被淘汰，开头是 s 的分支 shijian 和 shixi 符合要求。\n       \n       > text: we study on jikeshijian app ^ regex: jike(zhushou|shijian|shixi) ^ ^ ^ 淘汰 符合 符合\n     \n     * 然后 DFA 依次检查字符串，检测到 shijian 中的 j 时，只有 shijian 分支符合，淘汰 shixi，接着看分别文本后面的 ian，和正则比较，匹配成功。\n       \n       > text: we study on jikeshijian app ^ regex: jike(zhushou|shijian|shixi) ^ ^ 符合 淘汰\n   \n   * DFA与NFA\n     \n     * 一般来说，DFA 引擎会更快一些，因为整个匹配过程中，字符串只看一遍，不会发生回溯，相同的字符不会被测试两次。也就是说 DFA 引擎执行的时间一般是线性的。DFA 引擎可以确保匹配到可能的最长字符串。但由于 DFA 引擎只包含有限的状态，所以它没有反向引用功能；并且因为它不构造显示扩展，它也不支持捕获子组。\n     * NFA 以表达式为主导，它的引擎是使用贪心匹配回溯算法实现。NFA 通过构造特定扩展，支持子组和反向引用。但由于 NFA 引擎会发生回溯，即它会对字符串中的同一部分，进行很多次对比。因此，在最坏情况下，它的执行速度可能非常慢。\n   \n   * POSIX NFA\n     \n     * POSIX NFA 引擎与传统的 NFA 引擎类似，但不同之处在于，POSIX NFA 在找到可能的最长匹配之前会继续回溯，也就是说它会尽可能找最长的，如果分支一样长，以最左边的为准\n     * 比如使用正则 pos|posix 在文本 posix 中进行匹配，传统的 NFA 从文本中找到的是 pos，而不是 posix，而 POSIX NFA 找到的是 posix\n   \n   * 三者比较\n   \n   \n\n# 回溯\n\n回溯是 NFA 引擎才有的，并且只有在正则中出现量词或多选分支结构时，才可能会发生回溯。\n\n * +\n   \n   > 比如我们使用正则 a+ab 来匹配 文本 aab 的时候，过程是这样的，a+ 是贪婪匹配，会占用掉文本中的两个 a，但正则接着又是 a，文本部分只剩下 b，只能通过回溯，让 a+ 吐出一个 a，再次尝试\n\n * .*\n   \n   > 如果正则是使用 .*ab 去匹配一个比较长的字符串就更糟糕了，因为 .* 会吃掉整个字符串（不考虑换行，假设文本中没有换行），然后，你会发现正则中还有 ab 没匹配到内容，只能将 .* 匹配上的字符串吐出一个字符，再尝试，还不行，再吐出一个，不断尝试\n   \n   \n   \n   * 所以在工作中，我们要尽量不用 .*,可以用其他方式来替换\n     * 比如要提取引号中的内容时\n     * \".+?\"——非贪婪模式\n     * \"[^\"]+\"——双引号内部，非双引号的字符\n\n# 优化原则\n\n * 测试性能的方法\n   \n   * 使用ipyhon来测试正则表达式\n     \n     * win+R——输入 ipython\n     * ipython使用技巧\n     \n     In [1]: import re\n     In [2]: x = '-' * 1000000 + 'abc'\n     In [3]: timeit re.search('abc', x)\n     480 µs ± 8.06 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n     \n   \n   * 可以通过前面 regex101.com 查看正则和文本匹配的次数\n\n * 提前编译好正则\n   \n   * compile\n\n * 尽量准确表示匹配范围\n   \n   * 我们可以写成“[^\"]+”。使用[^\"]要比使用点号好很多\n\n * 提取出公共部分\n   \n   * 因为 NFA 以正则为主导，会导致字符串中的某些部分重复匹配多次，影响效率。\n   * (abcd|abxy)这样的表达式，可以优化成ab(cd|xy)\n   * (^this|^that) is——^th(is|at) is——锚点部分独立处理\n   * this|that——th(?:is|at)\n\n * 出现可能性大的放左边\n   \n   * 由于正则是从左到右看的，所有在分支中，把可能性大的放到左边\n\n * 在必要时才使用子组\n   \n   * 在正则中，括号可以用于归组，但如果某部分后续不会再用到，就不需要保存成子组。\n   * (?:)——不保存子组\n\n * 警惕嵌套的子组重复\n   \n   * (.*)*这个正则，匹配的次数会呈指数级增长\n\n * 避免不同分支重复匹配\n\n# 示例\n\n针对这个示例，NFA 引擎的匹配过程\n\n> 文本：a12----开始部分用△表示，结束部分用○表示\n> \n> 即：△a12○\n> \n> 正则：^(?=[a-z])[a-z0-9]+$\n\n> ^——△\n> \n> (?=[a-z])——a\n> \n> [a-z0-9]——△——a12\n> \n> $——○\n\n\n# 正则解决常见问题\n\n\n\n# 正则处理问题的基本思路\n\n 1. 比如将问题分解成多个小问题，每个小问题见招拆招。\n\n 2. 某个位置上可能有多个字符的话，就⽤字符组。\n\n 3. 某个位置上有多个字符串的话，就⽤多选结构。\n\n 4. 出现的次数不确定的话，就⽤量词。\n\n 5. 对出现的位置有要求的话，就⽤锚点锁定位置。\n\n 6. 在正则中比较难的是某些字符不能出现，这个情况又可以进一步分为以下两种\n    \n    * 组成中不能出现\n      * 要查找的内容中不能出现某些字符——[^aeiou]——不能出现元音\n      * 例如 要求密码6位，但是不能有两个连续数字出现\n        * ^((?!\\d\\d)\\w){6}$\n        * (?!\\d\\d)——代表右边不能是两个数字，但是左边没有正则，即为空字符串\n    * 要查找的内容前后不能出现——环视\n\n# 常见问题\n\n * 匹配数字\n   \n   * 数字—— \\d 或[0-9]\n   * 连续的多个数字—— \\d+ 或[0-9]+\n   * n 位数字—— \\d{n}。\n   * 至少 n 位数据——\\d{n,}。\n   * m-n 位数字—— \\d{m,n}。\n\n * 匹配正数、负数、小数（浮点数）\n   \n   * 3，3.14，-3，+2.7，20.12\n     * [+-]?\\d+[.]?\\d*\n     * [+-]?\\d+(?:\\.\\d+)?\n       * ——(?:)表示不保留子组——\\.表示转义.\n   * .5，+.5——考虑这种极端情况（负号的时候整数部分不能没有，而正数的时候整数部分可以没有）\n     * 正数：+?(?:\\d+(?:\\.\\d+)?|\\.\\d+)\n       * \\d+(?:\\.\\d+)?和\\.\\d+两个分支\n     * 负数：-\\d+(?:\\.\\d+)?\n     * 组合：-\\d+(?:\\.\\d+)?|\\+?(?:\\d+(?:\\.\\d+)?|\\.\\d+)——正数和负数两个分支\n\n * 十六进制数\n   \n   * [0-9a-fA-F]+\n\n * 手机号码\n   \n   \n   \n   * 限制前两位\n     * 1[3-9]\\d{9}\n   * 限制前三位\n     * 13:13[0-9]\\d{8}\n     * 14:14[5-9]\\d{8}\n     * 15:15[0-35-9]\\d{8}\n     * 16:16[2567]\\d{8}\n     * 17:17[0-8]\\d{8}\n     * 18:18[0-9]\\d{8}\n     * 19:19[1389]\\d{8}\n     * 组合：1(?:3[0-9]|4[5-9]|5[0-35-9]|6[2567]|7[0-8]|8[0-9]|9[1389])\\d{8}\n\n * 身份证号码\n   \n   * 第一代是 15 位，第二代是 18 位;如果是 18 位，最后一位可以是 X（或 x），两代开头都不能是 0\n   * 15:[1-9]\\d{14}\n   * 18:[1-9]\\d{16}[\\dXx]\n   * 组合：[1-9]\\d{14}(\\d{2}[\\dXx])?\n\n * 邮政编码\n   \n   * 邮编一般为 6 位数字,需要加断言\n   * (?<!\\d)\\d{6}(?!\\d)——左边不是数字，右边不是数字\n\n * QQ号码\n   \n   * QQ 号不能以 0 开头，最长的有 10 位，最短的从 10000（5 位）开始\n   * [1-9]\\d{4,9}?——\\d{4,9},表示数字出现4次到9次\n\n * 中文字符\n   \n   * 中文属于多字节 Unicode 字符，通过 Unicode 属性，但有一些语言是不支持这种属性的\n   * 另外一个办法，就是码值的范围，中文的范围是 4E00 - 9FFF 之间\n   * Python，Java，JavaScript 中，Unicode 可以写成 \\u码值 来表示，即匹配中文的正则可以写成 [\\u4E00-\\u9FFF]，在 PHP 中使用，Unicode 就需要写成 \\u{码值} 的样式。\n   \n   # 测试环境，Python3\n   >>> import re\n   >>> reg = re.compile(r'[\\u4E00-\\u9FFF]')\n   >>> reg.findall(\"和伟忠一起学正则regex\")\n   ['和', '伟', '忠', '一', '起', '学', '正', '则']\n   \n\n * IPV4地址\n   \n   * IPv4 地址通常表示成 27.86.1.226 的样式，4 个数字用点隔开，每一位范围是 0-255\n   * 简单版：\\d{1,3}(?:\\.\\d{1,3}){3}\n   * 按位数考虑\n     * 1位——1.--01.--001.\n       * 0{0,2}\\d\n     * 2位——10.--010.\n       * 0?\\d\\d\n     * 3位——100.--255.\n       * 1\\d\\d|2[0-5][0-5]\n     * 组合——0{0,2}\\d|0?\\d\\d|1\\d\\d|2[0-5][0-5]=x\n     * 组合改进——长的分支放到左边——1\\d\\d|2[0-5][0-5]|0?\\d\\d|0{0,2}\\d=x\n   * 加上点——x.x.x.x\n     * (?:x)(?:\\.(?:x)){3}\n     * (?:1\\d\\d|2[0-5][0-5]|0?\\d\\d|0{0,2}\\d)(?:\\.(?:1\\d\\d|2[0-5][0-5]|0?\\d\\d|0{0,2}\\d)){3}\n\n * 日期\n   \n   * 日期格式 yyyy-mm-dd\n   * 简单：\\d{4}-\\d{2}-\\d{2}\n   * 复杂：考虑月份 1-12，一位时0可带可不带；日期 1-31（像 30，29,28这种应该交给程序来完成，不要用正则完成所有的事情）\n     * 年：\\d{4}\n     * 月：1[0-2]|0?[1-9]——两位的写在前面\n     * 日：[12]\\d|3[01]|0?[1-9]\n     * 组合：\\d{4}-(?:1[0-2]|0?[1-9])-(?:[12]\\d|3[01]|0?[1-9])\n\n * 时间\n   \n   * 24小时制——23:24\n   * 小时：0-23；分钟：0-59\n   * (?:2[0-3]|1[0-9]|0?[0-9]):(?:[1-5][0-9|0?[0-9]])\n   * 12小时制——11:59\n   * 小时：0-11；分钟：0-59\n   * (?:1[01]|0?[0-9]):(?:[1-5][0-9|0?[0-9]])\n\n * 邮箱\n   \n   * 格式是 用户名 @主机名，用户名部分通常可以有英文字母，数字，下划线，点等组成，但其中点不能在开头，也不能重复出现\n   * 简单版：[\\w.]+@[\\w]+\\.[\\w]+\n\n * 网页标签\n   \n   * 配对出现的标签，不区分大小写,title\n     * (?si)<title.*?>.*<\\/title>\n   * 提取引号里面的内容\n     * \".+\"\n     * \"[^\"]+\"——带\"\"\n     * (?<=\")[^\"]+(?=\")——不带\"\"\n   * 提取尖括号<>里面的内容\n     * <[^>]+>——带<>\n     * (?<=<)[^>]+(?=>)——不带<>\n\n\n# 从编程语言的角度来理解正则表达式\n\n\n\n> 使用第 4 代语言来描述问题，而无需花费大量时间，去考虑具体的处理逻辑和算法实现，处理逻辑和算法实现是由编译器（Compiler）或解释器（Interpreter）这样的语言解析引擎来负责。\n\n * 正则表达式属于声明式编程范式\n\n> 声明式编程范式，主要是模拟人脑思维的过程。声明式重目标、轻过程，专注问题的分析和表达，而不是算法实现。它不用指明执行顺序，属于目标导向，强调的是定义问题的描述——即“做什么”，因而目标是显性而算法是隐性的。\n\n * * 因此，从编程范式的角度来看：\n   * 声明式编程的世界观是：程序是由若干目标任务组成的有序列表；\n   * 声明式编程的方法论是：用语法元素来描述任务，由解析引擎转化为指令并执行。\n\n * 正则表达式的语法元素本质上就是程序逻辑和算法\n   \n   * 正则表达式中的星号量词“*”这一元字符，就是高级语言的处理逻辑“循环结构”的体现。具体来说，星号量词“*”代表的是不定次数循环结构，而前后多个星号量词的嵌套就是多层不定次数循环结构的嵌套；\n   \n   * 或运算符，也就是竖线“|”这个元字符，就是高级语言的处理逻辑“分支结构”的体现；\n   \n   * 而用于分组的圆括号“()”，就相当于高级语言的作用域。\n   \n   * 而当或运算符“|”出现在由星号量词“*”所限定的分组圆括号“()”中时，其实就是在“循环结构”中嵌套了“分支结构”；\n   \n   * 而如果进一步地，“循环结构”所嵌套的“分支结构”中的某个分支，又被某个星号量词“*”所限定，那么则相当于“循环结构”所嵌套的“分支结构”又嵌套了“循环结构”。\n   \n   * (张三|李四 *)*\n     \n     \n\n",normalizedContent:"# 正则表达式\n\n\n# 克制\n\n那到底该怎么“克制”呢？我的经验有以下三点。\n\n> **第⼀，能用普通字符串处理的，坚决⽤普通字符串处理。**字符串处理的速度不见得差，可读性却好上很多。如果要在大段文本中定位所有的 today 或者 tomorrow，用最简单的字符串查找，直接找两遍，明显比 to(day|morrow) 看起来更清楚。\n\n> **第⼆，能写注释的正则表达式，⼀定要写注释。**正则表达式的语法非常古老，不够直观，为了便于阅读和维护，如今大部分语言里都可以通过 x 打开注释模式。有了注释，复杂正则表达式的结构也能一目了然。\n\n> **第三，能用多个简单正则表达式解决的，⼀定不要苛求用一个复杂的正则表达式。**这里最明显的例子就是输入条件的验证。比如说，常见的密码要求“必须包含数字、小写字母、大写字母、特殊符号中的至少两种，且长度在 8 到 16 之间”。\n> \n> 你当然可以绞尽脑汁用一个正则表达式来验证，但如果放下执念，⽤多个正则表达式分别验证“包含数字”“包含小写字母”“包含大写字母”“包含特殊符号”这四个条件，要求验证成功结果数大于等于 2，再配合一个正则表达式验证长度，这样做也是可行的。虽然看起来繁琐，但可维护性绝对远远强于单个正则表达式。\n\n\n# 元字符的分类\n\n所谓元字符就是指 那些 在正则表达式中具有特殊意义的专用字符，正则就是由一系列的元字符组成\n\n# 特殊单字符\n\n * .——任意字符(换行除外)\n * \\d——任意数字 \\d——任意非数字\n * \\w—— 任意字母，数字，下划线 \\w——任意非字母，数字，下划线\n * \\s——任意空白符 \\s——任意非空白字符\n\n\n\n# 空白符\n\n * \\n——换行符（常用）\n * \\——里面有一个空格，用来匹配空格\n * \\s——可以匹配各种空白字符——（常用）\n\n\n\n# 量词\n\n * 英文的星号（*）——0 到多次\n\n * 加号（+）——1 到多次\n\n * 问号（?）——0 到 1 次\n\n * {m,n}——m 到 n 次\n\n * {m}——出现m次\n\n * {m,}——至少出现m次(注意后面的逗号)\n\n * .*——会匹配出长度最长的结果——默认贪婪匹配\n\n * .*?——会匹配出最短的结果——非贪婪匹配\n\n\n\n# 范围\n\n * |——用来隔开多个正则，表示满足其中任意一个就行（类似于或）\n * []——代表多选一，可以表示里面的任意单个元素\n   * [aeiou]——任意元音字符\n   * [a-z]——所有小写字符\n   * [^a-z]——^表示非，表达的是不能是里面的任何单个字符\n\n\n\n# 断言\n\n正则中提供了一些结构，只用于匹配位置，而不是文本内容本身，这种结构就是断言\n\n\n\n * 单词边界(word bounday)\n   \n   * \\b——表示单词边界\n   \n   \n   \n   * \\btom\\b——保证只有一个tom\n\n * 行的开始/结束(^,$)\n   \n   回车(\\r)和换行(\\n)是两个概念\n   \n   * 回车&换行\n     \n     \n   \n   * 开始和结束\n     \n     * 要求6位数字——^\\d{6}$\n   \n   \n   \n   * 多行模式下，^ $可以匹配每一行的开头或者结尾\n     \n     * 所以对于输入数据的开头或者结尾使用\\a和\\z(python)\n   \n   * 解决这个问题还有一种做法，我们可以在使用正则校验前，先判断一下字符串的长度，如果不满足长度要求，那就不需要再用正则去判断了。\n     \n     * 相当于你用正则解决主要的问题，而不是所有问题，这也是前面说的使用正则要克制.\n\n\n\n * 环视(look around)\n   \n   * 环视就是要求匹配部分的前面或后面要满足（或不满足）某种规则，有些地方也称环视为零宽断言\n   * 左尖括号代表看左边，没有尖括号是看右边，感叹号是非的意思。\n   \n   \n   \n   * 环视与子组\n     * 环视中虽然也有括号，但不会保存成子组\n     * 保存成子组的一般是匹配到的文本内容，后续用于替换等操作，而环视是表示对文本左右环境的要求，即环视只匹配位置，不匹配文本内容\n   * 六位邮编\n     * 中间6位数字，左边不是数字，右边不是数字\n     * (?<!\\d)\\d{6}(?!\\d)\n   * 单词边界\n     * \\b\\w+\\b——一个带边界的单词\n     * (?<!w)\\w+(?!\\w)或者(?<w)\\w+(?!\\w)\n\n# 示例\n\n * 某个资源，https://，http://，ftp://\n   \n   * (https?|ftp):\\/\\/\n     * s?——表示有一个或者没有s——匹配 http和https\n     * \\/——\\表示转义字符，\\/——表示正常的/\n\n * 正则——手机号——第一版\n   \n   * 第 1 位固定为数字 1；\n   * 第 2 位可能是 3，4，5，6，7，8，9；\n   * 第 3 位到第 11 位我们认为可能是 0-9 任意数字\n   * 1[3-9][0-9]{9}\n\n * 替换重复出现的单词\n   \n   * 以前的做法\n     \n     * the little cat cat is in the hat hat, we like it.\n     * 查找——(\\w+)\\s\\1\n     * 替换——\\1\n     * 这种做法无法满足\n   \n   * 现在的做法\n     \n     * the little cat cat2 is in the hat hat2, we like it.\n     \n     * cat与cat2属于不同的单词，但是(\\w+)\\s\\1达不到预计的效果\n       \n       \n     \n     * 查找——(\\w+)( \\1\\b)+\n       \n       * 严谨点——(\\w+)(?:\\s+\\1\\b)+\n       * ?:——不保存子组\n       * \\s+——可以匹配多个空白字符串\n     \n     * 替换——\\1\n       \n       \n\n\n# 正则三种模式\n\n贪婪匹配，非贪婪匹配，独占模式\n\n# 贪婪匹配\n\n> 在正则中，表示次数的量词默认是贪婪的，在贪婪模式下，会尝试尽可能最大长度去匹配\n\n * 在字符串 aaabb 中使用正则 a* 的匹配过程\n\n\n\n * a* 在匹配开头的 a 时，会尝试尽量匹配更多的 a，直到第一个字母 b 不满足要求为止，匹配上三个 a，后面每次匹配时都得到了空字符串。\n * 贪婪模式的特点就是尽可能进行最大长度匹配\n\n# 非贪婪匹配\n\n> 那么如何将贪婪模式变成非贪婪模式呢？我们可以在量词后面加上英文的问号 (?)——a？*\n\n\n\n * 这次匹配到的结果都是单个的 a，就连每个 a 左边的空字符串也匹配上了。\n * 非贪婪模式会尽可能短地去匹配\n * 两种模式对比——查找引号字符\n\n\n\n# 独占模式\n\n> 不管是贪婪模式，还是非贪婪模式，都需要发生回溯才能完成相应的功能。\n> \n> 独占模式，它类似贪婪匹配，但匹配过程不会发生回溯。\n\n * 贪婪模式（默认）\n\n\n\n * 非贪婪模式（量词？）\n\n\n\n * 独占模式（量词+）\n\n\n\n * 对比\n   \n   \n\n * 案例\n   \n   * we found “the little cat” is in the hat, we like “the little cat”\n   * 要求，提取出所有单词，引号中的单词（the little cat）看做一个\n   * \\w+|“.+?”——双引号内部 非贪婪模式\n   * \\w+|“[\\w\\s]+”——双引号内部 单词，空白符，一个或多个\n   * \\w+|“[^”]+”——双引号内部，不包含引号的一个或多个字符\n\n\n# 分组与引用\n\n * 案例1\n   \n   * \\d{18}|\\d{15}\n     \n     * 能识别18位或者15位数字\n     * 但是当每行数字超过18位后，只能识别18位的\n   \n   * \\d{15}|\\d{18}\n     \n     * 只能识别15位\n     * 因为18位中包含15位，且在大多数正则实现中，多分支选择都是左边的优先\n   \n   * \\d{15}(\\d{3})?\n     \n     * 前面表示15位，后面括号作为一个整体，加问号表示0个或者1个\n     * 由多个元字符组成某个部分，应该被看成一个整体的时候，可以用括号括起来表示一个整体，这是括号的一个重要功能\n     * 其实用括号括起来还有另外一个作用，那就是“复用”\n\n# 分组与编号\n\n * 子组\n   \n   * 保存子组(默认)\n     \n     * 用括号括起来的部分 子表达式 会被保存成一个子组\n     * 默认情况下，子组会被保存，并分配编号\n     \n     \n   \n   * 不保存子组\n     \n     * 你可能只想用括号将某些部分看成一个整体，后续不用再用它。这时我们可以使用(?:) 不保存子组。\n     \n     * 不保存子组可以理解为，括号只用于归组，把某个部分当成“单个元素”，不分配编号，后面不会再进行这部分的引用。\n     \n     * 不保存子组，性能会比较高。\n   \n   * 区别\n     \n     \n\n----------------------------------------\n\n * 括号嵌套查找编号\n   \n   * 对嵌套的括号查询-编号——数左边开括号的个数即可\n\n * 命名分组\n   \n   * 一些编程语言提供了命名分组（named grouping），这样和数字相比更容易辨识，不容易出错。命名分组的格式为(?p<分组名>正则)。\n\n# 分组引用\n\n * 各编程语言\n   \n   * \\num——\\1(1为分组编号)——\n   \n   * $1\n   \n   \n   \n   * 分组引用在查找和替换中使用\n     \n     * (\\w+) \\1——（中间有空格）\\w+表示一个单词，(\\w+)\\1表示引用分组的第一个\n     \n     * (\\w+)\\s\\1——空格尽量用 \\s\n       \n       \n     \n     * \\1表示第一个分组，即 ((\\d{4})-(\\d{2})-(\\d{2}))=2020-05-10\n     \n     * \\2表示第二个分组，即 (\\d{4})=2020\n     \n     * 在替换的时候，引用\\1,表示引用2020-05-10\n       \n       \n\n# 子组引用(*重要*)\n\n * \\d(\\d){2}——\\d(\\d)+——120\n   * 此时正则保存了一个子组，即\\1=0\n   * 这种()量词的情况 只保留最后一个作为子组\n * \\d(\\d)(\\d)——120\n   * 此时正则保存了两个子组，即\\1=2 && \\2=0\n * \\d(\\d{2})——\\d(\\d+)——120\n   * 此时正则保存了一个子组，即\\1=20\n   * 这种(量词)，量词在括号里面的情况，整个括号表示一个整体，即一个子组\n * (\\w+)\\s\\1——cat cat\n   * 第一个cat为子组1,后面的\\1表示引用之前的子组cat\n   * 这样即可表示 前后两个单词相同，即可表示 重复单词\n * ((\\w+)\\s\\2)——cat cat\n   * 第一个子组为 cat cat ,第二个子组为第一个 cat\n   * \\2 表示引用 第二个子组即第一个cat，这样也可以匹配上 重复单词\n * ([a-z]+)(\\d)\\s\\2——cat2 2\n   * 第一个子组为 cat， 因为+在括号里面\n   * 第二个子组是2， \\2表示引用第二个子组，即2,所有可以匹配上重复的字符2\n * (\\w+)(\\s\\1)+——cat cat或者cat cat cat或者cat cat cat cat\n   \n   * 第一个子组为第一个 cat\n   \n   * \\1表示引用第一个子组，即引用 cat\n   \n   * +表示一个或多个，再次引用第一个子组 cat，这样就可以表示 匹配到多个重复的单词\n   \n   * 示例\n     \n     * 查找\n       \n       * (\\w+)(\\s\\1)+\n     \n     * 替换\n       \n       * \\1\n\n\n# 匹配模式\n\n模式--mode\n\n * 模式修饰符——(?模式标识)\n   \n   * 不区分大小写——(?i)——insensitive首字母的小写\n   * 点号通配模式——(?s)——single\n   * 多行匹配模式——(?m)——multiline\n   * 注释模式——(?#)\n   \n   \n\n# 不区分大小写模式（case-insensitive）\n\n * 找出所有cat，不区分大小写\n   \n   * cat=cat=cat=cat=cat=cat=cat=cat\n   * [cc][aa][tt]——笨方法\n   * (?i)cat\n\n * 匹配两个连续出现的cat，不区分大小写\n   \n   * (?i)(cat)\\s\\1\n   \n   \n\n * 不区分大小写，匹配两个连续出现的cat，且第一次和第二次大小写一致\n   \n   * ((?i)cat) \\1\n   * 用一个大括号包裹起来，表示不区分大小写只作用于这个括号里的内容\n   \n   \n\n * 部分区分大小写——the cat 中 the不区分，cat区分\n   \n   * ((?i)the) cat\n   * 让模式修饰符(?i)只作用于the,而不作用于cat\n   * 如果不加括号，默认是两个都会被作用（即作用于整个正则），加了括号，相当于加了限定条件，只作用于the\n\n * 总结\n   \n   * 不区分大小写模式的指定方式，使用模式修饰符 (?i)；\n   * 修饰符如果在括号内，作用范围是这个括号内的正则，而不是整个正则；\n   * 使用编程语言时可以使用预定义好的常量来指定匹配模式。\n\n# 点号通配模式（single）\n\n也叫单行匹配模式，但是和多行匹配模式没有关系\n\n * .——可以匹配除了换行之外的任何字符\n * (?s).——可以匹配包括换行在内的任何字符\n\n# 多行匹配模式（multiline）\n\n * 默认\n   \n   * ^——匹配整个字符串的开头\n   \n   * $——匹配整个字符串的结尾\n   \n   \n\n * (?m)——多行匹配模式\n   \n   * ^——匹配每行的开头\n   * $——匹配每行的结尾\n   \n   \n\n# 注释模式（comment）\n\n * 正则表达式很复杂，可以在正则表达式内部加入注释\n * (?#)\n * (\\w+)(?#word)\\s\\1(?#word repeat again)\n\n# 示例\n\n * html 标签是不区分大小写的，比如我们要提取网页中的 head 标签中的内容，用正则如何实现呢？\n\n * >  <meta charset=\"utf-8\">\n\n * (?si)<head.*?>.*<\\/head>\n\n\n# 转义\n\n\n\n * 常用转义字符\n   \n   \n\n# 字符串转义和正则转义\n\n * 正则中：\n   \n   * \\d——表示单个数字\n   \n   * \\\\d——表示\\d——\\\\表示真正的反斜杠\n   \n   * \\\\|d——表示\\或者d\n\n * 程序中：\n   \n   * \\\\\\\\——\\\n     \n     * 四个反斜杠表示一个反斜杠\n     \n     * 因为先经过字符串转义\\\\\\\\——\\\\,真正被读入正则的只有\\\\\n     \n     * 然后，正则转义\\\\——\\\n       \n       \n   \n   * python中，可以使用原生字符串的方式来避免出现上面的情况\n     \n     \n     >>> import re\n     >>> re.findall('\\\\\\\\', 'a*b+c?\\\\d123d\\\\')\n     ['\\\\', '\\\\']// 第一个\\表示转义字符，第二个反斜杠表示真正的反斜杠\n     \n     \n     >>> import re\n     >>> re.findall(r'\\\\', 'a*b+c?\\\\d123d\\\\') //r表示读入原生字符\n     ['\\\\', '\\\\']\n     \n     \n     \n\n# 元字符的转义\n\n * 一般转义(直接在前面加\\即可)\n   \n   * \\*——*\n   * \\+——+\n   * \\?——?\n   * \\-——-``\n   * \\^——^\n   * \\$——$\n   * \\|——|\n\n * 括号的转义\n   \n   * 方括号 [] 和 花括号 {} 只需转义开括号\n   * 圆括号 () 两个都要转义\n   \n   >>> import re\n   >>> re.findall('\\(\\)\\[]\\{}', '()[]{}')\n   ['()[]{}']\n   >>> re.findall('\\(\\)\\[\\]\\{\\}', '()[]{}')  # 方括号和花括号都转义也可以\n   ['()[]{}']\n   \n\n * 转义函数\n   \n   * 使用转义函数可以将整个文本转义，将整个文本看做是一个正常的字符串，将其中的特殊字符加上转义。\n   \n   >>> import re\n   >>> re.escape('\\d')  # 反斜杠和字母d转义\n   '\\\\\\\\d'\n   >>> re.findall(re.escape('\\d'), '\\d')\n   ['\\\\d']\n   \n   >>> re.escape('[+]')  # 中括号和加号\n   '\\\\[\\\\+\\\\]'\n   >>> re.findall(re.escape('[+]'), '[+]')\n   ['[+]']\n   \n   \n   * 其他编程语言的函数\n   \n   \n\n# 字符组中的转义\n\n在字符组里只有三种情况需要转义\n\n * 脱字符^在中括号中，且在第一个位置需要转义\n   * 转义前代表非\n   * 转义后代表普通字符\n\n>>> import re\n>>> re.findall(r'[^ab]', '^ab')  # 转义前代表\"非\"\n['^']\n>>> re.findall(r'[\\^ab]', '^ab')  # 转义后代表普通字符\n['^', 'a', 'b']\n\n\n * 中划线-在中括号中，且不在首尾位置\n   * 在开头或者结尾不需要转义\n   * 在中间\n     * 转义前表示范围\n     * 转义后表示普通字符\n\n>>> import re\n>>> re.findall(r'[a-c]', 'abc-')  # 中划线在中间，代表\"范围\"\n['a', 'b', 'c']\n>>> re.findall(r'[a\\-c]', 'abc-')  # 中划线在中间，转义后的\n['a', 'c', '-']\n>>> re.findall(r'[-ac]', 'abc-')  # 在开头，不需要转义\n['a', 'c', '-']\n>>> re.findall(r'[ac-]', 'abc-')  # 在结尾，不需要转义\n['a', 'c', '-']\n\n\n\n * 右括号]在中括号中，且不在首位\n   * 在首位不需要转义，表示普通字符\n   * 不在首位，需要转义，不然[]会被认为提前结束\n\n>>> import re\n>>> re.findall(r'[]ab]', ']ab')  # 右括号不转义，在首位\n[']', 'a', 'b']\n>>> re.findall(r'[a]b]', ']ab')  # 右括号不转义，不在首位\n[]  # 匹配不上，因为含义是 a后面跟上b]\n>>> re.findall(r'[a\\]b]', ']ab')  # 转义后代表普通字符\n[']', 'a', 'b']\n\n\n其他字符，无需转义\n\n * []内部，+,*,?,.,()等不需要转义，就代表原来的字符.\n * 但如果在中括号中出现\\d或\\w 等符号时，他们还是元字符本身的含义。\n   * \\d=数字——\\w=字符\n\n>>> import re\n>>> re.findall(r'[.*+?()]', '[.*+?()]')  # 单个长度的元字符 \n['.', '*', '+', '?', '(', ')']\n>>> re.findall(r'[\\d]', 'd12\\\\')  # \\w，\\d等在中括号中还是元字符的功能\n['1', '2']  # 匹配上了数字，而不是反斜杠\\和字母d\n\n\n# 示例\n\n * \\\\n\\n\\\\——换行符\\n,用△表示\n * 即\\\\ n \\n \\\\——>字符串转义\\n△\\\n * 输入字符串——字符串转义——正则转义\n   * \\n——>△——>△=换行符\n   * \\\\n——>\\n——>△=换行符\n   * \\\\\\n——>\\△——>△=换行符\n   * \\\\\\\\n——>\\\\n——>\\n=反斜杠+n\n\n\n>>> import re\n>>> re.findall('\\n', '\\\\n\\n\\\\')\n['\\n']  # 找到了换行符\n\n\n>>> re.findall('\\\\n', '\\\\n\\n\\\\')  //输入`\\n`,匹配到`\\n`\n['\\n']  # 找到了换行符\n\n>>> re.findall('\\\\\\n', '\\\\n\\n\\\\') //输入`\\△`，\n['\\n']  # 找到了换行符\n\n>>> re.findall('\\\\\\\\n', '\\\\n\\n\\\\')\n['\\\\n'] # 找到了反斜杠和字母n\n\n\n\n# 正则流派\n\n\n\n# posix 流派\n\n * bre\n   * gnu bre 只有一个 e，使用时“花圆问管加”({}()?|+)时都要转义\n   * 早期标准，bre不支持 ? + |\n   * gnu bre 支持，但是需要加转义字符，即,\\? \\+ \\|\n * ere\n   * gnu ere 名称中有两个 e，不需要再转义\n\n\n\n# pcre 流派\n\n * 来源于 prel分支\n   * 这个流派显著特征是有\\d、\\w、\\s 这类字符fin组简记方式\n * 现在大部分编程语言支持的都是这个流派\n\n# linux中使用正则\n\n * 按照 bre 标准 实现的有 grep、sed 和 vi/vim 等\n * 按照 ere 标准 实现的有 egrep、awk 等\n\n\n\n * 可以使用 man grep来查看支持那些标准\n   \n   \n\n * grep默认是 bre流派\n\n * egrep=grep -e,是ere流派\n\n * grep -p是pcre 流派\n   \n   \n\n\n# 正则应用\n\n# 正则处理unicode编码\n\n\n\n * \\w——不能匹配汉字\n\n * 量词正常使用\n\n * '极客{3}'\n   \n   * 匹配极客客客\n   * 表示的是客这个汉字重复3次，而不是客这个汉字对应的编码最后一个字节重复3次\n   * 如果重复是最后一个字节，应该极(?:客){3}——分组的形式\n\n# 在编辑器中使用正则（sublime text）\n\n * sublime 的一些快捷键\n   * shift+alt+1-9(非小键盘)\n     * 使屏幕显示相等数字的小窗口\n   * ctrl+h——替换\n   * ctrl+l——选择整行\n\n\n\n\n\n * 光标移动和文本选择\n   \n   * 按住shift键选中，然后按左右键可以左右选择文本块\n   * 按住shift+alt，光标可以按块移动，快速移动到下一个单词。\n\n * 多焦点编辑\n   \n   * 查找——>快速查找全部\n   \n   * 例子：提取json中的姓名和手机号\n     \n     {\n       \"error_code\": 0,\n       \"result\": {\n         \"data\": [\n           {\n             \"name\": \"朱小明\",\n             \"tel\": \"138xx138000\"\n           },\n           {\n             \"name\": \"王五\",\n             \"tel\": \"139xx139000\"\n           }\n         ]\n       }\n     }\n     \n     朱小明\n     138xx138000\n     王五\n     139xx139000\n     \n     \n     \n     * 选中\": \",快速查找全部\n     * 按右方向键，将光标移动到引号右边\n     * 按住shift+alt，快速选择整个引号内的内容\n     * 复制，粘贴即可\n\n * 竖向编辑\n   \n   * shift+鼠标右键,可同时操作处于同一列的文本，同时编辑\n   * ctrl+鼠标左键，可以选中多处文本，同时编辑。\n\n * 内容提取\n   \n   * \\s+@\\s+\\.\\s+(?=;)——简单邮箱提取\n     \n     * \\s——任意非空白字符\n     * (?=;)——环视;\n   \n   * 小李： jkmkqhvrc@265.com;\n     小王： atvl@sogou@.com;\n     小红： vtoispm@tom.com;\n     小真： olncckkerlikb@citiz.com;\n     小爱： mddbatlosa@msn.com;\n     \n     jkmkqhvrc@265.com\n     atvl@sogou@.com\n     vtoispm@tom.com\n     olncckkerlikb@citiz.com\n     mddbatlosa@msn.com\n     \n\n * 内容替换\n   \n   * (\\s+@(\\s+)\\.\\s+)==查找\n     \n     * 两个括号，两个分组，均可以用来引用\n       * \\1=jkmkqhvrc@265.com\n       * \\2=265\n   \n   * \\2邮箱 ===\\1==替换\n   \n   * jkmkqhvrc@265.com\n     atvl@sogou@.com\n     vtoispm@tom.com\n     olncckkerlikb@citiz.com\n     mddbatlosa@msn.com\n     \n     265邮箱 ===jkmkqhvrc@265.com\n     sogou@邮箱 ===atvl@sogou@.com\n     tom邮箱 ===vtoispm@tom.com\n     citiz邮箱 ===olncckkerlikb@citiz.com\n     msn邮箱 ===mddbatlosa@msn.com\n     \n\n * 统计一篇英文文章中每个单词出现的次数\n   \n   * 处理成一行一个单词\n     \n     * \\w——\\n\n     \n     * 将非字符转换成空格\n     \n     * (?<!\\w)\\s——``\n       \n       * 将左边不是字符的空格去掉\n       \n       i have a cat, cat is a dog, dog is i cat.\n       \n       \n       i\n       have\n       a\n       cat\n       \n       cat\n       is\n       a\n       dog\n       \n       dog\n       is\n       i\n       cat\n       \n       \n       i\n       have\n       a\n       cat\n       cat\n       is\n       a\n       dog\n       dog\n       is\n       i\n       cat\n       \n   \n   * 使用sort 命令排序，uniq -c统计次数\n     \n     uniq -c：   统计每行出现次数\n     sort :\n     \t-n  按数字排序\n     \t-r\t逆序排序\n     \t-k1\t根据-t的分割，分成几域，取第1个域排序\n     \t-t  指定分隔符，默认的分隔符为空白字符和非空白字符之间的空字符\n     head -n10： 取前10行数据\n     \n     \n     $ sort word.txt | uniq -c\n           2 i\n           2 a\n           3 cat\n           2 dog\n           1 have\n           2 is\n     \n     $ sort word.txt | uniq -c | sort -nr\n           3 cat\n           2 is\n           2 dog\n           2 a\n           2 i\n           1 have\n     \n     \n\n# 在语言中用正则\n\n\n\n * 校验\n   \n   验证日期 2022-06-01\n   \n   * python\n     \n     # 测试环境 python3\n     >>> import re\n     >>> re.match(r'\\d{4}-\\d{2}-\\d{2}', '2020-06-01')\n     <re.match object; span=(0, 10), match='2020-06-01'>\n     # 这个输出是匹配到了，范围是从下标0到下标10，匹配结果是2020-06-01\n     # re.search 输出结果也是类似的\n     \n     \n     * \\a \\z表示文本的开头和结尾\n     * 不建议使用^和$,因为在多行模式下，可以表示每行的开头和结尾\n     \n     # 测试环境 python3\n     >>> import re\n     >>> reg = re.compile(r'\\a\\d{4}-\\d{2}-\\d{2}\\z')  # 建议先编译，提高效率\n     >>> reg.search('2020-06-01') is not none\n     true\n     >>> reg.match('2020-06-01') is not none  # 使用match时\\a可省略\n     true\n     \n     \n   \n   * java\n     \n     * \\a \\z表示文本的开头和结尾\n     \n     \n     import java.util.regex.matcher;\n     import java.util.regex.pattern;\n     \n     class main {\n       public static void main(string[] args) {\n         //方法1，可以不加 \\a 和 \\z\n         system.out.println(pattern.matches(\"\\\\d{4}-\\\\d{2}-\\\\d{2}\", \"2020-06-01\")); // true\n     \n         //方法2，可以不加 \\a 和 \\z\n         system.out.println(\"2020-06-01\".matches(\"\\\\d{4}-\\\\d{2}-\\\\d{2}\")); // true\n         \n         //方法3，必须加上 \\a 和 \\z\n         pattern pattern = pattern.compile(\"\\\\a\\\\d{4}-\\\\d{2}-\\\\d{2}\\\\z\");\n         system.out.println(pattern.matcher(\"2020-06-01\").find()); // true\n       }\n     }\n     \n\n * 提取\n   \n   日志时间提取\n   \n   * python\n   \n   # 没有子组时\n   >>> import re\n   >>> reg = re.compile(r'\\d{4}-\\d{2}')\n   >>> reg.findall('2020-05 2020-06')\n   ['2020-05', '2020-06']\n   \n   # 有子组时\n   >>> reg = re.compile(r'(\\d{4})-(\\d{2})')\n   >>> reg.findall('2020-05 2020-06')\n   [('2020', '05'), ('2020', '06')]\n   \n   \n   * 节约内存，使用迭代器\n   \n   \n   >>> import re\n   >>> reg = re.compile(r'(\\d{4})-(\\d{2})')\n   >>> for match in reg.finditer('2020-05 2020-06'):\n   ...     print('date: ', match[0])  # 整个正则匹配到的内容\n   ...     print('year: ', match[1])  # 第一个子组\n   ...     print('month:', match[2])  # 第二个子组\n   ...\n   date:  2020-05\n   year:  2020\n   month: 05\n   date:  2020-06\n   year:  2020\n   month: 06\n   \n   \n   * java\n     \n     \n     import java.util.regex.matcher;\n     import java.util.regex.pattern;\n     \n     class main {\n       public static void main(string[] args) {    \n         pattern pattern = pattern.compile(\"\\\\d{4}-\\\\d{2}\");\n         matcher match = pattern.matcher(\"2020-06 2020-07\");\n         while (match.find()) {\n           system.out.println(match.group());      \n         }\n       }\n     }\n     \n\n * 替换\n   \n   (02-20-2022)月日年——年月日(2022年02月20日)\n   \n   * python\n   \n   >>> import re\n   >>> reg = re.compile(r'(\\d{2})-(\\d{2})-(\\d{4})')\n   \n   >>> reg.sub(r'\\3年\\1月\\2日', '02-20-2020 05-21-2020')\n   '2020年02月20日 2020年05月21日'\n   \n   # 可以在替换中使用 \\g<数字>，如果分组多于10个时避免歧义\n   >>> reg.sub(r'\\g<3>年\\g<1>月\\g<2>日', '02-20-2020 05-21-2020')\n   '2020年02月20日 2020年05月21日'\n   \n   # 返回替换次数\n   >>> reg.subn(r'\\3年\\1月\\2日', '02-20-2020 05-21-2020')\n   ('2020年02月20日 2020年05月21日', 2)\n   \n   \n   \n   * java\n     * 替换时 引用子组要用 $\n   \n   import java.util.regex.matcher;\n   import java.util.regex.pattern;\n   \n   class main {\n     public static void main(string[] args) {\n       //方法1，输出 2020年02月20日 2020年05月21日\n       system.out.println(\"02-20-2020 05-21-2020\".replaceall(\"(\\\\d{2})-(\\\\d{2})-(\\\\d{4})\", \"$3年$1月$2日\"));\n       \n       //方法2，输出 2020年02月20日 2020年05月21日\n       final pattern pattern = pattern.compile(\"(\\\\d{2})-(\\\\d{2})-(\\\\d{4})\");\n       matcher match = pattern.matcher(\"02-20-2020 05-21-2020\");\n       system.out.println(match.replaceall(\"$3年$1月$2日\"));\n     }\n   }\n   \n\n * 切割\n   \n   切割得到单词\n   \n   * python\n   \n   >>> import re\n   >>> reg = re.compile(r'\\w+')\n   >>> reg.split(\"apple, pear! orange; tea\")\n   ['apple', 'pear', 'orange', 'tea']\n   \n   # 限制切割次数，比如切一刀，变成两部分\n   >>> reg.split(\"apple, pear! orange; tea\", 1)\n   ['apple', 'pear! orange; tea']\n   \n   \n   * java\n   \n   import java.util.regex.matcher;\n   import java.util.regex.pattern;\n   \n   class main {\n     public static void main(string[] args) {\n       pattern pattern = pattern.compile(\"\\\\w+\");\n       for(string s : pattern.split(\"apple, pear! orange; tea\")) {\n         system.out.println(s);\n       }\n     }\n   }\n   \n\n * 示例\n   \n   xxx#163.com (请把#换成@)\n   \n   * python\n   \n   // 替换——在用正则提取邮箱\n   >>> reg=re.compile(r'#')\n   >>> reg.sub(r'@','xxx#163.com')\n   'xxx@163.com'\n   \n   //提取\n   >>> reg=re.compile(r'\\w+[#@]\\w+.\\w+')\n   >>> reg.findall('联系邮箱：xxx#163.com (请把#换成@) 联系邮箱：xxx@163.com')\n   ['xxx#163.com', 'xxx@163.com']\n   \n   \n   \n   * java\n\nimport java.util.regex.matcher;\nimport java.util.regex.pattern;\n\n//替换\nclass main {\n  public static void main(string[] args) {\n   \t\tpattern compile = pattern.compile(\"#\");\n\t\tmatcher matcher = compile.matcher(\"xxx#163.com\");\n\t\tsystem.out.println(matcher.replaceall(\"@\"));\n    }\n}\n\n//提取\nclass main {\n  public static void main(string[] args){\n        //提取 带#或者带@的邮箱\n        pattern pattern=pattern.compile(\"\\\\w+[@#]\\\\w+.\\\\w+\");\n        matcher matcher = pattern.matcher(\"联系邮箱：xxx#163.com (请把#换成@) 联系邮箱：xxx@163.com\");\n        while (matcher.find())\n        {\n            system.out.println(matcher.group());\n        }\n\n    }\n}\n\n\n\n# 原理\n\n\n\n# 正则匹配原理\n\n * 有穷状态自动机\n   \n   * 正则之所以能够处理复杂文本，就是因为采用了有穷状态自动机\n   * 有穷自动机的具体实现称为正则引擎，主要有 dfa 和 nfa 两种\n     * dfa：确定性有穷自动机（deterministic finite automaton） nfa：非确定性有穷自动机（non-deterministic finite automaton）\n\n * 正则匹配过程\n   \n   * 编译**(compile)**的过程，其实就是生成自动机的过程\n   \n   * nfa工作机制\n     \n     nfa 引擎的工作方式是，先看正则，再看文本，而且以正则为主导\n     \n     > 字符串：we study on jikeshijian app 正则：jike(zhushou|shijian|shixi)\n     \n     * 正则中的第一个字符是 j，nfa 引擎在字符串中查找 j，接着匹配其后是否为 i ，如果是 i 则继续，这样一直找到 jike\n       \n       > regex: jike(zhushou|shijian|shixi) ^ text: we study on jikeshijian app ^\n     \n     * 我们再根据正则看文本后面是不是 z，发现不是，此时 zhushou 分支淘汰\n       \n       > regex: jike(zhushou|shijian|shixi) ^ 淘汰此分支(zhushou) text: we study on jikeshijian app ^\n     \n     * 我们接着看其它的分支，看文本部分是不是 s，直到 shijian 整个匹配上。\n     \n     * shijian 在匹配过程中如果不失败，就不会看后面的 shixi 分支。\n     \n     * 假设这里文本改一下，把 jikeshijian 变成 jikeshixi，正则 shijian 的 j 匹配不上时 shixi 的 x，会接着使用正则 shixi 来进行匹配，重新从 s 开始（nfa 引擎会记住这里）。\n       \n       > 第二个分支匹配失败 regex: jike(zhushou|shijian|shixi) ^ 淘汰此分支(正则j匹配不上文本x) text: we study on jikeshixi app ^\n       > \n       > 再次尝试第三个分支 regex: jike(zhushou|shijian|shixi) ^ text: we study on jikeshixi app ^\n   \n   * dfa工作机制\n     \n     dfa 会先看文本，再看正则表达式，是以文本为主导的\n     \n     * dfa 会从 we 中的 w 开始依次查找 j，定位到 j ，这个字符后面是 i。所以我们接着看正则部分是否有 i ，如果正则后面是个 i ，那就以同样的方式，匹配到后面的 ke\n       \n       > text: we study on jikeshijian app ^ regex: jike(zhushou|shijian|shixi) ^\n     \n     * 继续进行匹配，文本 e 后面是字符 s ，dfa 接着看正则表达式部分，此时 zhushou 分支被淘汰，开头是 s 的分支 shijian 和 shixi 符合要求。\n       \n       > text: we study on jikeshijian app ^ regex: jike(zhushou|shijian|shixi) ^ ^ ^ 淘汰 符合 符合\n     \n     * 然后 dfa 依次检查字符串，检测到 shijian 中的 j 时，只有 shijian 分支符合，淘汰 shixi，接着看分别文本后面的 ian，和正则比较，匹配成功。\n       \n       > text: we study on jikeshijian app ^ regex: jike(zhushou|shijian|shixi) ^ ^ 符合 淘汰\n   \n   * dfa与nfa\n     \n     * 一般来说，dfa 引擎会更快一些，因为整个匹配过程中，字符串只看一遍，不会发生回溯，相同的字符不会被测试两次。也就是说 dfa 引擎执行的时间一般是线性的。dfa 引擎可以确保匹配到可能的最长字符串。但由于 dfa 引擎只包含有限的状态，所以它没有反向引用功能；并且因为它不构造显示扩展，它也不支持捕获子组。\n     * nfa 以表达式为主导，它的引擎是使用贪心匹配回溯算法实现。nfa 通过构造特定扩展，支持子组和反向引用。但由于 nfa 引擎会发生回溯，即它会对字符串中的同一部分，进行很多次对比。因此，在最坏情况下，它的执行速度可能非常慢。\n   \n   * posix nfa\n     \n     * posix nfa 引擎与传统的 nfa 引擎类似，但不同之处在于，posix nfa 在找到可能的最长匹配之前会继续回溯，也就是说它会尽可能找最长的，如果分支一样长，以最左边的为准\n     * 比如使用正则 pos|posix 在文本 posix 中进行匹配，传统的 nfa 从文本中找到的是 pos，而不是 posix，而 posix nfa 找到的是 posix\n   \n   * 三者比较\n   \n   \n\n# 回溯\n\n回溯是 nfa 引擎才有的，并且只有在正则中出现量词或多选分支结构时，才可能会发生回溯。\n\n * +\n   \n   > 比如我们使用正则 a+ab 来匹配 文本 aab 的时候，过程是这样的，a+ 是贪婪匹配，会占用掉文本中的两个 a，但正则接着又是 a，文本部分只剩下 b，只能通过回溯，让 a+ 吐出一个 a，再次尝试\n\n * .*\n   \n   > 如果正则是使用 .*ab 去匹配一个比较长的字符串就更糟糕了，因为 .* 会吃掉整个字符串（不考虑换行，假设文本中没有换行），然后，你会发现正则中还有 ab 没匹配到内容，只能将 .* 匹配上的字符串吐出一个字符，再尝试，还不行，再吐出一个，不断尝试\n   \n   \n   \n   * 所以在工作中，我们要尽量不用 .*,可以用其他方式来替换\n     * 比如要提取引号中的内容时\n     * \".+?\"——非贪婪模式\n     * \"[^\"]+\"——双引号内部，非双引号的字符\n\n# 优化原则\n\n * 测试性能的方法\n   \n   * 使用ipyhon来测试正则表达式\n     \n     * win+r——输入 ipython\n     * ipython使用技巧\n     \n     in [1]: import re\n     in [2]: x = '-' * 1000000 + 'abc'\n     in [3]: timeit re.search('abc', x)\n     480 µs ± 8.06 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n     \n   \n   * 可以通过前面 regex101.com 查看正则和文本匹配的次数\n\n * 提前编译好正则\n   \n   * compile\n\n * 尽量准确表示匹配范围\n   \n   * 我们可以写成“[^\"]+”。使用[^\"]要比使用点号好很多\n\n * 提取出公共部分\n   \n   * 因为 nfa 以正则为主导，会导致字符串中的某些部分重复匹配多次，影响效率。\n   * (abcd|abxy)这样的表达式，可以优化成ab(cd|xy)\n   * (^this|^that) is——^th(is|at) is——锚点部分独立处理\n   * this|that——th(?:is|at)\n\n * 出现可能性大的放左边\n   \n   * 由于正则是从左到右看的，所有在分支中，把可能性大的放到左边\n\n * 在必要时才使用子组\n   \n   * 在正则中，括号可以用于归组，但如果某部分后续不会再用到，就不需要保存成子组。\n   * (?:)——不保存子组\n\n * 警惕嵌套的子组重复\n   \n   * (.*)*这个正则，匹配的次数会呈指数级增长\n\n * 避免不同分支重复匹配\n\n# 示例\n\n针对这个示例，nfa 引擎的匹配过程\n\n> 文本：a12----开始部分用△表示，结束部分用○表示\n> \n> 即：△a12○\n> \n> 正则：^(?=[a-z])[a-z0-9]+$\n\n> ^——△\n> \n> (?=[a-z])——a\n> \n> [a-z0-9]——△——a12\n> \n> $——○\n\n\n# 正则解决常见问题\n\n\n\n# 正则处理问题的基本思路\n\n 1. 比如将问题分解成多个小问题，每个小问题见招拆招。\n\n 2. 某个位置上可能有多个字符的话，就⽤字符组。\n\n 3. 某个位置上有多个字符串的话，就⽤多选结构。\n\n 4. 出现的次数不确定的话，就⽤量词。\n\n 5. 对出现的位置有要求的话，就⽤锚点锁定位置。\n\n 6. 在正则中比较难的是某些字符不能出现，这个情况又可以进一步分为以下两种\n    \n    * 组成中不能出现\n      * 要查找的内容中不能出现某些字符——[^aeiou]——不能出现元音\n      * 例如 要求密码6位，但是不能有两个连续数字出现\n        * ^((?!\\d\\d)\\w){6}$\n        * (?!\\d\\d)——代表右边不能是两个数字，但是左边没有正则，即为空字符串\n    * 要查找的内容前后不能出现——环视\n\n# 常见问题\n\n * 匹配数字\n   \n   * 数字—— \\d 或[0-9]\n   * 连续的多个数字—— \\d+ 或[0-9]+\n   * n 位数字—— \\d{n}。\n   * 至少 n 位数据——\\d{n,}。\n   * m-n 位数字—— \\d{m,n}。\n\n * 匹配正数、负数、小数（浮点数）\n   \n   * 3，3.14，-3，+2.7，20.12\n     * [+-]?\\d+[.]?\\d*\n     * [+-]?\\d+(?:\\.\\d+)?\n       * ——(?:)表示不保留子组——\\.表示转义.\n   * .5，+.5——考虑这种极端情况（负号的时候整数部分不能没有，而正数的时候整数部分可以没有）\n     * 正数：+?(?:\\d+(?:\\.\\d+)?|\\.\\d+)\n       * \\d+(?:\\.\\d+)?和\\.\\d+两个分支\n     * 负数：-\\d+(?:\\.\\d+)?\n     * 组合：-\\d+(?:\\.\\d+)?|\\+?(?:\\d+(?:\\.\\d+)?|\\.\\d+)——正数和负数两个分支\n\n * 十六进制数\n   \n   * [0-9a-fa-f]+\n\n * 手机号码\n   \n   \n   \n   * 限制前两位\n     * 1[3-9]\\d{9}\n   * 限制前三位\n     * 13:13[0-9]\\d{8}\n     * 14:14[5-9]\\d{8}\n     * 15:15[0-35-9]\\d{8}\n     * 16:16[2567]\\d{8}\n     * 17:17[0-8]\\d{8}\n     * 18:18[0-9]\\d{8}\n     * 19:19[1389]\\d{8}\n     * 组合：1(?:3[0-9]|4[5-9]|5[0-35-9]|6[2567]|7[0-8]|8[0-9]|9[1389])\\d{8}\n\n * 身份证号码\n   \n   * 第一代是 15 位，第二代是 18 位;如果是 18 位，最后一位可以是 x（或 x），两代开头都不能是 0\n   * 15:[1-9]\\d{14}\n   * 18:[1-9]\\d{16}[\\dxx]\n   * 组合：[1-9]\\d{14}(\\d{2}[\\dxx])?\n\n * 邮政编码\n   \n   * 邮编一般为 6 位数字,需要加断言\n   * (?<!\\d)\\d{6}(?!\\d)——左边不是数字，右边不是数字\n\n * qq号码\n   \n   * qq 号不能以 0 开头，最长的有 10 位，最短的从 10000（5 位）开始\n   * [1-9]\\d{4,9}?——\\d{4,9},表示数字出现4次到9次\n\n * 中文字符\n   \n   * 中文属于多字节 unicode 字符，通过 unicode 属性，但有一些语言是不支持这种属性的\n   * 另外一个办法，就是码值的范围，中文的范围是 4e00 - 9fff 之间\n   * python，java，javascript 中，unicode 可以写成 \\u码值 来表示，即匹配中文的正则可以写成 [\\u4e00-\\u9fff]，在 php 中使用，unicode 就需要写成 \\u{码值} 的样式。\n   \n   # 测试环境，python3\n   >>> import re\n   >>> reg = re.compile(r'[\\u4e00-\\u9fff]')\n   >>> reg.findall(\"和伟忠一起学正则regex\")\n   ['和', '伟', '忠', '一', '起', '学', '正', '则']\n   \n\n * ipv4地址\n   \n   * ipv4 地址通常表示成 27.86.1.226 的样式，4 个数字用点隔开，每一位范围是 0-255\n   * 简单版：\\d{1,3}(?:\\.\\d{1,3}){3}\n   * 按位数考虑\n     * 1位——1.--01.--001.\n       * 0{0,2}\\d\n     * 2位——10.--010.\n       * 0?\\d\\d\n     * 3位——100.--255.\n       * 1\\d\\d|2[0-5][0-5]\n     * 组合——0{0,2}\\d|0?\\d\\d|1\\d\\d|2[0-5][0-5]=x\n     * 组合改进——长的分支放到左边——1\\d\\d|2[0-5][0-5]|0?\\d\\d|0{0,2}\\d=x\n   * 加上点——x.x.x.x\n     * (?:x)(?:\\.(?:x)){3}\n     * (?:1\\d\\d|2[0-5][0-5]|0?\\d\\d|0{0,2}\\d)(?:\\.(?:1\\d\\d|2[0-5][0-5]|0?\\d\\d|0{0,2}\\d)){3}\n\n * 日期\n   \n   * 日期格式 yyyy-mm-dd\n   * 简单：\\d{4}-\\d{2}-\\d{2}\n   * 复杂：考虑月份 1-12，一位时0可带可不带；日期 1-31（像 30，29,28这种应该交给程序来完成，不要用正则完成所有的事情）\n     * 年：\\d{4}\n     * 月：1[0-2]|0?[1-9]——两位的写在前面\n     * 日：[12]\\d|3[01]|0?[1-9]\n     * 组合：\\d{4}-(?:1[0-2]|0?[1-9])-(?:[12]\\d|3[01]|0?[1-9])\n\n * 时间\n   \n   * 24小时制——23:24\n   * 小时：0-23；分钟：0-59\n   * (?:2[0-3]|1[0-9]|0?[0-9]):(?:[1-5][0-9|0?[0-9]])\n   * 12小时制——11:59\n   * 小时：0-11；分钟：0-59\n   * (?:1[01]|0?[0-9]):(?:[1-5][0-9|0?[0-9]])\n\n * 邮箱\n   \n   * 格式是 用户名 @主机名，用户名部分通常可以有英文字母，数字，下划线，点等组成，但其中点不能在开头，也不能重复出现\n   * 简单版：[\\w.]+@[\\w]+\\.[\\w]+\n\n * 网页标签\n   \n   * 配对出现的标签，不区分大小写,title\n     * (?si)<title.*?>.*<\\/title>\n   * 提取引号里面的内容\n     * \".+\"\n     * \"[^\"]+\"——带\"\"\n     * (?<=\")[^\"]+(?=\")——不带\"\"\n   * 提取尖括号<>里面的内容\n     * <[^>]+>——带<>\n     * (?<=<)[^>]+(?=>)——不带<>\n\n\n# 从编程语言的角度来理解正则表达式\n\n\n\n> 使用第 4 代语言来描述问题，而无需花费大量时间，去考虑具体的处理逻辑和算法实现，处理逻辑和算法实现是由编译器（compiler）或解释器（interpreter）这样的语言解析引擎来负责。\n\n * 正则表达式属于声明式编程范式\n\n> 声明式编程范式，主要是模拟人脑思维的过程。声明式重目标、轻过程，专注问题的分析和表达，而不是算法实现。它不用指明执行顺序，属于目标导向，强调的是定义问题的描述——即“做什么”，因而目标是显性而算法是隐性的。\n\n * * 因此，从编程范式的角度来看：\n   * 声明式编程的世界观是：程序是由若干目标任务组成的有序列表；\n   * 声明式编程的方法论是：用语法元素来描述任务，由解析引擎转化为指令并执行。\n\n * 正则表达式的语法元素本质上就是程序逻辑和算法\n   \n   * 正则表达式中的星号量词“*”这一元字符，就是高级语言的处理逻辑“循环结构”的体现。具体来说，星号量词“*”代表的是不定次数循环结构，而前后多个星号量词的嵌套就是多层不定次数循环结构的嵌套；\n   \n   * 或运算符，也就是竖线“|”这个元字符，就是高级语言的处理逻辑“分支结构”的体现；\n   \n   * 而用于分组的圆括号“()”，就相当于高级语言的作用域。\n   \n   * 而当或运算符“|”出现在由星号量词“*”所限定的分组圆括号“()”中时，其实就是在“循环结构”中嵌套了“分支结构”；\n   \n   * 而如果进一步地，“循环结构”所嵌套的“分支结构”中的某个分支，又被某个星号量词“*”所限定，那么则相当于“循环结构”所嵌套的“分支结构”又嵌套了“循环结构”。\n   \n   * (张三|李四 *)*\n     \n     \n\n",charsets:{cjk:!0}},{title:"Linux",frontmatter:{autoSort:100,title:"Linux",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/373d5a/",categories:["技术","Linux"],tags:["知识","技术","Linux"]},regularPath:"/03.%E6%8A%80%E6%9C%AF/02.Linux/05.Linux%E5%9F%BA%E7%A1%80.html",relativePath:"03.技术/02.Linux/05.Linux基础.md",key:"v-1db4fa63",path:"/pages/373d5a/",headers:[{level:2,title:"快捷键",slug:"快捷键",normalizedTitle:"快捷键",charIndex:14},{level:2,title:"常用命令",slug:"常用命令",normalizedTitle:"常用命令",charIndex:263},{level:2,title:"高级指令",slug:"高级指令",normalizedTitle:"高级指令",charIndex:10058},{level:2,title:"指令应用",slug:"指令应用",normalizedTitle:"指令应用",charIndex:13496},{level:2,title:"Linux自有服务",slug:"linux自有服务",normalizedTitle:"linux自有服务",charIndex:14134},{level:2,title:"任务调度",slug:"任务调度",normalizedTitle:"任务调度",charIndex:19865},{level:2,title:"Linux分区",slug:"linux分区",normalizedTitle:"linux分区",charIndex:21549},{level:2,title:"网络配置",slug:"网络配置",normalizedTitle:"网络配置",charIndex:22884},{level:2,title:"Linux软件安装",slug:"linux软件安装",normalizedTitle:"linux软件安装",charIndex:24257},{level:2,title:"Linux项目部署",slug:"linux项目部署",normalizedTitle:"linux项目部署",charIndex:27032}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"快捷键 常用命令 高级指令 指令应用 Linux自有服务 任务调度 Linux分区 网络配置 Linux软件安装 Linux项目部署",content:'# Linux基础\n\n\n# 快捷键\n\n * Ctrl+C——停止命令\n * Ctrl+u——快速删除光标前的内容\n * Ctrl+k——快速删除光标后的内容\n * Ctrl+shift+c/Ctrl+Insert——复制\n * Ctrl+shift+v/Shift+Insert——粘贴\n * 选中+鼠标中键——自动填充到命令行\n * Ctrl+Alt+T——打开终端（乌班图）\n * Tab键自动补全\n * 连续两次Tab,给出操作提示\n * 上下箭头调用层用命令\n * 使用clear命令或者Ctrl+l实现清屏\n\n\n# 常用命令\n\n# 命令格式\n\n\n\n# 文件目录操作命令\n\n * pwd ——查看当前目录位置\n\n * tree——查看树形目录结构\n\n * head [-n] fileName——查看文件前n行内容，默认n=10\n\n * wc -l fileName——统计文件行数（-l 行数 ，-w 单词数，-c字节数）\n\n * free -h——查询内存\n\n * df -h——查看磁盘空间\n\n * echo—— 输出环境变量；输出字符串\n   \n   * echo $PATH\n   \n   * echo hello world!\n\n * ls——查看当前目录下所有文件\n   \n   * 命令说明 ls [-al] [dir]\n     \n     * -a ：显示所有文件及目录（.开头的隐藏文件也会列出）\n     * -l：处文件名称外，同时将文件形态（d表示目录，-表示文件）、权限、拥有者、文件大小等详细信息列出\n   \n   * 实例\n     \n     * ls -a——查看所有文件，包含隐藏文件，不显示详细信息\n     * ls -l——查看非隐藏文件的详细信息\n     * ll——ls -l的简写\n     * ls -al——查看所有文件的详细信息\n\n * cd 目录名——进入某个文件夹下\n   \n   * cd \\——进入根目录\n   * cd ..——表示目前目录位置的上级目录\n   * cd .——表示目前所在目录\n   * cd ~——表示用户的home目录\n   * cd diana——进入当前目录下的diana文件目录中\n   * cd /diana——进入根目录下的diana文件目录中，开头的第一个/表示根目录\n\n * cat [-n] fileName——显示文件内容（全部）\n   \n   * 说明\n     * -n 由1开始对所有输出的行数的编号\n   * 实例\n     * cat /etc/profile——查看/etc目录下的profile文件内容\n\n * cat fileName1 fileName2 fileName3 > fileName4——将前三个文件的内容合并到第四个文件中，并打开\n   \n   * 实例\n     * cat 1.txt 2.txt >3.txt——将前1.txt，2.txt文件的内容合并到3.txt文件中，并打开\n\n * more fileName——以分页的形式显示文件内容\n   \n   一次性将文件全部加载，效率比较低\n   \n   * 说明\n     * 回车键， 向下滚动一行\n     * 空格键，ctrl+F 向下滚动一屏\n     * b，ctrl+B 返回上一屏\n     * q，ctrl+c，退出more\n     * = 输出当前行号\n     * :f 输出文件名和当前行的行号\n   * 实例\n     * more /etc/profile——以分页方式展示/etc目录下的profile文件内容\n\n * less fileName——以分页的形式显示文件内容\n   \n   显示一部分记载一部分，速度更快,适合大文件\n   \n   * 说明\n     * 方向上键——向上一行\n     * 回车键，方向下键 ——向下滚动一行\n     * 空格键，pg dn——向下滚动一屏\n     * pg up ——向上滚动一屏\n     * q，ctrl+c，退出more\n     * /字符串 ——向下搜寻字符串的功能， n:向下查找 ；N:向上查找\n     * ?字符串 ——向下搜寻字符串的功能， n:向下查找 ；N:向上查找\n   * 实例\n     * less /etc/profile——以分页方式展示/etc目录下的profile文件内容\n\n * tail [-f] fileName——查看文件末尾的内容（默认后10行）（自己手动的不可以显示）（一般用来查看日志）\n   \n   * 说明\n     * -f 动态读取文件末尾内容并显示，通常用于日志文件的内容输出\n   * 实例\n     * tail /etc/profile——显示/etc目录下profile文件末尾10行的内容\n     * tail -20/etc/profile——显示 /etc目录下的profile文件末尾20行的内容\n     * tail -f/diana/my.log——动态读取 /diana目录下的my.log文件末尾内容并显示\n\n * mkdir [-p] dirName——创建目录\n   \n   * 说明\n     \n     * -p 确保目录名称存在，不存在的就创建一个。通过此选项，可是实现多层目录同时创建\n   \n   * 实例\n     \n     * mkdir java——在当前目录下，创建名为java的子目录\n     \n     * mkdir -p diana/java——在工作目录diana中创建一个java子目录，如果diana目录不存在，则创建。即可以一次性创建多层文件目录\n\n * rmdir [-p] dirName——删除空目录\n   \n   * 说明\n     * -p 当子目录被删除后，使父目录为空的话，一并删除\n   * 实例\n     * rmdir diana——删除名为diana的空目录\n     * rmdir -p diana/test——删除diana目录下的test目录，若删除后，diana为空目录，则删除diana\n     * rmdir diana*——删除名称以diana开头的空目录\n\n * rm[-rf] name——删除文件或目录\n   \n   * 说明\n     \n     * -r 将目录及目录中所有文件（目录）逐一删除，即递归删除\n     * -f无需确认，强制删除\n   \n   * 实例\n   \n   * rf 1.txt——删除1.txt文件，会询问是否删除\n   \n   * rm -f 1.txt——强制删除1.txt，不会询问\n   \n   * rm -rf /*——强制删除根目录下所有文件\n   \n   * rm -r diana/——删除名为diana的目录和目录中的所有文件，删除前需确认\n\n * touch fileName——如果文件不存在，新建文件\n   \n   * touch 1.txt——创建1.txt\n\n * 输出重定向\n   \n   * > ——覆盖\n     \n     * 新输入的内容会覆盖原来全部的内容\n   \n   * >> ——追加\n     \n     * 追加写，继续在后面写\n   \n   * ls -a > ls.txt——将当前目录信息输出到ls.txt中\n   \n   * cat file1 > file 2——将文件1的内容覆盖到文件2 可以用于快速复制粘贴改名\n\n * ln——符号链接，类似于快捷方式\n   \n   * ln -s [原文件或目录] [软链接名]——给原文件创建一个软连接(快捷方式)\n   * ln -s /root /home/diana/myroot——在diana目录下创建一个root目录的软连接\n   * ln -s /etc/sysconfig/network-scripts/ifcfg-ens33 ~/ifcfg-ens33——将网卡配置文件软连接到家（root用户的目录是root）目录下\n\n * history——查看历史指令\n   \n   * history 10——显示历史的近10行指令\n   * !15——执行第历史命令的第15条命令\n\n# 拷贝移动命令\n\n * cp [-r] source dest——用于复制文件或目录\n   \n   * 说明\n     * -r 如果复制的是目录需要使用此选项，此时将复刻该目录下所有的子目录和文件\n     * \\cp 强制覆盖 不给出提示\n   * 实例\n     * cp 1.txt diana/ ——将1.txt文件复制到diana目录中\n     * cp 1.txt ./2.txt——将1.txt复制到当前目录下，并改名为2.txt\n     * cp -r diana/ ./diane——将diana目录及目录下所有文件复制到当前目录下的diane目录下\n       * 这就相当于将整个文件夹diana作为一个整体copy了过去\n       * copy完后目录结构为 diane/diana/file\n     * cp -r diana/* ./diane——将diana目录下所有文件复制到当前目录下的diane目录下\n       * 这就相当于将整个文件夹下的所有文件作为一个整体copy了过去\n       * copy完后目录结构为 diane/file\n\n * mv source dest——为文件或目录改名、或将文件或目录移动到其他位置\n   \n   * 实例\n     * mv 1.txt 2.txt——将1.txt重命名为2.txt\n     * mv 1.txt diana/——将文件1.txt移动到diana文件目录下\n     * mv 1.txt diana/2.txt——将1.txt移动到diana目录下，并改名为2.txt\n     * mv diana/ diane/——如果diane目录不存在，将diana目录改名为diane\n     * mv diana/ diane/——如果diane目录存在，将diana目录移动到diane目录下\n     * mv diana/* diane/——如果diane目录存在，将diane目录下的所有文件移动到diane目录下\n     * mv diana/* diane/——如果diane目录不存在，报错\n\n# 打包压缩命令\n\n * gzip/gunzip——压缩/解压缩 文件\n   \n   * gzip ls.txt——压缩文件 ls.txt——>ls.txt.gz\n   * gunzip ls.txt.gz\n\n * zip/unzip——压缩/解压缩 文件\n   \n   * zip -r 1.zip diana——将diana目录及目录下所有文件 压缩成1.zip\n   * unzip -d ./lb 1.zip——将1.zip 解压到当前的lb目录下\n\n * tar [-zcxvf] fileName [files]——对文件进行 打包、解包、压缩、解压\n   \n   * 包文件后缀为 .tar表示只是完成了打包，并没有压缩\n   \n   * 包文件后缀为.tar.gz表示打包的同时还进行了压缩\n   \n   * 说明\n     \n     * -z: z代表的是 gzip，通过gzip命令处理文件，gzip可以对文件压缩**（gz）**或者解压\n     \n     * -c： c代表的是create，即创建新的包**（tar）**文件,与x互斥\n     \n     * -x： x代表的是extract，实现从包文件中还原文件，与c互斥\n     \n     * -v： v代表的是verbose，显示命令的执行过程\n     \n     * -f： f代表的是file, 用于指定打包文件的名称\n   \n   * 常用组合\n     \n     * -zcvf\n       \n       * tar -zcvf test.tar.gz diane——对diane文件目录进行压缩，打包，命名为 test.tar.gz\n     \n     * -cvf\n       \n       * tar -cvf test.tar diane——对diane文件目录进行打包，命名为 test.tar\n     \n     * -zxvf\n       \n       * tar -zxvf test.tar.gz——将test.tar.gz进行解压缩，解包操作，并放在当前目录下\n       * tar -zxvf test.tar.gz -C /user/local——将文件解压缩，解包后，放在 /user/local 目录中\n     \n     * -xvf\n       \n       * tar -xvf test.tar——将test.tar进行解包操作，并放在当前目录下\n\n# 文本编辑命令\n\n * vi/vim\n   \n   * vim 是一个文本编辑工具，在编辑文件时，可以对文本内容进行着色，比vi功能要好\n   \n   * 需要自己安装vim—— yum install vim\n   \n   * 使用vim命令编辑文件时，如果指定的文件存在则直接打开文件，如果不存在，则新建文件\n   \n   * vim在编辑文本时，分为3种模式，可以相互切换\n     \n     * > 命令模式，插入模式，底行模式\n   \n   * vim打开文件的方式\n     \n     * vim 文件路径——打开指定的文件\n     * vim +数字 文件路径——打开指定的文件，并且将光标移动到指定行\n     * vim +/关键词 文件路径——打开指定的文件，并且高亮显示关键词\n     * vim 文件1 文件2 文件3——同时打开多个文件\n   \n   * 命令模式（删除行，复制行，移动光标，粘贴等等）\n     \n     * 命令模式下可以查看文件内容、移动光标（上下左右箭头，gg（快速移动到开头），G（快速移动到末尾））\n     \n     * 通过vim命令打开文件后，默认进入命令模式\n     \n     * 另外两种模式需要首先进入命令模式，才能进入彼此\n       \n       * 底行模式——》命令模式——》插入模式\n       \n       * 插入模式——》命令模式——》底行模式\n   \n   * 插入模式（编辑）\n     \n     * 插入模式下，可以对文件内容进行编辑\n     * 在命令模式下按[i,a,o]任意一个键，可以进入插入模式。进入插入模式后，下方会出现[insert]字样\n     * 在插入模式下按下ESC,回到命令模式\n   \n   * 底行模式（搜索，替换，保存，退出，撤销，高亮等等）\n     \n     * 底行模式下可以通过命令对文件内容进行查找、显示行号、退出等操作\n     * 在命令模式下按下[:,/]任意一个，可以进入底行模式\n     * 通过 /方式进入底行模式后，可以对文件内容进行查找 / name(name为你要查询的词)\n     * 通过 :方式进入底行模式后，可以输入 wq（保存并退出），q!(不保存退出)，set nu(显示行号)\n   \n   \n\n# 进程命令\n\n * ps——进程查看命令\n   \n   * ps -ef——查看当前运行的所有进程的详细信息\n   * ps -ef|grep tomcat——查看tomcat的进程信息\n     * |在linux中称为管道符，可以将前一个命令的结果输出给后一个命令作为输入\n   * ps -ef|grep xxx——查看xxx的进程信息\n\n * kill -9 id\n   \n   * -9表示强制删除\n   * -15表示正常关闭\n   * id为进程名称\n\n * nohup Command[Arg ...][&]——用于不挂断地运行指定命令，退出终端不会影响程序的运行\n   \n   * 说明\n     * Command要执行的命令\n     * Arg一些参数，可以指定输出文件\n     * &让命令在后台运行\n   * 实例\n     * nohub java -jar xxx.jar &>hello.log &\n     * &>hello.log将日志输出到hello.log\n     * & 后台运行\n\n# 防火墙命令\n\n * firewall-cmd --state——查看防火墙状态\n * systemctl stop firewalld——暂时关闭防火墙（重启后开启防火墙）\n * systemctl disable firewalld——永久关闭防火墙\n * systemctl start firewalld——开启防火墙\n   * systemctl start mysqld.service——开启mysql服务\n * firewall-cmd --zone=public --add-port=8080/tcp --permanent——开放8080端口\n   * 8080——指定要开放的端口\n   * /tcp——指定链接协议\n * firewall-cmd --zone=public --remove-port=8080/tcp --permanent——关闭8080端口\n * firewall-cmd --reload——立即生效\n   * 开放或者关闭端口后，需要执行这个，才能立即生效\n * firewall-cmd --zone=public --list-ports——查看开放的端口\n\n# 操作软件命令\n\n * yum list xxx——查找有没有当前软件可安装\n   \n   * yum list lrzsz——查找软件lrzsz\n\n * yum install vim——安装vim软件\n\n * rpm -qa——查询当前系统安装的所有软件\n   \n   * rpm -qa|mysql——查询当前系统安装的名称带mysql的软件\n\n * rpm -e --nodeps xxx——删除软件xxx\n\n * source /etc/profile——重新加载配置文件，立即生效\n\n# 查找命令\n\n * find dirName -option fileName ——在指定目录下查找文件\n   \n   * -name——按名称\n     \n     * find .-name *.java——在当前目录及其子目录下，查找.java结尾的所有文件\n     * find /diana -name *.java——在/diana 目录及其子目录下，查找.java结尾的所有文件\n   \n   * -user——按用户名\n     \n     * find /opt -user root——查找opt目录下 root用户的文件\n   \n   * -size——按大小查找\n     \n     * find / -size +200M——查找大于200M的文件\n     * -200M——小于200M 200M——等于200M\n     * k,M,G\n\n * locate——快速定位文件路径\n   \n   利用事先建立好的数据库来快速定位文件，无需遍历整个文件系统，查询速度较快\n   \n   * update——定位前，需要先更新数据库\n   * locate ls.txt——搜索文件\n\n * which——可以查看某个指令在哪个目录下\n   \n   * which ls\n\n * grep [选项] word fileName——从指定文件中查找指定的文本内容\n   \n   一般与管道符结合使用\n   \n   * -n——显示匹配行及行号\n   \n   * -i——忽略字母大小写\n   \n   * grep Hello HelloWord.java——查找HelloWorld.java文件中出现Hello字符串的位置\n   \n   * grep hello *.java——查找当前目录中所有.java结尾的文件中包含hello字符串的位置\n   \n   * cat ls.txt |grep hello——查看文件中 带hello的某几行\n   \n   * grep hello ls.txt——等价于上面的写法\n\n * netstat -tunlp——查看所有启动的服务\n   \n   * netstat -tunlp|grep mysql——查看mysql的启动服务\n\n * ps -ef——查看当前运行的所有进程的详细信息\n   \n   * ps -ef|grep tomcat——查看tomcat的进程信息\n\n# 时间命令\n\n * date指令\n   \n   * date——输出时间\n   * 输出——2022年 06月 05日 星期日 10:55:53 CST\n   * date +%Y——输出年份\n   * date +%m——输出月份\n   * date +%d——输出日期\n   * date +%F——格式化输出时间（年月日）——等价于date +"%Y-%m-%d"\n     * 输出——2022-06-05\n   * date +"%F %T"——格式化输出时间（年月日，时分秒）（必须加引号，表示不可分隔的整体）\n     * 等价于date +"%Y-%m-%d %H:%M:%S"\n     * 输出——2022-06-05 11:02:53\n   * date -d "-1 day"——获取之前或者之后的某个时间（备份用）\n     * +表示之后，-表示之前\n     * hour-小时 day-天 year-年 month-月\n   * date -s "2022-06-05 10:10:10"\n     * 修改系统当前时间\n\n * cal指令\n   \n   * cal——直接输出当前月份的日历，相当于 cal -1\n   * cal -3——输出3个月的日历（前一个月，本月，后一个月）\n   * cal -y 2022——输出一年（2022）的日历\n\n# 管道命令\n\n管道符：|\n\n可以用于过滤，特殊，扩展处理，主要是起一个辅助作用\n\n * 过滤 |grep\n   * ls |grep g——过滤当前目录下，名字带g的\n   * |,以管道符作为分界线，前面的命令的输出作为后面命令的输入\n   * grep,主要用于过滤\n * 特殊案例\n   * cat 1.txt | less——等价于less 1.txt（多此一举）\n * 扩展处理\n   * ls | wc -l——统计某个目录下文档的总个数\n\n# 帮助命令\n\n * man\n   \n   manual，手册，（包含了Linux中全部的命令手册，英文）\n   \n   * man 命令 ——进入手册\n   * 按q——退出手册\n\n# 其他命令\n\n * man 命令——man head——查看命令手册\n\n * ifconfig——查看ip配置\n\n * ip addr——查看ip地址\n\n * sudo apt install git ——安装git(sudo 是乌班图系统下的)\n\n * sudo apt install xxx ——安装xxx——没有东西会给出提示\n\n * du -sh *——查询占用空间\n\n * uname -a——查看系统版本\n\n * which java——查看java安装目录\n\n * wget +url——下载指定url的压缩包\n\n * 正常指令 > 文件路径 ——ll > ll.txt——将ll命令展示的东西，存储到文本中，而不显示在屏幕上\n   \n   ——覆盖输出，会覆盖掉原先的文件内容\n\n * 正常指令 >> 文件路径 ——ll > ll.txt——将ll命令展示的东西，存储到文本中，而不显示在屏幕上\n   \n     \t\t\t\t\t\t\t——追加输出，从原文件内容末尾继续添加\n   \n   \n   \n   # 高级指令\n\n# init\n\n运行级别\n\n * init0——关机\n * init3——命令行页面\n * init5——图形页面\n * init6——重启\n\n# hostname\n\n操作服务器的主机名(读取、设置)\n\n * /etc/hostname——文件可以指定主机名\n\n * hostname——表示输出完整的主机名\n   \n   * 输出——localhost.localdomain\n\n * hostname -f——表示输出当前主机名中的FQDN(全限定域名)\n   \n   * 输出——localhost\n\n# id\n\n查看一个用户的一些基本信息（包含用户id，用户组id，附加组id...）,该指令如果不指定用户则默认当前用户\n\n * id——默认显示当前执行该命令的用户的信息\n   * 输出——uid=0(root) gid=0(root) 组=0(root) 环境=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023\n * id 用户名——默认指定用户的基本信息\n\n# whoami\n\n显示当前登陆的用户名，一般用于shell脚本,用于获取当前操作的用户名\n\n * whoami\n   * 输出——root\n\n# ps （重点）\n\n进程查询指令\n\n * ps -ef\n\n * ps -ef|grep xxx——查看xxx的进程信息\n   \n   * -e—— 等价于-A,表示列出全部的进程\n   * -f——显示全部的列(显示全部字段)\n   * 执行结果\n   \n   \n   \n   * * UID: 该进程执行的用户id。\n     \n     * PID：进程id。\n     \n     * PPID：该进程的父级进程id，如果一个程序的父级进程找不到，改程序的进程称之为僵尸进程。\n     \n     * C：Cpu的占用率，其形式是百分数。\n     \n     * STIME：进程的启动时间。\n     \n     * TTY：终端设备，发起该进程的设备识别符号。如果显示"?"则表示该进程并不是由终端设备发起。\n     \n     * TIME：进程的执行时间\n     \n     * CMD：该进程的名称或者对应的路径。\n\n# top（重点）\n\n查看服务器的进程占的资源\n\n * 进入命令： top (动态显示)\n\n * 退出命令：q\n\n * 快捷键：\n   \n   * M——表示将结果按照内存（MEM）降序排列\n   * P——表示将结果按照CPU使用率降序排列\n   * 1——切换各个cpu的详细信息\n\n * 执行结果\n   \n   \n   \n   * PID: 进程id。\n   \n   * USER：该进程对应的用户。\n   \n   * PR： 优先级。（越大越好）\n   \n   * VIRT： 虚拟内存 （进程申请的内存）\n   \n   * RES： 常驻内存（进程总共使用的内存，包含了依赖使用的内存）\n   \n   * SHR： 共享内存（依赖进程所使用的内存）\n   \n   * 计算一个进程实际使用的内存=常驻内存（RES）— 共享内存（SHR）\n   \n   * S：表示进程的状态(S表示睡眠，R表示运行)\n   \n   * %CPU： 表示CPU的占用百分比\n   \n   * %MEM：表示内存的占用百分比\n   \n   * TIME+：进程执行的时间\n   \n   * COMMAND： 进程的名称或者路径\n\n# du -sh\n\n查看目录的真实大小\n\n * du-sh 目录路径\n   \n   * -s—— 只显示汇总的大小\n   \n   * -h——以高可读性的形式进行显示\n\n * 示例：du -sh java——392M java\n\n# find\n\n用于查找文件\n\n * find 路径范围 选项 参数值\n   \n   * name——按照文档名称进行搜索\n   \n   * type——按照文档的类型进行搜索\n     \n     * -表示文件（在使用find的时候需要使用f来替换）\n     * d表示文件夹\n\n * find / -name ls.txt——从根目录(包含其子文件夹)开始找，找到名字为ls.txt的文件\n   \n   * / ——根目录--路径范围\n   * -name——按照文档名称搜索--选项\n   * ls.txt——文档名--参数值\n\n * find /diane -name *.conf\n   \n   * 从/diane目录(包含其子文件夹)开始找，找到所有后缀为.conf的文件\n\n * find ./ -name *.conf\n   \n   * 从当前目录及其子目录下开始找，找到所有后缀为.conf的文件\n\n * find /diane -type f\n   \n   * 从/diane目录(包含其子文件夹)开始找，找到所有的文件\n\n * find /diane -type f|wc -l\n\n * 从/diane目录(包含其子文件夹)开始找，找到所有的文件,并将其作为管道的输入，统计一下文件的数量\n\n * find /diane -type d\n   \n   * 从/diane目录(包含其子文件夹)开始找，找到所有的文件夹\n   \n   # service\n\n用于控制一些软件的服务 启动/停止/重启\n\n * service 服务名 start/stop/restart\n * service NetworkManager stop ——禁用网络管理\n * service network restart——重启网络服务\n\n# kill\n\n使用kill杀死进程\n\n * kill 进程PID\n * killall 进程名称\n * ps -ef|grep xxx ——通过ps命令查看进行PID\n\n# ifconfig\n\n获取网卡信息\n\n * ifconfig\n   \n   \n\n# reboot\n\n重新启动计算机\n\n * reboot——重新启动计算机\n * reboot -w——模拟重新启动计算机，但是不重启(只写关机与开机的日志信息)\n\n# shutdown\n\n关机\n\n * shutdown -h 1==shutdown\n   * 1分钟后关机\n * shutdown -h now "关机提示"\n   * 立即关机，并给出关机提示\n * shutdown -h 15:25 “关机提示”\n   * 设置关机时间，并给出关机提示\n * shutdown -c\n   * 取消定时关机命令\n * shutdown -r now\n   * 立即重启计算机\n\n其他关机命令\n\n * init 0\n\n * halt\n\n * poweroff\n\n * sync——将内存的数据同步到磁盘\n   \n   * 以免数据丢失，可以在关机前执行该指令\n   * 现在的关机指令，基本都在执行关机前，底层调用了这个命令\n\n# uptime\n\n输出计算机的持续在线时间（计算机从开机到现在运行的时间）\n\n * uptime\n   * 输出——09:40:31 up 5 min, 3 users, load average: 0.04, 0.08, 0.05\n\n# uname\n\n获取操作系统的信息\n\n * uname——获取操作系统的类型\n   * Linux\n * uname -a——获取全部的系统信息**(类型，全部主机名，内核版本，发布时间，开源计划)**\n   * Linux localhost.localdomain 3.10.0-1160.el7.x86_64 #1 SMP Mon Oct 19 16:18:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\n\n# netstat\n\n查看网络的连接状态\n\n * netstat -tnlp\n   \n   \n   \n   * t——表示只列出tcp协议的连接\n   * n——表示将地址从字母组合转换成ip地址，将协议转换成端口号来显示\n   * l——表示过滤出“state(状态)”列中其值为LISTEN(监听)的连接\n   * p——表示显示发起连接的进程PID和进程名称\n\n\n# 指令应用\n\n * 删除/diane下所有A开头的文件\n   \n   * rm -f /diane/A*\n\n * 系统重要文件需要备份，如何把/diane/java备份到/diana目录下\n   \n   * cp -r /diane/java /diana\n\n * 如何查看系统最后创建的3个账户\n   \n   * tail -3 /etc/passwd\n\n * 什么命令可以统计当前系统中一共有多少账户\n   \n   * wc -l /etc/passwd\n\n * 如何创建/diane/ll.txt文件\n   \n   * touch /diane/ll.txt\n\n * 如何通过vim编辑打开/diane/ll.txt\n   \n   * vim /diane/ll.txt\n\n * 如何查看/etc/passwd的头三行和尾三行\n   \n   * head -3 /etc/passwd\n   * tail -3 /etc/passwd\n\n * 如何在diane目录下**一次性(-p)**创建目录 text 1 2 3 4\n   \n   * mkdir -p test 1 2 3 4（在diane目录下）\n\n * 如何最快的返回当前账户的家目录\n   \n   * cd ~\n   * cd\n\n * 如果查看/etc所占的磁盘空间\n   \n   * du -sh /etc\n\n * 如何删除/diana下所有文件\n   \n   * rm -rf /diana/*\n\n\n# Linux自有服务\n\n# 组管理和权限管理\n\n * 其它组——除文件的所有者和所在组的用户外，系统的其他用户都是文件的其他组\n\n\n\n * 文件所有者，及所有组\n   \n   * 用户lisi信息\n   \n   \n   \n   * lisi创建文件的信息\n     * 可以看出，第一个是文件所有者 lisi；第二个是文件所在组diana，默认用户所在组\n   \n   \n\n * chown 用户名 文件名—— 修改文件的所有者\n   \n   * -R——如果是目录，则使其下所有子文件或者目录递归生效\n   \n   * chown -R diana file——将file文件夹及其下所有文件的所有者改为diana\n   \n   * chown diana 1.txt——将1.txt的文件所有者改成diana\n   \n   * 修改前\n   \n   * 修改后\n\n * chgrp 组名 文件名——修改文件所在组\n   \n   * -R——如果是目录，则使其下所有子文件或者目录递归生效\n   \n   * 继续前文修改\n   \n   * chgrp liangbing 1.txt——将1.txt文件组修改至liangbing\n   \n   * 再次修改后\n   \n   # 用户管理\n\n * 每个用户有唯一的用户名\n\n * 下面所有的用户操作的命令，除了修改密码，其他均只有root用户有权限操作\n\n * 重要三个文件\n   \n   * /etc/passwd——存储用户的关键信息\n   \n   * /etc/group——存储用户组的关键信息\n   \n   * /etc/shadow——存储用户的密码信息\n\n * useradd 选项 用户名——添加用户\n   \n   * 常用选项（选项可以为空）\n     \n     * -g——表示指定用户的用户主组，选项的值可以是用户组的id，也可以是组名\n     * -G——表示指定用户的用户附加组，选项的值可以是用户组的id，也可以是组名\n     * -u——uid 用户的id(用户标识符)，系统默认会从500以后按顺序分配uid，如果不想使用系统分配的，可以通过该选项自定义\n   \n   * \n   \n   * \n   \n   * 添加用户(无选项)——会自动生成 用户主组，用户附加组，uid\n     \n     * useradd diane——添加用户 diane\n       * uid——1000\n       * 用户主组——1000\n       * 用户附加组——1000\n     * useradd diana——添加用户 diana\n       * uid——1001\n       * 用户主组——1001\n       * 用户附加组——1001\n   \n   * 添加用户(指定选项)\n     \n     * useradd -g 1000 -G 1001 -u 666 liangbing——添加指定用户liangbing\n       \n       * uid——666——passwd\n       * 用户主组——1000——passwd\n       * 用户附加组——1001——group\n       * 用户名—— liangbing\n   \n   * 验证创建用户是否成功\n     \n     * 验证/etc/passwd的最后一行，查看是否有新创建的用户信息\n     * 验证是否存在home目录(在centos下创建好用户之后随之产生一个同名的家目录)\n   \n   * /etc/passwd补充\n     \n     \n     \n     * 用户名：密码位：用户ID：用户组ID：注释：家目录：解释器shell\n     * 用户名： 创建新用户名称，后期登陆需要输入\n     * 密码：一般都是x，表示密码的占位，并不是真正的密码\n     * 用户ID：用户的标识符(uid)\n     * 用户组ID：该用户所属的主组ID\n     * 注释：解释该用户是做什么的\n     * 家目录：用户登陆进入系统后默认的位置\n     * 解释器shell：等待用户进入系统之后，用户输入指令之后，该解释器会收集用户输入的指令，传递给内核处理\n\n * usermod 选项 用户名——修改用户\n   \n   * 常用选项\n     * -g,-G,-u——同上\n     * -l——修改用户名(小写的L)\n   * usermod -l lisi zhangsan——将zhangsan用户名修改为lisi\n   * usermod -g wudang zwj——将zwj的组修改至wudang\n\n * passwd 用户名——设置密码\n   \n   * Linux不允许没有密码的用户登陆到系统，因此没有密码的用户处于锁定状态，不允许登陆\n   \n   * su 用户名——切换用户名\n     \n     * 从root往普通用户切换不需要密码，但是反之需要root密码\n     * 切换用户前后的工作路径不变\n     * 普通用户没有办法访问root用户家目录，但是反之可以\n\n * userdel 选项 用户名\n   \n   * 选项 -r——表示删除用户的同时，删除其家目录，谨慎使用\n   * 不带选项的话，表示删除用户的同时，保留其家目录，一般建议保留\n   * 可以直接删除没有登陆的用户\n   * 如果用户在登陆，需要先杀死其进程(kill)，然后在删除用户\n\n# 用户组管理\n\n * groupadd 选项 用户名组\n   \n   * 常用选项\n     * -g——类似于用户添加里面的-u，表示自己设置一个自定义的用户组ID数字，如果自己不指定，系统会自动生成\n   * groupadd liang——创建用户组 liang\n\n * groupmod 选项 用户组名\n   \n   * 常用选项\n     * -g\n     * -n——类似于用户修改里面的-l,表示设置新的用户组的名称\n   * groupmod -g 500 -n bing liang——将用户组liang 修改名称为bing，修改组号为500\n\n * groupdel 用户组名\n   \n   * groupdel bing——删除用户组 bing\n     * 组内没有人，可以直接删除\n   * group lisi——删除用户组lisi\n     * lisi的组号是用户zhangsan的主组，不允许直接删除用户组\n     * 需要先将组内用户移走或者删除，才可以删除用户组\n\n# 权限\n\n * - rw- r-- r--.\n\n * d rwx r-x r-x.\n\n * l rwx rwx rwx.\n\n * b rw- rw- ---.\n\n * c rw- rw- ---.\n\n * 0--9位说明\n   \n   * 第0位表示文件类型（d,-l,c,b）\n     \n     * l——表示链接，相当于windows的快捷方式\n     \n     * d——表示目录，相当于windows的文件夹\n     \n     * c——表示字符设备文件，例如，鼠标，键盘，\n     \n     * b——表示块设备，比如 硬盘\n     \n     * -——普通文件\n   \n   * 1-3位——确定文件所有者拥有该文件的权限——User\n   \n   * 4-6位——确定文件所在组（同用户组）拥有该文件的权限——Group\n   \n   * 7-9位——确定其他用户拥有该文件的权限——Other\n\n * r,w,x,-——文件——r=4,w=2,x=1——rwx=4+2+1=7\n   \n   * r——代表可读，可以读取，查看\n   \n   * w——代表可写，可以修改，但是不代表可以删除该文件；\n   \n   * ——删除一个文件的前提条件是对该文件所在的目录有写权限，才能删除该文件\n   \n   * x——代表可执行，文件可以被执行\n\n * r,w,x,-——目录\n   \n   * r——代表可读，可以ls查看目录内容\n     * 当目录没有r权限时，其他用户不能通过ls查看目录内文件\n     * 但是若其他用户对目录内文件有写权限，虽然无法通过ls列出该目录下文件，仍然可以直接对写该文件进行修改\n   * w——代表可写，可以修改，对目录内部文件创建+删除，重命名目录；\n   * x——代表可执行，可以进入该目录——cd目录\n\n * ll解读\n   \n   * drwxr-xr-x. 3 root root 16 7月 19 21:04 2\n     * 3 ——文件：硬连接数 目录：一层子目录内的文件数\n     * 第一个root ——表示 文件所有者\n     * 第二个root——表示文件所在组\n     * 16——文件大小（字节）\n     * 时间表示文件的修改时间\n     * 2 ——表示的文件的名称\n\n * chmod——修改文件或目录的权限（绿色表示可执行文件）\n   \n   * 第一种方式：+,-,= 变更权限\n     \n     u:所有者，g:所有组，o:其他人 ，a:所有人(u,g,o的总和)\n     \n     * chmod u=rwx,g=rw,o=x abc——给abc文件直接赋予权限\n     * chmod o+w abc ——给其他人 增加写权限\n     * chmod a-x abc——给所有人 减少执行权限\n     * chmod a-x-w abc——给所有人 减少执行权限,减少写权限\n   \n   * ==第二种：通过数字变更权限==\n     \n     r=4,w=2,x=1——rwx=4+2+1=7\n     \n     wx=3，rx=5，rw=6，rwx=7\n     \n     * chmod u=rwx,g=rw,o=x abc——给abc文件直接赋予权限\n     * chmod 761 abc——与上等价\n\n# 权限实践\n\n * 实践1——警察，土匪\n   \n   * 警察组 polic ， 土匪组 bandit\n   \n   * jack，jerry 警察\n   \n   * xh，xq 土匪\n   \n   * 创建组\n     \n     * root:groupadd polic,groupadd bandit\n   \n   * 创建用户\n     \n     * root:useradd -g polic jack useradd -g polic jerry\n       * useradd -g bandit xh useradd -g bandit xq\n   \n   * jack创建一个文件，自己可以读写，本组人可以读，其他组没有权限\n     \n     * jack：touch jack.txt——在jack家目录下\n       * chmod 640 jack.txt\n   \n   * jack修改文件，让其他组可以读，本组人可以读写\n     \n     * jack：chmod 664 jack.txt\n   \n   * xh投靠警察，看看是否可以读写\n     \n     * root: usermod -g polic xh\n     \n     * xh 无法进入jack家目录，因此无法对文件进行修改\n       \n       * 如果要对目录内的文件进行操作，需要有该目录的相应权限\n     \n     * root: chmod 777 /home/jack——开放jack家目录的所有权限\n     \n     * xh:cd /home/jack,vim jack.txt——可以成功读写\n     \n     * xq: cd /home/jack,vim jack.txt——只读文件，不可写入\n       \n       * wq!——强制保存，导致jack.txt的所有者修改为xq\n\n * 实践2——西游记\n   \n   * 建立两个组（神仙-sx，妖怪-yg）\n     \n     * root: groupadd sx groupadd yg\n   \n   * 建立4个用户（wk，bj，--yg）（ts，ss，--sx）\n     \n     * root: useradd -g sx ts useradd -g sx ss\n     * root: useradd -g yg wk useradd -g yg bj\n   \n   * 用wk建立一个文件（monkey.java 该文件写入 i am monkey）\n     \n     这里不考虑目录影响，该目录对所有人开放所有权限\n     \n     * wk:touch mokey.java\n     * wk:echo "i am mokey" >> mokey.java\n   \n   * 给bj一个 r，w的权限,ts，ss对该文件没有权限\n     \n     * wk:chmod 760 mokey.java\n   \n   * 八戒修改monkey.java ，加入一句话（i am pig）\n     \n     * bj:echo "i am pig" >> mokey.java\n   \n   * 把ss放入妖怪组\n     \n     * root: usermod -g yg ss\n   \n   * 让ss修改该文件 monkey.java 加入一句话 （i am person）\n     \n     * ss:echo "i am pig" >> mokey.java\n\n\n# 任务调度\n\n# 循环任务调度\n\n\n\n设置任务调度文件 ：/etc/crontab\n\n * 1. 设置个人任务调度——执行crontab -e命令\n * 2. 输入任务到调度文件（有空格）\n   * */1 * * * * ls -l /etc/ > /tmp/to.txt\n   * 每小时的每分钟执行一次 ls -l /etc/ > /tmp/to.txt\n   * 占位符：\n   * 特殊符号：![](/assets/后端/Linux/任务调度 特特殊符号.jpg)\n   * 例子 ![](/assets/后端/Linux/任务调度 例子.jpg)\n\n * 定时调度shell脚本，在shell脚本中放入多个调度任务\n   \n   * 一分钟一次，将当前日期和日历追加到/diane/time.txt中\n     \n     1. 写脚本\n        \n        * vim my.sh\n        * 输入：date >> /diane/time.txt\n        * 输入：cal >> /diane/time.txt\n     \n     2. 给脚本增加执行权限\n        \n        * chmod u+x my.sh\n     \n     3. 定时调度脚本\n        \n        * crontab -e\n        * 输入：*/1 * * * * my.sh\n\n * crontab -r——终止任务调度——删除了写入的任务调度\n\n * crontab -l——列出当前有哪些任务调度\n\n * service crond restart——重启任务调度\n\n# 一次性任务调度\n\n>  1. at 命令是一次性定时计划任务，at 的守护进程 atd 会以后台模式运行，检测作业队列来运行\n>  2. 默认情况下，atd 守护进程每60s检查一次作业队列，有作业时，会检查作业运行时间，如果时间与当前时间匹配，则运行此作业。\n>  3. at 命令 是一次性定时计划任务，执行完一个任务后 不在执行此任务\n>  4. 在使用 at命令的时候，一定要保证 atd 进程的启动\n\n * at [选项] [时间]\n   \n   * 选项\n     \n     ![](/assets/后端/Linux/at 选项.jpg)\n   \n   * 时间\n     \n     ![](/assets/后端/Linux/at 时间.jpg)\n   \n   * Ctrl + D结束at命令的输入\n\n * at 定时任务 案例\n   \n   * 输入过程\n     \n     * 输入错误，按住 ctrl+backspace来删除\n     * 按ctrl +u 来进行撤销\n     * 输入报错：Can\'t open /var/run/atd.pid to signal atd. No atd running?\n       * 解决方法：systemctl restart atd\n   \n   * 2天后的下午5点执行 /bin/ls/home\n     \n     * at 5pm + 2days\n     * 输入：/bin/ls /home\n   \n   * atq 命令来查看系统中没有执行的工作任务\n     \n     * atq\n   \n   * 明天17点钟，输出时间到指定文件内，如 /root/date100.log\n     \n     * at 17:00 +1days\n     * 输入：date >> /root/date100.log\n   \n   * 2分钟后，输出时间到指定文件内，如/root/date200.log\n     \n     * at now + 2minutes\n   \n   * 删除已经设置的任务， atrm编号\n     \n     * atrm 5——删除5号任务\n     * atrm 4 5 6——删除 4 5 6 号任务\n\n\n# Linux分区\n\n# 目录\n\nlinux一切皆文件\n\n\n\n\n\n\n\n# 磁盘分区机制\n\n * 挂载机制——将硬盘的某个分区跟文件链接起来\n\n\n\n * 硬盘情况\n   \n   * SCSI硬盘，标识为sdx~——x=a,b,c……——~=1,2,3……\n   * sda1---sda2---sda3---sda4\n   \n   \n\n * 查看所有设备的挂载情况\n\n * lsblk--lsblk -f\n\n\n\n * 对新增硬盘分区\n   \n   * 新增一块硬盘，需要重启\n   \n   * 分区命令——fdisk /dev/sdb—(sdb是新增硬盘名称)\n     \n     * m——显示命令\n     * p——显示磁盘分区\n     * n——新增分区\n     * d——删除分区\n     * w——写入并退出\n     * 开始分区后，输入n，新增分区，然后选择p，分区类型为主分区，两次回车选择默认，最后输入w保存分区并退出，若不保存，则输入q\n   \n   * 格式化磁盘（获得了一个UUID）——mkfs -t ext4 /dev/sdb1—(ext4是文件格式，sdb1是新增硬盘分区名称)\n   \n   * 挂载——mount /dev/sdb1 /newdisk/-(sdb1是新增硬盘分区名称,newdisk是用来挂载的新建文件夹)\n     \n     * 重启后 挂载关系会失效\n   \n   * 永久挂载\n     \n     * vim /etc/fstab\n     * /dev/sdb1 /newdisk ext4 defaults 0 0\n   \n   * 取消挂载——umount /dev/sdb1或者umount /newdisk\n\n# 磁盘实用指令\n\n * df -h——查询系统磁盘使用情况\n\n * du [选项] /目录——查询指定目录的磁盘占用情况\n   \n   * -s——指定目录大小汇总（只显示目录的总用量，显示一个）\n   * -h——带计量单位\n   * -a——含文件（默认不含文件）\n   * --max-depth=1——子目录深度（默认是最深处）\n   * -c——列出明细的同时，增加汇总值（没啥用）\n\n * 统计/opt文件夹下文件的个数\n   \n   * ll /opt | grep "^-" | wc -l\n   * "^-"——表示以-开头的，正则表达式，即普通文件\n\n * 统计/opt文件夹下目录的个数\n   \n   * ll /opt | grep "^d" | wc -l\n   * "^d"——表示以d开头的，正则表达式，即目录\n\n * 统计/opt文件夹下文件的个数，包括子文件里的\n   \n   * ll -R /opt | grep "^-" | wc -l\n   * -R——表示递归，深入子文件内\n\n * 统计/opt文件夹下目录的个数，包括子文件里的\n   \n   * ll -R /opt | grep "^d" | wc -l\n\n * 以树形显示目录结构\n   \n   * tree /opt\n   * tree -L 1 /opt——只显示1层\n\n\n# 网络配置\n\n# 网络基础\n\n * 三种模式\n\n\n\n * 查看ip地址\n   \n   * Window下\n     * ipconfig\n     * 找到VMnet8\n     * \n   * Linux下\n     * ifconfig\n     * 找到ens33\n     * \n\n * 网络链接测试\n   \n   * ping ip=ping 192.168.159.1\n   * ping 网站=ping www.baidu.com\n\n * NAT模式（共享主机ip）\n\n\n\n# 网卡设置\n\n * 网络问题(当连不上网时，可以这样操作一下)\n   \n   * service NetworkManager stop ——禁用网络管理\n   * service network restart——重启网络服务\n\n * 和git命令，解决合并冲突时差不多\n\n * 网卡的配置文件太深，可以通过创建软连接的方式\n   \n   * ln -s 原路径 现路径\n   * ln -s /etc/sysconfig/network-scripts/ifcfg-ens33 ~/ifcfg-ens33——将网卡配置文件软连接到家目录下\n\n\n\n# 远程链接\n\n安装SSH连接工具\n\n * SSH，建立在应用层基础上的安全协议\n * 通过SSH连接工具，可以实现从本地连接到远程的Linux服务器\n * 这里安装SSH连接工具，是为了最大限度的模拟企业工作\n * 链接操作\n   * 点击左上角文件夹，点开，在点击左上角文件夹，选择SSH链接\n   * 输入名称（任意），主机输入要链接的ip地址\n   * 输入用户名和密码\n   * 再次进入，有个快速连接，里面保存了之前的链接信息，直接点击即可\n\n# 主机名和hosts映射\n\n * hostname\n   \n   操作服务器的主机名(读取、设置)\n   \n   * /etc/hostname——文件可以指定主机名\n   \n   * hostname——表示输出完整的主机名\n     \n     * 输出——localhost.localdomain\n     * 修改后——diana\n   \n   * hostname -f——表示输出当前主机名中的FQDN(全限定域名)\n     \n     * 输出——localhost\n\n * 设置hosts映射\n   \n   通过ping 主机名找到Linux系统\n   \n   * Windows\n     * C:\\Windows\\System32\\drivers\\etc\\hosts——在该文件下写入映射关系\n     * 192.168.159.100 diana——(ip 主机名)\n     * \n   * Linux\n     * etc/hosts——在该文件下写入映射关系\n     * 192.168.137.1 diana-pc——(ip windows 主机名)\n\n * 主机名解析机制分析（Hosts、DNS）\n   \n   * Hosts 是一个文本文件，用来记录 IP和 Hostname的映射关系\n   * DNS 是域名系统，是互联网上作为域名和IP地址相互映射的一个分布式数据库\n   * DNS 实例\n\n\n# Linux软件安装\n\n# 软件安装方式\n\n * 二进制发布包安装\n   * 软件已经针对具体平台编译打包发布，只要解压，修改配置即可\n * rpm安装\n   * 软件已经按照redhat的包管理规范进行打包，使用rpm命令进行安装，不能自行解决库依赖问题\n * yum安装\n   * 一种在线安装软件方式，本质上还是rpm安装，自动下载安装包并安装，安装过程中自动解决库依赖问题\n * 源码编译安装\n   * 软件以源码工程的形式发布，需要自己编译打包\n\n# jdk\n\nJAVA_HOME=/usr/local/jdk1.8.0_361\n\nPATH=$JAVA_HOME/bin:$PATH\n\n\n\n# Tomcat\n\n * 安装\n\n\n\n * 启动验证\n\n\n\n * 防火墙命令\n   * 在window下，访问tomcat需要开放 8080端口\n\n\n\n# Mysql\n\n * 安装前检测\n\n\n\n * 安装网站介绍\n   * 安装报错\n     * 失败的软件包是：mysql-community-libs-compat-5.7.37-1.el7.x86_64 GPG 密钥配置为：file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql\n   * 解决方案\n     * 去这个文件下面 /etc/yum.repos.d/mysql-community.repo 修改对应安装版本的gpgcheck=0即可,默认值为1\n * 安装后启动，并查看\n\n\n\n * 设置mysql密码\n   \n   \n\n * 从远程链接mysql服务器\n   \n   * 开放3306端口\n     \n     * firewall-cmd --zone=public --add-port=3306/tcp --permanent\n   \n   * 允许远程链接(在mysql下运行)\n     \n     * use mysql;\n     * update user set host=\'%\' where user=\'root\';\n     * FLUSH PRIVILEGES;\n\n# lrzsz\n\n * yum list lrzsz——查看可安装软件\n * yum install lrzsz.x86_64——安装软件\n * 使用\n   * rz 弹出对话框，选择文件，上传文件\n\n# Redis\n\n * 安装\n\n\n\n * 使用\n   * src 目录下\n     * redis-server——启动服务\n     * redis-cli -h localhost -p 6379 -a 123456 —— -h选择连接地方，-p选择连接端口，-a选择密码\n       * auth 123456——也可以启动服务之后，显示输入密码\n   * src 上层目录下，有个redis.conf配置文件\n     * src/redis-server ./redis.conf——修改配置文件后，启动服务\n     * 修改配置\n       * requirepass 123456 ——输入密码登陆\n       * daemonize yes——后台运行\n       * # bind 127.0.0.1——注释掉这句话，表示允许远程访问\n\n# Nginx\n\n * 下载安装\n   \n   \n\n * 目录结构\n\n\n\n * Nginx命令(sbin/nginx)\n   \n   * ./nginx -v——查看版本号\n   * ./nginx -t——检测配置文件（/conf/nginx.conf）是否有问题\n   * ./nginx——启动nginx服务（master,worker）默认两个进程，，（默认端口号 80）\n   * ./nginx -s stop——关闭nginx服务\n   * ./nginx -s reload——重新加载配置文件\n\n * Nginx配置文件结构\n   \n   \n   \n   * 全局块\n   * events块\n   * http块\n\n * Nginx具体应用\n   \n   * 部署静态资源\n     * 标红的地方是 语法， 黑色的地方是可以修改的地方\n     * 可以配置多个server，监听多个端口\n   \n   \n   \n   * 反向代理概念\n     * 正向代理概念 ———— 翻墙，vpn\n     * 反向代理——用户无需感知，用户访问反向代理服务器，即可得到web服务器上的内容，但是用户不知道它访问的是反向代理服务器.\n   \n   \n   \n   \n   \n   * 反向代理配置\n     \n     * 配置\n       \n       server {\n           listen       81;\n           server_name  localhost;\n           location / {\n           \tproxy_pass http://169.254.176.155; #反向代理，将请求转发到指定服务      \n           }\n       }\n       \n     \n     * 效果\n       \n       访问http://192.168.159.100:81/,nginx服务器的端口\n       \n       可以访问到http://169.254.176.155/,window 下 tomcat服务器端口\n   \n   * 负载均衡概念\n   \n   \n   \n   * 负载均衡配置\n     \n     * 理论上来说，里面配置的应该是多台服务器，但是咱们没有多余的服务器，就用不同的端口号来代替\n     * weight为权重，权重越多，访问机会越大\n     \n     upstream targetserver{  #upstream指令可以定义一组服务器\n     \tserver 169.254.176.155:80 weight=100;\n     \tserver 169.254.176.155:8080 weight=50;  #只能配置到端口，后面不能跟路径了   \n     }\n     server{\n     \tlisten  82;\n     \tserver_name localhost;\n     \tlocation / {\n     \t\tproxy_pass http://targetserver;\n     \t}\n     }\n     \n     \n     * 负载均衡测量\n     \n     \n\n\n# Linux项目部署\n\n# 手工部署项目（步骤）\n\n * 将web项目打jar包，并传到llinux中\n\n * 在linux中运行jar包\n   \n   * java -jar xxx.jar\n\n * 检测防火墙，确保8080端口对外开放\n   \n   * firewall-cmd --zone=public --list-ports\n\n * 在浏览器中访问192.168.159.131:8080/host\n\n * 改为后台运行springboot程序，并将日志输出到日志文件\n   \n   * nohup Command[Arg ...][&]——用于不挂断地运行指定命令，退出终端不会影响程序的运行\n     * 说明\n       * Command要执行的命令\n       * Arg一些参数，可以指定输出文件\n       * &让命令在后台运行\n     * 实例\n       * nohup java -jar xxx.jar &>hello.log &\n       * &>hello.log将日志输出到hello.log\n       * & 后台运行\n\n * 停止服务\n   \n   * ps -ef|grep java——查找java进程——39390\n   \n   * kill -9 -39390——杀死java对应进程\n\n# shell脚本自动部署项目\n\n\n\n * 安装git\n   \n   * yum list git——查看git可安装版本——git.x86_64\n   * yum install git.x86_64——安装git\n     * git config --global user.name "diana"——设置用户名\n     * git config --global user.email "diana_liangbing@126.com"——设置邮箱\n   * git clone https://gitee.com/diana-liangbing/hellogit.git——克隆项目\n\n * 安装maven\n   \n   \n\n * 编写shell脚本\n   \n   #!/bin/sh\n   echo ==============================\n   echo 自动化部署脚本启动\n   echo ==============================\n   \n   echo 停止原来运行的工程\n   #定义变量 方便查询进程\n   APP_NAME=hellogit\n   \n   #查找包含hellogit的进程，并将其停掉\n   tpid=`ps -ef|grep $APP_NAME|grep -v grep|grep -v kill|awk \'{print $2}\'`\n   if [ ${tpid} ];then\n           echo \'STOP Process...\'\n           kill -15 $tpid\n   fi\n   sleep 2\n   #再次查找进程，如果还存在，强制杀死，否则输出 停止成功\n   tpid=`ps -ef|grep $APP_NAME|grep -v grep|grep -v kill|awk \'{print $2}\'`\n   if [ ${tpid} ];then\n           echo \'Kill Process!\'\n           kill -9 $tpid\n   else\n           echo \'Stop Success!\'\n   fi\n   \n   echo 准备从Git仓库拉取最新代码\n   #进入存放项目的目录\n   cd /diane/java-project/project/hellogit\n   \n   echo 开始从Git仓库拉取最新代码\n   git pull\n   echo 代码拉取完成\n   \n   echo 开始打包\n   output=`mvn clean package -Dmaven.test.skip=true`   #clean，package，并跳过测试\n   \n   cd target\n   \n   echo 启动项目\n   #后台启动项目，并将日志输出到当前目录下的hellolinux.log中\n   nohup java -jar hello-linux-0.0.1-SNAPSHOT.jar &>hellolinux.log &   \n   echo 项目启动完成\n   \n   \n\n * 为用户授予执行shell脚本权限\n\n\n\n\n\n * 执行shell脚本\n   * ./bootStart.sh——直接打开就是执行\n * 设置静态ip\n   * static 没有引号\n\n',normalizedContent:'# linux基础\n\n\n# 快捷键\n\n * ctrl+c——停止命令\n * ctrl+u——快速删除光标前的内容\n * ctrl+k——快速删除光标后的内容\n * ctrl+shift+c/ctrl+insert——复制\n * ctrl+shift+v/shift+insert——粘贴\n * 选中+鼠标中键——自动填充到命令行\n * ctrl+alt+t——打开终端（乌班图）\n * tab键自动补全\n * 连续两次tab,给出操作提示\n * 上下箭头调用层用命令\n * 使用clear命令或者ctrl+l实现清屏\n\n\n# 常用命令\n\n# 命令格式\n\n\n\n# 文件目录操作命令\n\n * pwd ——查看当前目录位置\n\n * tree——查看树形目录结构\n\n * head [-n] filename——查看文件前n行内容，默认n=10\n\n * wc -l filename——统计文件行数（-l 行数 ，-w 单词数，-c字节数）\n\n * free -h——查询内存\n\n * df -h——查看磁盘空间\n\n * echo—— 输出环境变量；输出字符串\n   \n   * echo $path\n   \n   * echo hello world!\n\n * ls——查看当前目录下所有文件\n   \n   * 命令说明 ls [-al] [dir]\n     \n     * -a ：显示所有文件及目录（.开头的隐藏文件也会列出）\n     * -l：处文件名称外，同时将文件形态（d表示目录，-表示文件）、权限、拥有者、文件大小等详细信息列出\n   \n   * 实例\n     \n     * ls -a——查看所有文件，包含隐藏文件，不显示详细信息\n     * ls -l——查看非隐藏文件的详细信息\n     * ll——ls -l的简写\n     * ls -al——查看所有文件的详细信息\n\n * cd 目录名——进入某个文件夹下\n   \n   * cd \\——进入根目录\n   * cd ..——表示目前目录位置的上级目录\n   * cd .——表示目前所在目录\n   * cd ~——表示用户的home目录\n   * cd diana——进入当前目录下的diana文件目录中\n   * cd /diana——进入根目录下的diana文件目录中，开头的第一个/表示根目录\n\n * cat [-n] filename——显示文件内容（全部）\n   \n   * 说明\n     * -n 由1开始对所有输出的行数的编号\n   * 实例\n     * cat /etc/profile——查看/etc目录下的profile文件内容\n\n * cat filename1 filename2 filename3 > filename4——将前三个文件的内容合并到第四个文件中，并打开\n   \n   * 实例\n     * cat 1.txt 2.txt >3.txt——将前1.txt，2.txt文件的内容合并到3.txt文件中，并打开\n\n * more filename——以分页的形式显示文件内容\n   \n   一次性将文件全部加载，效率比较低\n   \n   * 说明\n     * 回车键， 向下滚动一行\n     * 空格键，ctrl+f 向下滚动一屏\n     * b，ctrl+b 返回上一屏\n     * q，ctrl+c，退出more\n     * = 输出当前行号\n     * :f 输出文件名和当前行的行号\n   * 实例\n     * more /etc/profile——以分页方式展示/etc目录下的profile文件内容\n\n * less filename——以分页的形式显示文件内容\n   \n   显示一部分记载一部分，速度更快,适合大文件\n   \n   * 说明\n     * 方向上键——向上一行\n     * 回车键，方向下键 ——向下滚动一行\n     * 空格键，pg dn——向下滚动一屏\n     * pg up ——向上滚动一屏\n     * q，ctrl+c，退出more\n     * /字符串 ——向下搜寻字符串的功能， n:向下查找 ；n:向上查找\n     * ?字符串 ——向下搜寻字符串的功能， n:向下查找 ；n:向上查找\n   * 实例\n     * less /etc/profile——以分页方式展示/etc目录下的profile文件内容\n\n * tail [-f] filename——查看文件末尾的内容（默认后10行）（自己手动的不可以显示）（一般用来查看日志）\n   \n   * 说明\n     * -f 动态读取文件末尾内容并显示，通常用于日志文件的内容输出\n   * 实例\n     * tail /etc/profile——显示/etc目录下profile文件末尾10行的内容\n     * tail -20/etc/profile——显示 /etc目录下的profile文件末尾20行的内容\n     * tail -f/diana/my.log——动态读取 /diana目录下的my.log文件末尾内容并显示\n\n * mkdir [-p] dirname——创建目录\n   \n   * 说明\n     \n     * -p 确保目录名称存在，不存在的就创建一个。通过此选项，可是实现多层目录同时创建\n   \n   * 实例\n     \n     * mkdir java——在当前目录下，创建名为java的子目录\n     \n     * mkdir -p diana/java——在工作目录diana中创建一个java子目录，如果diana目录不存在，则创建。即可以一次性创建多层文件目录\n\n * rmdir [-p] dirname——删除空目录\n   \n   * 说明\n     * -p 当子目录被删除后，使父目录为空的话，一并删除\n   * 实例\n     * rmdir diana——删除名为diana的空目录\n     * rmdir -p diana/test——删除diana目录下的test目录，若删除后，diana为空目录，则删除diana\n     * rmdir diana*——删除名称以diana开头的空目录\n\n * rm[-rf] name——删除文件或目录\n   \n   * 说明\n     \n     * -r 将目录及目录中所有文件（目录）逐一删除，即递归删除\n     * -f无需确认，强制删除\n   \n   * 实例\n   \n   * rf 1.txt——删除1.txt文件，会询问是否删除\n   \n   * rm -f 1.txt——强制删除1.txt，不会询问\n   \n   * rm -rf /*——强制删除根目录下所有文件\n   \n   * rm -r diana/——删除名为diana的目录和目录中的所有文件，删除前需确认\n\n * touch filename——如果文件不存在，新建文件\n   \n   * touch 1.txt——创建1.txt\n\n * 输出重定向\n   \n   * > ——覆盖\n     \n     * 新输入的内容会覆盖原来全部的内容\n   \n   * >> ——追加\n     \n     * 追加写，继续在后面写\n   \n   * ls -a > ls.txt——将当前目录信息输出到ls.txt中\n   \n   * cat file1 > file 2——将文件1的内容覆盖到文件2 可以用于快速复制粘贴改名\n\n * ln——符号链接，类似于快捷方式\n   \n   * ln -s [原文件或目录] [软链接名]——给原文件创建一个软连接(快捷方式)\n   * ln -s /root /home/diana/myroot——在diana目录下创建一个root目录的软连接\n   * ln -s /etc/sysconfig/network-scripts/ifcfg-ens33 ~/ifcfg-ens33——将网卡配置文件软连接到家（root用户的目录是root）目录下\n\n * history——查看历史指令\n   \n   * history 10——显示历史的近10行指令\n   * !15——执行第历史命令的第15条命令\n\n# 拷贝移动命令\n\n * cp [-r] source dest——用于复制文件或目录\n   \n   * 说明\n     * -r 如果复制的是目录需要使用此选项，此时将复刻该目录下所有的子目录和文件\n     * \\cp 强制覆盖 不给出提示\n   * 实例\n     * cp 1.txt diana/ ——将1.txt文件复制到diana目录中\n     * cp 1.txt ./2.txt——将1.txt复制到当前目录下，并改名为2.txt\n     * cp -r diana/ ./diane——将diana目录及目录下所有文件复制到当前目录下的diane目录下\n       * 这就相当于将整个文件夹diana作为一个整体copy了过去\n       * copy完后目录结构为 diane/diana/file\n     * cp -r diana/* ./diane——将diana目录下所有文件复制到当前目录下的diane目录下\n       * 这就相当于将整个文件夹下的所有文件作为一个整体copy了过去\n       * copy完后目录结构为 diane/file\n\n * mv source dest——为文件或目录改名、或将文件或目录移动到其他位置\n   \n   * 实例\n     * mv 1.txt 2.txt——将1.txt重命名为2.txt\n     * mv 1.txt diana/——将文件1.txt移动到diana文件目录下\n     * mv 1.txt diana/2.txt——将1.txt移动到diana目录下，并改名为2.txt\n     * mv diana/ diane/——如果diane目录不存在，将diana目录改名为diane\n     * mv diana/ diane/——如果diane目录存在，将diana目录移动到diane目录下\n     * mv diana/* diane/——如果diane目录存在，将diane目录下的所有文件移动到diane目录下\n     * mv diana/* diane/——如果diane目录不存在，报错\n\n# 打包压缩命令\n\n * gzip/gunzip——压缩/解压缩 文件\n   \n   * gzip ls.txt——压缩文件 ls.txt——>ls.txt.gz\n   * gunzip ls.txt.gz\n\n * zip/unzip——压缩/解压缩 文件\n   \n   * zip -r 1.zip diana——将diana目录及目录下所有文件 压缩成1.zip\n   * unzip -d ./lb 1.zip——将1.zip 解压到当前的lb目录下\n\n * tar [-zcxvf] filename [files]——对文件进行 打包、解包、压缩、解压\n   \n   * 包文件后缀为 .tar表示只是完成了打包，并没有压缩\n   \n   * 包文件后缀为.tar.gz表示打包的同时还进行了压缩\n   \n   * 说明\n     \n     * -z: z代表的是 gzip，通过gzip命令处理文件，gzip可以对文件压缩**（gz）**或者解压\n     \n     * -c： c代表的是create，即创建新的包**（tar）**文件,与x互斥\n     \n     * -x： x代表的是extract，实现从包文件中还原文件，与c互斥\n     \n     * -v： v代表的是verbose，显示命令的执行过程\n     \n     * -f： f代表的是file, 用于指定打包文件的名称\n   \n   * 常用组合\n     \n     * -zcvf\n       \n       * tar -zcvf test.tar.gz diane——对diane文件目录进行压缩，打包，命名为 test.tar.gz\n     \n     * -cvf\n       \n       * tar -cvf test.tar diane——对diane文件目录进行打包，命名为 test.tar\n     \n     * -zxvf\n       \n       * tar -zxvf test.tar.gz——将test.tar.gz进行解压缩，解包操作，并放在当前目录下\n       * tar -zxvf test.tar.gz -c /user/local——将文件解压缩，解包后，放在 /user/local 目录中\n     \n     * -xvf\n       \n       * tar -xvf test.tar——将test.tar进行解包操作，并放在当前目录下\n\n# 文本编辑命令\n\n * vi/vim\n   \n   * vim 是一个文本编辑工具，在编辑文件时，可以对文本内容进行着色，比vi功能要好\n   \n   * 需要自己安装vim—— yum install vim\n   \n   * 使用vim命令编辑文件时，如果指定的文件存在则直接打开文件，如果不存在，则新建文件\n   \n   * vim在编辑文本时，分为3种模式，可以相互切换\n     \n     * > 命令模式，插入模式，底行模式\n   \n   * vim打开文件的方式\n     \n     * vim 文件路径——打开指定的文件\n     * vim +数字 文件路径——打开指定的文件，并且将光标移动到指定行\n     * vim +/关键词 文件路径——打开指定的文件，并且高亮显示关键词\n     * vim 文件1 文件2 文件3——同时打开多个文件\n   \n   * 命令模式（删除行，复制行，移动光标，粘贴等等）\n     \n     * 命令模式下可以查看文件内容、移动光标（上下左右箭头，gg（快速移动到开头），g（快速移动到末尾））\n     \n     * 通过vim命令打开文件后，默认进入命令模式\n     \n     * 另外两种模式需要首先进入命令模式，才能进入彼此\n       \n       * 底行模式——》命令模式——》插入模式\n       \n       * 插入模式——》命令模式——》底行模式\n   \n   * 插入模式（编辑）\n     \n     * 插入模式下，可以对文件内容进行编辑\n     * 在命令模式下按[i,a,o]任意一个键，可以进入插入模式。进入插入模式后，下方会出现[insert]字样\n     * 在插入模式下按下esc,回到命令模式\n   \n   * 底行模式（搜索，替换，保存，退出，撤销，高亮等等）\n     \n     * 底行模式下可以通过命令对文件内容进行查找、显示行号、退出等操作\n     * 在命令模式下按下[:,/]任意一个，可以进入底行模式\n     * 通过 /方式进入底行模式后，可以对文件内容进行查找 / name(name为你要查询的词)\n     * 通过 :方式进入底行模式后，可以输入 wq（保存并退出），q!(不保存退出)，set nu(显示行号)\n   \n   \n\n# 进程命令\n\n * ps——进程查看命令\n   \n   * ps -ef——查看当前运行的所有进程的详细信息\n   * ps -ef|grep tomcat——查看tomcat的进程信息\n     * |在linux中称为管道符，可以将前一个命令的结果输出给后一个命令作为输入\n   * ps -ef|grep xxx——查看xxx的进程信息\n\n * kill -9 id\n   \n   * -9表示强制删除\n   * -15表示正常关闭\n   * id为进程名称\n\n * nohup command[arg ...][&]——用于不挂断地运行指定命令，退出终端不会影响程序的运行\n   \n   * 说明\n     * command要执行的命令\n     * arg一些参数，可以指定输出文件\n     * &让命令在后台运行\n   * 实例\n     * nohub java -jar xxx.jar &>hello.log &\n     * &>hello.log将日志输出到hello.log\n     * & 后台运行\n\n# 防火墙命令\n\n * firewall-cmd --state——查看防火墙状态\n * systemctl stop firewalld——暂时关闭防火墙（重启后开启防火墙）\n * systemctl disable firewalld——永久关闭防火墙\n * systemctl start firewalld——开启防火墙\n   * systemctl start mysqld.service——开启mysql服务\n * firewall-cmd --zone=public --add-port=8080/tcp --permanent——开放8080端口\n   * 8080——指定要开放的端口\n   * /tcp——指定链接协议\n * firewall-cmd --zone=public --remove-port=8080/tcp --permanent——关闭8080端口\n * firewall-cmd --reload——立即生效\n   * 开放或者关闭端口后，需要执行这个，才能立即生效\n * firewall-cmd --zone=public --list-ports——查看开放的端口\n\n# 操作软件命令\n\n * yum list xxx——查找有没有当前软件可安装\n   \n   * yum list lrzsz——查找软件lrzsz\n\n * yum install vim——安装vim软件\n\n * rpm -qa——查询当前系统安装的所有软件\n   \n   * rpm -qa|mysql——查询当前系统安装的名称带mysql的软件\n\n * rpm -e --nodeps xxx——删除软件xxx\n\n * source /etc/profile——重新加载配置文件，立即生效\n\n# 查找命令\n\n * find dirname -option filename ——在指定目录下查找文件\n   \n   * -name——按名称\n     \n     * find .-name *.java——在当前目录及其子目录下，查找.java结尾的所有文件\n     * find /diana -name *.java——在/diana 目录及其子目录下，查找.java结尾的所有文件\n   \n   * -user——按用户名\n     \n     * find /opt -user root——查找opt目录下 root用户的文件\n   \n   * -size——按大小查找\n     \n     * find / -size +200m——查找大于200m的文件\n     * -200m——小于200m 200m——等于200m\n     * k,m,g\n\n * locate——快速定位文件路径\n   \n   利用事先建立好的数据库来快速定位文件，无需遍历整个文件系统，查询速度较快\n   \n   * update——定位前，需要先更新数据库\n   * locate ls.txt——搜索文件\n\n * which——可以查看某个指令在哪个目录下\n   \n   * which ls\n\n * grep [选项] word filename——从指定文件中查找指定的文本内容\n   \n   一般与管道符结合使用\n   \n   * -n——显示匹配行及行号\n   \n   * -i——忽略字母大小写\n   \n   * grep hello helloword.java——查找helloworld.java文件中出现hello字符串的位置\n   \n   * grep hello *.java——查找当前目录中所有.java结尾的文件中包含hello字符串的位置\n   \n   * cat ls.txt |grep hello——查看文件中 带hello的某几行\n   \n   * grep hello ls.txt——等价于上面的写法\n\n * netstat -tunlp——查看所有启动的服务\n   \n   * netstat -tunlp|grep mysql——查看mysql的启动服务\n\n * ps -ef——查看当前运行的所有进程的详细信息\n   \n   * ps -ef|grep tomcat——查看tomcat的进程信息\n\n# 时间命令\n\n * date指令\n   \n   * date——输出时间\n   * 输出——2022年 06月 05日 星期日 10:55:53 cst\n   * date +%y——输出年份\n   * date +%m——输出月份\n   * date +%d——输出日期\n   * date +%f——格式化输出时间（年月日）——等价于date +"%y-%m-%d"\n     * 输出——2022-06-05\n   * date +"%f %t"——格式化输出时间（年月日，时分秒）（必须加引号，表示不可分隔的整体）\n     * 等价于date +"%y-%m-%d %h:%m:%s"\n     * 输出——2022-06-05 11:02:53\n   * date -d "-1 day"——获取之前或者之后的某个时间（备份用）\n     * +表示之后，-表示之前\n     * hour-小时 day-天 year-年 month-月\n   * date -s "2022-06-05 10:10:10"\n     * 修改系统当前时间\n\n * cal指令\n   \n   * cal——直接输出当前月份的日历，相当于 cal -1\n   * cal -3——输出3个月的日历（前一个月，本月，后一个月）\n   * cal -y 2022——输出一年（2022）的日历\n\n# 管道命令\n\n管道符：|\n\n可以用于过滤，特殊，扩展处理，主要是起一个辅助作用\n\n * 过滤 |grep\n   * ls |grep g——过滤当前目录下，名字带g的\n   * |,以管道符作为分界线，前面的命令的输出作为后面命令的输入\n   * grep,主要用于过滤\n * 特殊案例\n   * cat 1.txt | less——等价于less 1.txt（多此一举）\n * 扩展处理\n   * ls | wc -l——统计某个目录下文档的总个数\n\n# 帮助命令\n\n * man\n   \n   manual，手册，（包含了linux中全部的命令手册，英文）\n   \n   * man 命令 ——进入手册\n   * 按q——退出手册\n\n# 其他命令\n\n * man 命令——man head——查看命令手册\n\n * ifconfig——查看ip配置\n\n * ip addr——查看ip地址\n\n * sudo apt install git ——安装git(sudo 是乌班图系统下的)\n\n * sudo apt install xxx ——安装xxx——没有东西会给出提示\n\n * du -sh *——查询占用空间\n\n * uname -a——查看系统版本\n\n * which java——查看java安装目录\n\n * wget +url——下载指定url的压缩包\n\n * 正常指令 > 文件路径 ——ll > ll.txt——将ll命令展示的东西，存储到文本中，而不显示在屏幕上\n   \n   ——覆盖输出，会覆盖掉原先的文件内容\n\n * 正常指令 >> 文件路径 ——ll > ll.txt——将ll命令展示的东西，存储到文本中，而不显示在屏幕上\n   \n     \t\t\t\t\t\t\t——追加输出，从原文件内容末尾继续添加\n   \n   \n   \n   # 高级指令\n\n# init\n\n运行级别\n\n * init0——关机\n * init3——命令行页面\n * init5——图形页面\n * init6——重启\n\n# hostname\n\n操作服务器的主机名(读取、设置)\n\n * /etc/hostname——文件可以指定主机名\n\n * hostname——表示输出完整的主机名\n   \n   * 输出——localhost.localdomain\n\n * hostname -f——表示输出当前主机名中的fqdn(全限定域名)\n   \n   * 输出——localhost\n\n# id\n\n查看一个用户的一些基本信息（包含用户id，用户组id，附加组id...）,该指令如果不指定用户则默认当前用户\n\n * id——默认显示当前执行该命令的用户的信息\n   * 输出——uid=0(root) gid=0(root) 组=0(root) 环境=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023\n * id 用户名——默认指定用户的基本信息\n\n# whoami\n\n显示当前登陆的用户名，一般用于shell脚本,用于获取当前操作的用户名\n\n * whoami\n   * 输出——root\n\n# ps （重点）\n\n进程查询指令\n\n * ps -ef\n\n * ps -ef|grep xxx——查看xxx的进程信息\n   \n   * -e—— 等价于-a,表示列出全部的进程\n   * -f——显示全部的列(显示全部字段)\n   * 执行结果\n   \n   \n   \n   * * uid: 该进程执行的用户id。\n     \n     * pid：进程id。\n     \n     * ppid：该进程的父级进程id，如果一个程序的父级进程找不到，改程序的进程称之为僵尸进程。\n     \n     * c：cpu的占用率，其形式是百分数。\n     \n     * stime：进程的启动时间。\n     \n     * tty：终端设备，发起该进程的设备识别符号。如果显示"?"则表示该进程并不是由终端设备发起。\n     \n     * time：进程的执行时间\n     \n     * cmd：该进程的名称或者对应的路径。\n\n# top（重点）\n\n查看服务器的进程占的资源\n\n * 进入命令： top (动态显示)\n\n * 退出命令：q\n\n * 快捷键：\n   \n   * m——表示将结果按照内存（mem）降序排列\n   * p——表示将结果按照cpu使用率降序排列\n   * 1——切换各个cpu的详细信息\n\n * 执行结果\n   \n   \n   \n   * pid: 进程id。\n   \n   * user：该进程对应的用户。\n   \n   * pr： 优先级。（越大越好）\n   \n   * virt： 虚拟内存 （进程申请的内存）\n   \n   * res： 常驻内存（进程总共使用的内存，包含了依赖使用的内存）\n   \n   * shr： 共享内存（依赖进程所使用的内存）\n   \n   * 计算一个进程实际使用的内存=常驻内存（res）— 共享内存（shr）\n   \n   * s：表示进程的状态(s表示睡眠，r表示运行)\n   \n   * %cpu： 表示cpu的占用百分比\n   \n   * %mem：表示内存的占用百分比\n   \n   * time+：进程执行的时间\n   \n   * command： 进程的名称或者路径\n\n# du -sh\n\n查看目录的真实大小\n\n * du-sh 目录路径\n   \n   * -s—— 只显示汇总的大小\n   \n   * -h——以高可读性的形式进行显示\n\n * 示例：du -sh java——392m java\n\n# find\n\n用于查找文件\n\n * find 路径范围 选项 参数值\n   \n   * name——按照文档名称进行搜索\n   \n   * type——按照文档的类型进行搜索\n     \n     * -表示文件（在使用find的时候需要使用f来替换）\n     * d表示文件夹\n\n * find / -name ls.txt——从根目录(包含其子文件夹)开始找，找到名字为ls.txt的文件\n   \n   * / ——根目录--路径范围\n   * -name——按照文档名称搜索--选项\n   * ls.txt——文档名--参数值\n\n * find /diane -name *.conf\n   \n   * 从/diane目录(包含其子文件夹)开始找，找到所有后缀为.conf的文件\n\n * find ./ -name *.conf\n   \n   * 从当前目录及其子目录下开始找，找到所有后缀为.conf的文件\n\n * find /diane -type f\n   \n   * 从/diane目录(包含其子文件夹)开始找，找到所有的文件\n\n * find /diane -type f|wc -l\n\n * 从/diane目录(包含其子文件夹)开始找，找到所有的文件,并将其作为管道的输入，统计一下文件的数量\n\n * find /diane -type d\n   \n   * 从/diane目录(包含其子文件夹)开始找，找到所有的文件夹\n   \n   # service\n\n用于控制一些软件的服务 启动/停止/重启\n\n * service 服务名 start/stop/restart\n * service networkmanager stop ——禁用网络管理\n * service network restart——重启网络服务\n\n# kill\n\n使用kill杀死进程\n\n * kill 进程pid\n * killall 进程名称\n * ps -ef|grep xxx ——通过ps命令查看进行pid\n\n# ifconfig\n\n获取网卡信息\n\n * ifconfig\n   \n   \n\n# reboot\n\n重新启动计算机\n\n * reboot——重新启动计算机\n * reboot -w——模拟重新启动计算机，但是不重启(只写关机与开机的日志信息)\n\n# shutdown\n\n关机\n\n * shutdown -h 1==shutdown\n   * 1分钟后关机\n * shutdown -h now "关机提示"\n   * 立即关机，并给出关机提示\n * shutdown -h 15:25 “关机提示”\n   * 设置关机时间，并给出关机提示\n * shutdown -c\n   * 取消定时关机命令\n * shutdown -r now\n   * 立即重启计算机\n\n其他关机命令\n\n * init 0\n\n * halt\n\n * poweroff\n\n * sync——将内存的数据同步到磁盘\n   \n   * 以免数据丢失，可以在关机前执行该指令\n   * 现在的关机指令，基本都在执行关机前，底层调用了这个命令\n\n# uptime\n\n输出计算机的持续在线时间（计算机从开机到现在运行的时间）\n\n * uptime\n   * 输出——09:40:31 up 5 min, 3 users, load average: 0.04, 0.08, 0.05\n\n# uname\n\n获取操作系统的信息\n\n * uname——获取操作系统的类型\n   * linux\n * uname -a——获取全部的系统信息**(类型，全部主机名，内核版本，发布时间，开源计划)**\n   * linux localhost.localdomain 3.10.0-1160.el7.x86_64 #1 smp mon oct 19 16:18:59 utc 2020 x86_64 x86_64 x86_64 gnu/linux\n\n# netstat\n\n查看网络的连接状态\n\n * netstat -tnlp\n   \n   \n   \n   * t——表示只列出tcp协议的连接\n   * n——表示将地址从字母组合转换成ip地址，将协议转换成端口号来显示\n   * l——表示过滤出“state(状态)”列中其值为listen(监听)的连接\n   * p——表示显示发起连接的进程pid和进程名称\n\n\n# 指令应用\n\n * 删除/diane下所有a开头的文件\n   \n   * rm -f /diane/a*\n\n * 系统重要文件需要备份，如何把/diane/java备份到/diana目录下\n   \n   * cp -r /diane/java /diana\n\n * 如何查看系统最后创建的3个账户\n   \n   * tail -3 /etc/passwd\n\n * 什么命令可以统计当前系统中一共有多少账户\n   \n   * wc -l /etc/passwd\n\n * 如何创建/diane/ll.txt文件\n   \n   * touch /diane/ll.txt\n\n * 如何通过vim编辑打开/diane/ll.txt\n   \n   * vim /diane/ll.txt\n\n * 如何查看/etc/passwd的头三行和尾三行\n   \n   * head -3 /etc/passwd\n   * tail -3 /etc/passwd\n\n * 如何在diane目录下**一次性(-p)**创建目录 text 1 2 3 4\n   \n   * mkdir -p test 1 2 3 4（在diane目录下）\n\n * 如何最快的返回当前账户的家目录\n   \n   * cd ~\n   * cd\n\n * 如果查看/etc所占的磁盘空间\n   \n   * du -sh /etc\n\n * 如何删除/diana下所有文件\n   \n   * rm -rf /diana/*\n\n\n# linux自有服务\n\n# 组管理和权限管理\n\n * 其它组——除文件的所有者和所在组的用户外，系统的其他用户都是文件的其他组\n\n\n\n * 文件所有者，及所有组\n   \n   * 用户lisi信息\n   \n   \n   \n   * lisi创建文件的信息\n     * 可以看出，第一个是文件所有者 lisi；第二个是文件所在组diana，默认用户所在组\n   \n   \n\n * chown 用户名 文件名—— 修改文件的所有者\n   \n   * -r——如果是目录，则使其下所有子文件或者目录递归生效\n   \n   * chown -r diana file——将file文件夹及其下所有文件的所有者改为diana\n   \n   * chown diana 1.txt——将1.txt的文件所有者改成diana\n   \n   * 修改前\n   \n   * 修改后\n\n * chgrp 组名 文件名——修改文件所在组\n   \n   * -r——如果是目录，则使其下所有子文件或者目录递归生效\n   \n   * 继续前文修改\n   \n   * chgrp liangbing 1.txt——将1.txt文件组修改至liangbing\n   \n   * 再次修改后\n   \n   # 用户管理\n\n * 每个用户有唯一的用户名\n\n * 下面所有的用户操作的命令，除了修改密码，其他均只有root用户有权限操作\n\n * 重要三个文件\n   \n   * /etc/passwd——存储用户的关键信息\n   \n   * /etc/group——存储用户组的关键信息\n   \n   * /etc/shadow——存储用户的密码信息\n\n * useradd 选项 用户名——添加用户\n   \n   * 常用选项（选项可以为空）\n     \n     * -g——表示指定用户的用户主组，选项的值可以是用户组的id，也可以是组名\n     * -g——表示指定用户的用户附加组，选项的值可以是用户组的id，也可以是组名\n     * -u——uid 用户的id(用户标识符)，系统默认会从500以后按顺序分配uid，如果不想使用系统分配的，可以通过该选项自定义\n   \n   * \n   \n   * \n   \n   * 添加用户(无选项)——会自动生成 用户主组，用户附加组，uid\n     \n     * useradd diane——添加用户 diane\n       * uid——1000\n       * 用户主组——1000\n       * 用户附加组——1000\n     * useradd diana——添加用户 diana\n       * uid——1001\n       * 用户主组——1001\n       * 用户附加组——1001\n   \n   * 添加用户(指定选项)\n     \n     * useradd -g 1000 -g 1001 -u 666 liangbing——添加指定用户liangbing\n       \n       * uid——666——passwd\n       * 用户主组——1000——passwd\n       * 用户附加组——1001——group\n       * 用户名—— liangbing\n   \n   * 验证创建用户是否成功\n     \n     * 验证/etc/passwd的最后一行，查看是否有新创建的用户信息\n     * 验证是否存在home目录(在centos下创建好用户之后随之产生一个同名的家目录)\n   \n   * /etc/passwd补充\n     \n     \n     \n     * 用户名：密码位：用户id：用户组id：注释：家目录：解释器shell\n     * 用户名： 创建新用户名称，后期登陆需要输入\n     * 密码：一般都是x，表示密码的占位，并不是真正的密码\n     * 用户id：用户的标识符(uid)\n     * 用户组id：该用户所属的主组id\n     * 注释：解释该用户是做什么的\n     * 家目录：用户登陆进入系统后默认的位置\n     * 解释器shell：等待用户进入系统之后，用户输入指令之后，该解释器会收集用户输入的指令，传递给内核处理\n\n * usermod 选项 用户名——修改用户\n   \n   * 常用选项\n     * -g,-g,-u——同上\n     * -l——修改用户名(小写的l)\n   * usermod -l lisi zhangsan——将zhangsan用户名修改为lisi\n   * usermod -g wudang zwj——将zwj的组修改至wudang\n\n * passwd 用户名——设置密码\n   \n   * linux不允许没有密码的用户登陆到系统，因此没有密码的用户处于锁定状态，不允许登陆\n   \n   * su 用户名——切换用户名\n     \n     * 从root往普通用户切换不需要密码，但是反之需要root密码\n     * 切换用户前后的工作路径不变\n     * 普通用户没有办法访问root用户家目录，但是反之可以\n\n * userdel 选项 用户名\n   \n   * 选项 -r——表示删除用户的同时，删除其家目录，谨慎使用\n   * 不带选项的话，表示删除用户的同时，保留其家目录，一般建议保留\n   * 可以直接删除没有登陆的用户\n   * 如果用户在登陆，需要先杀死其进程(kill)，然后在删除用户\n\n# 用户组管理\n\n * groupadd 选项 用户名组\n   \n   * 常用选项\n     * -g——类似于用户添加里面的-u，表示自己设置一个自定义的用户组id数字，如果自己不指定，系统会自动生成\n   * groupadd liang——创建用户组 liang\n\n * groupmod 选项 用户组名\n   \n   * 常用选项\n     * -g\n     * -n——类似于用户修改里面的-l,表示设置新的用户组的名称\n   * groupmod -g 500 -n bing liang——将用户组liang 修改名称为bing，修改组号为500\n\n * groupdel 用户组名\n   \n   * groupdel bing——删除用户组 bing\n     * 组内没有人，可以直接删除\n   * group lisi——删除用户组lisi\n     * lisi的组号是用户zhangsan的主组，不允许直接删除用户组\n     * 需要先将组内用户移走或者删除，才可以删除用户组\n\n# 权限\n\n * - rw- r-- r--.\n\n * d rwx r-x r-x.\n\n * l rwx rwx rwx.\n\n * b rw- rw- ---.\n\n * c rw- rw- ---.\n\n * 0--9位说明\n   \n   * 第0位表示文件类型（d,-l,c,b）\n     \n     * l——表示链接，相当于windows的快捷方式\n     \n     * d——表示目录，相当于windows的文件夹\n     \n     * c——表示字符设备文件，例如，鼠标，键盘，\n     \n     * b——表示块设备，比如 硬盘\n     \n     * -——普通文件\n   \n   * 1-3位——确定文件所有者拥有该文件的权限——user\n   \n   * 4-6位——确定文件所在组（同用户组）拥有该文件的权限——group\n   \n   * 7-9位——确定其他用户拥有该文件的权限——other\n\n * r,w,x,-——文件——r=4,w=2,x=1——rwx=4+2+1=7\n   \n   * r——代表可读，可以读取，查看\n   \n   * w——代表可写，可以修改，但是不代表可以删除该文件；\n   \n   * ——删除一个文件的前提条件是对该文件所在的目录有写权限，才能删除该文件\n   \n   * x——代表可执行，文件可以被执行\n\n * r,w,x,-——目录\n   \n   * r——代表可读，可以ls查看目录内容\n     * 当目录没有r权限时，其他用户不能通过ls查看目录内文件\n     * 但是若其他用户对目录内文件有写权限，虽然无法通过ls列出该目录下文件，仍然可以直接对写该文件进行修改\n   * w——代表可写，可以修改，对目录内部文件创建+删除，重命名目录；\n   * x——代表可执行，可以进入该目录——cd目录\n\n * ll解读\n   \n   * drwxr-xr-x. 3 root root 16 7月 19 21:04 2\n     * 3 ——文件：硬连接数 目录：一层子目录内的文件数\n     * 第一个root ——表示 文件所有者\n     * 第二个root——表示文件所在组\n     * 16——文件大小（字节）\n     * 时间表示文件的修改时间\n     * 2 ——表示的文件的名称\n\n * chmod——修改文件或目录的权限（绿色表示可执行文件）\n   \n   * 第一种方式：+,-,= 变更权限\n     \n     u:所有者，g:所有组，o:其他人 ，a:所有人(u,g,o的总和)\n     \n     * chmod u=rwx,g=rw,o=x abc——给abc文件直接赋予权限\n     * chmod o+w abc ——给其他人 增加写权限\n     * chmod a-x abc——给所有人 减少执行权限\n     * chmod a-x-w abc——给所有人 减少执行权限,减少写权限\n   \n   * ==第二种：通过数字变更权限==\n     \n     r=4,w=2,x=1——rwx=4+2+1=7\n     \n     wx=3，rx=5，rw=6，rwx=7\n     \n     * chmod u=rwx,g=rw,o=x abc——给abc文件直接赋予权限\n     * chmod 761 abc——与上等价\n\n# 权限实践\n\n * 实践1——警察，土匪\n   \n   * 警察组 polic ， 土匪组 bandit\n   \n   * jack，jerry 警察\n   \n   * xh，xq 土匪\n   \n   * 创建组\n     \n     * root:groupadd polic,groupadd bandit\n   \n   * 创建用户\n     \n     * root:useradd -g polic jack useradd -g polic jerry\n       * useradd -g bandit xh useradd -g bandit xq\n   \n   * jack创建一个文件，自己可以读写，本组人可以读，其他组没有权限\n     \n     * jack：touch jack.txt——在jack家目录下\n       * chmod 640 jack.txt\n   \n   * jack修改文件，让其他组可以读，本组人可以读写\n     \n     * jack：chmod 664 jack.txt\n   \n   * xh投靠警察，看看是否可以读写\n     \n     * root: usermod -g polic xh\n     \n     * xh 无法进入jack家目录，因此无法对文件进行修改\n       \n       * 如果要对目录内的文件进行操作，需要有该目录的相应权限\n     \n     * root: chmod 777 /home/jack——开放jack家目录的所有权限\n     \n     * xh:cd /home/jack,vim jack.txt——可以成功读写\n     \n     * xq: cd /home/jack,vim jack.txt——只读文件，不可写入\n       \n       * wq!——强制保存，导致jack.txt的所有者修改为xq\n\n * 实践2——西游记\n   \n   * 建立两个组（神仙-sx，妖怪-yg）\n     \n     * root: groupadd sx groupadd yg\n   \n   * 建立4个用户（wk，bj，--yg）（ts，ss，--sx）\n     \n     * root: useradd -g sx ts useradd -g sx ss\n     * root: useradd -g yg wk useradd -g yg bj\n   \n   * 用wk建立一个文件（monkey.java 该文件写入 i am monkey）\n     \n     这里不考虑目录影响，该目录对所有人开放所有权限\n     \n     * wk:touch mokey.java\n     * wk:echo "i am mokey" >> mokey.java\n   \n   * 给bj一个 r，w的权限,ts，ss对该文件没有权限\n     \n     * wk:chmod 760 mokey.java\n   \n   * 八戒修改monkey.java ，加入一句话（i am pig）\n     \n     * bj:echo "i am pig" >> mokey.java\n   \n   * 把ss放入妖怪组\n     \n     * root: usermod -g yg ss\n   \n   * 让ss修改该文件 monkey.java 加入一句话 （i am person）\n     \n     * ss:echo "i am pig" >> mokey.java\n\n\n# 任务调度\n\n# 循环任务调度\n\n\n\n设置任务调度文件 ：/etc/crontab\n\n * 1. 设置个人任务调度——执行crontab -e命令\n * 2. 输入任务到调度文件（有空格）\n   * */1 * * * * ls -l /etc/ > /tmp/to.txt\n   * 每小时的每分钟执行一次 ls -l /etc/ > /tmp/to.txt\n   * 占位符：\n   * 特殊符号：![](/assets/后端/linux/任务调度 特特殊符号.jpg)\n   * 例子 ![](/assets/后端/linux/任务调度 例子.jpg)\n\n * 定时调度shell脚本，在shell脚本中放入多个调度任务\n   \n   * 一分钟一次，将当前日期和日历追加到/diane/time.txt中\n     \n     1. 写脚本\n        \n        * vim my.sh\n        * 输入：date >> /diane/time.txt\n        * 输入：cal >> /diane/time.txt\n     \n     2. 给脚本增加执行权限\n        \n        * chmod u+x my.sh\n     \n     3. 定时调度脚本\n        \n        * crontab -e\n        * 输入：*/1 * * * * my.sh\n\n * crontab -r——终止任务调度——删除了写入的任务调度\n\n * crontab -l——列出当前有哪些任务调度\n\n * service crond restart——重启任务调度\n\n# 一次性任务调度\n\n>  1. at 命令是一次性定时计划任务，at 的守护进程 atd 会以后台模式运行，检测作业队列来运行\n>  2. 默认情况下，atd 守护进程每60s检查一次作业队列，有作业时，会检查作业运行时间，如果时间与当前时间匹配，则运行此作业。\n>  3. at 命令 是一次性定时计划任务，执行完一个任务后 不在执行此任务\n>  4. 在使用 at命令的时候，一定要保证 atd 进程的启动\n\n * at [选项] [时间]\n   \n   * 选项\n     \n     ![](/assets/后端/linux/at 选项.jpg)\n   \n   * 时间\n     \n     ![](/assets/后端/linux/at 时间.jpg)\n   \n   * ctrl + d结束at命令的输入\n\n * at 定时任务 案例\n   \n   * 输入过程\n     \n     * 输入错误，按住 ctrl+backspace来删除\n     * 按ctrl +u 来进行撤销\n     * 输入报错：can\'t open /var/run/atd.pid to signal atd. no atd running?\n       * 解决方法：systemctl restart atd\n   \n   * 2天后的下午5点执行 /bin/ls/home\n     \n     * at 5pm + 2days\n     * 输入：/bin/ls /home\n   \n   * atq 命令来查看系统中没有执行的工作任务\n     \n     * atq\n   \n   * 明天17点钟，输出时间到指定文件内，如 /root/date100.log\n     \n     * at 17:00 +1days\n     * 输入：date >> /root/date100.log\n   \n   * 2分钟后，输出时间到指定文件内，如/root/date200.log\n     \n     * at now + 2minutes\n   \n   * 删除已经设置的任务， atrm编号\n     \n     * atrm 5——删除5号任务\n     * atrm 4 5 6——删除 4 5 6 号任务\n\n\n# linux分区\n\n# 目录\n\nlinux一切皆文件\n\n\n\n\n\n\n\n# 磁盘分区机制\n\n * 挂载机制——将硬盘的某个分区跟文件链接起来\n\n\n\n * 硬盘情况\n   \n   * scsi硬盘，标识为sdx~——x=a,b,c……——~=1,2,3……\n   * sda1---sda2---sda3---sda4\n   \n   \n\n * 查看所有设备的挂载情况\n\n * lsblk--lsblk -f\n\n\n\n * 对新增硬盘分区\n   \n   * 新增一块硬盘，需要重启\n   \n   * 分区命令——fdisk /dev/sdb—(sdb是新增硬盘名称)\n     \n     * m——显示命令\n     * p——显示磁盘分区\n     * n——新增分区\n     * d——删除分区\n     * w——写入并退出\n     * 开始分区后，输入n，新增分区，然后选择p，分区类型为主分区，两次回车选择默认，最后输入w保存分区并退出，若不保存，则输入q\n   \n   * 格式化磁盘（获得了一个uuid）——mkfs -t ext4 /dev/sdb1—(ext4是文件格式，sdb1是新增硬盘分区名称)\n   \n   * 挂载——mount /dev/sdb1 /newdisk/-(sdb1是新增硬盘分区名称,newdisk是用来挂载的新建文件夹)\n     \n     * 重启后 挂载关系会失效\n   \n   * 永久挂载\n     \n     * vim /etc/fstab\n     * /dev/sdb1 /newdisk ext4 defaults 0 0\n   \n   * 取消挂载——umount /dev/sdb1或者umount /newdisk\n\n# 磁盘实用指令\n\n * df -h——查询系统磁盘使用情况\n\n * du [选项] /目录——查询指定目录的磁盘占用情况\n   \n   * -s——指定目录大小汇总（只显示目录的总用量，显示一个）\n   * -h——带计量单位\n   * -a——含文件（默认不含文件）\n   * --max-depth=1——子目录深度（默认是最深处）\n   * -c——列出明细的同时，增加汇总值（没啥用）\n\n * 统计/opt文件夹下文件的个数\n   \n   * ll /opt | grep "^-" | wc -l\n   * "^-"——表示以-开头的，正则表达式，即普通文件\n\n * 统计/opt文件夹下目录的个数\n   \n   * ll /opt | grep "^d" | wc -l\n   * "^d"——表示以d开头的，正则表达式，即目录\n\n * 统计/opt文件夹下文件的个数，包括子文件里的\n   \n   * ll -r /opt | grep "^-" | wc -l\n   * -r——表示递归，深入子文件内\n\n * 统计/opt文件夹下目录的个数，包括子文件里的\n   \n   * ll -r /opt | grep "^d" | wc -l\n\n * 以树形显示目录结构\n   \n   * tree /opt\n   * tree -l 1 /opt——只显示1层\n\n\n# 网络配置\n\n# 网络基础\n\n * 三种模式\n\n\n\n * 查看ip地址\n   \n   * window下\n     * ipconfig\n     * 找到vmnet8\n     * \n   * linux下\n     * ifconfig\n     * 找到ens33\n     * \n\n * 网络链接测试\n   \n   * ping ip=ping 192.168.159.1\n   * ping 网站=ping www.baidu.com\n\n * nat模式（共享主机ip）\n\n\n\n# 网卡设置\n\n * 网络问题(当连不上网时，可以这样操作一下)\n   \n   * service networkmanager stop ——禁用网络管理\n   * service network restart——重启网络服务\n\n * 和git命令，解决合并冲突时差不多\n\n * 网卡的配置文件太深，可以通过创建软连接的方式\n   \n   * ln -s 原路径 现路径\n   * ln -s /etc/sysconfig/network-scripts/ifcfg-ens33 ~/ifcfg-ens33——将网卡配置文件软连接到家目录下\n\n\n\n# 远程链接\n\n安装ssh连接工具\n\n * ssh，建立在应用层基础上的安全协议\n * 通过ssh连接工具，可以实现从本地连接到远程的linux服务器\n * 这里安装ssh连接工具，是为了最大限度的模拟企业工作\n * 链接操作\n   * 点击左上角文件夹，点开，在点击左上角文件夹，选择ssh链接\n   * 输入名称（任意），主机输入要链接的ip地址\n   * 输入用户名和密码\n   * 再次进入，有个快速连接，里面保存了之前的链接信息，直接点击即可\n\n# 主机名和hosts映射\n\n * hostname\n   \n   操作服务器的主机名(读取、设置)\n   \n   * /etc/hostname——文件可以指定主机名\n   \n   * hostname——表示输出完整的主机名\n     \n     * 输出——localhost.localdomain\n     * 修改后——diana\n   \n   * hostname -f——表示输出当前主机名中的fqdn(全限定域名)\n     \n     * 输出——localhost\n\n * 设置hosts映射\n   \n   通过ping 主机名找到linux系统\n   \n   * windows\n     * c:\\windows\\system32\\drivers\\etc\\hosts——在该文件下写入映射关系\n     * 192.168.159.100 diana——(ip 主机名)\n     * \n   * linux\n     * etc/hosts——在该文件下写入映射关系\n     * 192.168.137.1 diana-pc——(ip windows 主机名)\n\n * 主机名解析机制分析（hosts、dns）\n   \n   * hosts 是一个文本文件，用来记录 ip和 hostname的映射关系\n   * dns 是域名系统，是互联网上作为域名和ip地址相互映射的一个分布式数据库\n   * dns 实例\n\n\n# linux软件安装\n\n# 软件安装方式\n\n * 二进制发布包安装\n   * 软件已经针对具体平台编译打包发布，只要解压，修改配置即可\n * rpm安装\n   * 软件已经按照redhat的包管理规范进行打包，使用rpm命令进行安装，不能自行解决库依赖问题\n * yum安装\n   * 一种在线安装软件方式，本质上还是rpm安装，自动下载安装包并安装，安装过程中自动解决库依赖问题\n * 源码编译安装\n   * 软件以源码工程的形式发布，需要自己编译打包\n\n# jdk\n\njava_home=/usr/local/jdk1.8.0_361\n\npath=$java_home/bin:$path\n\n\n\n# tomcat\n\n * 安装\n\n\n\n * 启动验证\n\n\n\n * 防火墙命令\n   * 在window下，访问tomcat需要开放 8080端口\n\n\n\n# mysql\n\n * 安装前检测\n\n\n\n * 安装网站介绍\n   * 安装报错\n     * 失败的软件包是：mysql-community-libs-compat-5.7.37-1.el7.x86_64 gpg 密钥配置为：file:///etc/pki/rpm-gpg/rpm-gpg-key-mysql\n   * 解决方案\n     * 去这个文件下面 /etc/yum.repos.d/mysql-community.repo 修改对应安装版本的gpgcheck=0即可,默认值为1\n * 安装后启动，并查看\n\n\n\n * 设置mysql密码\n   \n   \n\n * 从远程链接mysql服务器\n   \n   * 开放3306端口\n     \n     * firewall-cmd --zone=public --add-port=3306/tcp --permanent\n   \n   * 允许远程链接(在mysql下运行)\n     \n     * use mysql;\n     * update user set host=\'%\' where user=\'root\';\n     * flush privileges;\n\n# lrzsz\n\n * yum list lrzsz——查看可安装软件\n * yum install lrzsz.x86_64——安装软件\n * 使用\n   * rz 弹出对话框，选择文件，上传文件\n\n# redis\n\n * 安装\n\n\n\n * 使用\n   * src 目录下\n     * redis-server——启动服务\n     * redis-cli -h localhost -p 6379 -a 123456 —— -h选择连接地方，-p选择连接端口，-a选择密码\n       * auth 123456——也可以启动服务之后，显示输入密码\n   * src 上层目录下，有个redis.conf配置文件\n     * src/redis-server ./redis.conf——修改配置文件后，启动服务\n     * 修改配置\n       * requirepass 123456 ——输入密码登陆\n       * daemonize yes——后台运行\n       * # bind 127.0.0.1——注释掉这句话，表示允许远程访问\n\n# nginx\n\n * 下载安装\n   \n   \n\n * 目录结构\n\n\n\n * nginx命令(sbin/nginx)\n   \n   * ./nginx -v——查看版本号\n   * ./nginx -t——检测配置文件（/conf/nginx.conf）是否有问题\n   * ./nginx——启动nginx服务（master,worker）默认两个进程，，（默认端口号 80）\n   * ./nginx -s stop——关闭nginx服务\n   * ./nginx -s reload——重新加载配置文件\n\n * nginx配置文件结构\n   \n   \n   \n   * 全局块\n   * events块\n   * http块\n\n * nginx具体应用\n   \n   * 部署静态资源\n     * 标红的地方是 语法， 黑色的地方是可以修改的地方\n     * 可以配置多个server，监听多个端口\n   \n   \n   \n   * 反向代理概念\n     * 正向代理概念 ———— 翻墙，vpn\n     * 反向代理——用户无需感知，用户访问反向代理服务器，即可得到web服务器上的内容，但是用户不知道它访问的是反向代理服务器.\n   \n   \n   \n   \n   \n   * 反向代理配置\n     \n     * 配置\n       \n       server {\n           listen       81;\n           server_name  localhost;\n           location / {\n           \tproxy_pass http://169.254.176.155; #反向代理，将请求转发到指定服务      \n           }\n       }\n       \n     \n     * 效果\n       \n       访问http://192.168.159.100:81/,nginx服务器的端口\n       \n       可以访问到http://169.254.176.155/,window 下 tomcat服务器端口\n   \n   * 负载均衡概念\n   \n   \n   \n   * 负载均衡配置\n     \n     * 理论上来说，里面配置的应该是多台服务器，但是咱们没有多余的服务器，就用不同的端口号来代替\n     * weight为权重，权重越多，访问机会越大\n     \n     upstream targetserver{  #upstream指令可以定义一组服务器\n     \tserver 169.254.176.155:80 weight=100;\n     \tserver 169.254.176.155:8080 weight=50;  #只能配置到端口，后面不能跟路径了   \n     }\n     server{\n     \tlisten  82;\n     \tserver_name localhost;\n     \tlocation / {\n     \t\tproxy_pass http://targetserver;\n     \t}\n     }\n     \n     \n     * 负载均衡测量\n     \n     \n\n\n# linux项目部署\n\n# 手工部署项目（步骤）\n\n * 将web项目打jar包，并传到llinux中\n\n * 在linux中运行jar包\n   \n   * java -jar xxx.jar\n\n * 检测防火墙，确保8080端口对外开放\n   \n   * firewall-cmd --zone=public --list-ports\n\n * 在浏览器中访问192.168.159.131:8080/host\n\n * 改为后台运行springboot程序，并将日志输出到日志文件\n   \n   * nohup command[arg ...][&]——用于不挂断地运行指定命令，退出终端不会影响程序的运行\n     * 说明\n       * command要执行的命令\n       * arg一些参数，可以指定输出文件\n       * &让命令在后台运行\n     * 实例\n       * nohup java -jar xxx.jar &>hello.log &\n       * &>hello.log将日志输出到hello.log\n       * & 后台运行\n\n * 停止服务\n   \n   * ps -ef|grep java——查找java进程——39390\n   \n   * kill -9 -39390——杀死java对应进程\n\n# shell脚本自动部署项目\n\n\n\n * 安装git\n   \n   * yum list git——查看git可安装版本——git.x86_64\n   * yum install git.x86_64——安装git\n     * git config --global user.name "diana"——设置用户名\n     * git config --global user.email "diana_liangbing@126.com"——设置邮箱\n   * git clone https://gitee.com/diana-liangbing/hellogit.git——克隆项目\n\n * 安装maven\n   \n   \n\n * 编写shell脚本\n   \n   #!/bin/sh\n   echo ==============================\n   echo 自动化部署脚本启动\n   echo ==============================\n   \n   echo 停止原来运行的工程\n   #定义变量 方便查询进程\n   app_name=hellogit\n   \n   #查找包含hellogit的进程，并将其停掉\n   tpid=`ps -ef|grep $app_name|grep -v grep|grep -v kill|awk \'{print $2}\'`\n   if [ ${tpid} ];then\n           echo \'stop process...\'\n           kill -15 $tpid\n   fi\n   sleep 2\n   #再次查找进程，如果还存在，强制杀死，否则输出 停止成功\n   tpid=`ps -ef|grep $app_name|grep -v grep|grep -v kill|awk \'{print $2}\'`\n   if [ ${tpid} ];then\n           echo \'kill process!\'\n           kill -9 $tpid\n   else\n           echo \'stop success!\'\n   fi\n   \n   echo 准备从git仓库拉取最新代码\n   #进入存放项目的目录\n   cd /diane/java-project/project/hellogit\n   \n   echo 开始从git仓库拉取最新代码\n   git pull\n   echo 代码拉取完成\n   \n   echo 开始打包\n   output=`mvn clean package -dmaven.test.skip=true`   #clean，package，并跳过测试\n   \n   cd target\n   \n   echo 启动项目\n   #后台启动项目，并将日志输出到当前目录下的hellolinux.log中\n   nohup java -jar hello-linux-0.0.1-snapshot.jar &>hellolinux.log &   \n   echo 项目启动完成\n   \n   \n\n * 为用户授予执行shell脚本权限\n\n\n\n\n\n * 执行shell脚本\n   * ./bootstart.sh——直接打开就是执行\n * 设置静态ip\n   * static 没有引号\n\n',charsets:{cjk:!0}},{title:"Redis篇",frontmatter:{autoSort:50,title:"Redis篇",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/edb686/",categories:["面试"],tags:["知识","面试","Redis"]},regularPath:"/04.%E9%9D%A2%E8%AF%95/10.Redis.html",relativePath:"04.面试/10.Redis.md",key:"v-3a79afdc",path:"/pages/edb686/",headers:[{level:2,title:"Redis与Memcache的区别？",slug:"redis与memcache的区别",normalizedTitle:"redis与memcache的区别？",charIndex:2},{level:2,title:"Redis的单线程问题",slug:"redis的单线程问题",normalizedTitle:"redis的单线程问题",charIndex:353},{level:2,title:"Redis的持久化方案由哪些？",slug:"redis的持久化方案由哪些",normalizedTitle:"redis的持久化方案由哪些？",charIndex:603},{level:2,title:"Redis的集群方式有哪些？",slug:"redis的集群方式有哪些",normalizedTitle:"redis的集群方式有哪些？",charIndex:2958},{level:2,title:"Redis的常用数据类型有哪些？",slug:"redis的常用数据类型有哪些",normalizedTitle:"redis的常用数据类型有哪些？",charIndex:3904},{level:2,title:"聊一下Redis事务机制",slug:"聊一下redis事务机制",normalizedTitle:"聊一下redis事务机制",charIndex:4099},{level:2,title:"Redis的Key过期策略",slug:"redis的key过期策略",normalizedTitle:"redis的key过期策略",charIndex:5517},{level:3,title:"参考资料：",slug:"参考资料",normalizedTitle:"参考资料：",charIndex:5535},{level:3,title:"面试话术：",slug:"面试话术",normalizedTitle:"面试话术：",charIndex:391},{level:2,title:"Redis在项目中的哪些地方有用到?",slug:"redis在项目中的哪些地方有用到",normalizedTitle:"redis在项目中的哪些地方有用到?",charIndex:7691},{level:2,title:"Redis的缓存击穿、缓存雪崩、缓存穿透",slug:"redis的缓存击穿、缓存雪崩、缓存穿透",normalizedTitle:"redis的缓存击穿、缓存雪崩、缓存穿透",charIndex:8096},{level:3,title:"1）缓存穿透",slug:"_1-缓存穿透",normalizedTitle:"1）缓存穿透",charIndex:8121},{level:3,title:"2）缓存击穿",slug:"_2-缓存击穿",normalizedTitle:"2）缓存击穿",charIndex:8908},{level:3,title:"3）缓存雪崩",slug:"_3-缓存雪崩",normalizedTitle:"3）缓存雪崩",charIndex:9580},{level:2,title:"缓存冷热数据分离",slug:"缓存冷热数据分离",normalizedTitle:"缓存冷热数据分离",charIndex:9948},{level:2,title:"Redis实现分布式锁",slug:"redis实现分布式锁",normalizedTitle:"redis实现分布式锁",charIndex:10451},{level:3,title:"1）最基本的分布式锁：",slug:"_1-最基本的分布式锁",normalizedTitle:"1）最基本的分布式锁：",charIndex:10593},{level:3,title:"2）可重入分布式锁",slug:"_2-可重入分布式锁",normalizedTitle:"2）可重入分布式锁",charIndex:11107},{level:3,title:"3）高可用的锁",slug:"_3-高可用的锁",normalizedTitle:"3）高可用的锁",charIndex:12790},{level:2,title:"如何实现数据库与缓存数据一致？",slug:"如何实现数据库与缓存数据一致",normalizedTitle:"如何实现数据库与缓存数据一致？",charIndex:13761}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"Redis与Memcache的区别？ Redis的单线程问题 Redis的持久化方案由哪些？ Redis的集群方式有哪些？ Redis的常用数据类型有哪些？ 聊一下Redis事务机制 Redis的Key过期策略 参考资料： 面试话术： Redis在项目中的哪些地方有用到? Redis的缓存击穿、缓存雪崩、缓存穿透 1）缓存穿透 2）缓存击穿 3）缓存雪崩 缓存冷热数据分离 Redis实现分布式锁 1）最基本的分布式锁： 2）可重入分布式锁 3）高可用的锁 如何实现数据库与缓存数据一致？",content:"# Redis与Memcache的区别？\n\n * redis支持更丰富的数据类型（支持更复杂的应用场景）：Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。memcache支持简单的数据类型，String。\n * Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而Memecache把数据全部存在内存之中。\n * 集群模式：memcached没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；但是 redis 目前是原生支持 cluster 模式的.\n * Redis使用单线程：Memcached是多线程，非阻塞IO复用的网络模型；Redis使用单线程的多路 IO 复用模型。\n\n\n\n\n# Redis的单线程问题\n\n面试官：Redis采用单线程，如何保证高并发？\n\n面试话术：\n\nRedis快的主要原因是：\n\n 1. 完全基于内存\n 2. 数据结构简单，对数据操作也简单\n 3. 使用多路 I/O 复用模型，充分利用CPU资源\n\n面试官：这样做的好处是什么？\n\n面试话术：\n\n单线程优势有下面几点：\n\n * 代码更清晰，处理逻辑更简单\n * 不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为锁而导致的性能消耗\n * 不存在多进程或者多线程导致的CPU切换，充分利用CPU资源\n\n\n# Redis的持久化方案由哪些？\n\n相关资料：\n\n1）RDB 持久化\n\nRDB持久化可以使用save或bgsave，为了不阻塞主进程业务，一般都使用bgsave，流程：\n\n * Redis 进程会 fork 出一个子进程（与父进程内存数据一致）。\n * 父进程继续处理客户端请求命令\n * 由子进程将内存中的所有数据写入到一个临时的 RDB 文件中。\n * 完成写入操作之后，旧的 RDB 文件会被新的 RDB 文件替换掉。\n\n下面是一些和 RDB 持久化相关的配置：\n\n * save 60 10000：如果在 60 秒内有 10000 个 key 发生改变，那就执行 RDB 持久化。\n * stop-writes-on-bgsave-error yes：如果 Redis 执行 RDB 持久化失败（常见于操作系统内存不足），那么 Redis 将不再接受 client 写入数据的请求。\n * rdbcompression yes：当生成 RDB 文件时，同时进行压缩。\n * dbfilename dump.rdb：将 RDB 文件命名为 dump.rdb。\n * dir /var/lib/redis：将 RDB 文件保存在/var/lib/redis目录下。\n\n当然在实践中，我们通常会将stop-writes-on-bgsave-error设置为false，同时让监控系统在 Redis 执行 RDB 持久化失败时发送告警，以便人工介入解决，而不是粗暴地拒绝 client 的写入请求。\n\nRDB持久化的优点：\n\n * RDB持久化文件小，Redis数据恢复时速度快\n * 子进程不影响父进程，父进程可以持续处理客户端命令\n * 子进程fork时采用copy-on-write方式，大多数情况下，没有太多的内存消耗，效率比较好。\n\nRDB 持久化的缺点：\n\n * 子进程fork时采用copy-on-write方式，如果Redis此时写操作较多，可能导致额外的内存占用，甚至内存溢出\n * RDB文件压缩会减小文件体积，但通过时会对CPU有额外的消耗\n * 如果业务场景很看重数据的持久性 (durability)，那么不应该采用 RDB 持久化。譬如说，如果 Redis 每 5 分钟执行一次 RDB 持久化，要是 Redis 意外奔溃了，那么最多会丢失 5 分钟的数据。\n\n2）AOF 持久化\n\n可以使用appendonly yes配置项来开启 AOF 持久化。Redis 执行 AOF 持久化时，会将接收到的写命令追加到 AOF 文件的末尾，因此 Redis 只要对 AOF 文件中的命令进行回放，就可以将数据库还原到原先的状态。 　　与 RDB 持久化相比，AOF 持久化的一个明显优势就是，它可以提高数据的持久性 (durability)。因为在 AOF 模式下，Redis 每次接收到 client 的写命令，就会将命令write()到 AOF 文件末尾。 　　然而，在 Linux 中，将数据write()到文件后，数据并不会立即刷新到磁盘，而会先暂存在 OS 的文件系统缓冲区。在合适的时机，OS 才会将缓冲区的数据刷新到磁盘（如果需要将文件内容刷新到磁盘，可以调用fsync()或fdatasync()）。 　　通过appendfsync配置项，可以控制 Redis 将命令同步到磁盘的频率：\n\n * always：每次 Redis 将命令write()到 AOF 文件时，都会调用fsync()，将命令刷新到磁盘。这可以保证最好的数据持久性，但却会给系统带来极大的开销。\n * no：Redis 只将命令write()到 AOF 文件。这会让 OS 决定何时将命令刷新到磁盘。\n * everysec：除了将命令write()到 AOF 文件，Redis 还会每秒执行一次fsync()。在实践中，推荐使用这种设置，一定程度上可以保证数据持久性，又不会明显降低 Redis 性能。\n\n然而，AOF 持久化并不是没有缺点的：Redis 会不断将接收到的写命令追加到 AOF 文件中，导致 AOF 文件越来越大。过大的 AOF 文件会消耗磁盘空间，并且导致 Redis 重启时更加缓慢。为了解决这个问题，在适当情况下，Redis 会对 AOF 文件进行重写，去除文件中冗余的命令，以减小 AOF 文件的体积。在重写 AOF 文件期间， Redis 会启动一个子进程，由子进程负责对 AOF 文件进行重写。 　　可以通过下面两个配置项，控制 Redis 重写 AOF 文件的频率：\n\n * auto-aof-rewrite-min-size 64mb\n * auto-aof-rewrite-percentage 100\n\n上面两个配置的作用：当 AOF 文件的体积大于 64MB，并且 AOF 文件的体积比上一次重写之后的体积大了至少一倍，那么 Redis 就会执行 AOF 重写。\n\n优点：\n\n * 持久化频率高，数据可靠性高\n * 没有额外的内存或CPU消耗\n\n缺点：\n\n * 文件体积大\n * 文件大导致服务数据恢复时效率较低\n\n面试话术：\n\nRedis 提供了两种数据持久化的方式，一种是 RDB，另一种是 AOF。默认情况下，Redis 使用的是 RDB 持久化。\n\nRDB持久化文件体积较小，但是保存数据的频率一般较低，可靠性差，容易丢失数据。另外RDB写数据时会采用Fork函数拷贝主进程，可能有额外的内存消耗，文件压缩也会有额外的CPU消耗。\n\nROF持久化可以做到每秒钟持久化一次，可靠性高。但是持久化文件体积较大，导致数据恢复时读取文件时间较长，效率略低\n\n\n# Redis的集群方式有哪些？\n\n面试话术：\n\nRedis集群可以分为主从集群和分片集群两类。\n\n主从集群一般一主多从，主库用来写数据，从库用来读数据。结合哨兵，可以再主库宕机时从新选主，目的是保证Redis的高可用。\n\n分片集群是数据分片，我们会让多个Redis节点组成集群，并将16383个插槽分到不同的节点上。存储数据时利用对key做hash运算，得到插槽值后存储到对应的节点即可。因为存储数据面向的是插槽而非节点本身，因此可以做到集群动态伸缩。目的是让Redis能存储更多数据。\n\n1）主从集群\n\n主从集群，也是读写分离集群。一般都是一主多从方式。\n\nRedis 的复制（replication）功能允许用户根据一个 Redis 服务器来创建任意多个该服务器的复制品，其中被复制的服务器为主服务器（master），而通过复制创建出来的服务器复制品则为从服务器（slave）。\n\n只要主从服务器之间的网络连接正常，主从服务器两者会具有相同的数据，主服务器就会一直将发生在自己身上的数据更新同步 给从服务器，从而一直保证主从服务器的数据相同。\n\n * 写数据时只能通过主节点完成\n * 读数据可以从任何节点完成\n * 如果配置了哨兵节点，当master宕机时，哨兵会从salve节点选出一个新的主。\n\n主从集群分两种：\n\n\n\n带有哨兵的集群：\n\n\n\n2）分片集群\n\n主从集群中，每个节点都要保存所有信息，容易形成木桶效应。并且当数据量较大时，单个机器无法满足需求。此时我们就要使用分片集群了。\n\n\n\n集群特征：\n\n * 每个节点都保存不同数据\n\n * 所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽.\n\n * 节点的fail是通过集群中超过半数的节点检测失效时才生效.\n\n * 客户端与redis节点直连,不需要中间proxy层连接集群中任何一个可用节点都可以访问到数据\n\n * redis-cluster把所有的物理节点映射到[0-16383]slot（插槽）上，实现动态伸缩\n\n为了保证Redis中每个节点的高可用，我们还可以给每个节点创建replication（slave节点），如图：\n\n\n\n出现故障时，主从可以及时切换：\n\n\n\n\n# Redis的常用数据类型有哪些？\n\n支持多种类型的数据结构，主要区别是value存储的数据格式不同：\n\n * string：最基本的数据类型，二进制安全的字符串，最大512M。\n\n * list：按照添加顺序保持顺序的字符串列表。\n\n * set：无序的字符串集合，不存在重复的元素。\n\n * sorted set：已排序的字符串集合。\n\n * hash：key-value对格式\n\n\n# 聊一下Redis事务机制\n\n相关资料：\n\n参考：http://redisdoc.com/topic/transaction.html\n\nRedis事务功能是通过MULTI、EXEC、DISCARD和WATCH 四个原语实现的。Redis会将一个事务中的所有命令序列化，然后按顺序执行。但是Redis事务不支持回滚操作，命令运行出错后，正确的命令会继续执行。\n\n * MULTI: 用于开启一个事务，它总是返回OK。 MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个待执行命令队列中\n * EXEC：按顺序执行命令队列内的所有命令。返回所有命令的返回值。事务执行过程中，Redis不会执行其它事务的命令。\n * DISCARD：清空命令队列，并放弃执行事务， 并且客户端会从事务状态中退出\n * WATCH：Redis的乐观锁机制，利用compare-and-set（CAS）原理，可以监控一个或多个键，一旦其中有一个键被修改，之后的事务就不会执行\n\n使用事务时可能会遇上以下两种错误：\n\n * 执行 EXEC 之前，入队的命令可能会出错。比如说，命令可能会产生语法错误（参数数量错误，参数名错误，等等），或者其他更严重的错误，比如内存不足（如果服务器使用 maxmemory 设置了最大内存限制的话）。\n   * Redis 2.6.5 开始，服务器会对命令入队失败的情况进行记录，并在客户端调用 EXEC 命令时，拒绝执行并自动放弃这个事务。\n * 命令可能在 EXEC 调用之后失败。举个例子，事务中的命令可能处理了错误类型的键，比如将列表命令用在了字符串键上面，诸如此类。\n   * 即使事务中有某个/某些命令在执行时产生了错误， 事务中的其他命令仍然会继续执行，不会回滚。\n\n为什么 Redis 不支持回滚（roll back）？\n\n以下是这种做法的优点：\n\n * Redis 命令只会因为错误的语法而失败（并且这些问题不能在入队时发现），或是命令用在了错误类型的键上面：这也就是说，从实用性的角度来说，失败的命令是由编程错误造成的，而这些错误应该在开发的过程中被发现，而不应该出现在生产环境中。\n * 因为不需要对回滚进行支持，所以 Redis 的内部可以保持简单且快速。\n\n鉴于没有任何机制能避免程序员自己造成的错误， 并且这类错误通常不会在生产环境中出现， 所以 Redis 选择了更简单、更快速的无回滚方式来处理事务。\n\n面试话术：\n\nRedis事务其实是把一系列Redis命令放入队列，然后批量执行，执行过程中不会有其它事务来打断。不过与关系型数据库的事务不同，Redis事务不支持回滚操作，事务中某个命令执行失败，其它命令依然会执行。\n\n为了弥补不能回滚的问题，Redis会在事务入队时就检查命令，如果命令异常则会放弃整个事务。\n\n因此，只要程序员编程是正确的，理论上说Redis会正确执行所有事务，无需回滚。\n\n面试官：如果事务执行一半的时候Redis宕机怎么办？\n\nRedis有持久化机制，因为可靠性问题，我们一般使用AOF持久化。事务的所有命令也会写入AOF文件，但是如果在执行EXEC命令之前，Redis已经宕机，则AOF文件中事务不完整。使用 redis-check-aof 程序可以移除 AOF 文件中不完整事务的信息，确保服务器可以顺利启动。\n\n\n# Redis的Key过期策略\n\n\n# 参考资料：\n\n# 为什么需要内存回收？\n\n * 1、在Redis中，set指令可以指定key的过期时间，当过期时间到达以后，key就失效了；\n * 2、Redis是基于内存操作的，所有的数据都是保存在内存中，一台机器的内存是有限且很宝贵的。\n\n基于以上两点，为了保证Redis能继续提供可靠的服务，Redis需要一种机制清理掉不常用的、无效的、多余的数据，失效后的数据需要及时清理，这就需要内存回收了。\n\nRedis的内存回收主要分为过期删除策略和内存淘汰策略两部分。\n\n# 过期删除策略\n\n删除达到过期时间的key。\n\n * 1）定时删除\n\n对于每一个设置了过期时间的key都会创建一个定时器，一旦到达过期时间就立即删除。该策略可以立即清除过期的数据，对内存较友好，但是缺点是占用了大量的CPU资源去处理过期的数据，会影响Redis的吞吐量和响应时间。\n\n * 2）惰性删除\n\n当访问一个key时，才判断该key是否过期，过期则删除。该策略能最大限度地节省CPU资源，但是对内存却十分不友好。有一种极端的情况是可能出现大量的过期key没有被再次访问，因此不会被清除，导致占用了大量的内存。\n\n> 在计算机科学中，懒惰删除（英文：lazy deletion）指的是从一个散列表（也称哈希表）中删除元素的一种方法。在这个方法中，删除仅仅是指标记一个元素被删除，而不是整个清除它。被删除的位点在插入时被当作空元素，在搜索之时被当作已占据。\n\n * 3）定期删除\n\n每隔一段时间，扫描Redis中过期key字典，并清除部分过期的key。该策略是前两者的一个折中方案，还可以通过调整定时扫描的时间间隔和每次扫描的限定耗时，在不同情况下使得CPU和内存资源达到最优的平衡效果。\n\n在Redis中，同时使用了定期删除和惰性删除。不过Redis定期删除采用的是随机抽取的方式删除部分Key，因此不能保证过期key 100%的删除。\n\nRedis结合了定期删除和惰性删除，基本上能很好的处理过期数据的清理，但是实际上还是有点问题的，如果过期key较多，定期删除漏掉了一部分，而且也没有及时去查，即没有走惰性删除，那么就会有大量的过期key堆积在内存中，导致redis内存耗尽，当内存耗尽之后，有新的key到来会发生什么事呢？是直接抛弃还是其他措施呢？有什么办法可以接受更多的key？\n\n# 内存淘汰策略\n\nRedis的内存淘汰策略，是指内存达到maxmemory极限时，使用某种算法来决定清理掉哪些数据，以保证新数据的存入。\n\nRedis的内存淘汰机制包括：\n\n * noeviction: 当内存不足以容纳新写入数据时，新写入操作会报错。\n * allkeys-lru：当内存不足以容纳新写入数据时，在键空间（server.db[i].dict）中，移除最近最少使用的 key（这个是最常用的）。\n * allkeys-random：当内存不足以容纳新写入数据时，在键空间（server.db[i].dict）中，随机移除某个 key。\n * volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间（server.db[i].expires）中，移除最近最少使用的 key。\n * volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间（server.db[i].expires）中，随机移除某个 key。\n * volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间（server.db[i].expires）中，有更早过期时间的 key 优先移除。\n\n> 在配置文件中，通过maxmemory-policy可以配置要使用哪一个淘汰机制。\n\n什么时候会进行淘汰？\n\nRedis会在每一次处理命令的时候（processCommand函数调用freeMemoryIfNeeded）判断当前redis是否达到了内存的最大限制，如果达到限制，则使用对应的算法去处理需要删除的key。\n\n在淘汰key时，Redis默认最常用的是LRU算法（Latest Recently Used）。Redis通过在每一个redisObject保存lru属性来保存key最近的访问时间，在实现LRU算法时直接读取key的lru属性。\n\n具体实现时，Redis遍历每一个db，从每一个db中随机抽取一批样本key，默认是3个key，再从这3个key中，删除最近最少使用的key。\n\n\n# 面试话术：\n\nRedis过期策略包含定期删除和惰性删除两部分。定期删除是在Redis内部有一个定时任务，会定期删除一些过期的key。惰性删除是当用户查询某个Key时，会检查这个Key是否已经过期，如果没过期则返回用户，如果过期则删除。\n\n但是这两个策略都无法保证过期key一定删除，漏网之鱼越来越多，还可能导致内存溢出。当发生内存不足问题时，Redis还会做内存回收。内存回收采用LRU策略，就是最近最少使用。其原理就是记录每个Key的最近使用时间，内存回收时，随机抽取一些Key，比较其使用时间，把最老的几个删除。\n\nRedis的逻辑是：最近使用过的，很可能再次被使用\n\n\n# Redis在项目中的哪些地方有用到?\n\n（1）共享session\n\n在分布式系统下，服务会部署在不同的tomcat，因此多个tomcat的session无法共享，以前存储在session中的数据无法实现共享，可以用redis代替session，解决分布式系统间数据共享问题。\n\n（2）数据缓存\n\nRedis采用内存存储，读写效率较高。我们可以把数据库的访问频率高的热点数据存储到redis中，这样用户请求时优先从redis中读取，减少数据库压力，提高并发能力。\n\n（3）异步队列\n\nReids在内存存储引擎领域的一大优点是提供 list 和 set 操作，这使得Redis能作为一个很好的消息队列平台来使用。而且Redis中还有pub/sub这样的专用结构，用于1对N的消息通信模式。\n\n（4）分布式锁\n\nRedis中的乐观锁机制，可以帮助我们实现分布式锁的效果，用于解决分布式系统下的多线程安全问题\n\n\n# Redis的缓存击穿、缓存雪崩、缓存穿透\n\n\n# 1）缓存穿透\n\n参考资料：\n\n * 什么是缓存穿透\n   \n   * 正常情况下，我们去查询数据都是存在。那么请求去查询一条压根儿数据库中根本就不存在的数据，也就是缓存和数据库都查询不到这条数据，但是请求每次都会打到数据库上面去。这种查询不存在数据的现象我们称为缓存穿透。\n\n * 穿透带来的问题\n   \n   * 试想一下，如果有黑客会对你的系统进行攻击，拿一个不存在的id 去查询数据，会产生大量的请求到数据库去查询。可能会导致你的数据库由于压力过大而宕掉。\n\n * 解决办法\n   \n   * 缓存空值：之所以会发生穿透，就是因为缓存中没有存储这些空数据的key。从而导致每次查询都到数据库去了。那么我们就可以为这些key对应的值设置为null 丢到缓存里面去。后面再出现查询这个key 的请求的时候，直接返回null 。这样，就不用在到数据库中去走一圈了，但是别忘了设置过期时间。\n   * BloomFilter（布隆过滤）：将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。在缓存之前在加一层 BloomFilter ，在查询的时候先去 BloomFilter 去查询 key 是否存在，如果不存在就直接返回，存在再走查缓存 -> 查 DB。\n\n话术：\n\n缓存穿透有两种解决方案：其一是把不存在的key设置null值到缓存中。其二是使用布隆过滤器，在查询缓存前先通过布隆过滤器判断key是否存在，存在再去查询缓存。\n\n设置null值可能被恶意针对，攻击者使用大量不存在的不重复key ，那么方案一就会缓存大量不存在key数据。此时我们还可以对Key规定格式模板，然后对不存在的key做正则规范匹配，如果完全不符合就不用存null值到redis，而是直接返回错误。\n\n\n# 2）缓存击穿\n\n相关资料：\n\n * 什么是缓存击穿？\n\nkey可能会在某些时间点被超高并发地访问，是一种非常“热点”的数据。这个时候，需要考虑一个问题：缓存被“击穿”的问题。\n\n当这个key在失效的瞬间，redis查询失败，持续的大并发就穿破缓存，直接请求数据库，就像在一个屏障上凿开了一个洞。\n\n * 解决方案：\n   * 使用互斥锁(mutex key)：mutex，就是互斥。简单地来说，就是在缓存失效的时候（判断拿出来的值为空），不是立即去load db，而是先使用Redis的SETNX去set一个互斥key，当操作返回成功时，再进行load db的操作并回设缓存；否则，就重试整个get缓存的方法。SETNX，是「SET if Not eXists」的缩写，也就是只有不存在的时候才设置，可以利用它来实现互斥的效果。\n   * 软过期：也就是逻辑过期，不使用redis提供的过期时间，而是业务层在数据中存储过期时间信息。查询时由业务程序判断是否过期，如果数据即将过期时，将缓存的时效延长，程序可以派遣一个线程去数据库中获取最新的数据，其他线程这时看到延长了的过期时间，就会继续使用旧数据，等派遣的线程获取最新数据后再更新缓存。\n\n推荐使用互斥锁，因为软过期会有业务逻辑侵入和额外的判断。\n\n面试话术：\n\n缓存击穿主要担心的是某个Key过期，更新缓存时引起对数据库的突发高并发访问。因此我们可以在更新缓存时采用互斥锁控制，只允许一个线程去更新缓存，其它线程等待并重新读取缓存。例如Redis的setnx命令就能实现互斥效果。\n\n\n# 3）缓存雪崩\n\n相关资料：\n\n缓存雪崩，是指在某一个时间段，缓存集中过期失效。对这批数据的访问查询，都落到了数据库上，对于数据库而言，就会产生周期性的压力波峰。\n\n解决方案：\n\n * 数据分类分批处理：采取不同分类数据，缓存不同周期\n * 相同分类数据：采用固定时长加随机数方式设置缓存\n * 热点数据缓存时间长一些，冷门数据缓存时间短一些\n * 避免redis节点宕机引起雪崩，搭建主从集群，保证高可用\n\n面试话术：\n\n解决缓存雪崩问题的关键是让缓存Key的过期时间分散。因此我们可以把数据按照业务分类，然后设置不同过期时间。相同业务类型的key，设置固定时长加随机数。尽可能保证每个Key的过期时间都不相同。\n\n另外，Redis宕机也可能导致缓存雪崩，因此我们还要搭建Redis主从集群及哨兵监控，保证Redis的高可用。\n\n\n# 缓存冷热数据分离\n\n背景资料：\n\nRedis使用的是内存存储，当需要海量数据存储时，成本非常高。\n\n经过调研发现，当前主流DDR3内存和主流SATA SSD的单位成本价格差距大概在20倍左右，为了优化redis机器综合成本，我们考虑实现基于热度统计 的数据分级存储及数据在RAM/FLASH之间的动态交换，从而大幅度降低成本，达到性能与成本的高平衡。\n\n基本思路：基于key访问次数(LFU)的热度统计算法识别出热点数据，并将热点数据保留在redis中，对于无访问/访问次数少的数据则转存到SSD上，如果SSD上的key再次变热，则重新将其加载到redis内存中。\n\n目前流行的高性能磁盘存储，并且遵循Redis协议的方案包括：\n\n * SSDB：http://ssdb.io/zh_cn/\n * RocksDB：https://rocksdb.org.cn/\n\n因此，我们就需要在应用程序与缓存服务之间引入代理，实现Redis和SSD之间的切换，如图：\n\n\n\n这样的代理方案阿里云提供的就有。当然也有一些开源方案，例如：https://github.com/JingchengLi/swapdb\n\n\n# Redis实现分布式锁\n\n分布式锁要满足的条件：\n\n * 多进程互斥：同一时刻，只有一个进程可以获取锁\n * 保证锁可以释放：任务结束或出现异常，锁一定要释放，避免死锁\n * 阻塞锁（可选）：获取锁失败时可否重试\n * 重入锁（可选）：获取锁的代码递归调用时，依然可以获取锁\n\n\n# 1）最基本的分布式锁：\n\n利用Redis的setnx命令，这个命令的特征时如果多次执行，只有第一次执行会成功，可以实现互斥的效果。但是为了保证服务宕机时也可以释放锁，需要利用expire命令给锁设置一个有效期\n\nsetnx lock thread-01 # 尝试获取锁\nexpire lock 10 # 设置有效期\n\n\n面试官问题1：如果expire之前服务宕机怎么办？\n\n要保证setnx和expire命令的原子性。redis的set命令可以满足：\n\nset key value [NX] [EX time] \n\n\n需要添加nx和ex的选项：\n\n * NX：与setnx一致，第一次执行成功\n * EX：设置过期时间\n\n面试官问题2：释放锁的时候，如果自己的锁已经过期了，此时会出现安全漏洞，如何解决？\n\n在锁中存储当前进程和线程标识，释放锁时对锁的标识判断，如果是自己的则删除，不是则放弃操作。\n\n但是这两步操作要保证原子性，需要通过Lua脚本来实现。\n\nif redis.call(\"get\",KEYS[1]) ** ARGV[1] then\n    redis.call(\"del\",KEYS[1])\nend\n\n\n\n# 2）可重入分布式锁\n\n如果有重入的需求，则除了在锁中记录进程标识，还要记录重试次数，流程如下：\n\n\n\n下面我们假设锁的key为“lock”，hashKey是当前线程的id：“threadId”，锁自动释放时间假设为20\n\n获取锁的步骤：\n\n * 1、判断lock是否存在 EXISTS lock\n   * 存在，说明有人获取锁了，下面判断是不是自己的锁\n     * 判断当前线程id作为hashKey是否存在：HEXISTS lock threadId\n       * 不存在，说明锁已经有了，且不是自己获取的，锁获取失败，end\n       * 存在，说明是自己获取的锁，重入次数+1：HINCRBY lock threadId 1，去到步骤3\n   * 2、不存在，说明可以获取锁，HSET key threadId 1\n   * 3、设置锁自动释放时间，EXPIRE lock 20\n\n释放锁的步骤：\n\n * 1、判断当前线程id作为hashKey是否存在：HEXISTS lock threadId\n   * 不存在，说明锁已经失效，不用管了\n   * 存在，说明锁还在，重入次数减1：HINCRBY lock threadId -1，获取新的重入次数\n * 2、判断重入次数是否为0：\n   * 为0，说明锁全部释放，删除key：DEL lock\n   * 大于0，说明锁还在使用，重置有效时间：EXPIRE lock 20\n\n对应的Lua脚本如下：\n\n首先是获取锁：\n\nlocal key = KEYS[1]; -- 锁的key\nlocal threadId = ARGV[1]; -- 线程唯一标识\nlocal releaseTime = ARGV[2]; -- 锁的自动释放时间\n\nif(redis.call('exists', key) ** 0) then -- 判断是否存在\n\tredis.call('hset', key, threadId, '1'); -- 不存在, 获取锁\n\tredis.call('expire', key, releaseTime); -- 设置有效期\n\treturn 1; -- 返回结果\nend;\n\nif(redis.call('hexists', key, threadId) ** 1) then -- 锁已经存在，判断threadId是否是自己\t\n\tredis.call('hincrby', key, threadId, '1'); -- 不存在, 获取锁，重入次数+1\n\tredis.call('expire', key, releaseTime); -- 设置有效期\n\treturn 1; -- 返回结果\nend;\nreturn 0; -- 代码走到这里,说明获取锁的不是自己，获取锁失败\n\n\n然后是释放锁：\n\nlocal key = KEYS[1]; -- 锁的key\nlocal threadId = ARGV[1]; -- 线程唯一标识\nlocal releaseTime = ARGV[2]; -- 锁的自动释放时间\n\nif (redis.call('HEXISTS', key, threadId) ** 0) then -- 判断当前锁是否还是被自己持有\n    return nil; -- 如果已经不是自己，则直接返回\nend;\nlocal count = redis.call('HINCRBY', key, threadId, -1); -- 是自己的锁，则重入次数-1\n\nif (count > 0) then -- 判断是否重入次数是否已经为0\n    redis.call('EXPIRE', key, releaseTime); -- 大于0说明不能释放锁，重置有效期然后返回\n    return nil;\nelse\n    redis.call('DEL', key); -- 等于0说明可以释放锁，直接删除\n    return nil;\nend;\n\n\n\n# 3）高可用的锁\n\n面试官问题：redis分布式锁依赖与redis，如果redis宕机则锁失效。如何解决？\n\n此时大多数同学会回答说：搭建主从集群，做数据备份。\n\n这样就进入了陷阱，因为面试官的下一个问题就来了：\n\n面试官问题：如果搭建主从集群做数据备份时，进程A获取锁，master还没有把数据备份到slave，master宕机，slave升级为master，此时原来锁失效，其它进程也可以获取锁，出现安全问题。如何解决？\n\n关于这个问题，Redis官网给出了解决方案，使用RedLock思路可以解决：\n\n> 在Redis的分布式环境中，我们假设有N个Redis master。这些节点完全互相独立，不存在主从复制或者其他集群协调机制。之前我们已经描述了在Redis单实例下怎么安全地获取和释放锁。我们确保将在每（N)个实例上使用此方法获取和释放锁。在这个样例中，我们假设有5个Redis master节点，这是一个比较合理的设置，所以我们需要在5台机器上面或者5台虚拟机上面运行这些实例，这样保证他们不会同时都宕掉。\n> \n> 为了取到锁，客户端应该执行以下操作:\n> \n>  1. 获取当前Unix时间，以毫秒为单位。\n>  2. 依次尝试从N个实例，使用相同的key和随机值获取锁。在步骤2，当向Redis设置锁时,客户端应该设置一个网络连接和响应超时时间，这个超时时间应该小于锁的失效时间。例如你的锁自动失效时间为10秒，则超时时间应该在5-50毫秒之间。这样可以避免服务器端Redis已经挂掉的情况下，客户端还在死死地等待响应结果。如果服务器端没有在规定时间内响应，客户端应该尽快尝试另外一个Redis实例。\n>  3. 客户端使用当前时间减去开始获取锁时间（步骤1记录的时间）就得到获取锁使用的时间。当且仅当从大多数（这里是3个节点）的Redis节点都取到锁，并且使用的时间小于锁失效时间时，锁才算获取成功。\n>  4. 如果取到了锁，key的真正有效时间等于有效时间减去获取锁所使用的时间（步骤3计算的结果）。\n>  5. 如果因为某些原因，获取锁失败（没有在至少N/2+1个Redis实例取到锁或者取锁时间已经超过了有效时间），客户端应该在所有的Redis实例上进行解锁（即便某些Redis实例根本就没有加锁成功）。\n\n\n# 如何实现数据库与缓存数据一致？\n\n面试话术：\n\n实现方案有下面几种：\n\n * 本地缓存同步：当前微服务的数据库数据与缓存数据同步，可以直接在数据库修改时加入对Redis的修改逻辑，保证一致。\n * 跨服务缓存同步：服务A调用了服务B，并对查询结果缓存。服务B数据库修改，可以通过MQ通知服务A，服务A修改Redis缓存数据\n * 通用方案：使用Canal框架，伪装成MySQL的salve节点，监听MySQL的binLog变化，然后修改Redis缓存数据",normalizedContent:"# redis与memcache的区别？\n\n * redis支持更丰富的数据类型（支持更复杂的应用场景）：redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。memcache支持简单的数据类型，string。\n * redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而memecache把数据全部存在内存之中。\n * 集群模式：memcached没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；但是 redis 目前是原生支持 cluster 模式的.\n * redis使用单线程：memcached是多线程，非阻塞io复用的网络模型；redis使用单线程的多路 io 复用模型。\n\n\n\n\n# redis的单线程问题\n\n面试官：redis采用单线程，如何保证高并发？\n\n面试话术：\n\nredis快的主要原因是：\n\n 1. 完全基于内存\n 2. 数据结构简单，对数据操作也简单\n 3. 使用多路 i/o 复用模型，充分利用cpu资源\n\n面试官：这样做的好处是什么？\n\n面试话术：\n\n单线程优势有下面几点：\n\n * 代码更清晰，处理逻辑更简单\n * 不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为锁而导致的性能消耗\n * 不存在多进程或者多线程导致的cpu切换，充分利用cpu资源\n\n\n# redis的持久化方案由哪些？\n\n相关资料：\n\n1）rdb 持久化\n\nrdb持久化可以使用save或bgsave，为了不阻塞主进程业务，一般都使用bgsave，流程：\n\n * redis 进程会 fork 出一个子进程（与父进程内存数据一致）。\n * 父进程继续处理客户端请求命令\n * 由子进程将内存中的所有数据写入到一个临时的 rdb 文件中。\n * 完成写入操作之后，旧的 rdb 文件会被新的 rdb 文件替换掉。\n\n下面是一些和 rdb 持久化相关的配置：\n\n * save 60 10000：如果在 60 秒内有 10000 个 key 发生改变，那就执行 rdb 持久化。\n * stop-writes-on-bgsave-error yes：如果 redis 执行 rdb 持久化失败（常见于操作系统内存不足），那么 redis 将不再接受 client 写入数据的请求。\n * rdbcompression yes：当生成 rdb 文件时，同时进行压缩。\n * dbfilename dump.rdb：将 rdb 文件命名为 dump.rdb。\n * dir /var/lib/redis：将 rdb 文件保存在/var/lib/redis目录下。\n\n当然在实践中，我们通常会将stop-writes-on-bgsave-error设置为false，同时让监控系统在 redis 执行 rdb 持久化失败时发送告警，以便人工介入解决，而不是粗暴地拒绝 client 的写入请求。\n\nrdb持久化的优点：\n\n * rdb持久化文件小，redis数据恢复时速度快\n * 子进程不影响父进程，父进程可以持续处理客户端命令\n * 子进程fork时采用copy-on-write方式，大多数情况下，没有太多的内存消耗，效率比较好。\n\nrdb 持久化的缺点：\n\n * 子进程fork时采用copy-on-write方式，如果redis此时写操作较多，可能导致额外的内存占用，甚至内存溢出\n * rdb文件压缩会减小文件体积，但通过时会对cpu有额外的消耗\n * 如果业务场景很看重数据的持久性 (durability)，那么不应该采用 rdb 持久化。譬如说，如果 redis 每 5 分钟执行一次 rdb 持久化，要是 redis 意外奔溃了，那么最多会丢失 5 分钟的数据。\n\n2）aof 持久化\n\n可以使用appendonly yes配置项来开启 aof 持久化。redis 执行 aof 持久化时，会将接收到的写命令追加到 aof 文件的末尾，因此 redis 只要对 aof 文件中的命令进行回放，就可以将数据库还原到原先的状态。 　　与 rdb 持久化相比，aof 持久化的一个明显优势就是，它可以提高数据的持久性 (durability)。因为在 aof 模式下，redis 每次接收到 client 的写命令，就会将命令write()到 aof 文件末尾。 　　然而，在 linux 中，将数据write()到文件后，数据并不会立即刷新到磁盘，而会先暂存在 os 的文件系统缓冲区。在合适的时机，os 才会将缓冲区的数据刷新到磁盘（如果需要将文件内容刷新到磁盘，可以调用fsync()或fdatasync()）。 　　通过appendfsync配置项，可以控制 redis 将命令同步到磁盘的频率：\n\n * always：每次 redis 将命令write()到 aof 文件时，都会调用fsync()，将命令刷新到磁盘。这可以保证最好的数据持久性，但却会给系统带来极大的开销。\n * no：redis 只将命令write()到 aof 文件。这会让 os 决定何时将命令刷新到磁盘。\n * everysec：除了将命令write()到 aof 文件，redis 还会每秒执行一次fsync()。在实践中，推荐使用这种设置，一定程度上可以保证数据持久性，又不会明显降低 redis 性能。\n\n然而，aof 持久化并不是没有缺点的：redis 会不断将接收到的写命令追加到 aof 文件中，导致 aof 文件越来越大。过大的 aof 文件会消耗磁盘空间，并且导致 redis 重启时更加缓慢。为了解决这个问题，在适当情况下，redis 会对 aof 文件进行重写，去除文件中冗余的命令，以减小 aof 文件的体积。在重写 aof 文件期间， redis 会启动一个子进程，由子进程负责对 aof 文件进行重写。 　　可以通过下面两个配置项，控制 redis 重写 aof 文件的频率：\n\n * auto-aof-rewrite-min-size 64mb\n * auto-aof-rewrite-percentage 100\n\n上面两个配置的作用：当 aof 文件的体积大于 64mb，并且 aof 文件的体积比上一次重写之后的体积大了至少一倍，那么 redis 就会执行 aof 重写。\n\n优点：\n\n * 持久化频率高，数据可靠性高\n * 没有额外的内存或cpu消耗\n\n缺点：\n\n * 文件体积大\n * 文件大导致服务数据恢复时效率较低\n\n面试话术：\n\nredis 提供了两种数据持久化的方式，一种是 rdb，另一种是 aof。默认情况下，redis 使用的是 rdb 持久化。\n\nrdb持久化文件体积较小，但是保存数据的频率一般较低，可靠性差，容易丢失数据。另外rdb写数据时会采用fork函数拷贝主进程，可能有额外的内存消耗，文件压缩也会有额外的cpu消耗。\n\nrof持久化可以做到每秒钟持久化一次，可靠性高。但是持久化文件体积较大，导致数据恢复时读取文件时间较长，效率略低\n\n\n# redis的集群方式有哪些？\n\n面试话术：\n\nredis集群可以分为主从集群和分片集群两类。\n\n主从集群一般一主多从，主库用来写数据，从库用来读数据。结合哨兵，可以再主库宕机时从新选主，目的是保证redis的高可用。\n\n分片集群是数据分片，我们会让多个redis节点组成集群，并将16383个插槽分到不同的节点上。存储数据时利用对key做hash运算，得到插槽值后存储到对应的节点即可。因为存储数据面向的是插槽而非节点本身，因此可以做到集群动态伸缩。目的是让redis能存储更多数据。\n\n1）主从集群\n\n主从集群，也是读写分离集群。一般都是一主多从方式。\n\nredis 的复制（replication）功能允许用户根据一个 redis 服务器来创建任意多个该服务器的复制品，其中被复制的服务器为主服务器（master），而通过复制创建出来的服务器复制品则为从服务器（slave）。\n\n只要主从服务器之间的网络连接正常，主从服务器两者会具有相同的数据，主服务器就会一直将发生在自己身上的数据更新同步 给从服务器，从而一直保证主从服务器的数据相同。\n\n * 写数据时只能通过主节点完成\n * 读数据可以从任何节点完成\n * 如果配置了哨兵节点，当master宕机时，哨兵会从salve节点选出一个新的主。\n\n主从集群分两种：\n\n\n\n带有哨兵的集群：\n\n\n\n2）分片集群\n\n主从集群中，每个节点都要保存所有信息，容易形成木桶效应。并且当数据量较大时，单个机器无法满足需求。此时我们就要使用分片集群了。\n\n\n\n集群特征：\n\n * 每个节点都保存不同数据\n\n * 所有的redis节点彼此互联(ping-pong机制),内部使用二进制协议优化传输速度和带宽.\n\n * 节点的fail是通过集群中超过半数的节点检测失效时才生效.\n\n * 客户端与redis节点直连,不需要中间proxy层连接集群中任何一个可用节点都可以访问到数据\n\n * redis-cluster把所有的物理节点映射到[0-16383]slot（插槽）上，实现动态伸缩\n\n为了保证redis中每个节点的高可用，我们还可以给每个节点创建replication（slave节点），如图：\n\n\n\n出现故障时，主从可以及时切换：\n\n\n\n\n# redis的常用数据类型有哪些？\n\n支持多种类型的数据结构，主要区别是value存储的数据格式不同：\n\n * string：最基本的数据类型，二进制安全的字符串，最大512m。\n\n * list：按照添加顺序保持顺序的字符串列表。\n\n * set：无序的字符串集合，不存在重复的元素。\n\n * sorted set：已排序的字符串集合。\n\n * hash：key-value对格式\n\n\n# 聊一下redis事务机制\n\n相关资料：\n\n参考：http://redisdoc.com/topic/transaction.html\n\nredis事务功能是通过multi、exec、discard和watch 四个原语实现的。redis会将一个事务中的所有命令序列化，然后按顺序执行。但是redis事务不支持回滚操作，命令运行出错后，正确的命令会继续执行。\n\n * multi: 用于开启一个事务，它总是返回ok。 multi执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个待执行命令队列中\n * exec：按顺序执行命令队列内的所有命令。返回所有命令的返回值。事务执行过程中，redis不会执行其它事务的命令。\n * discard：清空命令队列，并放弃执行事务， 并且客户端会从事务状态中退出\n * watch：redis的乐观锁机制，利用compare-and-set（cas）原理，可以监控一个或多个键，一旦其中有一个键被修改，之后的事务就不会执行\n\n使用事务时可能会遇上以下两种错误：\n\n * 执行 exec 之前，入队的命令可能会出错。比如说，命令可能会产生语法错误（参数数量错误，参数名错误，等等），或者其他更严重的错误，比如内存不足（如果服务器使用 maxmemory 设置了最大内存限制的话）。\n   * redis 2.6.5 开始，服务器会对命令入队失败的情况进行记录，并在客户端调用 exec 命令时，拒绝执行并自动放弃这个事务。\n * 命令可能在 exec 调用之后失败。举个例子，事务中的命令可能处理了错误类型的键，比如将列表命令用在了字符串键上面，诸如此类。\n   * 即使事务中有某个/某些命令在执行时产生了错误， 事务中的其他命令仍然会继续执行，不会回滚。\n\n为什么 redis 不支持回滚（roll back）？\n\n以下是这种做法的优点：\n\n * redis 命令只会因为错误的语法而失败（并且这些问题不能在入队时发现），或是命令用在了错误类型的键上面：这也就是说，从实用性的角度来说，失败的命令是由编程错误造成的，而这些错误应该在开发的过程中被发现，而不应该出现在生产环境中。\n * 因为不需要对回滚进行支持，所以 redis 的内部可以保持简单且快速。\n\n鉴于没有任何机制能避免程序员自己造成的错误， 并且这类错误通常不会在生产环境中出现， 所以 redis 选择了更简单、更快速的无回滚方式来处理事务。\n\n面试话术：\n\nredis事务其实是把一系列redis命令放入队列，然后批量执行，执行过程中不会有其它事务来打断。不过与关系型数据库的事务不同，redis事务不支持回滚操作，事务中某个命令执行失败，其它命令依然会执行。\n\n为了弥补不能回滚的问题，redis会在事务入队时就检查命令，如果命令异常则会放弃整个事务。\n\n因此，只要程序员编程是正确的，理论上说redis会正确执行所有事务，无需回滚。\n\n面试官：如果事务执行一半的时候redis宕机怎么办？\n\nredis有持久化机制，因为可靠性问题，我们一般使用aof持久化。事务的所有命令也会写入aof文件，但是如果在执行exec命令之前，redis已经宕机，则aof文件中事务不完整。使用 redis-check-aof 程序可以移除 aof 文件中不完整事务的信息，确保服务器可以顺利启动。\n\n\n# redis的key过期策略\n\n\n# 参考资料：\n\n# 为什么需要内存回收？\n\n * 1、在redis中，set指令可以指定key的过期时间，当过期时间到达以后，key就失效了；\n * 2、redis是基于内存操作的，所有的数据都是保存在内存中，一台机器的内存是有限且很宝贵的。\n\n基于以上两点，为了保证redis能继续提供可靠的服务，redis需要一种机制清理掉不常用的、无效的、多余的数据，失效后的数据需要及时清理，这就需要内存回收了。\n\nredis的内存回收主要分为过期删除策略和内存淘汰策略两部分。\n\n# 过期删除策略\n\n删除达到过期时间的key。\n\n * 1）定时删除\n\n对于每一个设置了过期时间的key都会创建一个定时器，一旦到达过期时间就立即删除。该策略可以立即清除过期的数据，对内存较友好，但是缺点是占用了大量的cpu资源去处理过期的数据，会影响redis的吞吐量和响应时间。\n\n * 2）惰性删除\n\n当访问一个key时，才判断该key是否过期，过期则删除。该策略能最大限度地节省cpu资源，但是对内存却十分不友好。有一种极端的情况是可能出现大量的过期key没有被再次访问，因此不会被清除，导致占用了大量的内存。\n\n> 在计算机科学中，懒惰删除（英文：lazy deletion）指的是从一个散列表（也称哈希表）中删除元素的一种方法。在这个方法中，删除仅仅是指标记一个元素被删除，而不是整个清除它。被删除的位点在插入时被当作空元素，在搜索之时被当作已占据。\n\n * 3）定期删除\n\n每隔一段时间，扫描redis中过期key字典，并清除部分过期的key。该策略是前两者的一个折中方案，还可以通过调整定时扫描的时间间隔和每次扫描的限定耗时，在不同情况下使得cpu和内存资源达到最优的平衡效果。\n\n在redis中，同时使用了定期删除和惰性删除。不过redis定期删除采用的是随机抽取的方式删除部分key，因此不能保证过期key 100%的删除。\n\nredis结合了定期删除和惰性删除，基本上能很好的处理过期数据的清理，但是实际上还是有点问题的，如果过期key较多，定期删除漏掉了一部分，而且也没有及时去查，即没有走惰性删除，那么就会有大量的过期key堆积在内存中，导致redis内存耗尽，当内存耗尽之后，有新的key到来会发生什么事呢？是直接抛弃还是其他措施呢？有什么办法可以接受更多的key？\n\n# 内存淘汰策略\n\nredis的内存淘汰策略，是指内存达到maxmemory极限时，使用某种算法来决定清理掉哪些数据，以保证新数据的存入。\n\nredis的内存淘汰机制包括：\n\n * noeviction: 当内存不足以容纳新写入数据时，新写入操作会报错。\n * allkeys-lru：当内存不足以容纳新写入数据时，在键空间（server.db[i].dict）中，移除最近最少使用的 key（这个是最常用的）。\n * allkeys-random：当内存不足以容纳新写入数据时，在键空间（server.db[i].dict）中，随机移除某个 key。\n * volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间（server.db[i].expires）中，移除最近最少使用的 key。\n * volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间（server.db[i].expires）中，随机移除某个 key。\n * volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间（server.db[i].expires）中，有更早过期时间的 key 优先移除。\n\n> 在配置文件中，通过maxmemory-policy可以配置要使用哪一个淘汰机制。\n\n什么时候会进行淘汰？\n\nredis会在每一次处理命令的时候（processcommand函数调用freememoryifneeded）判断当前redis是否达到了内存的最大限制，如果达到限制，则使用对应的算法去处理需要删除的key。\n\n在淘汰key时，redis默认最常用的是lru算法（latest recently used）。redis通过在每一个redisobject保存lru属性来保存key最近的访问时间，在实现lru算法时直接读取key的lru属性。\n\n具体实现时，redis遍历每一个db，从每一个db中随机抽取一批样本key，默认是3个key，再从这3个key中，删除最近最少使用的key。\n\n\n# 面试话术：\n\nredis过期策略包含定期删除和惰性删除两部分。定期删除是在redis内部有一个定时任务，会定期删除一些过期的key。惰性删除是当用户查询某个key时，会检查这个key是否已经过期，如果没过期则返回用户，如果过期则删除。\n\n但是这两个策略都无法保证过期key一定删除，漏网之鱼越来越多，还可能导致内存溢出。当发生内存不足问题时，redis还会做内存回收。内存回收采用lru策略，就是最近最少使用。其原理就是记录每个key的最近使用时间，内存回收时，随机抽取一些key，比较其使用时间，把最老的几个删除。\n\nredis的逻辑是：最近使用过的，很可能再次被使用\n\n\n# redis在项目中的哪些地方有用到?\n\n（1）共享session\n\n在分布式系统下，服务会部署在不同的tomcat，因此多个tomcat的session无法共享，以前存储在session中的数据无法实现共享，可以用redis代替session，解决分布式系统间数据共享问题。\n\n（2）数据缓存\n\nredis采用内存存储，读写效率较高。我们可以把数据库的访问频率高的热点数据存储到redis中，这样用户请求时优先从redis中读取，减少数据库压力，提高并发能力。\n\n（3）异步队列\n\nreids在内存存储引擎领域的一大优点是提供 list 和 set 操作，这使得redis能作为一个很好的消息队列平台来使用。而且redis中还有pub/sub这样的专用结构，用于1对n的消息通信模式。\n\n（4）分布式锁\n\nredis中的乐观锁机制，可以帮助我们实现分布式锁的效果，用于解决分布式系统下的多线程安全问题\n\n\n# redis的缓存击穿、缓存雪崩、缓存穿透\n\n\n# 1）缓存穿透\n\n参考资料：\n\n * 什么是缓存穿透\n   \n   * 正常情况下，我们去查询数据都是存在。那么请求去查询一条压根儿数据库中根本就不存在的数据，也就是缓存和数据库都查询不到这条数据，但是请求每次都会打到数据库上面去。这种查询不存在数据的现象我们称为缓存穿透。\n\n * 穿透带来的问题\n   \n   * 试想一下，如果有黑客会对你的系统进行攻击，拿一个不存在的id 去查询数据，会产生大量的请求到数据库去查询。可能会导致你的数据库由于压力过大而宕掉。\n\n * 解决办法\n   \n   * 缓存空值：之所以会发生穿透，就是因为缓存中没有存储这些空数据的key。从而导致每次查询都到数据库去了。那么我们就可以为这些key对应的值设置为null 丢到缓存里面去。后面再出现查询这个key 的请求的时候，直接返回null 。这样，就不用在到数据库中去走一圈了，但是别忘了设置过期时间。\n   * bloomfilter（布隆过滤）：将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。在缓存之前在加一层 bloomfilter ，在查询的时候先去 bloomfilter 去查询 key 是否存在，如果不存在就直接返回，存在再走查缓存 -> 查 db。\n\n话术：\n\n缓存穿透有两种解决方案：其一是把不存在的key设置null值到缓存中。其二是使用布隆过滤器，在查询缓存前先通过布隆过滤器判断key是否存在，存在再去查询缓存。\n\n设置null值可能被恶意针对，攻击者使用大量不存在的不重复key ，那么方案一就会缓存大量不存在key数据。此时我们还可以对key规定格式模板，然后对不存在的key做正则规范匹配，如果完全不符合就不用存null值到redis，而是直接返回错误。\n\n\n# 2）缓存击穿\n\n相关资料：\n\n * 什么是缓存击穿？\n\nkey可能会在某些时间点被超高并发地访问，是一种非常“热点”的数据。这个时候，需要考虑一个问题：缓存被“击穿”的问题。\n\n当这个key在失效的瞬间，redis查询失败，持续的大并发就穿破缓存，直接请求数据库，就像在一个屏障上凿开了一个洞。\n\n * 解决方案：\n   * 使用互斥锁(mutex key)：mutex，就是互斥。简单地来说，就是在缓存失效的时候（判断拿出来的值为空），不是立即去load db，而是先使用redis的setnx去set一个互斥key，当操作返回成功时，再进行load db的操作并回设缓存；否则，就重试整个get缓存的方法。setnx，是「set if not exists」的缩写，也就是只有不存在的时候才设置，可以利用它来实现互斥的效果。\n   * 软过期：也就是逻辑过期，不使用redis提供的过期时间，而是业务层在数据中存储过期时间信息。查询时由业务程序判断是否过期，如果数据即将过期时，将缓存的时效延长，程序可以派遣一个线程去数据库中获取最新的数据，其他线程这时看到延长了的过期时间，就会继续使用旧数据，等派遣的线程获取最新数据后再更新缓存。\n\n推荐使用互斥锁，因为软过期会有业务逻辑侵入和额外的判断。\n\n面试话术：\n\n缓存击穿主要担心的是某个key过期，更新缓存时引起对数据库的突发高并发访问。因此我们可以在更新缓存时采用互斥锁控制，只允许一个线程去更新缓存，其它线程等待并重新读取缓存。例如redis的setnx命令就能实现互斥效果。\n\n\n# 3）缓存雪崩\n\n相关资料：\n\n缓存雪崩，是指在某一个时间段，缓存集中过期失效。对这批数据的访问查询，都落到了数据库上，对于数据库而言，就会产生周期性的压力波峰。\n\n解决方案：\n\n * 数据分类分批处理：采取不同分类数据，缓存不同周期\n * 相同分类数据：采用固定时长加随机数方式设置缓存\n * 热点数据缓存时间长一些，冷门数据缓存时间短一些\n * 避免redis节点宕机引起雪崩，搭建主从集群，保证高可用\n\n面试话术：\n\n解决缓存雪崩问题的关键是让缓存key的过期时间分散。因此我们可以把数据按照业务分类，然后设置不同过期时间。相同业务类型的key，设置固定时长加随机数。尽可能保证每个key的过期时间都不相同。\n\n另外，redis宕机也可能导致缓存雪崩，因此我们还要搭建redis主从集群及哨兵监控，保证redis的高可用。\n\n\n# 缓存冷热数据分离\n\n背景资料：\n\nredis使用的是内存存储，当需要海量数据存储时，成本非常高。\n\n经过调研发现，当前主流ddr3内存和主流sata ssd的单位成本价格差距大概在20倍左右，为了优化redis机器综合成本，我们考虑实现基于热度统计 的数据分级存储及数据在ram/flash之间的动态交换，从而大幅度降低成本，达到性能与成本的高平衡。\n\n基本思路：基于key访问次数(lfu)的热度统计算法识别出热点数据，并将热点数据保留在redis中，对于无访问/访问次数少的数据则转存到ssd上，如果ssd上的key再次变热，则重新将其加载到redis内存中。\n\n目前流行的高性能磁盘存储，并且遵循redis协议的方案包括：\n\n * ssdb：http://ssdb.io/zh_cn/\n * rocksdb：https://rocksdb.org.cn/\n\n因此，我们就需要在应用程序与缓存服务之间引入代理，实现redis和ssd之间的切换，如图：\n\n\n\n这样的代理方案阿里云提供的就有。当然也有一些开源方案，例如：https://github.com/jingchengli/swapdb\n\n\n# redis实现分布式锁\n\n分布式锁要满足的条件：\n\n * 多进程互斥：同一时刻，只有一个进程可以获取锁\n * 保证锁可以释放：任务结束或出现异常，锁一定要释放，避免死锁\n * 阻塞锁（可选）：获取锁失败时可否重试\n * 重入锁（可选）：获取锁的代码递归调用时，依然可以获取锁\n\n\n# 1）最基本的分布式锁：\n\n利用redis的setnx命令，这个命令的特征时如果多次执行，只有第一次执行会成功，可以实现互斥的效果。但是为了保证服务宕机时也可以释放锁，需要利用expire命令给锁设置一个有效期\n\nsetnx lock thread-01 # 尝试获取锁\nexpire lock 10 # 设置有效期\n\n\n面试官问题1：如果expire之前服务宕机怎么办？\n\n要保证setnx和expire命令的原子性。redis的set命令可以满足：\n\nset key value [nx] [ex time] \n\n\n需要添加nx和ex的选项：\n\n * nx：与setnx一致，第一次执行成功\n * ex：设置过期时间\n\n面试官问题2：释放锁的时候，如果自己的锁已经过期了，此时会出现安全漏洞，如何解决？\n\n在锁中存储当前进程和线程标识，释放锁时对锁的标识判断，如果是自己的则删除，不是则放弃操作。\n\n但是这两步操作要保证原子性，需要通过lua脚本来实现。\n\nif redis.call(\"get\",keys[1]) ** argv[1] then\n    redis.call(\"del\",keys[1])\nend\n\n\n\n# 2）可重入分布式锁\n\n如果有重入的需求，则除了在锁中记录进程标识，还要记录重试次数，流程如下：\n\n\n\n下面我们假设锁的key为“lock”，hashkey是当前线程的id：“threadid”，锁自动释放时间假设为20\n\n获取锁的步骤：\n\n * 1、判断lock是否存在 exists lock\n   * 存在，说明有人获取锁了，下面判断是不是自己的锁\n     * 判断当前线程id作为hashkey是否存在：hexists lock threadid\n       * 不存在，说明锁已经有了，且不是自己获取的，锁获取失败，end\n       * 存在，说明是自己获取的锁，重入次数+1：hincrby lock threadid 1，去到步骤3\n   * 2、不存在，说明可以获取锁，hset key threadid 1\n   * 3、设置锁自动释放时间，expire lock 20\n\n释放锁的步骤：\n\n * 1、判断当前线程id作为hashkey是否存在：hexists lock threadid\n   * 不存在，说明锁已经失效，不用管了\n   * 存在，说明锁还在，重入次数减1：hincrby lock threadid -1，获取新的重入次数\n * 2、判断重入次数是否为0：\n   * 为0，说明锁全部释放，删除key：del lock\n   * 大于0，说明锁还在使用，重置有效时间：expire lock 20\n\n对应的lua脚本如下：\n\n首先是获取锁：\n\nlocal key = keys[1]; -- 锁的key\nlocal threadid = argv[1]; -- 线程唯一标识\nlocal releasetime = argv[2]; -- 锁的自动释放时间\n\nif(redis.call('exists', key) ** 0) then -- 判断是否存在\n\tredis.call('hset', key, threadid, '1'); -- 不存在, 获取锁\n\tredis.call('expire', key, releasetime); -- 设置有效期\n\treturn 1; -- 返回结果\nend;\n\nif(redis.call('hexists', key, threadid) ** 1) then -- 锁已经存在，判断threadid是否是自己\t\n\tredis.call('hincrby', key, threadid, '1'); -- 不存在, 获取锁，重入次数+1\n\tredis.call('expire', key, releasetime); -- 设置有效期\n\treturn 1; -- 返回结果\nend;\nreturn 0; -- 代码走到这里,说明获取锁的不是自己，获取锁失败\n\n\n然后是释放锁：\n\nlocal key = keys[1]; -- 锁的key\nlocal threadid = argv[1]; -- 线程唯一标识\nlocal releasetime = argv[2]; -- 锁的自动释放时间\n\nif (redis.call('hexists', key, threadid) ** 0) then -- 判断当前锁是否还是被自己持有\n    return nil; -- 如果已经不是自己，则直接返回\nend;\nlocal count = redis.call('hincrby', key, threadid, -1); -- 是自己的锁，则重入次数-1\n\nif (count > 0) then -- 判断是否重入次数是否已经为0\n    redis.call('expire', key, releasetime); -- 大于0说明不能释放锁，重置有效期然后返回\n    return nil;\nelse\n    redis.call('del', key); -- 等于0说明可以释放锁，直接删除\n    return nil;\nend;\n\n\n\n# 3）高可用的锁\n\n面试官问题：redis分布式锁依赖与redis，如果redis宕机则锁失效。如何解决？\n\n此时大多数同学会回答说：搭建主从集群，做数据备份。\n\n这样就进入了陷阱，因为面试官的下一个问题就来了：\n\n面试官问题：如果搭建主从集群做数据备份时，进程a获取锁，master还没有把数据备份到slave，master宕机，slave升级为master，此时原来锁失效，其它进程也可以获取锁，出现安全问题。如何解决？\n\n关于这个问题，redis官网给出了解决方案，使用redlock思路可以解决：\n\n> 在redis的分布式环境中，我们假设有n个redis master。这些节点完全互相独立，不存在主从复制或者其他集群协调机制。之前我们已经描述了在redis单实例下怎么安全地获取和释放锁。我们确保将在每（n)个实例上使用此方法获取和释放锁。在这个样例中，我们假设有5个redis master节点，这是一个比较合理的设置，所以我们需要在5台机器上面或者5台虚拟机上面运行这些实例，这样保证他们不会同时都宕掉。\n> \n> 为了取到锁，客户端应该执行以下操作:\n> \n>  1. 获取当前unix时间，以毫秒为单位。\n>  2. 依次尝试从n个实例，使用相同的key和随机值获取锁。在步骤2，当向redis设置锁时,客户端应该设置一个网络连接和响应超时时间，这个超时时间应该小于锁的失效时间。例如你的锁自动失效时间为10秒，则超时时间应该在5-50毫秒之间。这样可以避免服务器端redis已经挂掉的情况下，客户端还在死死地等待响应结果。如果服务器端没有在规定时间内响应，客户端应该尽快尝试另外一个redis实例。\n>  3. 客户端使用当前时间减去开始获取锁时间（步骤1记录的时间）就得到获取锁使用的时间。当且仅当从大多数（这里是3个节点）的redis节点都取到锁，并且使用的时间小于锁失效时间时，锁才算获取成功。\n>  4. 如果取到了锁，key的真正有效时间等于有效时间减去获取锁所使用的时间（步骤3计算的结果）。\n>  5. 如果因为某些原因，获取锁失败（没有在至少n/2+1个redis实例取到锁或者取锁时间已经超过了有效时间），客户端应该在所有的redis实例上进行解锁（即便某些redis实例根本就没有加锁成功）。\n\n\n# 如何实现数据库与缓存数据一致？\n\n面试话术：\n\n实现方案有下面几种：\n\n * 本地缓存同步：当前微服务的数据库数据与缓存数据同步，可以直接在数据库修改时加入对redis的修改逻辑，保证一致。\n * 跨服务缓存同步：服务a调用了服务b，并对查询结果缓存。服务b数据库修改，可以通过mq通知服务a，服务a修改redis缓存数据\n * 通用方案：使用canal框架，伪装成mysql的salve节点，监听mysql的binlog变化，然后修改redis缓存数据",charsets:{cjk:!0}},{title:"微服务篇",frontmatter:{autoSort:10,title:"微服务篇",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/fa00f8/",categories:["面试"],tags:["知识","面试","微服务"]},regularPath:"/04.%E9%9D%A2%E8%AF%95/20.%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%AF%87.html",relativePath:"04.面试/20.微服务篇.md",key:"v-142bc356",path:"/pages/fa00f8/",headers:[{level:2,title:"SpringCloud常见组件有哪些？",slug:"springcloud常见组件有哪些",normalizedTitle:"springcloud常见组件有哪些？",charIndex:2},{level:2,title:"Nacos的服务注册表结构是怎样的？",slug:"nacos的服务注册表结构是怎样的",normalizedTitle:"nacos的服务注册表结构是怎样的？",charIndex:256},{level:2,title:"Nacos如何支撑阿里内部数十万服务注册压力？",slug:"nacos如何支撑阿里内部数十万服务注册压力",normalizedTitle:"nacos如何支撑阿里内部数十万服务注册压力？",charIndex:701},{level:2,title:"Nacos如何避免并发读写冲突问题？",slug:"nacos如何避免并发读写冲突问题",normalizedTitle:"nacos如何避免并发读写冲突问题？",charIndex:859},{level:2,title:"Nacos与Eureka的区别有哪些？",slug:"nacos与eureka的区别有哪些",normalizedTitle:"nacos与eureka的区别有哪些？",charIndex:1038},{level:2,title:"Sentinel的限流与Gateway的限流有什么差别？",slug:"sentinel的限流与gateway的限流有什么差别",normalizedTitle:"sentinel的限流与gateway的限流有什么差别？",charIndex:1344},{level:2,title:"Sentinel的线程隔离与Hystix的线程隔离有什么差别?",slug:"sentinel的线程隔离与hystix的线程隔离有什么差别",normalizedTitle:"sentinel的线程隔离与hystix的线程隔离有什么差别?",charIndex:1552}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"SpringCloud常见组件有哪些？ Nacos的服务注册表结构是怎样的？ Nacos如何支撑阿里内部数十万服务注册压力？ Nacos如何避免并发读写冲突问题？ Nacos与Eureka的区别有哪些？ Sentinel的限流与Gateway的限流有什么差别？ Sentinel的线程隔离与Hystix的线程隔离有什么差别?",content:"# SpringCloud常见组件有哪些？\n\n问题说明：这个题目主要考察对SpringCloud的组件基本了解\n\n难易程度：简单\n\n参考话术：\n\nSpringCloud包含的组件很多，有很多功能是重复的。其中最常用组件包括：\n\n•注册中心组件：Eureka、Nacos等\n\n•负载均衡组件：Ribbon\n\n•远程调用组件：OpenFeign\n\n•网关组件：Zuul、Gateway\n\n•服务保护组件：Hystrix、Sentinel\n\n•服务配置管理组件：SpringCloudConfig、Nacos\n\n\n# Nacos的服务注册表结构是怎样的？\n\n问题说明：考察对Nacos数据分级结构的了解，以及Nacos源码的掌握情况\n\n难易程度：一般\n\n参考话术：\n\nNacos采用了数据的分级存储模型，最外层是Namespace，用来隔离环境。然后是Group，用来对服务分组。接下来就是服务（Service）了，一个服务包含多个实例，但是可能处于不同机房，因此Service下有多个集群（Cluster），Cluster下是不同的实例（Instance）。\n\n对应到Java代码中，Nacos采用了一个多层的Map来表示。结构为Map<String, Map<String, Service>>，其中最外层Map的key就是namespaceId，值是一个Map。内层Map的key是group拼接serviceName，值是Service对象。Service对象内部又是一个Map，key是集群名称，值是Cluster对象。而Cluster对象内部维护了Instance的集合。\n\n如图：\n\n\n\n\n# Nacos如何支撑阿里内部数十万服务注册压力？\n\n问题说明：考察对Nacos源码的掌握情况\n\n难易程度：难\n\n参考话术：\n\nNacos内部接收到注册的请求时，不会立即写数据，而是将服务注册的任务放入一个阻塞队列就立即响应给客户端。然后利用线程池读取阻塞队列中的任务，异步来完成实例更新，从而提高并发写能力。\n\n\n# Nacos如何避免并发读写冲突问题？\n\n问题说明：考察对Nacos源码的掌握情况\n\n难易程度：难\n\n参考话术：\n\nNacos在更新实例列表时，会采用CopyOnWrite技术，首先将旧的实例列表拷贝一份，然后更新拷贝的实例列表，再用更新后的实例列表来覆盖旧的实例列表。\n\n这样在更新的过程中，就不会对读实例列表的请求产生影响，也不会出现脏读问题了。\n\n\n# Nacos与Eureka的区别有哪些？\n\n问题说明：考察对Nacos、Eureka的底层实现的掌握情况\n\n难易程度：难\n\n参考话术：\n\nNacos与Eureka有相同点，也有不同之处，可以从以下几点来描述：\n\n * 接口方式：Nacos与Eureka都对外暴露了Rest风格的API接口，用来实现服务注册、发现等功能\n * 实例类型：Nacos的实例有永久和临时实例之分；而Eureka只支持临时实例\n * 健康检测：Nacos对临时实例采用心跳模式检测，对永久实例采用主动请求来检测；Eureka只支持心跳模式\n * 服务发现：Nacos支持定时拉取和订阅推送两种模式；Eureka只支持定时拉取模式\n\n\n# Sentinel的限流与Gateway的限流有什么差别？\n\n问题说明：考察对限流算法的掌握情况\n\n难易程度：难\n\n参考话术：\n\n限流算法常见的有三种实现：滑动时间窗口、令牌桶算法、漏桶算法。Gateway则采用了基于Redis实现的令牌桶算法。\n\n而Sentinel内部却比较复杂：\n\n * 默认限流模式是基于滑动时间窗口算法\n * 排队等待的限流模式则基于漏桶算法\n * 而热点参数限流则是基于令牌桶算法\n\n\n# Sentinel的线程隔离与Hystix的线程隔离有什么差别?\n\n问题说明：考察对线程隔离方案的掌握情况\n\n难易程度：一般\n\n参考话术：\n\nHystix默认是基于线程池实现的线程隔离，每一个被隔离的业务都要创建一个独立的线程池，线程过多会带来额外的CPU开销，性能一般，但是隔离性更强。\n\nSentinel是基于信号量（计数器）实现的线程隔离，不用创建线程池，性能较好，但是隔离性一般。",normalizedContent:"# springcloud常见组件有哪些？\n\n问题说明：这个题目主要考察对springcloud的组件基本了解\n\n难易程度：简单\n\n参考话术：\n\nspringcloud包含的组件很多，有很多功能是重复的。其中最常用组件包括：\n\n•注册中心组件：eureka、nacos等\n\n•负载均衡组件：ribbon\n\n•远程调用组件：openfeign\n\n•网关组件：zuul、gateway\n\n•服务保护组件：hystrix、sentinel\n\n•服务配置管理组件：springcloudconfig、nacos\n\n\n# nacos的服务注册表结构是怎样的？\n\n问题说明：考察对nacos数据分级结构的了解，以及nacos源码的掌握情况\n\n难易程度：一般\n\n参考话术：\n\nnacos采用了数据的分级存储模型，最外层是namespace，用来隔离环境。然后是group，用来对服务分组。接下来就是服务（service）了，一个服务包含多个实例，但是可能处于不同机房，因此service下有多个集群（cluster），cluster下是不同的实例（instance）。\n\n对应到java代码中，nacos采用了一个多层的map来表示。结构为map<string, map<string, service>>，其中最外层map的key就是namespaceid，值是一个map。内层map的key是group拼接servicename，值是service对象。service对象内部又是一个map，key是集群名称，值是cluster对象。而cluster对象内部维护了instance的集合。\n\n如图：\n\n\n\n\n# nacos如何支撑阿里内部数十万服务注册压力？\n\n问题说明：考察对nacos源码的掌握情况\n\n难易程度：难\n\n参考话术：\n\nnacos内部接收到注册的请求时，不会立即写数据，而是将服务注册的任务放入一个阻塞队列就立即响应给客户端。然后利用线程池读取阻塞队列中的任务，异步来完成实例更新，从而提高并发写能力。\n\n\n# nacos如何避免并发读写冲突问题？\n\n问题说明：考察对nacos源码的掌握情况\n\n难易程度：难\n\n参考话术：\n\nnacos在更新实例列表时，会采用copyonwrite技术，首先将旧的实例列表拷贝一份，然后更新拷贝的实例列表，再用更新后的实例列表来覆盖旧的实例列表。\n\n这样在更新的过程中，就不会对读实例列表的请求产生影响，也不会出现脏读问题了。\n\n\n# nacos与eureka的区别有哪些？\n\n问题说明：考察对nacos、eureka的底层实现的掌握情况\n\n难易程度：难\n\n参考话术：\n\nnacos与eureka有相同点，也有不同之处，可以从以下几点来描述：\n\n * 接口方式：nacos与eureka都对外暴露了rest风格的api接口，用来实现服务注册、发现等功能\n * 实例类型：nacos的实例有永久和临时实例之分；而eureka只支持临时实例\n * 健康检测：nacos对临时实例采用心跳模式检测，对永久实例采用主动请求来检测；eureka只支持心跳模式\n * 服务发现：nacos支持定时拉取和订阅推送两种模式；eureka只支持定时拉取模式\n\n\n# sentinel的限流与gateway的限流有什么差别？\n\n问题说明：考察对限流算法的掌握情况\n\n难易程度：难\n\n参考话术：\n\n限流算法常见的有三种实现：滑动时间窗口、令牌桶算法、漏桶算法。gateway则采用了基于redis实现的令牌桶算法。\n\n而sentinel内部却比较复杂：\n\n * 默认限流模式是基于滑动时间窗口算法\n * 排队等待的限流模式则基于漏桶算法\n * 而热点参数限流则是基于令牌桶算法\n\n\n# sentinel的线程隔离与hystix的线程隔离有什么差别?\n\n问题说明：考察对线程隔离方案的掌握情况\n\n难易程度：一般\n\n参考话术：\n\nhystix默认是基于线程池实现的线程隔离，每一个被隔离的业务都要创建一个独立的线程池，线程过多会带来额外的cpu开销，性能一般，但是隔离性更强。\n\nsentinel是基于信号量（计数器）实现的线程隔离，不用创建线程池，性能较好，但是隔离性一般。",charsets:{cjk:!0}},{title:"MQ相关",frontmatter:{autoSort:9,title:"MQ相关",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/8b5a99/",categories:["面试"],tags:["知识","面试","MQ"]},regularPath:"/04.%E9%9D%A2%E8%AF%95/30.MQ.html",relativePath:"04.面试/30.MQ.md",key:"v-a797ba30",path:"/pages/8b5a99/",headers:[{level:2,title:"你们为什么选择了RabbitMQ而不是其它的MQ？",slug:"你们为什么选择了rabbitmq而不是其它的mq",normalizedTitle:"你们为什么选择了rabbitmq而不是其它的mq？",charIndex:2},{level:2,title:"RabbitMQ如何确保消息的不丢失？",slug:"rabbitmq如何确保消息的不丢失",normalizedTitle:"rabbitmq如何确保消息的不丢失？",charIndex:391},{level:2,title:"RabbitMQ如何避免消息堆积？",slug:"rabbitmq如何避免消息堆积",normalizedTitle:"rabbitmq如何避免消息堆积？",charIndex:1528},{level:2,title:"RabbitMQ如何保证消息的有序性？",slug:"rabbitmq如何保证消息的有序性",normalizedTitle:"rabbitmq如何保证消息的有序性？",charIndex:2079},{level:2,title:"如何防止MQ消息被重复消费？",slug:"如何防止mq消息被重复消费",normalizedTitle:"如何防止mq消息被重复消费？",charIndex:2293},{level:2,title:"如何保证RabbitMQ的高可用？",slug:"如何保证rabbitmq的高可用",normalizedTitle:"如何保证rabbitmq的高可用？",charIndex:2564},{level:2,title:"使用MQ可以解决那些问题？",slug:"使用mq可以解决那些问题",normalizedTitle:"使用mq可以解决那些问题？",charIndex:2681}],lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:"你们为什么选择了RabbitMQ而不是其它的MQ？ RabbitMQ如何确保消息的不丢失？ RabbitMQ如何避免消息堆积？ RabbitMQ如何保证消息的有序性？ 如何防止MQ消息被重复消费？ 如何保证RabbitMQ的高可用？ 使用MQ可以解决那些问题？",content:"# 你们为什么选择了RabbitMQ而不是其它的MQ？\n\n如图：\n\n\n\n话术：\n\nkafka是以吞吐量高而闻名，不过其数据稳定性一般，而且无法保证消息有序性。我们公司的日志收集也有使用，业务模块中则使用的RabbitMQ。\n\n阿里巴巴的RocketMQ基于Kafka的原理，弥补了Kafka的缺点，继承了其高吞吐的优势，其客户端目前以Java为主。但是我们担心阿里巴巴开源产品的稳定性，所以就没有使用。\n\nRabbitMQ基于面向并发的语言Erlang开发，吞吐量不如Kafka，但是对我们公司来讲够用了。而且消息可靠性较好，并且消息延迟极低，集群搭建比较方便。支持多种协议，并且有各种语言的客户端，比较灵活。Spring对RabbitMQ的支持也比较好，使用起来比较方便，比较符合我们公司的需求。\n\n综合考虑我们公司的并发需求以及稳定性需求，我们选择了RabbitMQ。\n\n\n# RabbitMQ如何确保消息的不丢失？\n\n话术：\n\nRabbitMQ针对消息传递过程中可能发生问题的各个地方，给出了针对性的解决方案：\n\n * 生产者发送消息时可能因为网络问题导致消息没有到达交换机：\n   * RabbitMQ提供了publisher confirm机制\n     * 生产者发送消息后，可以编写ConfirmCallback函数\n     * 消息成功到达交换机后，RabbitMQ会调用ConfirmCallback通知消息的发送者，返回ACK\n     * 消息如果未到达交换机，RabbitMQ也会调用ConfirmCallback通知消息的发送者，返回NACK\n     * 消息超时未发送成功也会抛出异常\n * 消息到达交换机后，如果未能到达队列，也会导致消息丢失：\n   * RabbitMQ提供了publisher return机制\n     * 生产者可以定义ReturnCallback函数\n     * 消息到达交换机，未到达队列，RabbitMQ会调用ReturnCallback通知发送者，告知失败原因\n * 消息到达队列后，MQ宕机也可能导致丢失消息：\n   * RabbitMQ提供了持久化功能，集群的主从备份功能\n     * 消息持久化，RabbitMQ会将交换机、队列、消息持久化到磁盘，宕机重启可以恢复消息\n     * 镜像集群，仲裁队列，都可以提供主从备份功能，主节点宕机，从节点会自动切换为主，数据依然在\n * 消息投递给消费者后，如果消费者处理不当，也可能导致消息丢失\n   * SpringAMQP基于RabbitMQ提供了消费者确认机制、消费者重试机制，消费者失败处理策略：\n     * 消费者的确认机制：\n       * 消费者处理消息成功，未出现异常时，Spring返回ACK给RabbitMQ，消息才被移除\n       * 消费者处理消息失败，抛出异常，宕机，Spring返回NACK或者不返回结果，消息不被异常\n     * 消费者重试机制：\n       * 默认情况下，消费者处理失败时，消息会再次回到MQ队列，然后投递给其它消费者。Spring提供的消费者重试机制，则是在处理失败后不返回NACK，而是直接在消费者本地重试。多次重试都失败后，则按照消费者失败处理策略来处理消息。避免了消息频繁入队带来的额外压力。\n     * 消费者失败策略：\n       * 当消费者多次本地重试失败时，消息默认会丢弃。\n       * Spring提供了Republish策略，在多次重试都失败，耗尽重试次数后，将消息重新投递给指定的异常交换机，并且会携带上异常栈信息，帮助定位问题。\n\n\n# RabbitMQ如何避免消息堆积？\n\n话术：\n\n消息堆积问题产生的原因往往是因为消息发送的速度超过了消费者消息处理的速度。因此解决方案无外乎以下三点：\n\n * 提高消费者处理速度\n * 增加更多消费者\n * 增加队列消息存储上限\n\n1）提高消费者处理速度\n\n消费者处理速度是由业务代码决定的，所以我们能做的事情包括：\n\n * 尽可能优化业务代码，提高业务性能\n * 接收到消息后，开启线程池，并发处理多个消息\n\n优点：成本低，改改代码即可\n\n缺点：开启线程池会带来额外的性能开销，对于高频、低时延的任务不合适。推荐任务执行周期较长的业务。\n\n2）增加更多消费者\n\n一个队列绑定多个消费者，共同争抢任务，自然可以提供消息处理的速度。\n\n优点：能用钱解决的问题都不是问题。实现简单粗暴\n\n缺点：问题是没有钱。成本太高\n\n3）增加队列消息存储上限\n\n在RabbitMQ的1.8版本后，加入了新的队列模式：Lazy Queue\n\n这种队列不会将消息保存在内存中，而是在收到消息后直接写入磁盘中，理论上没有存储上限。可以解决消息堆积问题。\n\n优点：磁盘存储更安全；存储无上限；避免内存存储带来的Page Out问题，性能更稳定；\n\n缺点：磁盘存储受到IO性能的限制，消息时效性不如内存模式，但影响不大。\n\n\n# RabbitMQ如何保证消息的有序性？\n\n话术：\n\n其实RabbitMQ是队列存储，天然具备先进先出的特点，只要消息的发送是有序的，那么理论上接收也是有序的。不过当一个队列绑定了多个消费者时，可能出现消息轮询投递给消费者的情况，而消费者的处理顺序就无法保证了。\n\n因此，要保证消息的有序性，需要做的下面几点：\n\n * 保证消息发送的有序性\n * 保证一组有序的消息都发送到同一个队列\n * 保证一个队列只包含一个消费者\n\n\n# 如何防止MQ消息被重复消费？\n\n话术：\n\n消息重复消费的原因多种多样，不可避免。所以只能从消费者端入手，只要能保证消息处理的幂等性就可以确保消息不被重复消费。\n\n而幂等性的保证又有很多方案：\n\n * 给每一条消息都添加一个唯一id，在本地记录消息表及消息状态，处理消息时基于数据库表的id唯一性做判断\n * 同样是记录消息表，利用消息状态字段实现基于乐观锁的判断，保证幂等\n * 基于业务本身的幂等性。比如根据id的删除、查询业务天生幂等；新增、修改等业务可以考虑基于数据库id唯一性、或者乐观锁机制确保幂等。本质与消息表方案类似。\n\n\n# 如何保证RabbitMQ的高可用？\n\n话术：\n\n要实现RabbitMQ的高可用无外乎下面两点：\n\n * 做好交换机、队列、消息的持久化\n * 搭建RabbitMQ的镜像集群，做好主从备份。当然也可以使用仲裁队列代替镜像集群。\n\n\n# 使用MQ可以解决那些问题？\n\n话术：\n\nRabbitMQ能解决的问题很多，例如：\n\n * 解耦合：将几个业务关联的微服务调用修改为基于MQ的异步通知，可以解除微服务之间的业务耦合。同时还提高了业务性能。\n * 流量削峰：将突发的业务请求放入MQ中，作为缓冲区。后端的业务根据自己的处理能力从MQ中获取消息，逐个处理任务。流量曲线变的平滑很多\n * 延迟队列：基于RabbitMQ的死信队列或者DelayExchange插件，可以实现消息发送后，延迟接收的效果。",normalizedContent:"# 你们为什么选择了rabbitmq而不是其它的mq？\n\n如图：\n\n\n\n话术：\n\nkafka是以吞吐量高而闻名，不过其数据稳定性一般，而且无法保证消息有序性。我们公司的日志收集也有使用，业务模块中则使用的rabbitmq。\n\n阿里巴巴的rocketmq基于kafka的原理，弥补了kafka的缺点，继承了其高吞吐的优势，其客户端目前以java为主。但是我们担心阿里巴巴开源产品的稳定性，所以就没有使用。\n\nrabbitmq基于面向并发的语言erlang开发，吞吐量不如kafka，但是对我们公司来讲够用了。而且消息可靠性较好，并且消息延迟极低，集群搭建比较方便。支持多种协议，并且有各种语言的客户端，比较灵活。spring对rabbitmq的支持也比较好，使用起来比较方便，比较符合我们公司的需求。\n\n综合考虑我们公司的并发需求以及稳定性需求，我们选择了rabbitmq。\n\n\n# rabbitmq如何确保消息的不丢失？\n\n话术：\n\nrabbitmq针对消息传递过程中可能发生问题的各个地方，给出了针对性的解决方案：\n\n * 生产者发送消息时可能因为网络问题导致消息没有到达交换机：\n   * rabbitmq提供了publisher confirm机制\n     * 生产者发送消息后，可以编写confirmcallback函数\n     * 消息成功到达交换机后，rabbitmq会调用confirmcallback通知消息的发送者，返回ack\n     * 消息如果未到达交换机，rabbitmq也会调用confirmcallback通知消息的发送者，返回nack\n     * 消息超时未发送成功也会抛出异常\n * 消息到达交换机后，如果未能到达队列，也会导致消息丢失：\n   * rabbitmq提供了publisher return机制\n     * 生产者可以定义returncallback函数\n     * 消息到达交换机，未到达队列，rabbitmq会调用returncallback通知发送者，告知失败原因\n * 消息到达队列后，mq宕机也可能导致丢失消息：\n   * rabbitmq提供了持久化功能，集群的主从备份功能\n     * 消息持久化，rabbitmq会将交换机、队列、消息持久化到磁盘，宕机重启可以恢复消息\n     * 镜像集群，仲裁队列，都可以提供主从备份功能，主节点宕机，从节点会自动切换为主，数据依然在\n * 消息投递给消费者后，如果消费者处理不当，也可能导致消息丢失\n   * springamqp基于rabbitmq提供了消费者确认机制、消费者重试机制，消费者失败处理策略：\n     * 消费者的确认机制：\n       * 消费者处理消息成功，未出现异常时，spring返回ack给rabbitmq，消息才被移除\n       * 消费者处理消息失败，抛出异常，宕机，spring返回nack或者不返回结果，消息不被异常\n     * 消费者重试机制：\n       * 默认情况下，消费者处理失败时，消息会再次回到mq队列，然后投递给其它消费者。spring提供的消费者重试机制，则是在处理失败后不返回nack，而是直接在消费者本地重试。多次重试都失败后，则按照消费者失败处理策略来处理消息。避免了消息频繁入队带来的额外压力。\n     * 消费者失败策略：\n       * 当消费者多次本地重试失败时，消息默认会丢弃。\n       * spring提供了republish策略，在多次重试都失败，耗尽重试次数后，将消息重新投递给指定的异常交换机，并且会携带上异常栈信息，帮助定位问题。\n\n\n# rabbitmq如何避免消息堆积？\n\n话术：\n\n消息堆积问题产生的原因往往是因为消息发送的速度超过了消费者消息处理的速度。因此解决方案无外乎以下三点：\n\n * 提高消费者处理速度\n * 增加更多消费者\n * 增加队列消息存储上限\n\n1）提高消费者处理速度\n\n消费者处理速度是由业务代码决定的，所以我们能做的事情包括：\n\n * 尽可能优化业务代码，提高业务性能\n * 接收到消息后，开启线程池，并发处理多个消息\n\n优点：成本低，改改代码即可\n\n缺点：开启线程池会带来额外的性能开销，对于高频、低时延的任务不合适。推荐任务执行周期较长的业务。\n\n2）增加更多消费者\n\n一个队列绑定多个消费者，共同争抢任务，自然可以提供消息处理的速度。\n\n优点：能用钱解决的问题都不是问题。实现简单粗暴\n\n缺点：问题是没有钱。成本太高\n\n3）增加队列消息存储上限\n\n在rabbitmq的1.8版本后，加入了新的队列模式：lazy queue\n\n这种队列不会将消息保存在内存中，而是在收到消息后直接写入磁盘中，理论上没有存储上限。可以解决消息堆积问题。\n\n优点：磁盘存储更安全；存储无上限；避免内存存储带来的page out问题，性能更稳定；\n\n缺点：磁盘存储受到io性能的限制，消息时效性不如内存模式，但影响不大。\n\n\n# rabbitmq如何保证消息的有序性？\n\n话术：\n\n其实rabbitmq是队列存储，天然具备先进先出的特点，只要消息的发送是有序的，那么理论上接收也是有序的。不过当一个队列绑定了多个消费者时，可能出现消息轮询投递给消费者的情况，而消费者的处理顺序就无法保证了。\n\n因此，要保证消息的有序性，需要做的下面几点：\n\n * 保证消息发送的有序性\n * 保证一组有序的消息都发送到同一个队列\n * 保证一个队列只包含一个消费者\n\n\n# 如何防止mq消息被重复消费？\n\n话术：\n\n消息重复消费的原因多种多样，不可避免。所以只能从消费者端入手，只要能保证消息处理的幂等性就可以确保消息不被重复消费。\n\n而幂等性的保证又有很多方案：\n\n * 给每一条消息都添加一个唯一id，在本地记录消息表及消息状态，处理消息时基于数据库表的id唯一性做判断\n * 同样是记录消息表，利用消息状态字段实现基于乐观锁的判断，保证幂等\n * 基于业务本身的幂等性。比如根据id的删除、查询业务天生幂等；新增、修改等业务可以考虑基于数据库id唯一性、或者乐观锁机制确保幂等。本质与消息表方案类似。\n\n\n# 如何保证rabbitmq的高可用？\n\n话术：\n\n要实现rabbitmq的高可用无外乎下面两点：\n\n * 做好交换机、队列、消息的持久化\n * 搭建rabbitmq的镜像集群，做好主从备份。当然也可以使用仲裁队列代替镜像集群。\n\n\n# 使用mq可以解决那些问题？\n\n话术：\n\nrabbitmq能解决的问题很多，例如：\n\n * 解耦合：将几个业务关联的微服务调用修改为基于mq的异步通知，可以解除微服务之间的业务耦合。同时还提高了业务性能。\n * 流量削峰：将突发的业务请求放入mq中，作为缓冲区。后端的业务根据自己的处理能力从mq中获取消息，逐个处理任务。流量曲线变的平滑很多\n * 延迟队列：基于rabbitmq的死信队列或者delayexchange插件，可以实现消息发送后，延迟接收的效果。",charsets:{cjk:!0}},{title:"blog",frontmatter:{title:"blog",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/2aa63b/",categories:["项目记录","blog"],tags:["项目","记录","blog"]},regularPath:"/05.%E9%A1%B9%E7%9B%AE%E8%AE%B0%E5%BD%95/05.blog/05.blog.html",relativePath:"05.项目记录/05.blog/05.blog.md",key:"v-a382d2da",path:"/pages/2aa63b/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"# blog\n\n 1. vuepress无法渲染带表格的md文件？\n    \n    > 由于一篇博客在上传的时候显示空白页，起初以为是文件太大，进行了拆分，拆分出去的md文件都可以正常显示。但是带表格的那个md文件依旧无法显示。\n    > \n    > 新建了一个只带表格的md文件，结果可以成功显示。将原md文件中的表格删除也可以成功显示。\n    > \n    > 那么是哪里出现了问题呢？？\n    \n    经过仔细排查发现，原来是表格中的尖括号出现了问题，由于是渲染成html文件，所以要使用<>的话，最好加一个转义字符。\n    \n    Set\\<K> keySet()\n    \n    Set<K> keySet()\n\n 2. YAML front matter 的书写\n    \n    yml格式的书写，一定一定一定记得加空格！！！\n    \n    title: 123\n    \n    autoSort: 97\n\n 3. 图片的插入，不能使用点，正确方式\n    \n    /assets/后端/数据库/Redis/\n    \n    对应的目录结构为：D:\\blog\\docs\\.vuepress\\public\\assets\\后端\\数据库\\Redis\n\n 4. 最多只能到四级目录！！！！！ 切记\n    \n    * 第一层目录为docs/\n    \n    * 第二层目录为docs/后端\n    \n    * 第三层目录为docs/后端/数据库\n    \n    * 第四层目录为docs/后端/数据库/Redis\n    \n    * Redis中不能有第五层目录了！！！！切记！！！！",normalizedContent:"# blog\n\n 1. vuepress无法渲染带表格的md文件？\n    \n    > 由于一篇博客在上传的时候显示空白页，起初以为是文件太大，进行了拆分，拆分出去的md文件都可以正常显示。但是带表格的那个md文件依旧无法显示。\n    > \n    > 新建了一个只带表格的md文件，结果可以成功显示。将原md文件中的表格删除也可以成功显示。\n    > \n    > 那么是哪里出现了问题呢？？\n    \n    经过仔细排查发现，原来是表格中的尖括号出现了问题，由于是渲染成html文件，所以要使用<>的话，最好加一个转义字符。\n    \n    set\\<k> keyset()\n    \n    set<k> keyset()\n\n 2. yaml front matter 的书写\n    \n    yml格式的书写，一定一定一定记得加空格！！！\n    \n    title: 123\n    \n    autosort: 97\n\n 3. 图片的插入，不能使用点，正确方式\n    \n    /assets/后端/数据库/redis/\n    \n    对应的目录结构为：d:\\blog\\docs\\.vuepress\\public\\assets\\后端\\数据库\\redis\n\n 4. 最多只能到四级目录！！！！！ 切记\n    \n    * 第一层目录为docs/\n    \n    * 第二层目录为docs/后端\n    \n    * 第三层目录为docs/后端/数据库\n    \n    * 第四层目录为docs/后端/数据库/redis\n    \n    * redis中不能有第五层目录了！！！！切记！！！！",charsets:{cjk:!0}},{title:"README",frontmatter:{title:"README",date:"2023-06-30T20:30:40.000Z",permalink:"/DOA/",categories:["DOA"],tags:["知识","DOA"]},regularPath:"/06.DOA/01.DOA.html",relativePath:"06.DOA/01.DOA.md",key:"v-b795f742",path:"/DOA/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"# DOA估计\n\n这个板块将会记录一些研究领域的内容。\n\nDOA估计\n\n稀疏阵列DOA估计\n\n矩阵填充算法\n\n压缩感知算法",normalizedContent:"# doa估计\n\n这个板块将会记录一些研究领域的内容。\n\ndoa估计\n\n稀疏阵列doa估计\n\n矩阵填充算法\n\n压缩感知算法",charsets:{cjk:!0}},{title:"README",frontmatter:{title:"README",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/1b70fb/",categories:["DOA"],tags:[null]},regularPath:"/06.DOA/",relativePath:"06.DOA/README.md",key:"v-567d2eac",path:"/pages/1b70fb/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"# DOA估计\n\n这个板块将会记录一些研究领域的内容。\n\nDOA估计\n\n稀疏阵列DOA估计\n\n矩阵填充算法\n\n压缩感知算法",normalizedContent:"# doa估计\n\n这个板块将会记录一些研究领域的内容。\n\ndoa估计\n\n稀疏阵列doa估计\n\n矩阵填充算法\n\n压缩感知算法",charsets:{cjk:!0}},{title:"快乐收藏馆",frontmatter:{title:"快乐收藏馆",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/d591da/",categories:["life"],tags:["记录","生活"]},regularPath:"/07.life/01.%E6%B5%B7%E6%B5%AA.html",relativePath:"07.life/01.海浪.md",key:"v-6cf60d0d",path:"/pages/d591da/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"# 快乐收藏馆\n\n这个板块将会记录一些生活的精彩瞬间\n\n",normalizedContent:"# 快乐收藏馆\n\n这个板块将会记录一些生活的精彩瞬间\n\n",charsets:{cjk:!0}},{title:"分类",frontmatter:{categoriesPage:!0,title:"分类",permalink:"/categories/",article:!1},regularPath:"/@pages/categoriesPage.html",relativePath:"@pages/categoriesPage.md",key:"v-0bfafa42",path:"/categories/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"站外导航",frontmatter:{title:"站外导航",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/961a83/",categories:["任意门"],tags:["记录"]},regularPath:"/08.%E4%BB%BB%E6%84%8F%E9%97%A8/02.%E7%AB%99%E5%A4%96%E5%AF%BC%E8%88%AA.html",relativePath:"08.任意门/02.站外导航.md",key:"v-b1e04020",path:"/pages/961a83/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"# 站外导航\n\n这里会推荐一些宝藏网站！",normalizedContent:"# 站外导航\n\n这里会推荐一些宝藏网站！",charsets:{cjk:!0}},{title:"归档",frontmatter:{archivesPage:!0,title:"归档",permalink:"/archives/",article:!1},regularPath:"/@pages/archivesPage.html",relativePath:"@pages/archivesPage.md",key:"v-306f947f",path:"/archives/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"edg夺冠",frontmatter:{title:"edg夺冠",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/4607dc/",categories:["life"],tags:["记录","生活"]},regularPath:"/07.life/05.edg.html",relativePath:"07.life/05.edg.md",key:"v-56be318a",path:"/pages/4607dc/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"# EDG\n\n我们是冠军！！！\n\n.",normalizedContent:"# edg\n\n我们是冠军！！！\n\n.",charsets:{cjk:!0}},{title:"标签",frontmatter:{tagsPage:!0,title:"标签",permalink:"/tags/",article:!1},regularPath:"/@pages/tagsPage.html",relativePath:"@pages/tagsPage.md",key:"v-d87ad802",path:"/tags/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"Home",frontmatter:{home:!0,heroText:"凉冰的手记",tagline:"记录生活的精彩瞬间，记录一个程序猿的成长！",features:[{title:"后端",details:"Java、SSM、SpringBoot、微服务等",link:"/Java-C/",imgUrl:"/assets/img/java6.webp"},{title:"算法",details:"数据结构与算法，leetcode刷题记录",link:"/Algorithm/",imgUrl:"/assets/img/算法.jpg"},{title:"技术",details:"技术文档、教程、技巧、实用工具",link:"/technology/",imgUrl:"/assets/img/other.png"}],footer:"MIT Licensed | Copyright © 2018-2022 |  by 凉冰"},regularPath:"/",relativePath:"README.md",key:"v-2fc6a049",path:"/",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"干了这碗鸡汤！",frontmatter:{title:"干了这碗鸡汤！",date:"2023-06-30T20:30:40.000Z",permalink:"/pages/f2e63f",sidebar:"auto",categories:["随笔"],tags:["鸡汤"],author:{name:"凉冰",link:"https://github.com/diana-devil"}},regularPath:"/_posts/%E9%9A%8F%E7%AC%94/%E9%B8%A1%E6%B1%A4.html",relativePath:"_posts/随笔/鸡汤.md",key:"v-a21049fc",path:"/pages/f2e63f/",excerpt:"<p>不知道大家有没有发现，我们身边经常有这样的人，他们越是有能力的，越是有知识的，越是低调，越是谦逊，因为他们深知，知道的越多，不知道的也就越多。</p>\n",lastUpdated:"2023 07 3",lastUpdatedTimestamp:1688350531e3,headersStr:null,content:"不知道大家有没有发现，我们身边经常有这样的人，他们越是有能力的，越是有知识的，越是低调，越是谦逊，因为他们深知，知道的越多，不知道的也就越多。\n\n你知道的越多，你不知道的也就越多，这是一句非常有哲理的话。\n\n每个人的知识面都是有限的，你有可能在某个领域会有较深的研究，成为这个领域里的专家，等到你站在高处的时候，才会发现，自己是多么的渺小，才知道自己有多少没有涉及的领域。知道的越多，疑惑、问题就会越来越多，对已知的质疑、疑虑、困惑就会越来越多。\n\n即使如此，我们也应该努力，至少可以成为某个领域的佼佼者。\n\n鸡汤1\n\n弱小的人，才习惯嘲讽与否定，而内心强大的人，从不吝啬赞美与鼓励。\n\n鸡汤2\n\n当代青年人都应该摆脱冷气，只管向上走，不必听从自暴自弃者的流言。能做事的做事，能发声的发声。有一份热，发一份光，就像萤火一般，也可以在黑暗里发一点光，不必等候炬火。",normalizedContent:"不知道大家有没有发现，我们身边经常有这样的人，他们越是有能力的，越是有知识的，越是低调，越是谦逊，因为他们深知，知道的越多，不知道的也就越多。\n\n你知道的越多，你不知道的也就越多，这是一句非常有哲理的话。\n\n每个人的知识面都是有限的，你有可能在某个领域会有较深的研究，成为这个领域里的专家，等到你站在高处的时候，才会发现，自己是多么的渺小，才知道自己有多少没有涉及的领域。知道的越多，疑惑、问题就会越来越多，对已知的质疑、疑虑、困惑就会越来越多。\n\n即使如此，我们也应该努力，至少可以成为某个领域的佼佼者。\n\n鸡汤1\n\n弱小的人，才习惯嘲讽与否定，而内心强大的人，从不吝啬赞美与鼓励。\n\n鸡汤2\n\n当代青年人都应该摆脱冷气，只管向上走，不必听从自暴自弃者的流言。能做事的做事，能发声的发声。有一份热，发一份光，就像萤火一般，也可以在黑暗里发一点光，不必等候炬火。",charsets:{cjk:!0}}],themeConfig:{nav:[{text:"主页",link:"/"},{text:"后端",link:"/Java-C/",items:[{text:"Java",link:"/back/java/"},{text:"JavaWeb",link:"/back/javaWeb/"},{text:"数据库",link:"/back/sql/"},{text:"SSM",link:"/back/SSM/"},{text:"SpringBoot",link:"/back/SpringBoot/"},{text:"微服务",link:"/back/Microservice/"}]},{text:"算法",link:"/Algorithm/"},{text:"技术",link:"/technology/",items:[{text:"Git",link:"/technology/Git/"},{text:"Linux",link:"/technology/Linux/"},{text:"正则表达式",link:"/pages/b5d182/"}]},{text:"面试",link:"/pages/edb686/"},{text:"项目记录",link:"/pages/2aa63b/"},{text:"DOA",link:"/pages/1b70fb/"},{text:"精彩瞬间",link:"/pages/d591da/"},{text:"任意门",link:"/pages/961a83/"},{text:"GitHub",link:"https://github.com/diana-devil/diana-devil.github.io"}],sidebarDepth:2,logo:"/恶魔.webp",searchMaxSuggestions:10,lastUpdated:"上次更新",docsDir:"docs",editLinks:!0,editLinkText:"编辑",sidebar:{"/01.后端/":[{title:"Java",collapsable:!0,children:[{title:"集合",collapsable:!0,children:[["10.Java/50.集合/10.Map基础.md","Map基础","/pages/6862b9/"],["10.Java/50.集合/20.HashMap.md","HashMap","/pages/cfbd66/"],["10.Java/50.集合/25.TreeMap.md","TreeMap","/pages/bb8a3e/"],["10.Java/50.集合/30.可变参数.md","Collections类","/pages/9f2f57/"],["10.Java/50.集合/40.综合练习.md","综合练习","/pages/8b78d9/"],["10.Java/50.集合/50.斗地主练习.md","斗地主练习","/pages/5208ee/"],["10.Java/50.集合/60.不可变集合.md","不可变集合","/pages/d8ef9d/"],["10.Java/50.集合/70.stream流.md","stream流","/pages/bc84f5/"],["10.Java/50.集合/80.方法引用.md","方法引用","/pages/eeb1b8/"]]},{title:"流",collapsable:!0,children:[["10.Java/60.流/05.异常.md","异常","/pages/2ff7b9/"],["10.Java/60.流/10.File基础.md","File基础","/pages/217f27/"],["10.Java/60.流/15.File综合练习.md","File练习","/pages/cb6708/"],["10.Java/60.流/20.IO流概述.md","IO流概述","/pages/4fe65a/"],["10.Java/60.流/25.字节流.md","字节流","/pages/674de2/"],["10.Java/60.流/30.字符流.md","字符流","/pages/f0c1fd/"],["10.Java/60.流/35.IO流异常.md","IO异常处理","/pages/2ca28f/"],["10.Java/60.流/40.IO流综合练习.md","IO流练习","/pages/6e842a/"],["10.Java/60.流/45.缓冲流.md","缓冲流","/pages/bce7f1/"],["10.Java/60.流/47.转换流.md","转换流","/pages/d2a882/"],["10.Java/60.流/49.序列流.md","序列流","/pages/57821f/"],["10.Java/60.流/51.打印流.md","打印流","/pages/a9cd19/"],["10.Java/60.流/53.压缩流.md","压缩流","/pages/6201c3/"],["10.Java/60.流/70.工具包.md","工具包","/pages/732670/"]]}]},{title:"JavaWeb",collapsable:!0,children:[["20.JavaWeb/10.Filter.md","Filter","/pages/7a0933/"],["20.JavaWeb/15.Listener.md","Listener","/pages/3b2939/"],["20.JavaWeb/20.Ajax.md","Ajax","/pages/36c3d1/"],["20.JavaWeb/25.Axios.md","Axios","/pages/140b55/"],["20.JavaWeb/30.JSON.md","Json","/pages/63836e/"],["20.JavaWeb/35.JSP.md","JSP","/pages/4f9642/"],["20.JavaWeb/40.VUE.md","VUE","/pages/7250e2/"],["20.JavaWeb/45.Element.md","Element","/pages/f1ca40/"],["20.JavaWeb/50.会话技术.md","会话技术","/pages/a450d7/"],["20.JavaWeb/55.用户登录注册案例.md","登录注册案例","/pages/4f0756/"]]},{title:"数据库",collapsable:!0,children:[{title:"MySQL",collapsable:!0,children:[["30.数据库/01.MySQL/01.SQL语句.md","SQL语句","/pages/e5c957/"],["30.数据库/01.MySQL/05.函数.md","函数","/pages/e00b6b/"],["30.数据库/01.MySQL/10.约束.md","约束","/pages/ab672a/"],["30.数据库/01.MySQL/15.多表查询.md","多表查询","/pages/a367f0/"],["30.数据库/01.MySQL/20.事务.md","事务","/pages/05f9b5/"]]},{title:"Redis",collapsable:!0,children:[["30.数据库/05.Redis/05.Redis基础.md","Redis基础","/pages/31e22b/"],["30.数据库/05.Redis/10.Redis常见命令.md","Redis常见命令","/pages/948263/"],["30.数据库/05.Redis/15.Redis-API.md","RedisAPI","/pages/94a219/"],["30.数据库/05.Redis/20.Redis优化.md","Redis优化","/pages/444956/"],["30.数据库/05.Redis/23.Redis持久化.md","Redis持久化","/pages/7c7a2d/"],["30.数据库/05.Redis/25.Redis主从.md","Redis主从","/pages/ef6419/"],["30.数据库/05.Redis/27.Redis哨兵.md","Redis哨兵","/pages/74c463/"],["30.数据库/05.Redis/30.Redis分片集群.md","Redis分片集群","/pages/f86562/"],["30.数据库/05.Redis/35.分布式缓存总结.md","分布式缓存总结","/pages/981287/"],["30.数据库/05.Redis/40.Redis数据结构.md","Redis数据结构","/pages/a0085d/"],["30.数据库/05.Redis/45.Redis网络模型.md","Redis通信协议","/pages/11137c/"],["30.数据库/05.Redis/50.Redis通信协议.md","Redis网络模型","/pages/638026/"],["30.数据库/05.Redis/55.Redis内存回收.md","Redis内存回收","/pages/11b084/"],["30.数据库/05.Redis/101.单机安装Redis.md","单机安装Redis","/pages/2d9387/"],["30.数据库/05.Redis/102.主从集群搭建.md","主从集群搭建","/pages/564491/"],["30.数据库/05.Redis/103.哨兵集群搭建.md","哨兵集群搭建","/pages/ca02f2/"],["30.数据库/05.Redis/104.分片集群搭建.md","分片集群搭建","/pages/fdb88d/"]]}]},{title:"SSM",collapsable:!0,children:[["40.SSM/05.Spring.md","Spring 基础","/pages/b2f859/"],["40.SSM/10.SpringMVC-XML.md","SpringMVC-XML开发","/pages/1d17ee/"],["40.SSM/15.SpringMVC-注解.md","SpringMVC-注解开发","/pages/ee4b7e/"],["40.SSM/20.MyBatisPlus.md","MybatisPlus","/pages/3c9c2a/"],["40.SSM/25.SSM 整合-XML方式.md","SSM整合—XML开发","/pages/f036b6/"],["40.SSM/30.SSM整合-注解.md","SSM整合—注解开发","/pages/e67134/"]]},{title:"SpringBoot",collapsable:!0,children:[["50.SpringBoot/05.基础篇.md","基础篇","/pages/d74c9d/"],["50.SpringBoot/10.开发实用篇.md","开发实用篇","/pages/164be2/"],["50.SpringBoot/15.数据层解决方案.md","数据层解决方案","/pages/0446de/"],["50.SpringBoot/20.第三方技术整合.md","第三方技术整合","/pages/e76fff/"],["50.SpringBoot/50.原理篇.md","原理篇","/pages/cd183e/"]]},{title:"微服务",collapsable:!0,children:[{title:"Docker",collapsable:!0,children:[["60.微服务/10.Docker/05.安装Docker.md","安装Docker","/pages/04da56/"],["60.微服务/10.Docker/10.Docker实用篇.md","Docker实用篇","/pages/247ef5/"]]},{title:"ES",collapsable:!0,children:[["60.微服务/20.ES/05.ES基础.md","ES基础","/pages/e9e22c/"],["60.微服务/20.ES/10.索引.md","索引","/pages/7bb700/"],["60.微服务/20.ES/15.文档操作.md","文档操作","/pages/7624a9/"],["60.微服务/20.ES/20.操作API.md","操作API","/pages/dac0d9/"],["60.微服务/20.ES/25.文档搜索.md","文档搜索","/pages/a93137/"],["60.微服务/20.ES/30.案例练习.md","案例练习","/pages/5b74f8/"],["60.微服务/20.ES/35.ES进阶.md","ES进阶","/pages/18dc29/"],["60.微服务/20.ES/100.es操作文档.md","操作练习","/pages/06e2e5/"]]},{title:"RabbitMQ",collapsable:!0,children:[["60.微服务/30.RabbitMQ/05.RabbitMQ介绍.md","RabbitMQ","/pages/0af5eb/"],["60.微服务/30.RabbitMQ/10.RabbitMQ入门.md","RabbitMQ入门","/pages/bdbadd/"],["60.微服务/30.RabbitMQ/15.SpringAMQP.md","SpringAMQP","/pages/4b3e75/"],["60.微服务/30.RabbitMQ/20.RabbitMQ进阶.md","RabbitMQ进阶","/pages/bac98f/"]]},{title:"SpringCloud",collapsable:!0,children:[["60.微服务/40.SpringCloud/05.认识微服务.md","认识微服务","/pages/49c1ac/"],["60.微服务/40.SpringCloud/10.服务拆分和远程调用.md","服务拆分和远程调用","/pages/1b3d4d/"],["60.微服务/40.SpringCloud/20.Eureka注册中心.md","Eureka注册中心","/pages/938522/"],["60.微服务/40.SpringCloud/30.Ribbon负载均衡.md","Ribbon负载均衡","/pages/5546ef/"],["60.微服务/40.SpringCloud/40.Nacos注册中心.md","Nacos注册中心","/pages/9b099d/"],["60.微服务/40.SpringCloud/50.Nacos配置管理.md","Nacos配置管理","/pages/ffc82f/"],["60.微服务/40.SpringCloud/60.Feign远程调用.md","Feign远程调用","/pages/314ef0/"],["60.微服务/40.SpringCloud/70.Gateway服务网关.md","Gateway服务网关","/pages/89fc28/"]]},{title:"多级缓存",collapsable:!0,children:[["60.微服务/50.多级缓存/05.多级缓存.md","多级缓存","/pages/267113/"],["60.微服务/50.多级缓存/10.JVM进程缓存.md","JVM进程缓存","/pages/dc166e/"],["60.微服务/50.多级缓存/15.Lua语法入门.md","Lua语法入门","/pages/1725f1/"],["60.微服务/50.多级缓存/20.实现多级缓存.md","实现多级缓存","/pages/387b25/"],["60.微服务/50.多级缓存/25.缓存同步.md","缓存同步","/pages/92ac01/"],["60.微服务/50.多级缓存/100.多级缓存总结.md","多级缓存总结","/pages/52d2bc/"]]},{title:"分布式事务",collapsable:!0,children:[["60.微服务/60.分布式事务/05.分布式事务问题.md","分布式事务问题","/pages/d48085/"],["60.微服务/60.分布式事务/10.理论基础.md","分布式事务理论基础","/pages/8ffa15/"],["60.微服务/60.分布式事务/20.Seata基础.md","Seata基础","/pages/f26c95/"],["60.微服务/60.分布式事务/25.Seata事务模式.md","Seata事务模式","/pages/2bcba9/"]]},{title:"微服务保护",collapsable:!0,children:[["60.微服务/70.微服务保护/05.Sentinel基础.md","Sentinel基础","/pages/6e3b45/"],["60.微服务/70.微服务保护/20.流量控制.md","流量控制","/pages/0f114c/"],["60.微服务/70.微服务保护/30.隔离和降级.md","隔离和降级","/pages/b25c7c/"],["60.微服务/70.微服务保护/40.授权规则.md","授权规则","/pages/d7aaea/"]]}]}],catalogue:{},"/02.算法/":[{title:"算法基础",collapsable:!0,children:[["05.算法基础/05.数据结构与算法.md","数据结构与算法","/pages/17f632/"],["05.算法基础/07.时空间复杂度.md","时(空)间复杂度","/pages/f85993/"]]},{title:"数组",collapsable:!0,children:[["10.数组/05.数组.md","数组","/pages/552b44/"]]},{title:"链表",collapsable:!0,children:[["15.链表/05.链表.md","链表","/pages/1d3d1a/"]]},{title:"栈",collapsable:!0,children:[["20.栈/05.栈.md","栈","/pages/bfea35/"]]},{title:"队列",collapsable:!0,children:[["25.队列/05.队列.md","队列","/pages/7b2a41/"]]},{title:"堆",collapsable:!0,children:[["30.堆/05.堆.md","堆","/pages/abbe04/"]]},{title:"散列表",collapsable:!0,children:[["35.散列表/05.散列表.md","散列表","/pages/6215d9/"],["35.散列表/10.哈希算法.md","哈希算法","/pages/9227a5/"]]},{title:"跳表",collapsable:!0,children:[["38.跳表/05.跳表.md","跳表","/pages/e28ee4/"]]},{title:"字符串",collapsable:!0,children:[["40.字符串/05.字符串.md","字符串","/pages/658fbb/"]]},{title:"树",collapsable:!0,children:[["45.树/05.二叉树.md","二叉树","/pages/4d7338/"]]},{title:"图",collapsable:!0,children:[["50.图/05.图.md","图","/pages/dacecb/"]]},{title:"双指针",collapsable:!0,children:[]},{title:"排序",collapsable:!0,children:[["60.排序/05.排序.md","排序","/pages/60a4a5/"]]},{title:"二分查找",collapsable:!0,children:[["70.二分查找/05.二分查找.md","二分查找","/pages/4fe23f/"]]},{title:"贪心",collapsable:!0,children:[["75.贪心/01.贪心算法.md","贪心算法","/pages/1b322d/"]]},{title:"回溯法",collapsable:!0,children:[["80.回溯法/01.回溯.md","回溯算法","/pages/ec0ce8/"],["80.回溯法/50.递归.md","递归","/pages/1ee87b/"]]},{title:"动态规划",collapsable:!0,children:[["90.动态规划/01.动态规划.md","动态规划","/pages/f7cec8/"]]},{title:"分治",collapsable:!0,children:[["95.分治/01.分治算法.md","分治算法","/pages/5b1763/"]]}],"/03.技术/":[{title:"Git",collapsable:!0,children:[["01.Git/05.Git 概述.md","README","/pages/b02b7f/"],["01.Git/10.Git基础.md","Git基础","/pages/91f9c0/"]]},{title:"Linux",collapsable:!0,children:[["02.Linux/05.Linux基础.md","Linux","/pages/373d5a/"],["02.Linux/20.vim.md","vim编辑器","/pages/549714/"],["02.Linux/50.Linux面试题.md","Linux面试题","/pages/343c9a/"]]},{title:"正则表达式",collapsable:!0,children:[["10.正则表达式/05.正则表达式.md","README","/pages/b5d182/"]]}],"/04.面试/":[["10.Redis.md","Redis篇","/pages/edb686/"],["20.微服务篇.md","微服务篇","/pages/fa00f8/"],["30.MQ.md","MQ相关","/pages/8b5a99/"]],"/05.项目记录/":[{title:"blog",collapsable:!0,children:[["05.blog/05.blog.md","blog","/pages/2aa63b/"]]}],"/06.DOA/":[["01.DOA.md","README","/DOA/"]],"/07.life/":[["01.海浪.md","快乐收藏馆","/pages/d591da/"],["05.edg.md","edg夺冠","/pages/4607dc/"]],"/08.任意门/":[["02.站外导航.md","站外导航","/pages/961a83/"]]},author:{name:"凉冰",link:"https://github.com/diana-devil"},blogger:{avatar:"/devil.jpg",name:"凉冰",slogan:"奋勇向前的可爱小恶魔！"},social:{icons:[{iconClass:"icon-youjian",title:"发邮件",link:"mailto:1693849288@qq.com"},{iconClass:"icon-github",title:"GitHub",link:"https://github.com/diana-devil"},{iconClass:"icon-erji",title:"听音乐",link:"https://y.qq.com/n/ryqq/playlist/3202075040"}]},footer:{createYear:2023,copyrightInfo:"by 凉冰 "}}};var Ve=t(100),We=t(101),Ke=(t(28),t(13));var Ze={computed:{$filterPosts(){return this.$site.pages.filter(n=>{const{frontmatter:{pageComponent:e,article:t,home:r}}=n;return!(e||!1===t||!0===r)})},$sortPosts(){return(n=this.$filterPosts).sort((n,e)=>{const t=n.frontmatter.sticky,r=e.frontmatter.sticky;return t&&r?t==r?Object(Ke.a)(n,e):t-r:t&&!r?-1:!t&&r?1:Object(Ke.a)(n,e)}),n;var n},$sortPostsByDate(){return(n=this.$filterPosts).sort((n,e)=>Object(Ke.a)(n,e)),n;var n},$groupPosts(){return function(n){const e={},t={};for(let r=0,i=n.length;r<i;r++){const{frontmatter:{categories:i,tags:a}}=n[r];"array"===Object(Ke.n)(i)&&i.forEach(t=>{t&&(e[t]||(e[t]=[]),e[t].push(n[r]))}),"array"===Object(Ke.n)(a)&&a.forEach(e=>{e&&(t[e]||(t[e]=[]),t[e].push(n[r]))})}return{categories:e,tags:t}}(this.$sortPosts)},$categoriesAndTags(){return function(n){const e=[],t=[];for(let t in n.categories)e.push({key:t,length:n.categories[t].length});for(let e in n.tags)t.push({key:e,length:n.tags[e].length});return{categories:e,tags:t}}(this.$groupPosts)}}};r.a.component(Ve.default),r.a.component(We.default);function Xe(n){return n.toString().padStart(2,"0")}t(249);r.a.component("CodeGroup",()=>Promise.resolve().then(t.bind(null,101))),r.a.component("Badge",()=>Promise.all([t.e(0),t.e(4)]).then(t.bind(null,497))),r.a.component("CodeBlock",()=>Promise.resolve().then(t.bind(null,100)));t(250);class Ye{constructor(n){Object.defineProperty(this,"registration",{value:n,configurable:!0,writable:!0})}update(){return this.registration.update()}skipWaiting(){const n=this.registration.waiting;return n?(console.log("[vuepress:sw] Doing worker.skipWaiting()."),new Promise((e,t)=>{const r=new MessageChannel;r.port1.onmessage=n=>{console.log("[vuepress:sw] Done worker.skipWaiting()."),n.data.error?t(n.data.error):e(n.data)},n.postMessage({type:"skip-waiting"},[r.port2])})):Promise.resolve()}}var nt=t(15);r.a.component("SWUpdatePopup",()=>Promise.all([t.e(0),t.e(3)]).then(t.bind(null,342)));var et={name:"BackToTop",props:{threshold:{type:Number,default:300}},data:()=>({scrollTop:null}),computed:{show(){return this.scrollTop>this.threshold}},mounted(){this.scrollTop=this.getScrollTop(),window.addEventListener("scroll",se()(()=>{this.scrollTop=this.getScrollTop()},100))},methods:{getScrollTop:()=>window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,scrollToTop(){window.scrollTo({top:0,behavior:"smooth"}),this.scrollTop=0}}},tt=(t(251),Object(Je.a)(et,(function(){var n=this._self._c;return n("transition",{attrs:{name:"fade"}},[this.show?n("svg",{staticClass:"go-to-top",attrs:{xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 49.484 28.284"},on:{click:this.scrollToTop}},[n("g",{attrs:{transform:"translate(-229 -126.358)"}},[n("rect",{attrs:{fill:"currentColor",width:"35",height:"5",rx:"2",transform:"translate(229 151.107) rotate(-45)"}}),this._v(" "),n("rect",{attrs:{fill:"currentColor",width:"35",height:"5",rx:"2",transform:"translate(274.949 154.642) rotate(-135)"}})])]):this._e()])}),[],!1,null,"5fd4ef0c",null).exports),rt=(n,e,t)=>{if(!e.has(n))throw TypeError("Cannot "+t)},it=(n,e,t)=>(rt(n,e,"read from private field"),t?t.call(n):e.get(n)),at=(n,e,t)=>{if(e.has(n))throw TypeError("Cannot add the same private member more than once");e instanceof WeakSet?e.add(n):e.set(n,t)},st=(n,e,t,r)=>(rt(n,e,"write to private field"),r?r.call(n,t):e.set(n,t),t);var ot,lt;const ct=class{constructor(n,e,t,r=!0){at(this,ot,void 0),at(this,lt,void 0),st(this,ot,{width:0,height:0});const{el:i,ctx:a}=ct.initCanvas(n);this.el=i,this.ctx=a,st(this,lt,r),this.size={width:e||window.innerWidth,height:t||window.innerHeight}}get size(){return{...it(this,ot)}}set size({width:n,height:e}){var t;if(it(this,ot).width===n&&it(this,ot).height===e)return;it(this,ot).width=n,it(this,ot).height=e;const r=null!=(t=it(this,lt)?window.devicePixelRatio:1)?t:1;this.el.width=Math.round(it(this,ot).width*r),this.el.height=Math.round(it(this,ot).height*r),this.el.style.width=it(this,ot).width+"px",this.el.style.height=it(this,ot).height+"px",it(this,lt)&&this.ctx.scale(r,r)}clear(){ct.clearCanvas(this.ctx,{...it(this,ot)})}to(n){n.ctx.drawImage(this.el,0,0,it(this,ot).width,it(this,ot).height)}handleResize(n){this.size={width:window.innerWidth,height:window.innerHeight}}static setCanvasStyle(n,e,t){const r=n.style,{zIndex:i=0,opacity:a=1}=e;r.position="fixed",r.top="0",r.left="0",r.zIndex=i.toString(),r.width=(t?t.width:n.width).toString()+"px",r.height=(t?t.height:n.height).toString()+"px",1!==a&&(r.opacity=a.toString()),r.pointerEvents="none"}static initCanvas(n){n||(n=document.createElement("canvas"));const e=n.getContext("2d");return{el:n,ctx:e}}static createOffscreenCanvas(){return new ct}static clearCanvas(n,e){const{width:t,height:r}=e;n.clearRect(0,0,t,r)}};let dt=ct;var pt,ut;ot=new WeakMap,lt=new WeakMap;class mt{constructor(n,e,t,r=!0,i=!0,a={zIndex:0,opacity:1}){at(this,pt,void 0),at(this,ut,void 0),st(this,pt,new dt(n,e,t,r)),dt.setCanvasStyle(it(this,pt).el,a,{width:e,height:t}),st(this,ut,i?new dt(void 0,e,t,r):null)}get size(){return it(this,pt).size}draw(n){var e;const t=null!=(e=it(this,ut))?e:it(this,pt);t.clear(),n(t.ctx,{...t.size})}render(){!it(this,ut)||(it(this,pt).clear(),it(this,ut).to(it(this,pt)))}handleResize(n){it(this,pt).handleResize(n),it(this,ut)&&it(this,ut).handleResize(n)}clear(){it(this,pt).clear(),it(this,ut)&&it(this,ut).clear()}}pt=new WeakMap,ut=new WeakMap;var gt=(n,e,t)=>{if(!e.has(n))throw TypeError("Cannot "+t)},ht=(n,e,t)=>(gt(n,e,"read from private field"),t?t.call(n):e.get(n));var bt;class ft{constructor(){((n,e,t)=>{if(e.has(n))throw TypeError("Cannot add the same private member more than once");e instanceof WeakSet?e.add(n):e.set(n,t)})(this,bt,void 0),((n,e,t,r)=>{gt(n,e,"write to private field"),r?r.call(n,t):e.set(n,t)})(this,bt,new Map)}add(n,e,t=window){ht(this,bt).has(t)||ht(this,bt).set(t,new Map);const r=ht(this,bt).get(t);r.has(n)||r.set(n,new Set),r.get(n).add(e)}startAll(){for(const[n,e]of ht(this,bt))for(const[t,r]of e)for(const e of r)n.addEventListener(t,e)}stopAll(){for(const[n,e]of ht(this,bt))for(const[t,r]of e)for(const e of r)n.removeEventListener(t,e)}clear(){ht(this,bt).clear()}}function vt(n){return!!n.touches}bt=new WeakMap;class yt{static randomFloat(n,e){return Math.random()*(e-n)+n}static randomInt(n,e){return Math.floor(yt.randomFloat(n,e))}static choice(n){const e=n.length;return n[Math.floor(e*Math.random())]}static color(n="0123456789ABCDEF"){return"#"+yt.choice(n)+yt.choice(n)+yt.choice(n)+yt.choice(n)+yt.choice(n)+yt.choice(n)}}var xt,kt,wt,St,jt,Et=(n,e,t)=>{if(!e.has(n))throw TypeError("Cannot "+t)},Tt=(n,e,t)=>(Et(n,e,"read from private field"),t?t.call(n):e.get(n)),qt=(n,e,t)=>{if(e.has(n))throw TypeError("Cannot add the same private member more than once");e instanceof WeakSet?e.add(n):e.set(n,t)},_t=(n,e,t,r)=>(Et(n,e,"write to private field"),r?r.call(n,t):e.set(n,t),t),It=(n,e,t)=>(Et(n,e,"access private method"),t);class Ct{constructor(n,e,t,r,i){qt(this,xt,void 0),qt(this,kt,void 0),qt(this,wt,void 0),this.size=t,this.color=r,_t(this,wt,0),_t(this,xt,i),_t(this,kt,e),this.position={...n}}move(){this.position.x=Math.sin(Tt(this,xt))*Tt(this,kt)+this.position.x,this.position.y=Math.cos(Tt(this,xt))*Tt(this,kt)+this.position.y+.3*Tt(this,wt),((n,e,t,r)=>({set _(r){_t(n,e,r,t)},get _(){return Tt(n,e,r)}}))(this,wt)._++}shouleRemove(n){return this.position.x<0||this.position.x>n.width||this.position.y>n.height}}xt=new WeakMap,kt=new WeakMap,wt=new WeakMap;St=new WeakMap;class At{static create(n,e,t,r,i,a){return new(this.shapeMap.get(n))(e,t,r,i,a)}}At.shapeMap=new Map([["star",class extends Ct{constructor(n,e,t,r,i){super(n,e,t,r,i),qt(this,St,0)}draw(n,e){n.fillStyle=this.color,n.beginPath();const t=2*this.size,r=this.size;for(let e=0;e<5;e++)n.lineTo(Math.cos((18+72*e-Tt(this,St))/180*Math.PI)*t+this.position.x,-Math.sin((18+72*e-Tt(this,St))/180*Math.PI)*t+this.position.y),n.lineTo(Math.cos((54+72*e-Tt(this,St))/180*Math.PI)*r+this.position.x,-Math.sin((54+72*e-Tt(this,St))/180*Math.PI)*r+this.position.y);n.fill(),_t(this,St,Tt(this,St)+5)}}],["circle",class extends Ct{draw(n,e){n.fillStyle=this.color,n.beginPath(),n.arc(this.position.x,this.position.y,this.size,0,2*Math.PI),n.fill()}}]]);class Rt{constructor(n,e,t,r){qt(this,jt,void 0),this.stopped=!1,_t(this,jt,new Set);for(let i=0;i<r;i++){const r=At.create(n,e,yt.randomFloat(1,6),t,yt.color("89ABCDEF"),yt.randomFloat(Math.PI-1,Math.PI+1));Tt(this,jt).add(r)}}move(n){for(const e of Tt(this,jt))e.shouleRemove(n)?Tt(this,jt).delete(e):e.move();0===Tt(this,jt).size&&(this.stopped=!0)}draw(n,e){for(const t of Tt(this,jt))t.draw(n,e)}}jt=new WeakMap;var zt,Ot,Mt,Bt,Lt,Dt,Ft,Pt,Nt,Ut,Ht,Jt,$t,Qt,Gt,Vt,Wt,Kt,Zt,Xt;class Yt{constructor({shape:n="star",size:e=2,numParticles:t=10}={},r={}){qt(this,Nt),qt(this,Ht),qt(this,$t),qt(this,Gt),qt(this,Wt),qt(this,Zt),qt(this,zt,void 0),qt(this,Ot,void 0),qt(this,Mt,void 0),qt(this,Bt,null),qt(this,Lt,new Set),qt(this,Dt,!1),qt(this,Ft,void 0),qt(this,Pt,new ft),_t(this,zt,n),_t(this,Ot,e),_t(this,Mt,t),_t(this,Ft,r),this.animate=this.animate.bind(this)}mount(n){_t(this,Bt,new mt(n,window.innerWidth,window.innerHeight,!0,!0,Tt(this,Ft))),It(this,Nt,Ut).call(this),function(n,{leftColor:e="#fff",rightColor:t="#444",leftBgColor:r="#35495e",rightBgColor:i="#00ffc0"}={}){console.log(`%c ${n} %c v0.5.2 112fa81 %c`,`background: ${r}; padding: 2px; color: ${e}; font-weight: bold; text-transform: uppercase;`,`background: ${i}; padding: 2px; color: ${t}; font-weight: bold; text-transform: uppercase;`,"background: transparent")}("Theme Popper 🎉",{leftBgColor:"#ffb366"})}unmount(){It(this,Ht,Jt).call(this),_t(this,Dt,!1)}animate(){if(_t(this,Dt,!0),0===Tt(this,Lt).size)return _t(this,Dt,!1),void Tt(this,Bt).clear();requestAnimationFrame(this.animate);for(const n of Tt(this,Lt)){if(n.stopped)return void Tt(this,Lt).delete(n);n.move(Tt(this,Bt).size)}Tt(this,Bt).draw((n,e)=>{for(const t of Tt(this,Lt))t.draw(n,e)}),Tt(this,Bt).render()}}zt=new WeakMap,Ot=new WeakMap,Mt=new WeakMap,Bt=new WeakMap,Lt=new WeakMap,Dt=new WeakMap,Ft=new WeakMap,Pt=new WeakMap,Nt=new WeakSet,Ut=function(){/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)?Tt(this,Pt).add("touchstart",It(this,$t,Qt).bind(this)):Tt(this,Pt).add("mousedown",It(this,$t,Qt).bind(this)),Tt(this,Pt).add("visibilitychange",It(this,Wt,Kt).bind(this)),Tt(this,Pt).add("resize",function(n,e,t){var r,i,a;void 0===e&&(e=50),void 0===t&&(t={});var s=null!=(r=t.isImmediate)&&r,o=null!=(i=t.callback)&&i,l=t.maxWait,c=Date.now(),d=[];function p(){if(void 0!==l){var n=Date.now()-c;if(n+e>=l)return l-n}return e}var u=function(){var e=[].slice.call(arguments),t=this;return new Promise((function(r,i){var l=s&&void 0===a;if(void 0!==a&&clearTimeout(a),a=setTimeout((function(){if(a=void 0,c=Date.now(),!s){var r=n.apply(t,e);o&&o(r),d.forEach((function(n){return(0,n.resolve)(r)})),d=[]}}),p()),l){var u=n.apply(t,e);return o&&o(u),r(u)}d.push({resolve:r,reject:i})}))};return u.cancel=function(n){void 0!==a&&clearTimeout(a),d.forEach((function(e){return(0,e.reject)(n)})),d=[]},u}(It(this,Gt,Vt).bind(this),500)),Tt(this,Pt).startAll()},Ht=new WeakSet,Jt=function(){Tt(this,Pt).stopAll(),Tt(this,Pt).clear()},$t=new WeakSet,Qt=function(n){const e={x:vt(n)?n.touches[0].clientX:n.clientX,y:vt(n)?n.touches[0].clientY:n.clientY},t=new Rt(Tt(this,zt),{...e},Tt(this,Ot),Tt(this,Mt));Tt(this,Lt).add(t),Tt(this,Dt)||It(this,Zt,Xt).call(this)},Gt=new WeakSet,Vt=function(n){Tt(this,Bt).handleResize(n)},Wt=new WeakSet,Kt=function(n){Tt(this,Lt).clear(),_t(this,Dt,!1)},Zt=new WeakSet,Xt=function(){requestAnimationFrame(this.animate)};var nr={name:"CursorEffects",data:()=>({popper:new Yt({shape:"star",size:2},{opacity:1,zIndex:999999999})}),mounted(){this.popper.mount(this.$el)},beforeDestroy(){this.popper.unmount()}},er=Object(Je.a)(nr,(function(){return(0,this._self._c)("canvas",{attrs:{id:"vuepress-canvas-cursor"}})}),[],!1,null,null,null).exports,tr={data:()=>({volumeKey:"reco-bgm-volume"}),methods:{setVolume(n){sessionStorage.setItem(this.volumeKey,n)},getVolume(){return sessionStorage.getItem(this.volumeKey)},removeVolume(){sessionStorage.removeItem(this.volumeKey)}}},rr={name:"ModuleTransition",props:{delay:{type:String,default:"0"},duration:{type:String,default:".25"},position:{type:String,default:"left"}},methods:{setStyle(n){n.style.transition=`all ${this.duration}s ease-in-out ${this.delay}s`,n.style.transform="right"===this.position?"translateX(20px)":"translateX(-20px)",n.style.opacity=0},unsetStyle(n){n.style.transform="translateX(0)",n.style.opacity=1}}},ir=(t(252),Object(Je.a)(rr,(function(){return(0,this._self._c)("transition",{attrs:{name:"module"},on:{enter:this.setStyle,"after-enter":this.unsetStyle,"before-leave":this.setStyle}},[this._t("default")],2)}),[],!1,null,"41bcba48",null).exports);let ar,sr=0;var or={mixins:[tr],components:{ModuleTransition:ir},mounted(){"left"===this.floatPosition?this.floatStyle={...this.floatStyle,left:"0","border-top-right-radius":"20px","border-bottom-right-radius":"20px"}:this.floatStyle={...this.floatStyle,right:"0","border-top-left-radius":"20px","border-bottom-left-radius":"20px"},this.autoShrink&&this.changeBgmInfo(!0)},data:()=>({panelPosition:{left:"10px",bottom:"10px","z-index":"999999"},curIndex:0,curPlayStatus:"paused",audio:[{name:"FIND YOU",artist:"G_E_M_ 邓紫棋",url:"/bgm/G_E_M_ 邓紫棋 - FIND YOU.ogg",cover:"/bgm/GEM.jpg"},{name:"GLORIA",artist:"G_E_M_ 邓紫棋",url:"/bgm/G_E_M_ 邓紫棋 - GLORIA.ogg",cover:"/bgm/GEM.jpg"},{name:"HELL",artist:"G_E_M_ 邓紫棋",url:"/bgm/G_E_M_ 邓紫棋 - HELL.ogg",cover:"/bgm/GEM.jpg"},{name:"冰河时代",artist:"G_E_M_ 邓紫棋",url:"/bgm/G_E_M_ 邓紫棋 - 冰河时代.ogg",cover:"/bgm/GEM.jpg"},{name:"不想回家",artist:"G_E_M_ 邓紫棋",url:"/bgm/G_E_M_ 邓紫棋 - 不想回家.ogg",cover:"/bgm/GEM.jpg"},{name:"倒流时间",artist:"G_E_M_ 邓紫棋",url:"/bgm/G_E_M_ 邓紫棋 - 倒流时间.ogg",cover:"/bgm/GEM.jpg"},{name:"老人与海",artist:"G_E_M_ 邓紫棋",url:"/bgm/G_E_M_ 邓紫棋 - 老人与海.ogg",cover:"/bgm/GEM.jpg"},{name:"离心力",artist:"G_E_M_ 邓紫棋",url:"/bgm/G_E_M_ 邓紫棋 - 离心力.ogg",cover:"/bgm/GEM.jpg"},{name:"两个自己",artist:"G_E_M_ 邓紫棋",url:"/bgm/G_E_M_ 邓紫棋 - 两个自己.ogg",cover:"/bgm/GEM.jpg"},{name:"你不是第一个离开的人",artist:"G_E_M_ 邓紫棋",url:"/bgm/G_E_M_ 邓紫棋 - 你不是第一个离开的人.ogg",cover:"/bgm/GEM.jpg"},{name:"泡沫",artist:"G_E_M_ 邓紫棋",url:"/bgm/G_E_M_ 邓紫棋 - 泡沫.ogg",cover:"/bgm/GEM.jpg"},{name:"让世界暂停一分钟",artist:"G_E_M_ 邓紫棋",url:"/bgm/G_E_M_ 邓紫棋 - 让世界暂停一分钟.ogg",cover:"/bgm/GEM.jpg"},{name:"少年与海",artist:"G_E_M_ 邓紫棋",url:"/bgm/G_E_M_ 邓紫棋 - 少年与海.ogg",cover:"/bgm/GEM.jpg"},{name:"受难曲",artist:"G_E_M_ 邓紫棋",url:"/bgm/G_E_M_ 邓紫棋 - 受难曲.ogg",cover:"/bgm/GEM.jpg"},{name:"天空没有极限",artist:"G_E_M_ 邓紫棋",url:"/bgm/G_E_M_ 邓紫棋 - 天空没有极限.ogg",cover:"/bgm/GEM.jpg"},{name:"夜的尽头",artist:"G_E_M_ 邓紫棋",url:"/bgm/G_E_M_ 邓紫棋 - 夜的尽头.ogg",cover:"/bgm/GEM.jpg"},{name:"只有我和你的地方",artist:"G_E_M_ 邓紫棋",url:"/bgm/G_E_M_ 邓紫棋 - 只有我和你的地方.ogg",cover:"/bgm/GEM.jpg"},{name:"江南",artist:"林俊杰",url:"/bgm/林俊杰 - 江南.ogg",cover:"/bgm/JJ.webp"},{name:"我还想她",artist:"林俊杰",url:"/bgm/林俊杰 - 我还想她.ogg",cover:"/bgm/JJ.webp"},{name:"醉赤壁",artist:"林俊杰",url:"/bgm/林俊杰 - 醉赤壁.ogg",cover:"/bgm/JJ.webp"}],autoplay:!1,isFloat:!1,isMini:!1,firstLoad:!0,isMute:!1,isFault:!1,floatPosition:"left",floatStyle:{bottom:"200px","z-index":"999999"},autoShrink:!1,shrinkMode:"float"}),watch:{curPlayStatus:function(n){"playing"===n?ar=setInterval((function(){const n=document.querySelector(".reco-bgm-cover"),e=document.querySelector(".mini-operation"),t=document.querySelector(".falut-message");sr+=1,n.style.transform="rotate("+sr+"deg)",n.style.transition="0.1s linear",e.style.transform="rotate(-"+sr+"deg)",e.style.transition="0.1s linear",t.style.transform="rotate(-"+sr+"deg)",t.style.transition="0.1s linear"}),100):clearInterval(ar)}},methods:{changeBgmInfo(n){const e=!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);e||"float"===this.shrinkMode?this.isFloat=n:e||"mini"!==this.shrinkMode||(this.isMini=n)},playReady(){if(this.firstLoad){if(this.getVolume()){const n=this.getVolume();this.$refs.vbar.style.width=100*n+"%",this.$refs.bgm.volume=n}else{const n=100*this.$refs.bgm.volume+"%";this.$refs.vbar.style.width=n}if(this.firstLoad=!1,this.autoplay){const n=this.$refs.bgm.play();void 0!==n&&n.then(n=>{console.log("vuepress-plugin-bgm-player: 自动播放成功"),this.curPlayStatus="playing"}).catch(n=>{console.log("vuepress-plugin-bgm-player: 自动播放失败"),window.addEventListener("click",this.pageClickHandle)})}}"playing"===this.curPlayStatus&&this.playBgm()},pageClickHandle(){this.autoplay&&this.playBgm(),window.removeEventListener("click",this.pageClickHandle)},pauseBgm(){this.$refs.bgm.pause(),this.curPlayStatus="paused"},playBgm(){const n=this.$refs.bgm.play();void 0!==n&&n.then(n=>{this.isFault&&(this.isFault=!1)}).catch(n=>{console.log(n),this.isFault=!0,this.pauseBgm()}),this.curPlayStatus="playing"},playNext(){this.$refs.pbar.style.width=0,this.isFault=!1,this.curIndex>=this.audio.length-1?this.curIndex=0:this.curIndex++},playLast(){this.$refs.pbar.style.width=0,this.isFault=!1,this.curIndex<=0?this.curIndex=this.audio.length-1:this.curIndex--},bgmEnded(){this.$refs.pbar.style.width=0,this.playNext()},timeUpdate(){const n=this.$refs.bgm.duration,e=this.$refs.bgm.currentTime/n*100+"%";this.$refs.pbar.style.width=e},progressJump(n){const e=this.$refs.bgm.duration,t=n.offsetX/150;isNaN(e)||(this.$refs.bgm.currentTime=t*e)},volumeJump(n){const e=n.offsetX/57;e>=0&&e<=1&&(this.isMute=!(e>0),this.$refs.vbar.style.width=100*e+"%",this.$refs.bgm.volume=e,this.setVolume(this.$refs.bgm.volume))},muteBgm(){this.isMute=!0,this.setVolume(this.$refs.bgm.volume),this.$refs.vbar.style.width=0,this.$refs.bgm.volume=0},unMuteBgm(){if(this.isMute=!1,this.getVolume()){const n=this.getVolume();this.$refs.vbar.style.width=100*n+"%",this.$refs.bgm.volume=n}else this.$refs.vbar.style.width="100%",this.$refs.bgm.volume=1}}},lr=(t(253),Object(Je.a)(or,(function(){var n=this,e=n._self._c;return e("div",{staticClass:"reco-bgm-panel"},[e("audio",{ref:"bgm",attrs:{id:"bgm",src:n.audio[n.curIndex].url},on:{ended:n.bgmEnded,canplay:n.playReady,timeupdate:n.timeUpdate}}),n._v(" "),e("module-transition",{attrs:{position:n.floatPosition}},[e("div",{directives:[{name:"show",rawName:"v-show",value:n.isFloat,expression:"isFloat"}],staticClass:"reco-float-box",style:n.floatStyle,on:{click:function(e){return n.changeBgmInfo(!1)}}},[e("img",{attrs:{src:n.audio[n.curIndex].cover}})])]),n._v(" "),e("module-transition",[e("div",{directives:[{name:"show",rawName:"v-show",value:!n.isFloat,expression:"!isFloat"}],staticClass:"reco-bgm-box",style:n.panelPosition},[e("div",{staticClass:"reco-bgm-cover",style:`background-image:url(${n.audio[n.curIndex].cover})`,on:{click:function(e){return n.changeBgmInfo(!1)}}},[e("div",{directives:[{name:"show",rawName:"v-show",value:n.isMini,expression:"isMini"}],staticClass:"mini-operation"},[e("i",{directives:[{name:"show",rawName:"v-show",value:"playing"===this.curPlayStatus&&n.isMini,expression:"this.curPlayStatus === 'playing' && isMini"}],staticClass:"reco-bgm reco-bgm-pause",on:{click:function(e){return e.stopPropagation(),n.pauseBgm.apply(null,arguments)}}}),n._v(" "),e("i",{directives:[{name:"show",rawName:"v-show",value:"paused"===this.curPlayStatus&&n.isMini,expression:"this.curPlayStatus === 'paused' && isMini"}],staticClass:"reco-bgm reco-bgm-play",on:{click:function(e){return e.stopPropagation(),n.playBgm.apply(null,arguments)}}})]),n._v(" "),e("div",{directives:[{name:"show",rawName:"v-show",value:n.isFault,expression:"isFault"}],staticClass:"falut-message"},[n._v("\n          播放失败\n        ")])]),n._v(" "),e("module-transition",{attrs:{duration:".15"}},[e("div",{directives:[{name:"show",rawName:"v-show",value:!n.isMini,expression:"!isMini"}],staticClass:"reco-bgm-info"},[e("div",{staticClass:"info-box"},[e("i",{staticClass:"reco-bgm reco-bgm-music music"}),n._v(n._s(n.audio[n.curIndex].name))]),n._v(" "),e("div",{staticClass:"info-box"},[e("i",{staticClass:"reco-bgm reco-bgm-artist"}),n._v(n._s(n.audio[n.curIndex].artist))]),n._v(" "),e("div",{staticClass:"reco-bgm-progress"},[e("div",{staticClass:"progress-bar",on:{click:n.progressJump}},[e("div",{ref:"pbar",staticClass:"bar"})])]),n._v(" "),e("div",{staticClass:"reco-bgm-operation"},[e("i",{staticClass:"reco-bgm reco-bgm-last last",on:{click:n.playLast}}),n._v(" "),e("i",{directives:[{name:"show",rawName:"v-show",value:"playing"===n.curPlayStatus,expression:"curPlayStatus === 'playing'"}],staticClass:"reco-bgm reco-bgm-pause pause",on:{click:n.pauseBgm}}),n._v(" "),e("i",{directives:[{name:"show",rawName:"v-show",value:"paused"===n.curPlayStatus,expression:"curPlayStatus === 'paused'"}],ref:"play",staticClass:"reco-bgm reco-bgm-play play",on:{click:n.playBgm}}),n._v(" "),e("i",{staticClass:"reco-bgm reco-bgm-next next",on:{click:n.playNext}}),n._v(" "),e("i",{directives:[{name:"show",rawName:"v-show",value:!n.isMute,expression:"!isMute"}],staticClass:"reco-bgm reco-bgm-volume1 volume",on:{click:n.muteBgm}}),n._v(" "),e("i",{directives:[{name:"show",rawName:"v-show",value:n.isMute,expression:"isMute"}],staticClass:"reco-bgm reco-bgm-mute mute",on:{click:n.unMuteBgm}}),n._v(" "),e("div",{staticClass:"volume-bar",on:{click:n.volumeJump}},[e("div",{ref:"vbar",staticClass:"bar"})])])])]),n._v(" "),e("module-transition",{attrs:{duration:".15"}},[e("div",{directives:[{name:"show",rawName:"v-show",value:!n.isMini,expression:"!isMini"}],staticClass:"reco-bgm-left-box",on:{click:function(e){return n.changeBgmInfo(!0)}}},[e("i",{staticClass:"reco-bgm reco-bgm-left"})])])],1)])],1)}),[],!1,null,"b1d3339e",null).exports),cr=[({Vue:n,options:e,router:t,siteData:r})=>{},({Vue:n,options:e,router:t,siteData:r})=>{r.pages.map(n=>{const{frontmatter:{date:e,author:t}}=n;"string"==typeof e&&"Z"===e.charAt(e.length-1)&&(n.frontmatter.date=function(n){n instanceof Date||(n=new Date(n));return`${n.getUTCFullYear()}-${Xe(n.getUTCMonth()+1)}-${Xe(n.getUTCDate())} ${Xe(n.getUTCHours())}:${Xe(n.getUTCMinutes())}:${Xe(n.getUTCSeconds())}`}(e)),t?n.author=t:r.themeConfig.author&&(n.author=r.themeConfig.author)}),n.mixin(Ze)},{},({Vue:n})=>{n.mixin({computed:{$dataBlock(){return this.$options.__data__block__}}})},{},{},async({router:n,isServer:e})=>{if(!e){const{register:e}=await t.e(159).then(t.bind(null,340));n.onReady(()=>{e("/service-worker.js",{registrationOptions:{},ready(){console.log("[vuepress:sw] Service worker is active."),nt.a.$emit("sw-ready")},cached(n){console.log("[vuepress:sw] Content has been cached for offline use."),nt.a.$emit("sw-cached",new Ye(n))},updated(n){console.log("[vuepress:sw] Content updated."),nt.a.$emit("sw-updated",new Ye(n))},offline(){console.log("[vuepress:sw] No internet connection found. App is running in offline mode."),nt.a.$emit("sw-offline")},error(n){console.error("[vuepress:sw] Error during service worker registration:",n),nt.a.$emit("sw-error",n),GA_ID&&ga("send","exception",{exDescription:n.message,exFatal:!1})}})})}},({Vue:n})=>{n.component("BackToTop",tt)},({Vue:n})=>{n.component("CursorEffects",er)},({router:n})=>{"undefined"!=typeof window&&function(){var n=document.createElement("script"),e=window.location.protocol.split(":")[0];n.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(n,t)}()},({Vue:n})=>{n.component("BgMusic",lr)}],dr=["SWUpdatePopup","BackToTop","CursorEffects","BgMusic"];class pr extends class{constructor(){this.store=new r.a({data:{state:{}}})}$get(n){return this.store.state[n]}$set(n,e){r.a.set(this.store.state,n,e)}$emit(...n){this.store.$emit(...n)}$on(...n){this.store.$on(...n)}}{}Object.assign(pr.prototype,{getPageAsyncComponent:Wn.d,getLayoutAsyncComponent:Wn.c,getAsyncComponent:Wn.b,getVueComponent:Wn.e});var ur={install(n){const e=new pr;n.$vuepress=e,n.prototype.$vuepress=e}};function mr(n,e){const t=e.toLowerCase();return n.options.routes.some(n=>n.path.toLowerCase()===t)}var gr={props:{pageKey:String,slotKey:{type:String,default:"default"}},render(n){const e=this.pageKey||this.$parent.$page.key;return Object(Wn.h)("pageKey",e),r.a.component(e)||r.a.component(e,Object(Wn.d)(e)),r.a.component(e)?n(e):n("")}},hr={functional:!0,props:{slotKey:String,required:!0},render:(n,{props:e,slots:t})=>n("div",{class:["content__"+e.slotKey]},t()[e.slotKey])},br={computed:{openInNewWindowTitle(){return this.$themeLocaleConfig.openNewWindowText||"(opens new window)"}}},fr=(t(254),t(255),Object(Je.a)(br,(function(){var n=this._self._c;return n("span",[n("svg",{staticClass:"icon outbound",attrs:{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",focusable:"false",x:"0px",y:"0px",viewBox:"0 0 100 100",width:"15",height:"15"}},[n("path",{attrs:{fill:"currentColor",d:"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"}}),this._v(" "),n("polygon",{attrs:{fill:"currentColor",points:"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"}})]),this._v(" "),n("span",{staticClass:"sr-only"},[this._v(this._s(this.openInNewWindowTitle))])])}),[],!1,null,null,null).exports),vr={functional:!0,render(n,{parent:e,children:t}){if(e._isMounted)return t;e.$once("hook:mounted",()=>{e.$forceUpdate()})}};r.a.config.productionTip=!1,r.a.use(Gn),r.a.use(ur),r.a.mixin(function(n,e,t=r.a){!function(n){n.locales&&Object.keys(n.locales).forEach(e=>{n.locales[e].path=e});Object.freeze(n)}(e),t.$vuepress.$set("siteData",e);const i=new(n(t.$vuepress.$get("siteData"))),a=Object.getOwnPropertyDescriptors(Object.getPrototypeOf(i)),s={};return Object.keys(a).reduce((n,e)=>(e.startsWith("$")&&(n[e]=a[e].get),n),s),{computed:s}}(n=>class{setPage(n){this.__page=n}get $site(){return n}get $themeConfig(){return this.$site.themeConfig}get $frontmatter(){return this.$page.frontmatter}get $localeConfig(){const{locales:n={}}=this.$site;let e,t;for(const r in n)"/"===r?t=n[r]:0===this.$page.path.indexOf(r)&&(e=n[r]);return e||t||{}}get $siteTitle(){return this.$localeConfig.title||this.$site.title||""}get $canonicalUrl(){const{canonicalUrl:n}=this.$page.frontmatter;return"string"==typeof n&&n}get $title(){const n=this.$page,{metaTitle:e}=this.$page.frontmatter;if("string"==typeof e)return e;const t=this.$siteTitle,r=n.frontmatter.home?null:n.frontmatter.title||n.title;return t?r?r+" | "+t:t:r||"VuePress"}get $description(){const n=function(n){if(n){const e=n.filter(n=>"description"===n.name)[0];if(e)return e.content}}(this.$page.frontmatter.meta);return n||(this.$page.frontmatter.description||this.$localeConfig.description||this.$site.description||"")}get $lang(){return this.$page.frontmatter.lang||this.$localeConfig.lang||"en-US"}get $localePath(){return this.$localeConfig.path||"/"}get $themeLocaleConfig(){return(this.$site.themeConfig.locales||{})[this.$localePath]||{}}get $page(){return this.__page?this.__page:function(n,e){for(let t=0;t<n.length;t++){const r=n[t];if(r.path.toLowerCase()===e.toLowerCase())return r}return{path:"",frontmatter:{}}}(this.$site.pages,this.$route.path)}},Ge)),r.a.component("Content",gr),r.a.component("ContentSlotsDistributor",hr),r.a.component("OutboundLink",fr),r.a.component("ClientOnly",vr),r.a.component("Layout",Object(Wn.c)("Layout")),r.a.component("NotFound",Object(Wn.c)("NotFound")),r.a.prototype.$withBase=function(n){const e=this.$site.base;return"/"===n.charAt(0)?e+n.slice(1):n},window.__VUEPRESS__={version:"1.9.5",hash:"6dca86f"},async function(n){const e="undefined"!=typeof window&&window.__VUEPRESS_ROUTER_BASE__?window.__VUEPRESS_ROUTER_BASE__:Ge.routerBase||Ge.base,t=new Gn({base:e,mode:"history",fallback:!1,routes:Qe,scrollBehavior:(n,e,t)=>t||(n.hash?!r.a.$vuepress.$get("disableScrollBehavior")&&{selector:decodeURIComponent(n.hash)}:{x:0,y:0})});!function(n){n.beforeEach((e,t,r)=>{if(mr(n,e.path))r();else if(/(\/|\.html)$/.test(e.path))if(/\/$/.test(e.path)){const t=e.path.replace(/\/$/,"")+".html";mr(n,t)?r(t):r()}else r();else{const t=e.path+"/",i=e.path+".html";mr(n,i)?r(i):mr(n,t)?r(t):r()}})}(t);const i={};try{await Promise.all(cr.filter(n=>"function"==typeof n).map(e=>e({Vue:r.a,options:i,router:t,siteData:Ge,isServer:n})))}catch(n){console.error(n)}return{app:new r.a(Object.assign(i,{router:t,render:n=>n("div",{attrs:{id:"app"}},[n("RouterView",{ref:"layout"}),n("div",{class:"global-ui"},dr.map(e=>n(e)))])})),router:t}}(!1).then(({app:n,router:e})=>{e.onReady(()=>{n.$mount("#app")})})}]);